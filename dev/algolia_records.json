[{"content":"Documentation for development version: 62d70d1 . Pydantic is the most widely used data validation library for Python. Fast and extensible, Pydantic plays nicely with your linters/IDE/brain. Define how data should be in pure, canonical Python 3.9+; validate it with Pydantic. Monitor Pydantic with Pydantic Logfire Pydantic Logfire is an application monitoring tool that is as simple to use and powerful as Pydantic itself. Logfire integrates with many popular Python libraries including FastAPI, OpenAI and Pydantic itself, so you can use Logfire to monitor Pydantic validations and understand why some inputs fail validation: from datetime import datetime\n\nimport logfire\n\nfrom pydantic import BaseModel\n\nlogfire.configure()\nlogfire.instrument_pydantic()  # (1)!\n\n\nclass Delivery(BaseModel):\n    timestamp: datetime\n    dimensions: tuple[int, int]\n\n\n# this will record details of a successful validation to logfire\nm = Delivery(timestamp='2020-01-02T03:04:05Z', dimensions=['10', '20'])\nprint(repr(m.timestamp))\n#> datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\nprint(m.dimensions)\n#> (10, 20)\n\nDelivery(timestamp='2020-01-02T03:04:05Z', dimensions=['10'])  # (2)! Set logfire record all both successful and failed validations, use record='failure' to only record failed validations, learn more . This will raise a ValidationError since there are too few dimensions , details of the input data and validation errors will be recorded in Logfire. Would give you a view like this in the Logfire platform: This is just a toy example, but hopefully makes clear the potential value of instrumenting a more complex application. Learn more about Pydantic Logfire Sign up to The Pydantic Stack newsletter, with updates & tutorials on Logfire, Pydantic AI and Pydantic: Subscribe","pageID":"Welcome to Pydantic","abs_url":"/latest/#pydantic-validation","title":"Welcome to Pydantic","objectID":"/latest/#pydantic-validation","rank":100},{"content":"Powered by type hints — with Pydantic, schema validation and serialization are controlled by type annotations; less to learn, less code to write, and integration with your IDE and static analysis tools. Learn more… Speed — Pydantic's core validation logic is written in Rust. As a result, Pydantic is among the fastest data validation libraries for Python. Learn more… JSON Schema — Pydantic models can emit JSON Schema, allowing for easy integration with other tools. Learn more… Strict and Lax mode — Pydantic can run in either strict mode (where data is not converted) or lax mode where Pydantic tries to coerce data to the correct type where appropriate. Learn more… Dataclasses , TypedDicts and more — Pydantic supports validation of many standard library types including dataclass and TypedDict . Learn more… Customisation — Pydantic allows custom validators and serializers to alter how data is processed in many powerful ways. Learn more… Ecosystem — around 8,000 packages on PyPI use Pydantic, including massively popular libraries like FastAPI , huggingface , Django Ninja , SQLModel , & LangChain . Learn more… Battle tested — Pydantic is downloaded over 360M times/month and is used by all FAANG companies and 20 of the 25 largest companies on NASDAQ. If you're trying to do something with Pydantic, someone else has probably already done it. Learn more… Installing Pydantic is as simple as: pip install pydantic","pageID":"Welcome to Pydantic","abs_url":"/latest/#why-use-pydantic","title":"Welcome to Pydantic - Why use Pydantic?","objectID":"/latest/#why-use-pydantic","rank":95},{"content":"To see Pydantic at work, let's start with a simple example, creating a custom class that inherits from BaseModel : from datetime import datetime\n\nfrom pydantic import BaseModel, PositiveInt\n\n\nclass User(BaseModel):\n    id: int  # (1)!\n    name: str = 'John Doe'  # (2)!\n    signup_ts: datetime | None  # (3)!\n    tastes: dict[str, PositiveInt]  # (4)!\n\n\nexternal_data = {\n    'id': 123,\n    'signup_ts': '2019-06-01 12:22',  # (5)!\n    'tastes': {\n        'wine': 9,\n        b'cheese': 7,  # (6)!\n        'cabbage': '1',  # (7)!\n    },\n}\n\nuser = User(**external_data)  # (8)!\n\nprint(user.id)  # (9)!\n#> 123\nprint(user.model_dump())  # (10)!\n\"\"\"\n{\n    'id': 123,\n    'name': 'John Doe',\n    'signup_ts': datetime.datetime(2019, 6, 1, 12, 22),\n    'tastes': {'wine': 9, 'cheese': 7, 'cabbage': 1},\n}\n\"\"\" id is of type int ; the annotation-only declaration tells Pydantic that this field is required. Strings,\n   bytes, or floats will be coerced to integers if possible; otherwise an exception will be raised. name is a string; because it has a default, it is not required. signup_ts is a  field that is required, but the value None may be provided;\n   Pydantic will process either a Unix timestamp integer (e.g. 1496498400 )\n   or a string representing the date and time. tastes is a dictionary with string keys and positive integer values. The PositiveInt type is\n   shorthand for Annotated[int, annotated_types.Gt(0)] . The input here is an ISO 8601 formatted datetime, but Pydantic will\n   convert it to a  object. The key here is bytes , but Pydantic will take care of coercing it to a string. Similarly, Pydantic will coerce the string '1' to the integer 1 . We create instance of User by passing our external data to User as keyword arguments. We can access fields as attributes of the model. We can convert the model to a dictionary with . If validation fails, Pydantic will raise an error with a breakdown of what was wrong: # continuing the above example...\n\nfrom datetime import datetime\nfrom pydantic import BaseModel, PositiveInt, ValidationError\n\n\nclass User(BaseModel):\n    id: int\n    name: str = 'John Doe'\n    signup_ts: datetime | None\n    tastes: dict[str, PositiveInt]\n\n\nexternal_data = {'id': 'not an int', 'tastes': {}}  # (1)!\n\ntry:\n    User(**external_data)  # (2)!\nexcept ValidationError as e:\n    print(e.errors())\n    \"\"\"\n    [\n        {\n            'type': 'int_parsing',\n            'loc': ('id',),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'not an int',\n            'url': 'https://errors.pydantic.dev/2/v/int_parsing',\n        },\n        {\n            'type': 'missing',\n            'loc': ('signup_ts',),\n            'msg': 'Field required',\n            'input': {'id': 'not an int', 'tastes': {}},\n            'url': 'https://errors.pydantic.dev/2/v/missing',\n        },\n    ]\n    \"\"\" The input data is wrong here — id is not a valid integer, and signup_ts is missing. Trying to instantiate User will raise a  with a list of errors.","pageID":"Welcome to Pydantic","abs_url":"/latest/#pydantic-examples","title":"Welcome to Pydantic - Pydantic examples","objectID":"/latest/#pydantic-examples","rank":90},{"content":"Hundreds of organisations and packages are using Pydantic. Some of the prominent companies and organizations around the world who are using Pydantic include: For a more comprehensive list of open-source projects using Pydantic see the list of dependents on github , or you can find some awesome projects using Pydantic in awesome-pydantic .","pageID":"Welcome to Pydantic","abs_url":"/latest/#who-is-using-pydantic","title":"Welcome to Pydantic - Who is using Pydantic?","objectID":"/latest/#who-is-using-pydantic","rank":85},{"content":"GitHub release This is the first alpha release of the upcoming 2.12 release, which adds initial support for Python 3.14.","pageID":"Changelog","abs_url":"/latest/changelog/#v2120a1-2025-07-26","title":"Changelog - v2.12.0a1 (2025-07-26)","objectID":"/latest/changelog/#v2120a1-2025-07-26","rank":100},{"content":"New Features ¶ Add __pydantic_on_complete__() hook that is called once model is fully ready to be used by @DouweM in #11762 Add initial support for Python 3.14 by @Viicos in #11991 Add regex patterns to JSON schema for Decimal type by @Dima-Bulavenko in #11987 Add support for doc attribute on dataclass fields by @Viicos in #12077 Add experimental MISSING sentinel by @Viicos in #11883 Changes ¶ Allow config and bases to be specified together in create_model() by @Viicos in #11714 Move some field logic out of the GenerateSchema class by @Viicos in #11733 Always make use of inspect.getsourcelines() for docstring extraction on Python 3.13 and greater by @Viicos in #11829 Only support the latest Mypy version by @Viicos in #11832 Do not implicitly convert after model validators to class methods by @Viicos in #11957 Refactor FieldInfo creation implementation by @Viicos in #11898 Make Secret covariant by @bluenote10 in #12008 Emit warning when field-specific metadata is used in invalid contexts by @Viicos in #12028 Fixes ¶ Properly fetch plain serializer function when serializing default value in JSON Schema by @Viicos in #11721 Remove generics cache workaround by @Viicos in #11755 Remove coercion of decimal constraints by @Viicos in #11772 Fix crash when expanding root type in the mypy plugin by @Viicos in #11735 Only mark model as complete once all fields are complete by @DouweM in #11759 Do not provide field_name in validator core schemas by @DouweM in #11761 Fix issue with recursive generic models by @Viicos in #11775 Fix qualified name comparison of private attributes during namespace inspection by @karta9821 in #11803 Make sure Pydantic dataclasses with slots and validate_assignment can be unpickled by @Viicos in #11769 Traverse function-before schemas during schema gathering by @Viicos in #11801 Fix check for stdlib dataclasses by @Viicos in #11822 Check if FieldInfo is complete after applying type variable map by @Viicos in #11855 Do not delete mock validator/serializer in model_rebuild() by @Viicos in #11890 Rebuild dataclass fields before schema generation by @Viicos in #11949 Always store the original field assignment on FieldInfo by @Viicos in #11946 Do not use deprecated methods as default field values by @Viicos in #11914 Allow callable discriminator to be applied on PEP 695 type aliases by @Viicos in #11941 Suppress core schema generation warning when using SkipValidation by @ygsh0816 in #12002 Do not emit typechecking error for invalid Field() default with validate_default set to True by @Viicos in #11988 Refactor logic to support Pydantic's Field() function in dataclasses by @Viicos in #12051 Packaging ¶ Update project metadata to use PEP 639 by @Viicos in #11694 Bump mkdocs-llmstxt to v0.2.0 by @Viicos in #11725 Bump pydantic-core to v2.35.1 by @Viicos in #11963 Bump dawidd6/action-download-artifact from 10 to 11 by @dependabot [bot] in #12033 Bump astral-sh/setup-uv from 5 to 6 by @dependabot [bot] in #11826 Update mypy to 1.17.0 by @Viicos in #12076","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed","title":"Changelog - v2.12.0a1 (2025-07-26) - What's Changed","objectID":"/latest/changelog/#whats-changed","rank":95},{"content":"@parth-paradkar made their first contribution in #11695 @dqkqd made their first contribution in #11739 @fhightower made their first contribution in #11722 @gbaian10 made their first contribution in #11766 @DouweM made their first contribution in #11759 @bowenliang123 made their first contribution in #11719 @rawwar made their first contribution in #11799 @karta9821 made their first contribution in #11803 @jinnovation made their first contribution in #11834 @zmievsa made their first contribution in #11861 @Otto-AA made their first contribution in #11860 @ygsh0816 made their first contribution in #12002 @lukland made their first contribution in #12015 @Dima-Bulavenko made their first contribution in #11987 @GSemikozov made their first contribution in #12050 @hannah-heywa made their first contribution in #12082","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors","title":"Changelog - v2.12.0a1 (2025-07-26) - New Contributors","objectID":"/latest/changelog/#new-contributors","rank":90},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2117-2025-06-14","title":"Changelog - v2.11.7 (2025-06-14)","objectID":"/latest/changelog/#v2117-2025-06-14","rank":85},{"content":"Fixes ¶ Copy FieldInfo instance if necessary during FieldInfo build by @Viicos in #11898","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_1","title":"Changelog - v2.11.7 (2025-06-14) - What's Changed","objectID":"/latest/changelog/#whats-changed_1","rank":80},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2116-2025-06-13","title":"Changelog - v2.11.6 (2025-06-13)","objectID":"/latest/changelog/#v2116-2025-06-13","rank":75},{"content":"Fixes ¶ Rebuild dataclass fields before schema generation by @Viicos in #11949 Always store the original field assignment on FieldInfo by @Viicos in #11946","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_2","title":"Changelog - v2.11.6 (2025-06-13) - What's Changed","objectID":"/latest/changelog/#whats-changed_2","rank":70},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2115-2025-05-22","title":"Changelog - v2.11.5 (2025-05-22)","objectID":"/latest/changelog/#v2115-2025-05-22","rank":65},{"content":"Fixes ¶ Check if FieldInfo is complete after applying type variable map by @Viicos in #11855 Do not delete mock validator/serializer in model_rebuild() by @Viicos in #11890 Do not duplicate metadata on model rebuild by @Viicos in #11902","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_3","title":"Changelog - v2.11.5 (2025-05-22) - What's Changed","objectID":"/latest/changelog/#whats-changed_3","rank":60},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2114-2025-04-29","title":"Changelog - v2.11.4 (2025-04-29)","objectID":"/latest/changelog/#v2114-2025-04-29","rank":55},{"content":"Packaging ¶ Bump mkdocs-llmstxt to v0.2.0 by @Viicos in #11725 Changes ¶ Allow config and bases to be specified together in create_model() by @Viicos in #11714 .\n  This change was backported as it was previously possible (although not meant to be supported)\n  to provide model_config as a field, which would make it possible to provide both configuration\n  and bases. Fixes ¶ Remove generics cache workaround by @Viicos in #11755 Remove coercion of decimal constraints by @Viicos in #11772 Fix crash when expanding root type in the mypy plugin by @Viicos in #11735 Fix issue with recursive generic models by @Viicos in #11775 Traverse function-before schemas during schema gathering by @Viicos in #11801","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_4","title":"Changelog - v2.11.4 (2025-04-29) - What's Changed","objectID":"/latest/changelog/#whats-changed_4","rank":50},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2113-2025-04-08","title":"Changelog - v2.11.3 (2025-04-08)","objectID":"/latest/changelog/#v2113-2025-04-08","rank":45},{"content":"Packaging ¶ Update V1 copy to v1.10.21 by @Viicos in #11706 Fixes ¶ Preserve field description when rebuilding model fields by @Viicos in #11698","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_5","title":"Changelog - v2.11.3 (2025-04-08) - What's Changed","objectID":"/latest/changelog/#whats-changed_5","rank":40},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2112-2025-04-03","title":"Changelog - v2.11.2 (2025-04-03)","objectID":"/latest/changelog/#v2112-2025-04-03","rank":35},{"content":"Fixes ¶ Bump pydantic-core to v2.33.1 by @Viicos in #11678 Make sure __pydantic_private__ exists before setting private attributes by @Viicos in #11666 Do not override FieldInfo._complete when using field from parent class by @Viicos in #11668 Provide the available definitions when applying discriminated unions by @Viicos in #11670 Do not expand root type in the mypy plugin for variables by @Viicos in #11676 Mention the attribute name in model fields deprecation message by @Viicos in #11674 Properly validate parameterized mappings by @Viicos in #11658","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_6","title":"Changelog - v2.11.2 (2025-04-03) - What's Changed","objectID":"/latest/changelog/#whats-changed_6","rank":30},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2111-2025-03-28","title":"Changelog - v2.11.1 (2025-03-28)","objectID":"/latest/changelog/#v2111-2025-03-28","rank":25},{"content":"Fixes ¶ Do not override 'definitions-ref' schemas containing serialization schemas or metadata by @Viicos in #11644","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_7","title":"Changelog - v2.11.1 (2025-03-28) - What's Changed","objectID":"/latest/changelog/#whats-changed_7","rank":20},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2110-2025-03-27","title":"Changelog - v2.11.0 (2025-03-27)","objectID":"/latest/changelog/#v2110-2025-03-27","rank":15},{"content":"Pydantic v2.11 is a version strongly focused on build time performance of Pydantic models (and core schema generation in general).\nSee the blog post for more details. Packaging ¶ Bump pydantic-core to v2.33.0 by @Viicos in #11631 New Features ¶ Add encoded_string() method to the URL types by @YassinNouh21 in #11580 Add support for defer_build with @validate_call decorator by @Viicos in #11584 Allow @with_config decorator to be used with keyword arguments by @Viicos in #11608 Simplify customization of default value inclusion in JSON Schema generation by @Viicos in #11634 Add generate_arguments_schema() function by @Viicos in #11572 Fixes ¶ Allow generic typed dictionaries to be used for unpacked variadic keyword parameters by @Viicos in #11571 Fix runtime error when computing model string representation involving cached properties and self-referenced models by @Viicos in #11579 Preserve other steps when using the ellipsis in the pipeline API by @Viicos in #11626 Fix deferred discriminator application logic by @Viicos in #11591","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_8","title":"Changelog - v2.11.0 (2025-03-27) - What's Changed","objectID":"/latest/changelog/#whats-changed_8","rank":10},{"content":"@cmenon12 made their first contribution in #11562 @Jeukoh made their first contribution in #11611","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_1","title":"Changelog - v2.11.0 (2025-03-27) - New Contributors","objectID":"/latest/changelog/#new-contributors_1","rank":5},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2110b2-2025-03-17","title":"Changelog - v2.11.0b2 (2025-03-17)","objectID":"/latest/changelog/#v2110b2-2025-03-17","rank":0},{"content":"Packaging ¶ Bump pydantic-core to v2.32.0 by @Viicos in #11567 New Features ¶ Add experimental support for free threading by @Viicos in #11516 Fixes ¶ Fix NotRequired qualifier not taken into account in stringified annotation by @Viicos in #11559","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_9","title":"Changelog - v2.11.0b2 (2025-03-17) - What's Changed","objectID":"/latest/changelog/#whats-changed_9","rank":-5},{"content":"@joren485 made their first contribution in #11547","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_2","title":"Changelog - v2.11.0b2 (2025-03-17) - New Contributors","objectID":"/latest/changelog/#new-contributors_2","rank":-10},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2110b1-2025-03-06","title":"Changelog - v2.11.0b1 (2025-03-06)","objectID":"/latest/changelog/#v2110b1-2025-03-06","rank":-15},{"content":"Packaging ¶ Add a check_pydantic_core_version() function by @Viicos in https://github.com/pydantic/pydantic/pull/11324 Remove greenlet development dependency by @Viicos in https://github.com/pydantic/pydantic/pull/11351 Use the typing-inspection library by @Viicos in https://github.com/pydantic/pydantic/pull/11479 Bump pydantic-core to v2.31.1 by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11526 New Features ¶ Support unsubstituted type variables with both a default and a bound or constraints by @FyZzyss in https://github.com/pydantic/pydantic/pull/10789 Add a default_factory_takes_validated_data property to FieldInfo by @Viicos in https://github.com/pydantic/pydantic/pull/11034 Raise a better error when a generic alias is used inside type[] by @Viicos in https://github.com/pydantic/pydantic/pull/11088 Properly support PEP 695 generics syntax by @Viicos in https://github.com/pydantic/pydantic/pull/11189 Properly support type variable defaults by @Viicos in https://github.com/pydantic/pydantic/pull/11332 Add support for validating v6, v7, v8 UUIDs by @astei in https://github.com/pydantic/pydantic/pull/11436 Improve alias configuration APIs by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11468 Changes ¶ Rework create_model field definitions format by @Viicos in https://github.com/pydantic/pydantic/pull/11032 Raise a deprecation warning when a field is annotated as final with a default value by @Viicos in https://github.com/pydantic/pydantic/pull/11168 Deprecate accessing model_fields and model_computed_fields on instances by @Viicos in https://github.com/pydantic/pydantic/pull/11169 Breaking Change: Move core schema generation logic for path types inside the GenerateSchema class by @sydney-runkle in https://github.com/pydantic/pydantic/pull/10846 Remove Python 3.8 Support by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11258 Optimize calls to get_type_ref by @Viicos in https://github.com/pydantic/pydantic/pull/10863 Disable pydantic-core core schema validation by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11271 Performance ¶ Only evaluate FieldInfo annotations if required during schema building by @Viicos in https://github.com/pydantic/pydantic/pull/10769 Improve __setattr__ performance of Pydantic models by caching setter functions by @MarkusSintonen in https://github.com/pydantic/pydantic/pull/10868 Improve annotation application performance by @Viicos in https://github.com/pydantic/pydantic/pull/11186 Improve performance of _typing_extra module by @Viicos in https://github.com/pydantic/pydantic/pull/11255 Refactor and optimize schema cleaning logic by @Viicos in https://github.com/pydantic/pydantic/pull/11244 Create a single dictionary when creating a CoreConfig instance by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11384 Bump pydantic-core and thus use SchemaValidator and SchemaSerializer caching by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11402 Reuse cached core schemas for parametrized generic Pydantic models by @MarkusSintonen in https://github.com/pydantic/pydantic/pull/11434 Fixes ¶ Improve TypeAdapter instance repr by @sydney-runkle in https://github.com/pydantic/pydantic/pull/10872 Use the correct frame when instantiating a parametrized TypeAdapter by @Viicos in https://github.com/pydantic/pydantic/pull/10893 Infer final fields with a default value as class variables in the mypy plugin by @Viicos in https://github.com/pydantic/pydantic/pull/11121 Recursively unpack Literal values if using PEP 695 type aliases by @Viicos in https://github.com/pydantic/pydantic/pull/11114 Override __subclasscheck__ on ModelMetaclass to avoid memory leak and performance issues by @Viicos in https://github.com/pydantic/pydantic/pull/11116 Remove unused _extract_get_pydantic_json_schema() parameter by @Viicos in https://github.com/pydantic/pydantic/pull/11155 Improve discriminated union error message for invalid union variants by @Viicos in https://github.com/pydantic/pydantic/pull/11161 Unpack PEP 695 type aliases if using the Annotated form by @Viicos in https://github.com/pydantic/pydantic/pull/11109 Add missing stacklevel in deprecated_instance_property warning by @Viicos in https://github.com/pydantic/pydantic/pull/11200 Copy WithJsonSchema schema to avoid sharing mutated data by @thejcannon in https://github.com/pydantic/pydantic/pull/11014 Do not cache parametrized models when in the process of parametrizing another model by @Viicos in https://github.com/pydantic/pydantic/pull/10704 Add discriminated union related metadata entries to the CoreMetadata definition by @Viicos in https://github.com/pydantic/pydantic/pull/11216 Consolidate schema definitions logic in the _Definitions class by @Viicos in https://github.com/pydantic/pydantic/pull/11208 Support initializing root model fields with values of the root type in the mypy plugin by @Viicos in https://github.com/pydantic/pydantic/pull/11212 Fix various issues with dataclasses and use_attribute_docstrings by @Viicos in https://github.com/pydantic/pydantic/pull/11246 Only compute normalized decimal places if necessary in decimal_places_validator by @misrasaurabh1 in https://github.com/pydantic/pydantic/pull/11281 Add support for validation_alias in the mypy plugin by @Viicos in https://github.com/pydantic/pydantic/pull/11295 Fix JSON Schema reference collection with \"examples\" keys by @Viicos in https://github.com/pydantic/pydantic/pull/11305 Do not transform model serializer functions as class methods in the mypy plugin by @Viicos in https://github.com/pydantic/pydantic/pull/11298 Simplify GenerateJsonSchema.literal_schema() implementation by @misrasaurabh1 in https://github.com/pydantic/pydantic/pull/11321 Add additional allowed schemes for ClickHouseDsn by @Maze21127 in https://github.com/pydantic/pydantic/pull/11319 Coerce decimal constraints to Decimal instances by @Viicos in https://github.com/pydantic/pydantic/pull/11350 Use the correct JSON Schema mode when handling function schemas by @Viicos in https://github.com/pydantic/pydantic/pull/11367 Improve exception message when encountering recursion errors during type evaluation by @Viicos in https://github.com/pydantic/pydantic/pull/11356 Always include additionalProperties: True for arbitrary dictionary schemas by @austinyu in https://github.com/pydantic/pydantic/pull/11392 Expose fallback parameter in serialization methods by @Viicos in https://github.com/pydantic/pydantic/pull/11398 Fix path serialization behavior by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11416 Do not reuse validators and serializers during model rebuild by @Viicos in https://github.com/pydantic/pydantic/pull/11429 Collect model fields when rebuilding a model by @Viicos in https://github.com/pydantic/pydantic/pull/11388 Allow cached properties to be altered on frozen models by @Viicos in https://github.com/pydantic/pydantic/pull/11432 Fix tuple serialization for Sequence types by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11435 Fix: do not check for __get_validators__ on classes where __get_pydantic_core_schema__ is also defined by @tlambert03 in https://github.com/pydantic/pydantic/pull/11444 Allow callable instances to be used as serializers by @Viicos in https://github.com/pydantic/pydantic/pull/11451 Improve error thrown when overriding field with a property by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11459 Fix JSON Schema generation with referenceable core schemas holding JSON metadata by @Viicos in https://github.com/pydantic/pydantic/pull/11475 Support strict specification on union member types by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11481 Implicitly set validate_by_name to True when validate_by_alias is False by @sydney-runkle in https://github.com/pydantic/pydantic/pull/11503 Change type of Any when synthesizing BaseSettings.__init__ signature in the mypy plugin by @Viicos in https://github.com/pydantic/pydantic/pull/11497 Support type variable defaults referencing other type variables by @Viicos in https://github.com/pydantic/pydantic/pull/11520 Fix ValueError on year zero by @davidhewitt in https://github.com/pydantic/pydantic-core/pull/1583 dataclass InitVar shouldn't be required on serialization by @sydney-runkle in https://github.com/pydantic/pydantic-core/pull/1602","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_10","title":"Changelog - v2.11.0b1 (2025-03-06) - What's Changed","objectID":"/latest/changelog/#whats-changed_10","rank":-20},{"content":"@FyZzyss made their first contribution in https://github.com/pydantic/pydantic/pull/10789 @tamird made their first contribution in https://github.com/pydantic/pydantic/pull/10948 @felixxm made their first contribution in https://github.com/pydantic/pydantic/pull/11077 @alexprabhat99 made their first contribution in https://github.com/pydantic/pydantic/pull/11082 @Kharianne made their first contribution in https://github.com/pydantic/pydantic/pull/11111 @mdaffad made their first contribution in https://github.com/pydantic/pydantic/pull/11177 @thejcannon made their first contribution in https://github.com/pydantic/pydantic/pull/11014 @thomasfrimannkoren made their first contribution in https://github.com/pydantic/pydantic/pull/11251 @usernameMAI made their first contribution in https://github.com/pydantic/pydantic/pull/11275 @ananiavito made their first contribution in https://github.com/pydantic/pydantic/pull/11302 @pawamoy made their first contribution in https://github.com/pydantic/pydantic/pull/11311 @Maze21127 made their first contribution in https://github.com/pydantic/pydantic/pull/11319 @kauabh made their first contribution in https://github.com/pydantic/pydantic/pull/11369 @jaceklaskowski made their first contribution in https://github.com/pydantic/pydantic/pull/11353 @tmpbeing made their first contribution in https://github.com/pydantic/pydantic/pull/11375 @petyosi made their first contribution in https://github.com/pydantic/pydantic/pull/11405 @austinyu made their first contribution in https://github.com/pydantic/pydantic/pull/11392 @mikeedjones made their first contribution in https://github.com/pydantic/pydantic/pull/11402 @astei made their first contribution in https://github.com/pydantic/pydantic/pull/11436 @dsayling made their first contribution in https://github.com/pydantic/pydantic/pull/11522 @sobolevn made their first contribution in https://github.com/pydantic/pydantic-core/pull/1645","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_3","title":"Changelog - New Contributors","objectID":"/latest/changelog/#new-contributors_3","rank":-25},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2110a2-2025-02-10","title":"Changelog - v2.11.0a2 (2025-02-10)","objectID":"/latest/changelog/#v2110a2-2025-02-10","rank":-30},{"content":"Pydantic v2.11 is a version strongly focused on build time performance of Pydantic models (and core schema generation in general).\nThis is another early alpha release, meant to collect early feedback from users having issues with core schema builds. Packaging ¶ Bump ruff from 0.9.2 to 0.9.5 by @Viicos in #11407 Bump pydantic-core to v2.29.0 by @mikeedjones in #11402 Use locally-built rust with symbols & pgo by @davidhewitt in #11403 Performance ¶ Create a single dictionary when creating a CoreConfig instance by @sydney-runkle in #11384 Fixes ¶ Use the correct JSON Schema mode when handling function schemas by @Viicos in #11367 Fix JSON Schema reference logic with examples keys by @Viicos in #11366 Improve exception message when encountering recursion errors during type evaluation by @Viicos in #11356 Always include additionalProperties: True for arbitrary dictionary schemas by @austinyu in #11392 Expose fallback parameter in serialization methods by @Viicos in #11398 Fix path serialization behavior by @sydney-runkle in #11416","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_11","title":"Changelog - v2.11.0a2 (2025-02-10) - What's Changed","objectID":"/latest/changelog/#whats-changed_11","rank":-35},{"content":"@kauabh made their first contribution in #11369 @jaceklaskowski made their first contribution in #11353 @tmpbeing made their first contribution in #11375 @petyosi made their first contribution in #11405 @austinyu made their first contribution in #11392 @mikeedjones made their first contribution in #11402","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_4","title":"Changelog - v2.11.0a2 (2025-02-10) - New Contributors","objectID":"/latest/changelog/#new-contributors_4","rank":-40},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2110a1-2025-01-30","title":"Changelog - v2.11.0a1 (2025-01-30)","objectID":"/latest/changelog/#v2110a1-2025-01-30","rank":-45},{"content":"Pydantic v2.11 is a version strongly focused on build time performance of Pydantic models (and core schema generation in general).\nThis is an early alpha release, meant to collect early feedback from users having issues with core schema builds. Packaging ¶ Bump dawidd6/action-download-artifact from 6 to 7 by @dependabot in #11018 Re-enable memray related tests on Python 3.12+ by @Viicos in #11191 Bump astral-sh/setup-uv to 5 by @dependabot in #11205 Bump ruff to v0.9.0 by @sydney-runkle in #11254 Regular uv.lock deps update by @sydney-runkle in #11333 Add a check_pydantic_core_version() function by @Viicos in #11324 Remove greenlet development dependency by @Viicos in #11351 Bump pydantic-core to v2.28.0 by @Viicos in #11364 New Features ¶ Support unsubstituted type variables with both a default and a bound or constraints by @FyZzyss in #10789 Add a default_factory_takes_validated_data property to FieldInfo by @Viicos in #11034 Raise a better error when a generic alias is used inside type[] by @Viicos in #11088 Properly support PEP 695 generics syntax by @Viicos in #11189 Properly support type variable defaults by @Viicos in #11332 Changes ¶ Rework create_model field definitions format by @Viicos in #11032 Raise a deprecation warning when a field is annotated as final with a default value by @Viicos in #11168 Deprecate accessing model_fields and model_computed_fields on instances by @Viicos in #11169 Move core schema generation logic for path types inside the GenerateSchema class by @sydney-runkle in #10846 Move deque schema gen to GenerateSchema class by @sydney-runkle in #11239 Move Mapping schema gen to GenerateSchema to complete removal of prepare_annotations_for_known_type workaround by @sydney-runkle in #11247 Remove Python 3.8 Support by @sydney-runkle in #11258 Disable pydantic-core core schema validation by @sydney-runkle in #11271 Performance ¶ Only evaluate FieldInfo annotations if required during schema building by @Viicos in #10769 Optimize calls to get_type_ref by @Viicos in #10863 Improve __setattr__ performance of Pydantic models by caching setter functions by @MarkusSintonen in #10868 Improve annotation application performance by @Viicos in #11186 Improve performance of _typing_extra module by @Viicos in #11255 Refactor and optimize schema cleaning logic by @Viicos and @MarkusSintonen in #11244 Fixes ¶ Add validation tests for _internal/_validators.py by @tkasuz in #10763 Improve TypeAdapter instance repr by @sydney-runkle in #10872 Revert \"ci: use locally built pydantic-core with debug symbols by @sydney-runkle in #10942 Re-enable all FastAPI tests by @tamird in #10948 Fix typo in HISTORY.md. by @felixxm in #11077 Infer final fields with a default value as class variables in the mypy plugin by @Viicos in #11121 Recursively unpack Literal values if using PEP 695 type aliases by @Viicos in #11114 Override __subclasscheck__ on ModelMetaclass to avoid memory leak and performance issues by @Viicos in #11116 Remove unused _extract_get_pydantic_json_schema() parameter by @Viicos in #11155 Add FastAPI and SQLModel to third-party tests by @sydney-runkle in #11044 Fix conditional expressions syntax for third-party tests by @Viicos in #11162 Move FastAPI tests to third-party workflow by @Viicos in #11164 Improve discriminated union error message for invalid union variants by @Viicos in #11161 Unpack PEP 695 type aliases if using the Annotated form by @Viicos in #11109 Include openapi-python-client check in issue creation for third-party failures, use main branch by @sydney-runkle in #11182 Add pandera third-party tests by @Viicos in #11193 Add ODMantic third-party tests by @sydney-runkle in #11197 Add missing stacklevel in deprecated_instance_property warning by @Viicos in #11200 Copy WithJsonSchema schema to avoid sharing mutated data by @thejcannon in #11014 Do not cache parametrized models when in the process of parametrizing another model by @Viicos in #10704 Re-enable Beanie third-party tests by @Viicos in #11214 Add discriminated union related metadata entries to the CoreMetadata definition by @Viicos in #11216 Consolidate schema definitions logic in the _Definitions class by @Viicos in #11208 Support initializing root model fields with values of the root type in the mypy plugin by @Viicos in #11212 Fix various issues with dataclasses and use_attribute_docstrings by @Viicos in #11246 Only compute normalized decimal places if necessary in decimal_places_validator by @misrasaurabh1 in #11281 Fix two misplaced sentences in validation errors documentation by @ananiavito in #11302 Fix mkdocstrings inventory example in documentation by @pawamoy in #11311 Add support for validation_alias in the mypy plugin by @Viicos in #11295 Do not transform model serializer functions as class methods in the mypy plugin by @Viicos in #11298 Simplify GenerateJsonSchema.literal_schema() implementation by @misrasaurabh1 in #11321 Add additional allowed schemes for ClickHouseDsn by @Maze21127 in #11319 Coerce decimal constraints to Decimal instances by @Viicos in #11350 Fix ValueError on year zero by @davidhewitt in pydantic-core#1583","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_12","title":"Changelog - v2.11.0a1 (2025-01-30) - What's Changed","objectID":"/latest/changelog/#whats-changed_12","rank":-50},{"content":"@FyZzyss made their first contribution in #10789 @tamird made their first contribution in #10948 @felixxm made their first contribution in #11077 @alexprabhat99 made their first contribution in #11082 @Kharianne made their first contribution in #11111 @mdaffad made their first contribution in #11177 @thejcannon made their first contribution in #11014 @thomasfrimannkoren made their first contribution in #11251 @usernameMAI made their first contribution in #11275 @ananiavito made their first contribution in #11302 @pawamoy made their first contribution in #11311 @Maze21127 made their first contribution in #11319","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_5","title":"Changelog - v2.11.0a1 (2025-01-30) - New Contributors","objectID":"/latest/changelog/#new-contributors_5","rank":-55},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2106-2025-01-23","title":"Changelog - v2.10.6 (2025-01-23)","objectID":"/latest/changelog/#v2106-2025-01-23","rank":-60},{"content":"Fixes ¶ Fix JSON Schema reference collection with 'examples' keys by @Viicos in #11325 Fix url python serialization by @sydney-runkle in #11331","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_13","title":"Changelog - v2.10.6 (2025-01-23) - What's Changed","objectID":"/latest/changelog/#whats-changed_13","rank":-65},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2105-2025-01-08","title":"Changelog - v2.10.5 (2025-01-08)","objectID":"/latest/changelog/#v2105-2025-01-08","rank":-70},{"content":"Fixes ¶ Remove custom MRO implementation of Pydantic models by @Viicos in #11184 Fix URL serialization for unions by @sydney-runkle in #11233","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_14","title":"Changelog - v2.10.5 (2025-01-08) - What's Changed","objectID":"/latest/changelog/#whats-changed_14","rank":-75},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2104-2024-12-18","title":"Changelog - v2.10.4 (2024-12-18)","objectID":"/latest/changelog/#v2104-2024-12-18","rank":-80},{"content":"Packaging ¶ Bump pydantic-core to v2.27.2 by @davidhewitt in #11138 Fixes ¶ Fix for comparison of AnyUrl objects by @alexprabhat99 in #11082 Properly fetch PEP 695 type params for functions, do not fetch annotations from signature by @Viicos in #11093 Include JSON Schema input core schema in function schemas by @Viicos in #11085 Add len to _BaseUrl to avoid TypeError by @Kharianne in #11111 Make sure the type reference is removed from the seen references by @Viicos in #11143","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_15","title":"Changelog - v2.10.4 (2024-12-18) - What's Changed","objectID":"/latest/changelog/#whats-changed_15","rank":-85},{"content":"@FyZzyss made their first contribution in #10789 @tamird made their first contribution in #10948 @felixxm made their first contribution in #11077 @alexprabhat99 made their first contribution in #11082 @Kharianne made their first contribution in #11111","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_6","title":"Changelog - v2.10.4 (2024-12-18) - New Contributors","objectID":"/latest/changelog/#new-contributors_6","rank":-90},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2103-2024-12-03","title":"Changelog - v2.10.3 (2024-12-03)","objectID":"/latest/changelog/#v2103-2024-12-03","rank":-95},{"content":"Fixes ¶ Set fields when defer_build is set on Pydantic dataclasses by @Viicos in #10984 Do not resolve the JSON Schema reference for dict core schema keys by @Viicos in #10989 Use the globals of the function when evaluating the return type for PlainSerializer and WrapSerializer functions by @Viicos in #11008 Fix host required enforcement for urls to be compatible with v2.9 behavior by @sydney-runkle in #11027 Add a default_factory_takes_validated_data property to FieldInfo by @Viicos in #11034 Fix url json schema in serialization mode by @sydney-runkle in #11035","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_16","title":"Changelog - v2.10.3 (2024-12-03) - What's Changed","objectID":"/latest/changelog/#whats-changed_16","rank":-100},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2102-2024-11-25","title":"Changelog - v2.10.2 (2024-11-25)","objectID":"/latest/changelog/#v2102-2024-11-25","rank":-105},{"content":"Fixes ¶ Only evaluate FieldInfo annotations if required during schema building by @Viicos in #10769 Do not evaluate annotations for private fields by @Viicos in #10962 Support serialization as any for Secret types and Url types by @sydney-runkle in #10947 Fix type hint of Field.default to be compatible with Python 3.8 and 3.9 by @Viicos in #10972 Add hashing support for URL types by @sydney-runkle in #10975 Hide BaseModel.__replace__ definition from type checkers by @Viicos in #10979","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_17","title":"Changelog - v2.10.2 (2024-11-25) - What's Changed","objectID":"/latest/changelog/#whats-changed_17","rank":-110},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v2101-2024-11-21","title":"Changelog - v2.10.1 (2024-11-21)","objectID":"/latest/changelog/#v2101-2024-11-21","rank":-115},{"content":"Packaging ¶ Bump pydantic-core version to v2.27.1 by @sydney-runkle in #10938 Fixes ¶ Use the correct frame when instantiating a parametrized TypeAdapter by @Viicos in #10893 Relax check for validated data in default_factory utils by @sydney-runkle in #10909 Fix type checking issue with model_fields and model_computed_fields by @sydney-runkle in #10911 Use the parent configuration during schema generation for stdlib dataclass es by @sydney-runkle in #10928 Use the globals of the function when evaluating the return type of serializers and computed_field s by @Viicos in #10929 Fix URL constraint application by @sydney-runkle in #10922 Fix URL equality with different validation methods by @sydney-runkle in #10934 Fix JSON schema title when specified as '' by @sydney-runkle in #10936 Fix python mode serialization for complex inference by @sydney-runkle in pydantic-core#1549","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_18","title":"Changelog - v2.10.1 (2024-11-21) - What's Changed","objectID":"/latest/changelog/#whats-changed_18","rank":-120},{"content":"","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_7","title":"Changelog - v2.10.1 (2024-11-21) - New Contributors","objectID":"/latest/changelog/#new-contributors_7","rank":-125},{"content":"The code released in v2.10.0 is practically identical to that of v2.10.0b2. GitHub release See the v2.10 release blog post for the highlights!","pageID":"Changelog","abs_url":"/latest/changelog/#v2100-2024-11-20","title":"Changelog - v2.10.0 (2024-11-20)","objectID":"/latest/changelog/#v2100-2024-11-20","rank":-130},{"content":"Packaging ¶ Bump pydantic-core to v2.27.0 by @sydney-runkle in #10825 Replaced pdm with uv by @frfahim in #10727 New Features ¶ Support fractions.Fraction by @sydney-runkle in #10318 Support Hashable for json validation by @sydney-runkle in #10324 Add a SocketPath type for linux systems by @theunkn0wn1 in #10378 Allow arbitrary refs in JSON schema examples by @sydney-runkle in #10417 Support defer_build for Pydantic dataclasses by @Viicos in #10313 Adding v1 / v2 incompatibility warning for nested v1 model by @sydney-runkle in #10431 Add support for unpacked TypedDict to type hint variadic keyword arguments with @validate_call by @Viicos in #10416 Support compiled patterns in protected_namespaces by @sydney-runkle in #10522 Add support for propertyNames in JSON schema by @FlorianSW in #10478 Adding __replace__ protocol for Python 3.13+ support by @sydney-runkle in #10596 Expose public sort method for JSON schema generation by @sydney-runkle in #10595 Add runtime validation of @validate_call callable argument by @kc0506 in #10627 Add experimental_allow_partial support by @samuelcolvin in #10748 Support default factories taking validated data as an argument by @Viicos in #10678 Allow subclassing ValidationError and PydanticCustomError by @Youssefares in pydantic/pydantic-core#1413 Add trailing-strings support to experimental_allow_partial by @sydney-runkle in #10825 Add rebuild() method for TypeAdapter and simplify defer_build patterns by @sydney-runkle in #10537 Improve TypeAdapter instance repr by @sydney-runkle in #10872 Changes ¶ Don't allow customization of SchemaGenerator until interface is more stable by @sydney-runkle in #10303 Cleanly defer_build on TypeAdapters , removing experimental flag by @sydney-runkle in #10329 Fix mro of generic subclass  by @kc0506 in #10100 Strip whitespaces on JSON Schema title generation by @sydney-runkle in #10404 Use b64decode and b64encode for Base64Bytes type by @sydney-runkle in #10486 Relax protected namespace config default by @sydney-runkle in #10441 Revalidate parametrized generics if instance's origin is subclass of OG class by @sydney-runkle in #10666 Warn if configuration is specified on the @dataclass decorator and with the __pydantic_config__ attribute by @sydney-runkle in #10406 Recommend against using Ellipsis (...) with Field by @Viicos in #10661 Migrate to subclassing instead of annotated approach for pydantic url types by @sydney-runkle in #10662 Change JSON schema generation of Literal s and Enums by @Viicos in #10692 Simplify unions involving Any or Never when replacing type variables by @Viicos in #10338 Do not require padding when decoding base64 bytes by @bschoenmaeckers in pydantic/pydantic-core#1448 Support dates all the way to 1BC by @changhc in pydantic/speedate#77 Performance ¶ Schema cleaning: skip unnecessary copies during schema walking by @Viicos in #10286 Refactor namespace logic for annotations evaluation by @Viicos in #10530 Improve email regexp on edge cases by @AlekseyLobanov in #10601 CoreMetadata refactor with an emphasis on documentation, schema build time performance, and reducing complexity by @sydney-runkle in #10675 Fixes ¶ Remove guarding check on computed_field with field_serializer by @nix010 in #10390 Fix Predicate issue in v2.9.0 by @sydney-runkle in #10321 Fixing annotated-types bound by @sydney-runkle in #10327 Turn tzdata install requirement into optional timezone dependency by @jakob-keller in #10331 Use correct types namespace when building namedtuple core schemas by @Viicos in #10337 Fix evaluation of stringified annotations during namespace inspection by @Viicos in #10347 Fix IncEx type alias definition by @Viicos in #10339 Do not error when trying to evaluate annotations of private attributes by @Viicos in #10358 Fix nested type statement by @kc0506 in #10369 Improve typing of ModelMetaclass.mro by @Viicos in #10372 Fix class access of deprecated computed_field s by @Viicos in #10391 Make sure inspect.iscoroutinefunction works on coroutines decorated with @validate_call by @MovisLi in #10374 Fix NameError when using validate_call with PEP 695 on a class by @kc0506 in #10380 Fix ZoneInfo with various invalid types by @sydney-runkle in #10408 Fix PydanticUserError on empty model_config with annotations by @cdwilson in #10412 Fix variance issue in _IncEx type alias, only allow True by @Viicos in #10414 Fix serialization schema generation when using PlainValidator by @Viicos in #10427 Fix schema generation error when serialization schema holds references by @Viicos in #10444 Inline references if possible when generating schema for json_schema_input_type by @Viicos in #10439 Fix recursive arguments in Representation by @Viicos in #10480 Fix representation for builtin function types by @kschwab in #10479 Add python validators for decimal constraints ( max_digits and decimal_places ) by @sydney-runkle in #10506 Only fetch __pydantic_core_schema__ from the current class during schema generation by @Viicos in #10518 Fix stacklevel on deprecation warnings for BaseModel by @sydney-runkle in #10520 Fix warning stacklevel in BaseModel.__init__ by @Viicos in #10526 Improve error handling for in-evaluable refs for discriminator application by @sydney-runkle in #10440 Change the signature of ConfigWrapper.core_config to take the title directly by @Viicos in #10562 Do not use the previous config from the stack for dataclasses without config by @Viicos in #10576 Fix serialization for IP types with mode='python' by @sydney-runkle in #10594 Support constraint application for Base64Etc types by @sydney-runkle in #10584 Fix validate_call ignoring Field in Annotated by @kc0506 in #10610 Raise an error when Self is invalid by @kc0506 in #10609 Using core_schema.InvalidSchema instead of metadata injection + checks by @sydney-runkle in #10523 Tweak type alias logic by @kc0506 in #10643 Support usage of type with typing.Self and type aliases by @kc0506 in #10621 Use overloads for Field and PrivateAttr functions by @Viicos in #10651 Clean up the mypy plugin implementation by @Viicos in #10669 Properly check for typing_extensions variant of TypeAliasType by @Daraan in #10713 Allow any mapping in BaseModel.model_copy() by @Viicos in #10751 Fix isinstance behavior for urls by @sydney-runkle in #10766 Ensure cached_property can be set on Pydantic models by @Viicos in #10774 Fix equality checks for primitives in literals by @sydney-runkle in pydantic/pydantic-core#1459 Properly enforce host_required for URLs by @Viicos in pydantic/pydantic-core#1488 Fix when coerce_numbers_to_str enabled and string has invalid Unicode character by @andrey-berenda in pydantic/pydantic-core#1515 Fix serializing complex values in Enum s by @changhc in pydantic/pydantic-core#1524 Refactor _typing_extra module by @Viicos in #10725 Support intuitive equality for urls by @sydney-runkle in #10798 Add bytearray to TypeAdapter.validate_json signature by @samuelcolvin in #10802 Ensure class access of method descriptors is performed when used as a default with Field by @Viicos in #10816 Fix circular import with validate_call by @sydney-runkle in #10807 Fix error when using type aliases referencing other type aliases by @Viicos in #10809 Fix IncEx type alias to be compatible with mypy by @Viicos in #10813 Make __signature__ a lazy property, do not deepcopy defaults by @Viicos in #10818 Make __signature__ lazy for dataclasses, too by @sydney-runkle in #10832 Subclass all single host url classes from AnyUrl to preserve behavior from v2.9 by @sydney-runkle in #10856","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_19","title":"Changelog - v2.10.0 (2024-11-20) - What's Changed","objectID":"/latest/changelog/#whats-changed_19","rank":-135},{"content":"@jakob-keller made their first contribution in #10331 @MovisLi made their first contribution in #10374 @joaopalmeiro made their first contribution in #10405 @theunkn0wn1 made their first contribution in #10378 @cdwilson made their first contribution in #10412 @dlax made their first contribution in #10421 @kschwab made their first contribution in #10479 @santibreo made their first contribution in #10453 @FlorianSW made their first contribution in #10478 @tkasuz made their first contribution in #10555 @AlekseyLobanov made their first contribution in #10601 @NiclasvanEyk made their first contribution in #10667 @mschoettle made their first contribution in #10677 @Daraan made their first contribution in #10713 @k4nar made their first contribution in #10736 @UriyaHarpeness made their first contribution in #10740 @frfahim made their first contribution in #10727","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_8","title":"Changelog - v2.10.0 (2024-11-20) - New Contributors","objectID":"/latest/changelog/#new-contributors_8","rank":-140},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v2100b2-2024-11-13","title":"Changelog - v2.10.0b2 (2024-11-13)","objectID":"/latest/changelog/#v2100b2-2024-11-13","rank":-145},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v2100b1-2024-11-06","title":"Changelog - v2.10.0b1 (2024-11-06)","objectID":"/latest/changelog/#v2100b1-2024-11-06","rank":-150},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v292-2024-09-17","title":"Changelog - v2.9.2 (2024-09-17)","objectID":"/latest/changelog/#v292-2024-09-17","rank":-155},{"content":"Fixes ¶ Do not error when trying to evaluate annotations of private attributes by @Viicos in #10358 Adding notes on designing sound Callable discriminators by @sydney-runkle in #10400 Fix serialization schema generation when using PlainValidator by @Viicos in #10427 Fix Union serialization warnings by @sydney-runkle in pydantic/pydantic-core#1449 Fix variance issue in _IncEx type alias, only allow True by @Viicos in #10414 Fix ZoneInfo validation with various invalid types by @sydney-runkle in #10408","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_20","title":"Changelog - v2.9.2 (2024-09-17) - What's Changed","objectID":"/latest/changelog/#whats-changed_20","rank":-160},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v291-2024-09-09","title":"Changelog - v2.9.1 (2024-09-09)","objectID":"/latest/changelog/#v291-2024-09-09","rank":-165},{"content":"Fixes ¶ Fix Predicate issue in v2.9.0 by @sydney-runkle in #10321 Fixing annotated-types bound to >=0.6.0 by @sydney-runkle in #10327 Turn tzdata install requirement into optional timezone dependency by @jakob-keller in #10331 Fix IncExc type alias definition by @Viicos in #10339 Use correct types namespace when building namedtuple core schemas by @Viicos in #10337 Fix evaluation of stringified annotations during namespace inspection by @Viicos in #10347 Fix tagged union serialization with alias generators by @sydney-runkle in pydantic/pydantic-core#1442","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_21","title":"Changelog - v2.9.1 (2024-09-09) - What's Changed","objectID":"/latest/changelog/#whats-changed_21","rank":-170},{"content":"GitHub release The code released in v2.9.0 is practically identical to that of v2.9.0b2.","pageID":"Changelog","abs_url":"/latest/changelog/#v290-2024-09-05","title":"Changelog - v2.9.0 (2024-09-05)","objectID":"/latest/changelog/#v290-2024-09-05","rank":-175},{"content":"Packaging ¶ Bump ruff to v0.5.0 and pyright to v1.1.369 by @sydney-runkle in #9801 Bump pydantic-extra-types to v2.9.0 by @sydney-runkle in #9832 Support compatibility with pdm v2.18.1 by @Viicos in #10138 Bump v1 version stub to v1.10.18 by @sydney-runkle in #10214 Bump pydantic-core to v2.23.2 by @sydney-runkle in #10311 New Features ¶ Add support for ZoneInfo by @Youssefares in #9896 Add Config.val_json_bytes by @josh-newman in #9770 Add DSN for Snowflake by @aditkumar72 in #10128 Support complex number by @changhc in #9654 Add support for annotated_types.Not by @aditkumar72 in #10210 Allow WithJsonSchema to inject $ref s w/ http or https links by @dAIsySHEng1 in #9863 Allow validators to customize validation JSON schema by @Viicos in #10094 Support parametrized PathLike types by @nix010 in #9764 Add tagged union serializer that attempts to use str or callable discriminators to select the correct serializer by @sydney-runkle in in pydantic/pydantic-core#1397 Changes ¶ Breaking Change: Merge dict type json_schema_extra by @sydney-runkle in #9792 For more info (how to replicate old behavior) on this change, see here Refactor annotation injection for known (often generic) types by @sydney-runkle in #9979 Move annotation compatibility errors to validation phase by @sydney-runkle in #9999 Improve runtime errors for string constraints like pattern for incompatible types by @sydney-runkle in #10158 Remove 'allOf' JSON schema workarounds by @dpeachey in #10029 Remove typed_dict_cls data from CoreMetadata by @sydney-runkle in #10180 Deprecate passing a dict to the Examples class by @Viicos in #10181 Remove initial_metadata from internal metadata construct by @sydney-runkle in #10194 Use re.Pattern.search instead of re.Pattern.match for consistency with rust behavior by @tinez in pydantic/pydantic-core#1368 Show value of wrongly typed data in pydantic-core serialization warning by @BoxyUwU in pydantic/pydantic-core#1377 Breaking Change: in pydantic-core , change metadata type hint in core schemas from Any -> Dict[str, Any] | None by @sydney-runkle in pydantic/pydantic-core#1411 Raise helpful warning when self isn't returned from model validator by @sydney-runkle in #10255 Performance ¶ Initial start at improving import times for modules, using caching primarily by @sydney-runkle in #10009 Using cached internal import for BaseModel by @sydney-runkle in #10013 Simplify internal generics logic - remove generator overhead by @sydney-runkle in #10059 Remove default module globals from types namespace by @sydney-runkle in #10123 Performance boost: skip caching parent namespaces in most cases by @sydney-runkle in #10113 Update ns stack with already copied ns by @sydney-runkle in #10267 Minor Internal Improvements ¶ ⚡️ Speed up multiple_of_validator() by 31% in pydantic/_internal/_validators.py by @misrasaurabh1 in #9839 ⚡️ Speed up ModelPrivateAttr.__set_name__() by 18% in pydantic/fields.py by @misrasaurabh1 in #9841 ⚡️ Speed up dataclass() by 7% in pydantic/dataclasses.py by @misrasaurabh1 in #9843 ⚡️ Speed up function _field_name_for_signature by 37% in pydantic/_internal/_signature.py by @misrasaurabh1 in #9951 ⚡️ Speed up method GenerateSchema._unpack_refs_defs by 26% in pydantic/_internal/_generate_schema.py by @misrasaurabh1 in #9949 ⚡️ Speed up function apply_each_item_validators by 100% in pydantic/_internal/_generate_schema.py by @misrasaurabh1 in #9950 ⚡️ Speed up method ConfigWrapper.core_config by 28% in pydantic/_internal/_config.py by @misrasaurabh1 in #9953 Fixes ¶ Respect use_enum_values on Literal types by @kwint in #9787 Prevent type error for exotic BaseModel/RootModel inheritance by @dmontagu in #9913 Fix typing issue with field_validator-decorated methods by @dmontagu in #9914 Replace str type annotation with Any in validator factories in documentation on validators by @maximilianfellhuber in #9885 Fix ComputedFieldInfo.wrapped_property pointer when a property setter is assigned by @tlambert03 in #9892 Fix recursive typing of main.IncEnx by @tlambert03 in #9924 Allow usage of type[Annotated[...]] by @Viicos in #9932 mypy plugin: handle frozen fields on a per-field basis by @dmontagu in #9935 Fix typo in invalid-annotated-type error code by @sydney-runkle in #9948 Simplify schema generation for uuid , url , and ip types by @sydney-runkle in #9975 Move date schemas to _generate_schema.py by @sydney-runkle in #9976 Move decimal.Decimal validation to _generate_schema.py by @sydney-runkle in #9977 Simplify IP address schema in _std_types_schema.py by @sydney-runkle in #9959 Fix type annotations for some potentially generic GenerateSchema.match_type options by @sydney-runkle in #9961 Add class name to \"has conflict\" warnings by @msabramo in #9964 Fix dataclass ignoring default_factory passed in Annotated by @kc0506 in #9971 Fix Sequence ignoring discriminator by @kc0506 in #9980 Fix typing for IPvAnyAddress and IPvAnyInterface by @haoyun in #9990 Fix false positives on v1 models in mypy plugin for from_orm check requiring from_attributes=True config by @radekwlsk in #9938 Apply strict=True to __init__ in mypy plugin by @kc0506 in #9998 Refactor application of deque annotations by @sydney-runkle in #10018 Raise a better user error when failing to evaluate a forward reference by @Viicos in #10030 Fix evaluation of __pydantic_extra__ annotation in specific circumstances by @Viicos in #10070 Fix frozen enforcement for dataclasses by @sydney-runkle in #10066 Remove logic to handle unused __get_pydantic_core_schema__ signature by @Viicos in #10075 Use is_annotated consistently by @Viicos in #10095 Fix PydanticDeprecatedSince26 typo by @kc0506 in #10101 Improve pyright tests, refactor model decorators signatures by @Viicos in #10092 Fix ip serialization logic by @sydney-runkle in #10112 Warn when frozen defined twice for dataclasses by @mochi22 in #10082 Do not compute JSON Schema default when plain serializers are used with when_used set to 'json-unless-none' and the default value is None by @Viicos in #10121 Fix ImportString special cases by @sydney-runkle in #10137 Blacklist default globals to support exotic user code with __ prefixed annotations by @sydney-runkle in #10136 Handle nullable schemas with serialization schema available during JSON Schema generation by @Viicos in #10132 Reorganize BaseModel annotations by @kc0506 in #10110 Fix core schema simplification when serialization schemas are involved in specific scenarios by @Viicos in #10155 Add support for stringified annotations when using PrivateAttr with Annotated by @Viicos in #10157 Fix JSON Schema number type for literal and enum schemas by @Viicos in #10172 Fix JSON Schema generation of fields with plain validators in serialization mode by @Viicos in #10167 Fix invalid JSON Schemas being generated for functions in certain scenarios by @Viicos in #10188 Make sure generated JSON Schemas are valid in tests by @Viicos in #10182 Fix key error with custom serializer by @sydney-runkle in #10200 Add 'wss' for allowed schemes in NatsDsn by @swelborn in #10224 Fix Mapping and MutableMapping annotations to use mapping schema instead of dict schema by @sydney-runkle in #10020 Fix JSON Schema generation for constrained dates by @Viicos in #10185 Fix discriminated union bug regression when using enums by @kfreezen in pydantic/pydantic-core#1286 Fix field_serializer with computed field when using * by @nix010 in pydantic/pydantic-core#1349 Try each option in Union serializer before inference by @sydney-runkle in pydantic/pydantic-core#1398 Fix float serialization behavior in strict mode by @sydney-runkle in pydantic/pydantic-core#1400 Introduce exactness into Decimal validation logic to improve union validation behavior by @sydney-runkle in in pydantic/pydantic-core#1405 Fix new warnings assertions to use pytest.warns() by @mgorny in #10241 Fix a crash when cleaning the namespace in ModelMetaclass by @Viicos in #10242 Fix parent namespace issue with model rebuilds by @sydney-runkle in #10257 Remove defaults filter for namespace by @sydney-runkle in #10261 Use identity instead of equality after validating model in __init__ by @Viicos in #10264 Support BigInt serialization for int subclasses by @kxx317 in pydantic/pydantic-core#1417 Support signature for wrap validators without info by @sydney-runkle in #10277 Ensure __pydantic_complete__ is set when rebuilding dataclasses by @Viicos in #10291 Respect schema_generator config value in TypeAdapter by @sydney-runkle in #10300","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_22","title":"Changelog - v2.9.0 (2024-09-05) - What's Changed","objectID":"/latest/changelog/#whats-changed_22","rank":-180},{"content":"pydantic ¶ @kwint made their first contribution in #9787 @seekinginfiniteloop made their first contribution in #9822 @a-alexander made their first contribution in #9848 @maximilianfellhuber made their first contribution in #9885 @karmaBonfire made their first contribution in #9945 @s-rigaud made their first contribution in #9958 @msabramo made their first contribution in #9964 @DimaCybr made their first contribution in #9972 @kc0506 made their first contribution in #9971 @haoyun made their first contribution in #9990 @radekwlsk made their first contribution in #9938 @dpeachey made their first contribution in #10029 @BoxyUwU made their first contribution in #10085 @mochi22 made their first contribution in #10082 @aditkumar72 made their first contribution in #10128 @changhc made their first contribution in #9654 @insumanth made their first contribution in #10229 @AdolfoVillalobos made their first contribution in #10240 @bllchmbrs made their first contribution in #10270 pydantic-core ¶ @kfreezen made their first contribution in pydantic/pydantic-core#1286 @tinez made their first contribution in pydantic/pydantic-core#1368 @fft001 made their first contribution in pydantic/pydantic-core#1362 @nix010 made their first contribution in pydantic/pydantic-core#1349 @BoxyUwU made their first contribution in pydantic/pydantic-core#1379 @candleindark made their first contribution in pydantic/pydantic-core#1404 @changhc made their first contribution in pydantic/pydantic-core#1331","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_9","title":"Changelog - v2.9.0 (2024-09-05) - New Contributors","objectID":"/latest/changelog/#new-contributors_9","rank":-185},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v290b2-2024-08-30","title":"Changelog - v2.9.0b2 (2024-08-30)","objectID":"/latest/changelog/#v290b2-2024-08-30","rank":-190},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v290b1-2024-08-26","title":"Changelog - v2.9.0b1 (2024-08-26)","objectID":"/latest/changelog/#v290b1-2024-08-26","rank":-195},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v282-2024-07-03","title":"Changelog - v2.8.2 (2024-07-03)","objectID":"/latest/changelog/#v282-2024-07-03","rank":-200},{"content":"Fixes ¶ Fix issue with assertion caused by pluggable schema validator by @dmontagu in #9838","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_23","title":"Changelog - v2.8.2 (2024-07-03) - What's Changed","objectID":"/latest/changelog/#whats-changed_23","rank":-205},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v281-2024-07-03","title":"Changelog - v2.8.1 (2024-07-03)","objectID":"/latest/changelog/#v281-2024-07-03","rank":-210},{"content":"Packaging ¶ Bump ruff to v0.5.0 and pyright to v1.1.369 by @sydney-runkle in #9801 Bump pydantic-core to v2.20.1 , pydantic-extra-types to v2.9.0 by @sydney-runkle in #9832 Fixes ¶ Fix breaking change in to_snake from v2.7 -> v2.8 by @sydney-runkle in #9812 Fix list constraint json schema application by @sydney-runkle in #9818 Support time duration more than 23 by @nix010 in pydantic/speedate#64 Fix millisecond fraction being handled with the wrong scale by @davidhewitt in pydantic/speedate#65 Handle negative fractional durations correctly by @sydney-runkle in pydantic/speedate#71","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_24","title":"Changelog - v2.8.1 (2024-07-03) - What's Changed","objectID":"/latest/changelog/#whats-changed_24","rank":-215},{"content":"GitHub release The code released in v2.8.0 is functionally identical to that of v2.8.0b1.","pageID":"Changelog","abs_url":"/latest/changelog/#v280-2024-07-01","title":"Changelog - v2.8.0 (2024-07-01)","objectID":"/latest/changelog/#v280-2024-07-01","rank":-220},{"content":"Packaging ¶ Update citation version automatically with new releases by @sydney-runkle in #9673 Bump pyright to v1.1.367 and add type checking tests for pipeline API by @adriangb in #9674 Update pydantic.v1 stub to v1.10.17 by @sydney-runkle in #9707 General package updates to prep for v2.8.0b1 by @sydney-runkle in #9741 Bump pydantic-core to v2.20.0 by @sydney-runkle in #9745 Add support for Python 3.13 by @sydney-runkle in #9743 Update pdm version used for pdm.lock to v2.16.1 by @sydney-runkle in #9761 Update to ruff v0.4.8 by @Viicos in #9585 New Features ¶ Experimental: support defer_build for TypeAdapter by @MarkusSintonen in #8939 Implement deprecated field in json schema by @NeevCohen in #9298 Experimental: Add pipeline API by @adriangb in #9459 Add support for programmatic title generation by @NeevCohen in #9183 Implement fail_fast feature by @uriyyo in #9708 Add ser_json_inf_nan='strings' mode to produce valid JSON by @josh-newman in pydantic/pydantic-core#1307 Changes ¶ Add warning when \"alias\" is set in ignored Annotated field by @nix010 in #9170 Support serialization of some serializable defaults in JSON schema by @sydney-runkle in #9624 Relax type specification for __validators__ values in create_model by @sydney-runkle in #9697 Breaking Change: Improve smart union matching logic by @sydney-runkle in pydantic/pydantic-core#1322 You can read more about our smart union matching logic here . In some cases, if the old behavior\nis desired, you can switch to left-to-right mode and change the order of your Union members. Performance ¶ Internal Improvements ¶ ⚡️ Speed up _display_error_loc() by 25% in pydantic/v1/error_wrappers.py by @misrasaurabh1 in #9653 ⚡️ Speed up _get_all_json_refs() by 34% in pydantic/json_schema.py by @misrasaurabh1 in #9650 ⚡️ Speed up is_pydantic_dataclass() by 41% in pydantic/dataclasses.py by @misrasaurabh1 in #9652 ⚡️ Speed up to_snake() by 27% in pydantic/alias_generators.py by @misrasaurabh1 in #9747 ⚡️ Speed up unwrap_wrapped_function() by 93% in pydantic/_internal/_decorators.py by @misrasaurabh1 in #9727 Fixes ¶ Replace __spec__.parent with __package__ by @hramezani in #9331 Fix Outputted Model JSON Schema for Sequence type by @anesmemisevic in #9303 Fix typing of _frame_depth by @Viicos in #9353 Make ImportString json schema compatible by @amitschang in #9344 Hide private attributes ( PrivateAttr ) from __init__ signature in type checkers by @idan22moral in #9293 Make detection of TypeVar defaults robust to the CPython PEP-696 implementation by @AlexWaygood in #9426 Fix usage of PlainSerializer with builtin types by @Viicos in #9450 Add more robust custom validation examples by @ChrisPappalardo in #9468 Fix ignored strict specification for StringConstraint(strict=False) by @vbmendes in #9476 Breaking Change: Use PEP 570 syntax by @Viicos in #9479 Use Self where possible by @Viicos in #9479 Do not alter RootModel.model_construct signature in the mypy plugin by @Viicos in #9480 Fixed type hint of validation_context by @OhioDschungel6 in #9508 Support context being passed to TypeAdapter's dump_json / dump_python by @alexcouper in #9495 Updates type signature for Field() constructor by @bjmc in #9484 Improve builtin alias generators by @sydney-runkle in #9561 Fix typing of TypeAdapter by @Viicos in #9570 Add fallback default value for private fields in __setstate__ of BaseModel by @anhpham1509 in #9584 Support PEP 746 by @adriangb in #9587 Allow validator and serializer functions to have default values by @Viicos in #9478 Fix bug with mypy plugin's handling of covariant TypeVar fields by @dmontagu in #9606 Fix multiple annotation / constraint application logic by @sydney-runkle in #9623 Respect regex flags in validation and json schema by @sydney-runkle in #9591 Fix type hint on IpvAnyAddress by @sydney-runkle in #9640 Allow a field specifier on __pydantic_extra__ by @dmontagu in #9659 Use normalized case for file path comparison by @sydney-runkle in #9737 Modify constraint application logic to allow field constraints on Optional[Decimal] by @lazyhope in #9754 validate_call type params fix by @sydney-runkle in #9760 Check all warnings returned by pytest.warns() by @s-t-e-v-e-n-k in #9702 Reuse re.Pattern object in regex patterns to allow for regex flags by @sydney-runkle in pydantic/pydantic-core#1318","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_25","title":"Changelog - v2.8.0 (2024-07-01) - What's Changed","objectID":"/latest/changelog/#whats-changed_25","rank":-225},{"content":"@idan22moral made their first contribution in #9294 @anesmemisevic made their first contribution in #9303 @max-muoto made their first contribution in #9338 @amitschang made their first contribution in #9344 @paulmartin91 made their first contribution in #9410 @OhioDschungel6 made their first contribution in #9405 @AlexWaygood made their first contribution in #9426 @kinuax made their first contribution in #9433 @antoni-jamiolkowski made their first contribution in #9431 @candleindark made their first contribution in #9448 @nix010 made their first contribution in #9170 @tomy0000000 made their first contribution in #9457 @vbmendes made their first contribution in #9470 @micheleAlberto made their first contribution in #9471 @ChrisPappalardo made their first contribution in #9468 @blueTurtz made their first contribution in #9475 @WinterBlue16 made their first contribution in #9477 @bittner made their first contribution in #9500 @alexcouper made their first contribution in #9495 @bjmc made their first contribution in #9484 @pjvv made their first contribution in #9529 @nedbat made their first contribution in #9530 @gunnellEvan made their first contribution in #9469 @jaymbans made their first contribution in #9531 @MarcBresson made their first contribution in #9534 @anhpham1509 made their first contribution in #9584 @K-dash made their first contribution in #9595 @s-t-e-v-e-n-k made their first contribution in #9527 @airwoodix made their first contribution in #9506 @misrasaurabh1 made their first contribution in #9653 @AlessandroMiola made their first contribution in #9740 @mylapallilavanyaa made their first contribution in #9746 @lazyhope made their first contribution in #9754 @YassinNouh21 made their first contribution in #9759","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_10","title":"Changelog - v2.8.0 (2024-07-01) - New Contributors","objectID":"/latest/changelog/#new-contributors_10","rank":-230},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v280b1-2024-06-27","title":"Changelog - v2.8.0b1 (2024-06-27)","objectID":"/latest/changelog/#v280b1-2024-06-27","rank":-235},{"content":"Github release","pageID":"Changelog","abs_url":"/latest/changelog/#v274-2024-06-12","title":"Changelog - v2.7.4 (2024-06-12)","objectID":"/latest/changelog/#v274-2024-06-12","rank":-240},{"content":"Packaging ¶ Bump pydantic.v1 to v1.10.16 reference by @sydney-runkle in #9639 Fixes ¶ Specify recursive_guard as kwarg in FutureRef._evaluate by @vfazio in #9612","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_26","title":"Changelog - v2.7.4 (2024-06-12) - What's Changed","objectID":"/latest/changelog/#whats-changed_26","rank":-245},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v273-2024-06-03","title":"Changelog - v2.7.3 (2024-06-03)","objectID":"/latest/changelog/#v273-2024-06-03","rank":-250},{"content":"Packaging ¶ Bump pydantic-core to v2.18.4 by @sydney-runkle in #9550 Fixes ¶ Fix u style unicode strings in python @samuelcolvin in pydantic/jiter#110","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_27","title":"Changelog - v2.7.3 (2024-06-03) - What's Changed","objectID":"/latest/changelog/#whats-changed_27","rank":-255},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v272-2024-05-28","title":"Changelog - v2.7.2 (2024-05-28)","objectID":"/latest/changelog/#v272-2024-05-28","rank":-260},{"content":"Packaging ¶ Bump pydantic-core to v2.18.3 by @sydney-runkle in #9515 Fixes ¶ Replace __spec__.parent with __package__ by @hramezani in #9331 Fix validation of int s with leading unary minus by @RajatRajdeep in pydantic/pydantic-core#1291 Fix str subclass validation for enums by @sydney-runkle in pydantic/pydantic-core#1273 Support BigInt s in Literal s and Enum s by @samuelcolvin in pydantic/pydantic-core#1297 Fix: uuid - allow str subclass as input by @davidhewitt in pydantic/pydantic-core#1296","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_28","title":"Changelog - v2.7.2 (2024-05-28) - What's Changed","objectID":"/latest/changelog/#whats-changed_28","rank":-265},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v271-2024-04-23","title":"Changelog - v2.7.1 (2024-04-23)","objectID":"/latest/changelog/#v271-2024-04-23","rank":-270},{"content":"Packaging ¶ Bump pydantic-core to v2.18.2 by @sydney-runkle in #9307 New Features ¶ Ftp and Websocket connection strings support by @CherrySuryp in #9205 Changes ¶ Use field description for RootModel schema description when there is … by @LouisGobert in #9214 Fixes ¶ Fix validation_alias behavior with model_construct for AliasChoices and AliasPath by @sydney-runkle in #9223 Revert typing.Literal and import it outside the TYPE_CHECKING block by @frost-nzcr4 in #9232 Fix Secret serialization schema, applicable for unions by @sydney-runkle in #9240 Fix strict application to function-after with use_enum_values by @sydney-runkle in #9279 Address case where model_construct on a class which defines model_post_init fails with AttributeError by @babygrimes in #9168 Fix model_json_schema with config types by @NeevCohen in #9287 Support multiple zeros as an int by @samuelcolvin in pydantic/pydantic-core#1269 Fix validation of int s with leading unary plus by @cknv in pydantic/pydantic-core#1272 Fix interaction between extra != 'ignore' and from_attributes=True by @davidhewitt in pydantic/pydantic-core#1276 Handle error from Enum 's missing function as ValidationError by @sydney-runkle in pydantic/pydantic-core#1274 Fix memory leak with Iterable validation by @davidhewitt in pydantic/pydantic-core#1271","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_29","title":"Changelog - v2.7.1 (2024-04-23) - What's Changed","objectID":"/latest/changelog/#whats-changed_29","rank":-275},{"content":"@zzstoatzz made their first contribution in #9219 @frost-nzcr4 made their first contribution in #9232 @CherrySuryp made their first contribution in #9205 @vagenas made their first contribution in #9268 @ollz272 made their first contribution in #9262 @babygrimes made their first contribution in #9168 @swelborn made their first contribution in #9296 @kf-novi made their first contribution in #9236 @lgeiger made their first contribution in #9288","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_11","title":"Changelog - v2.7.1 (2024-04-23) - New Contributors","objectID":"/latest/changelog/#new-contributors_11","rank":-280},{"content":"GitHub release The code released in v2.7.0 is practically identical to that of v2.7.0b1.","pageID":"Changelog","abs_url":"/latest/changelog/#v270-2024-04-11","title":"Changelog - v2.7.0 (2024-04-11)","objectID":"/latest/changelog/#v270-2024-04-11","rank":-285},{"content":"Packaging ¶ Reorganize pyproject.toml sections by @Viicos in #8899 Bump pydantic-core to v2.18.1 by @sydney-runkle in #9211 Adopt jiter v0.2.0 by @samuelcolvin in pydantic/pydantic-core#1250 New Features ¶ Extract attribute docstrings from FieldInfo.description by @Viicos in #6563 Add a with_config decorator to comply with typing spec by @Viicos in #8611 Allow an optional separator splitting the value and unit of the result of ByteSize.human_readable by @jks15satoshi in #8706 Add generic Secret base type by @conradogarciaberrotaran in #8519 Make use of Sphinx inventories for cross references in docs by @Viicos in #8682 Add environment variable to disable plugins by @geospackle in #8767 Add support for deprecated fields by @Viicos in #8237 Allow field_serializer('*') by @ornariece in #9001 Handle a case when model_config is defined as a model property by @alexeyt101 in #9004 Update create_model() to support typing.Annotated as input by @wannieman98 in #8947 Add ClickhouseDsn support by @solidguy7 in #9062 Add support for re.Pattern[str] to pattern field by @jag-k in #9053 Support for serialize_as_any runtime setting by @sydney-runkle in #8830 Add support for typing.Self by @Youssefares in #9023 Ability to pass context to serialization by @ornariece in #8965 Add feedback widget to docs with flarelytics integration by @sydney-runkle in #9129 Support for parsing partial JSON strings in Python by @samuelcolvin in pydantic/jiter#66 Finalized in v2.7.0, rather than v2.7.0b1: Add support for field level number to str coercion option by @NeevCohen in #9137 Update warnings parameter for serialization utilities to allow raising a warning by @Lance-Drane in #9166 Changes ¶ Correct docs, logic for model_construct behavior with extra by @sydney-runkle in #8807 Improve error message for improper RootModel subclasses by @sydney-runkle in #8857 Breaking Change: Use PEP570 syntax by @Viicos in #8940 Add enum and type to the JSON schema for single item literals by @dmontagu in #8944 Deprecate update_json_schema internal function by @sydney-runkle in #9125 Serialize duration to hour minute second, instead of just seconds by @kakilangit in pydantic/speedate#50 Trimming str before parsing to int and float by @hungtsetse in pydantic/pydantic-core#1203 Performance ¶ enum validator improvements by @samuelcolvin in #9045 Move enum validation and serialization to Rust by @samuelcolvin in #9064 Improve schema generation for nested dataclasses by @sydney-runkle in #9114 Fast path for ASCII python string creation in JSON by @samuelcolvin in in pydantic/jiter#72 SIMD integer and string JSON parsing on aarch64 ( Note: SIMD on x86 will be implemented in a future release) by @samuelcolvin in in pydantic/jiter#65 Support JSON Cow<str> from jiter by @davidhewitt in pydantic/pydantic-core#1231 MAJOR performance improvement: update to PyO3 0.21 final by @davidhewitt in pydantic/pydantic-core#1248 cache Python strings by @samuelcolvin in pydantic/pydantic-core#1240 Fixes ¶ Fix strict parsing for some Sequence s by @sydney-runkle in #8614 Add a check on the existence of __qualname__ by @anci3ntr0ck in #8642 Handle __pydantic_extra__ annotation being a string or inherited by @alexmojaki in #8659 Fix json validation for NameEmail by @Holi0317 in #8650 Fix type-safety of attribute access in BaseModel by @bluenote10 in #8651 Fix bug with mypy plugin and no_strict_optional = True by @dmontagu in #8666 Fix ByteSize error type change by @sydney-runkle in #8681 Fix inheriting annotations in dataclasses by @sydney-runkle in #8679 Fix regression in core schema generation for indirect definition references by @dmontagu in #8702 Fix unsupported types bug with plain validator by @sydney-runkle in #8710 Reverting problematic fix from 2.6 release, fixing schema building bug by @sydney-runkle in #8718 fixes __pydantic_config__ ignored for TypeDict by @13sin in #8734 Fix test failures with pytest v8.0.0 due to pytest.warns() starting to work inside pytest.raises() by @mgorny in #8678 Use is_valid_field from 1.x for mypy plugin by @DanielNoord in #8738 Better-support mypy strict equality flag by @dmontagu in #8799 model_json_schema export with Annotated types misses 'required' parameters by @LouisGobert in #8793 Fix default inclusion in FieldInfo.__repr_args__ by @sydney-runkle in #8801 Fix resolution of forward refs in dataclass base classes that are not present in the subclass module namespace by @matsjoyce-refeyn in #8751 Fix BaseModel type annotations to be resolvable by typing.get_type_hints by @devmonkey22 in #7680 Fix: allow empty string aliases with AliasGenerator by @sydney-runkle in #8810 Fix test along with date -> datetime timezone assumption fix by @sydney-runkle in #8823 Fix deprecation warning with usage of ast.Str by @Viicos in #8837 Add missing deprecated decorators by @Viicos in #8877 Fix serialization of NameEmail if name includes an email address by @NeevCohen in #8860 Add information about class in error message of schema generation by @Czaki in #8917 Make TypeAdapter 's typing compatible with special forms by @adriangb in #8923 Fix issue with config behavior being baked into the ref schema for enum s by @dmontagu in #8920 More helpful error re wrong model_json_schema usage by @sydney-runkle in #8928 Fix nested discriminated union schema gen, pt 2 by @sydney-runkle in #8932 Fix schema build for nested dataclasses / TypedDicts with discriminators by @sydney-runkle in #8950 Remove unnecessary logic for definitions schema gen with discriminated unions by @sydney-runkle in #8951 Fix handling of optionals in mypy plugin by @dmontagu in #9008 Fix PlainSerializer usage with std type constructor by @sydney-runkle in #9031 Remove unnecessary warning for config in plugin by @dmontagu in #9039 Fix default value serializing by @NeevCohen in #9066 Fix extra fields check in Model.__getattr__() by @NeevCohen in #9082 Fix ClassVar forward ref inherited from parent class by @alexmojaki in #9097 fix sequence like validator with strict True by @andresliszt in #8977 Improve warning message when a field name shadows a field in a parent model by @chan-vince in #9105 Do not warn about shadowed fields if they are not redefined in a child class by @chan-vince in #9111 Fix discriminated union bug with unsubstituted type var by @sydney-runkle in #9124 Support serialization of deque when passed to Sequence[blah blah blah] by @sydney-runkle in #9128 Init private attributes from super-types in model_post_init by @Viicos in #9134 fix model_construct with validation_alias by @ornariece in #9144 Ensure json-schema generator handles Literal null types by @bruno-f-cruz in #9135 Fixed in v2.7.0 : Fix allow extra generic by @dmontagu in #9193","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_30","title":"Changelog - v2.7.0 (2024-04-11) - What's Changed","objectID":"/latest/changelog/#whats-changed_30","rank":-290},{"content":"@hungtsetse made their first contribution in #8546 @StrawHatDrag0n made their first contribution in #8583 @anci3ntr0ck made their first contribution in #8642 @Holi0317 made their first contribution in #8650 @bluenote10 made their first contribution in #8651 @ADSteele916 made their first contribution in #8703 @musicinmybrain made their first contribution in #8731 @jks15satoshi made their first contribution in #8706 @13sin made their first contribution in #8734 @DanielNoord made their first contribution in #8738 @conradogarciaberrotaran made their first contribution in #8519 @chris-griffin made their first contribution in #8775 @LouisGobert made their first contribution in #8793 @matsjoyce-refeyn made their first contribution in #8751 @devmonkey22 made their first contribution in #7680 @adamency made their first contribution in #8847 @MamfTheKramf made their first contribution in #8851 @ornariece made their first contribution in #9001 @alexeyt101 made their first contribution in #9004 @wannieman98 made their first contribution in #8947 @solidguy7 made their first contribution in #9062 @kloczek made their first contribution in #9047 @jag-k made their first contribution in #9053 @priya-gitTest made their first contribution in #9088 @Youssefares made their first contribution in #9023 @chan-vince made their first contribution in #9105 @bruno-f-cruz made their first contribution in #9135 @Lance-Drane made their first contribution in #9166","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_12","title":"Changelog - v2.7.0 (2024-04-11) - New Contributors","objectID":"/latest/changelog/#new-contributors_12","rank":-295},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v270b1-2024-04-03","title":"Changelog - v2.7.0b1 (2024-04-03)","objectID":"/latest/changelog/#v270b1-2024-04-03","rank":-300},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v264-2024-03-12","title":"Changelog - v2.6.4 (2024-03-12)","objectID":"/latest/changelog/#v264-2024-03-12","rank":-305},{"content":"Fixes ¶ Fix usage of AliasGenerator with computed_field decorator by @sydney-runkle in #8806 Fix nested discriminated union schema gen, pt 2 by @sydney-runkle in #8932 Fix bug with no_strict_optional=True caused by API deferral by @dmontagu in #8826","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_31","title":"Changelog - v2.6.4 (2024-03-12) - What's Changed","objectID":"/latest/changelog/#whats-changed_31","rank":-310},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v263-2024-02-27","title":"Changelog - v2.6.3 (2024-02-27)","objectID":"/latest/changelog/#v263-2024-02-27","rank":-315},{"content":"Packaging ¶ Update pydantic-settings version in the docs by @hramezani in #8906 Fixes ¶ Fix discriminated union schema gen bug by @sydney-runkle in #8904","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_32","title":"Changelog - v2.6.3 (2024-02-27) - What's Changed","objectID":"/latest/changelog/#whats-changed_32","rank":-320},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v262-2024-02-23","title":"Changelog - v2.6.2 (2024-02-23)","objectID":"/latest/changelog/#v262-2024-02-23","rank":-325},{"content":"Packaging ¶ Upgrade to pydantic-core 2.16.3 by @sydney-runkle in #8879 Fixes ¶ 'YYYY-MM-DD' date string coerced to datetime shouldn't infer timezone by @sydney-runkle in pydantic/pydantic-core#1193","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_33","title":"Changelog - v2.6.2 (2024-02-23) - What's Changed","objectID":"/latest/changelog/#whats-changed_33","rank":-330},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v261-2024-02-05","title":"Changelog - v2.6.1 (2024-02-05)","objectID":"/latest/changelog/#v261-2024-02-05","rank":-335},{"content":"Packaging ¶ Upgrade to pydantic-core 2.16.2 by @sydney-runkle in #8717 Fixes ¶ Fix bug with mypy plugin and no_strict_optional = True by @dmontagu in #8666 Fix ByteSize error type change by @sydney-runkle in #8681 Fix inheriting Field annotations in dataclasses by @sydney-runkle in #8679 Fix regression in core schema generation for indirect definition references by @dmontagu in #8702 Fix unsupported types bug with PlainValidator by @sydney-runkle in #8710 Reverting problematic fix from 2.6 release, fixing schema building bug by @sydney-runkle in #8718 Fix warning for tuple of wrong size in Union by @davidhewitt in pydantic/pydantic-core#1174 Fix computed_field JSON serializer exclude_none behavior by @sydney-runkle in pydantic/pydantic-core#1187","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_34","title":"Changelog - v2.6.1 (2024-02-05) - What's Changed","objectID":"/latest/changelog/#whats-changed_34","rank":-340},{"content":"GitHub release The code released in v2.6.0 is practically identical to that of v2.6.0b1.","pageID":"Changelog","abs_url":"/latest/changelog/#v260-2024-01-23","title":"Changelog - v2.6.0 (2024-01-23)","objectID":"/latest/changelog/#v260-2024-01-23","rank":-345},{"content":"Packaging ¶ Check for email-validator version >= 2.0 by @commonism in #6033 Upgrade `ruff`` target version to Python 3.8 by @Elkiwa in #8341 Update to pydantic-extra-types==2.4.1 by @yezz123 in #8478 Update to pyright==1.1.345 by @Viicos in #8453 Update pydantic-core from 2.14.6 to 2.16.1, significant changes from these updates are described below, full changelog here New Features ¶ Add NatsDsn by @ekeew in #6874 Add ConfigDict.ser_json_inf_nan by @davidhewitt in #8159 Add types.OnErrorOmit by @adriangb in #8222 Support AliasGenerator usage by @sydney-runkle in #8282 Add Pydantic People Page to docs by @sydney-runkle in #8345 Support yyyy-MM-DD datetime parsing by @sydney-runkle in #8404 Added bits conversions to the ByteSize class #8415 by @luca-matei in #8507 Enable json schema creation with type ByteSize by @geospackle in #8537 Add eval_type_backport to handle union operator and builtin generic subscripting in older Pythons by @alexmojaki in #8209 Add support for dataclass fields init by @dmontagu in #8552 Implement pickling for ValidationError by @davidhewitt in pydantic/pydantic-core#1119 Add unified tuple validator that can handle \"variadic\" tuples via PEP-646 by @dmontagu in pydantic/pydantic-core#865 Changes ¶ Drop Python3.7 support by @hramezani in #7188 Drop Python 3.7, and PyPy 3.7 and 3.8 by @davidhewitt in pydantic/pydantic-core#1129 Use positional-only self in BaseModel constructor, so no field name can ever conflict with it by @ariebovenberg in #8072 Make @validate_call return a function instead of a custom descriptor - fixes binding issue with inheritance and adds self/cls argument to validation errors by @alexmojaki in #8268 Exclude BaseModel docstring from JSON schema description by @sydney-runkle in #8352 Introducing classproperty decorator for model_computed_fields by @Jocelyn-Gas in #8437 Explicitly raise an error if field names clashes with types by @Viicos in #8243 Use stricter serializer for unions of simple types by @alexdrydew pydantic/pydantic-core#1132 Performance ¶ Add Codspeed profiling Actions workflow  by @lambertsbennett in #8054 Improve int extraction by @samuelcolvin in pydantic/pydantic-core#1155 Improve performance of recursion guard by @samuelcolvin in pydantic/pydantic-core#1156 dataclass serialization speedups by @samuelcolvin in pydantic/pydantic-core#1162 Avoid HashMap creation when looking up small JSON objects in LazyIndexMaps by @samuelcolvin in pydantic/jiter#55 use hashbrown to speedup python string caching by @davidhewitt in pydantic/jiter#51 Replace Peak with more efficient Peek by @davidhewitt in pydantic/jiter#48 Fixes ¶ Move getattr warning in deprecated BaseConfig by @tlambert03 in #7183 Only hash model_fields , not whole __dict__ by @alexmojaki in #7786 Fix mishandling of unions while freezing types in the mypy plugin by @dmontagu in #7411 Fix mypy error on untyped ClassVar by @vincent-hachin-wmx in #8138 Only compare pydantic fields in BaseModel.__eq__ instead of whole __dict__ by @QuentinSoubeyranAqemia in #7825 Update strict docstring in model_validate method. by @LukeTonin in #8223 Fix overload position of computed_field by @Viicos in #8227 Fix custom type type casting used in multiple attributes by @ianhfc in #8066 Fix issue not allowing validate_call decorator to be dynamically assigned to a class method by @jusexton in #8249 Fix issue unittest.mock deprecation warnings  by @ibleedicare in #8262 Added tests for the case JsonValue contains subclassed primitive values by @jusexton in #8286 Fix mypy error on free before validator (classmethod) by @sydney-runkle in #8285 Fix to_snake conversion by @jevins09 in #8316 Fix type annotation of ModelMetaclass.__prepare__ by @slanzmich in #8305 Disallow config specification when initializing a TypeAdapter when the annotated type has config already by @sydney-runkle in #8365 Fix a naming issue with JSON schema for generics parametrized by recursive type aliases by @dmontagu in #8389 Fix type annotation in pydantic people script by @shenxiangzhuang in #8402 Add support for field alias in dataclass signature by @NeevCohen in #8387 Fix bug with schema generation with Field(...) in a forward ref by @dmontagu in #8494 Fix ordering of keys in __dict__ with model_construct call by @sydney-runkle in #8500 Fix module path_type creation when globals does not contain __name__ by @hramezani in #8470 Fix for namespace issue with dataclasses with from __future__ import annotations by @sydney-runkle in #8513 Fix: make function validator types positional-only by @pmmmwh in #8479 Fix usage of @deprecated by @Viicos in #8294 Add more support for private attributes in model_construct call by @sydney-runkle in #8525 Use a stack for the types namespace by @dmontagu in #8378 Fix schema-building bug with TypeAliasType for types with refs by @dmontagu in #8526 Support pydantic.Field(repr=False) in dataclasses by @tigeryy2 in #8511 Override dataclass_transform behavior for RootModel by @Viicos in #8163 Refactor signature generation for simplicity by @sydney-runkle in #8572 Fix ordering bug of PlainValidator annotation by @Anvil in #8567 Fix exclude_none for json serialization of computed_field s by @sydney-runkle in pydantic/pydantic-core#1098 Support yyyy-MM-DD string for datetimes by @sydney-runkle in pydantic/pydantic-core#1124 Tweak ordering of definitions in generated schemas by @StrawHatDrag0n in #8583","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_35","title":"Changelog - v2.6.0 (2024-01-23) - What's Changed","objectID":"/latest/changelog/#whats-changed_35","rank":-350},{"content":"pydantic ¶ @ekeew made their first contribution in #6874 @lambertsbennett made their first contribution in #8054 @vincent-hachin-wmx made their first contribution in #8138 @QuentinSoubeyranAqemia made their first contribution in #7825 @ariebovenberg made their first contribution in #8072 @LukeTonin made their first contribution in #8223 @denisart made their first contribution in #8231 @ianhfc made their first contribution in #8066 @eonu made their first contribution in #8255 @amandahla made their first contribution in #8263 @ibleedicare made their first contribution in #8262 @jevins09 made their first contribution in #8316 @cuu508 made their first contribution in #8322 @slanzmich made their first contribution in #8305 @jensenbox made their first contribution in #8331 @szepeviktor made their first contribution in #8356 @Elkiwa made their first contribution in #8341 @parhamfh made their first contribution in #8395 @shenxiangzhuang made their first contribution in #8402 @NeevCohen made their first contribution in #8387 @zby made their first contribution in #8497 @patelnets made their first contribution in #8491 @edwardwli made their first contribution in #8503 @luca-matei made their first contribution in #8507 @Jocelyn-Gas made their first contribution in #8437 @bL34cHig0 made their first contribution in #8501 @tigeryy2 made their first contribution in #8511 @geospackle made their first contribution in #8537 @Anvil made their first contribution in #8567 @hungtsetse made their first contribution in #8546 @StrawHatDrag0n made their first contribution in #8583 pydantic-core ¶ @mariuswinger made their first contribution in pydantic/pydantic-core#1087 @adamchainz made their first contribution in pydantic/pydantic-core#1090 @akx made their first contribution in pydantic/pydantic-core#1123","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_13","title":"Changelog - v2.6.0 (2024-01-23) - New Contributors","objectID":"/latest/changelog/#new-contributors_13","rank":-355},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v260b1-2024-01-19","title":"Changelog - v2.6.0b1 (2024-01-19)","objectID":"/latest/changelog/#v260b1-2024-01-19","rank":-360},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v253-2023-12-22","title":"Changelog - v2.5.3 (2023-12-22)","objectID":"/latest/changelog/#v253-2023-12-22","rank":-365},{"content":"Packaging ¶ uprev pydantic-core to 2.14.6 Fixes ¶ Fix memory leak with recursive definitions creating reference cycles by @davidhewitt in pydantic/pydantic-core#1125","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_36","title":"Changelog - v2.5.3 (2023-12-22) - What's Changed","objectID":"/latest/changelog/#whats-changed_36","rank":-370},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v252-2023-11-22","title":"Changelog - v2.5.2 (2023-11-22)","objectID":"/latest/changelog/#v252-2023-11-22","rank":-375},{"content":"Packaging ¶ uprev pydantic-core to 2.14.5 New Features ¶ Add ConfigDict.ser_json_inf_nan by @davidhewitt in #8159 Fixes ¶ Fix validation of Literal from JSON keys when used as dict key by @sydney-runkle in pydantic/pydantic-core#1075 Fix bug re custom_init on members of Union by @sydney-runkle in pydantic/pydantic-core#1076 Fix JsonValue bool serialization by @sydney-runkle in #8190 Fix handling of unhashable inputs with Literal in Union s by @sydney-runkle in pydantic/pydantic-core#1089","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_37","title":"Changelog - v2.5.2 (2023-11-22) - What's Changed","objectID":"/latest/changelog/#whats-changed_37","rank":-380},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v251-2023-11-15","title":"Changelog - v2.5.1 (2023-11-15)","objectID":"/latest/changelog/#v251-2023-11-15","rank":-385},{"content":"Packaging ¶ uprev pydantic-core to 2.14.3 by @samuelcolvin in #8120 Fixes ¶ Fix package description limit by @dmontagu in #8097 Fix ValidateCallWrapper error when creating a model which has a @validate_call wrapped field annotation by @sydney-runkle in #8110","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_38","title":"Changelog - v2.5.1 (2023-11-15) - What's Changed","objectID":"/latest/changelog/#whats-changed_38","rank":-390},{"content":"GitHub release The code released in v2.5.0 is functionally identical to that of v2.5.0b1.","pageID":"Changelog","abs_url":"/latest/changelog/#v250-2023-11-13","title":"Changelog - v2.5.0 (2023-11-13)","objectID":"/latest/changelog/#v250-2023-11-13","rank":-395},{"content":"Packaging ¶ Update pydantic-core from 2.10.1 to 2.14.1, significant changes from these updates are described below, full changelog here Update to pyright==1.1.335 by @Viicos in #8075 New Features ¶ Allow plugins to catch non ValidationError errors by @adriangb in #7806 Support __doc__ argument in create_model() by @chris-spann in #7863 Expose regex_engine flag - meaning you can use with the Rust or Python regex libraries in constraints by @utkini in #7768 Save return type generated from type annotation in ComputedFieldInfo by @alexmojaki in #7889 Adopting ruff formatter by @Luca-Blight in #7930 Added validation_error_cause to config by @zakstucke in #7626 Make path of the item to validate available in plugin by @hramezani in #7861 Add CallableDiscriminator and Tag by @dmontagu in #7983 CallableDiscriminator renamed to Discriminator by @dmontagu in #8047 Make union case tags affect union error messages by @dmontagu in #8001 Add examples and json_schema_extra to @computed_field by @alexmojaki in #8013 Add JsonValue type by @dmontagu in #7998 Allow str as argument to Discriminator by @dmontagu in #8047 Add SchemaSerializer.__reduce__ method to enable pickle serialization by @edoakes in pydantic/pydantic-core#1006 Changes ¶ Significant Change: replace ultra_strict with new smart union implementation, the way unions are validated has changed significantly to improve performance and correctness, we have worked hard to absolutely minimise the number of cases where behaviour has changed, see the PR for details - by @davidhewitt in pydantic/pydantic-core#867 Add support for instance method reassignment when extra='allow' by @sydney-runkle in #7683 Support JSON schema generation for Enum types with no cases by @sydney-runkle in #7927 Warn if a class inherits from Generic before BaseModel by @alexmojaki in #7891 Performance ¶ New custom JSON parser, jiter by @samuelcolvin in pydantic/pydantic-core#974 PGO build for MacOS M1 by @samuelcolvin in pydantic/pydantic-core#1063 Use __getattr__ for all package imports, improve import time by @samuelcolvin in #7947 Fixes ¶ Fix mypy issue with subclasses of RootModel by @sydney-runkle in #7677 Properly rebuild the FieldInfo when a forward ref gets evaluated by @dmontagu in #7698 Fix failure to load SecretStr from JSON (regression in v2.4) by @sydney-runkle in #7729 Fix defer_build behavior with TypeAdapter by @sydney-runkle in #7736 Improve compatibility with legacy mypy versions by @dmontagu in #7742 Fix: update TypeVar handling when default is not set by @pmmmwh in #7719 Support specification of strict on Enum type fields by @sydney-runkle in #7761 Wrap weakref.ref instead of subclassing to fix cloudpickle serialization by @edoakes in #7780 Keep values of private attributes set within model_post_init in subclasses by @alexmojaki in #7775 Add more specific type for non-callable json_schema_extra by @alexmojaki in #7803 Raise an error when deleting frozen (model) fields by @alexmojaki in #7800 Fix schema sorting bug with default values by @sydney-runkle in #7817 Use generated alias for aliases that are not specified otherwise by @alexmojaki in #7802 Support strict specification for UUID types by @sydney-runkle in #7865 JSON schema: fix extra parameter handling by @me-and in #7810 Fix: support pydantic.Field(kw_only=True) with inherited dataclasses by @PrettyWood in #7827 Support validate_call decorator for methods in classes with __slots__ by @sydney-runkle in #7883 Fix pydantic dataclass problem with dataclasses.field default by @hramezani in #7898 Fix schema generation for generics with union type bounds by @sydney-runkle in #7899 Fix version for importlib_metadata on python 3.7 by @sydney-runkle in #7904 Support | operator (Union) in PydanticRecursiveRef by @alexmojaki in #7892 Fix display_as_type for TypeAliasType in python 3.12 by @dmontagu in #7929 Add support for NotRequired generics in TypedDict by @sydney-runkle in #7932 Make generic TypeAliasType specifications produce different schema definitions by @alexdrydew in #7893 Added fix for signature of inherited dataclass by @howsunjow in #7925 Make the model name generation more robust in JSON schema by @joakimnordling in #7881 Fix plurals in validation error messages (in tests) by @Iipin in #7972 PrivateAttr is passed from Annotated default position by @tabassco in #8004 Don't decode bytes (which may not be UTF8) when displaying SecretBytes by @alexmojaki in #8012 Use classmethod instead of classmethod[Any, Any, Any] by @Mr-Pepe in #7979 Clearer error on invalid Plugin by @samuelcolvin in #8023 Correct pydantic dataclasses import by @samuelcolvin in #8027 Fix misbehavior for models referencing redefined type aliases by @dmontagu in #8050 Fix Optional field with validate_default only performing one field validation by @sydney-runkle in pydantic/pydantic-core#1002 Fix definition-ref bug with Dict keys by @sydney-runkle in pydantic/pydantic-core#1014 Fix bug allowing validation of bool types with coerce_numbers_to_str=True by @sydney-runkle in pydantic/pydantic-core#1017 Don't accept NaN in float and decimal constraints by @davidhewitt in pydantic/pydantic-core#1037 Add lax_str and lax_int support for enum values not inherited from str/int by @michaelhly in pydantic/pydantic-core#1015 Support subclasses in lists in Union of List types by @sydney-runkle in pydantic/pydantic-core#1039 Allow validation against max_digits and decimals to pass if normalized or non-normalized input is valid by @sydney-runkle in pydantic/pydantic-core#1049 Fix: proper pluralization in ValidationError messages by @Iipin in pydantic/pydantic-core#1050 Disallow the string '-' as datetime input by @davidhewitt in pydantic/speedate#52 & pydantic/pydantic-core#1060 Fix: NaN and Inf float serialization by @davidhewitt in pydantic/pydantic-core#1062 Restore manylinux-compatible PGO builds by @davidhewitt in pydantic/pydantic-core#1068","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_39","title":"Changelog - v2.5.0 (2023-11-13) - What's Changed","objectID":"/latest/changelog/#whats-changed_39","rank":-400},{"content":"pydantic ¶ @schneebuzz made their first contribution in #7699 @edoakes made their first contribution in #7780 @alexmojaki made their first contribution in #7775 @NickG123 made their first contribution in #7751 @gowthamgts made their first contribution in #7830 @jamesbraza made their first contribution in #7848 @laundmo made their first contribution in #7850 @rahmatnazali made their first contribution in #7870 @waterfountain1996 made their first contribution in #7878 @chris-spann made their first contribution in #7863 @me-and made their first contribution in #7810 @utkini made their first contribution in #7768 @bn-l made their first contribution in #7744 @alexdrydew made their first contribution in #7893 @Luca-Blight made their first contribution in #7930 @howsunjow made their first contribution in #7925 @joakimnordling made their first contribution in #7881 @icfly2 made their first contribution in #7976 @Yummy-Yums made their first contribution in #8003 @Iipin made their first contribution in #7972 @tabassco made their first contribution in #8004 @Mr-Pepe made their first contribution in #7979 @0x00cl made their first contribution in #8010 @barraponto made their first contribution in #8032 pydantic-core ¶ @sisp made their first contribution in pydantic/pydantic-core#995 @michaelhly made their first contribution in pydantic/pydantic-core#1015","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_14","title":"Changelog - v2.5.0 (2023-11-13) - New Contributors","objectID":"/latest/changelog/#new-contributors_14","rank":-405},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v250b1-2023-11-09","title":"Changelog - v2.5.0b1 (2023-11-09)","objectID":"/latest/changelog/#v250b1-2023-11-09","rank":-410},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v242-2023-09-27","title":"Changelog - v2.4.2 (2023-09-27)","objectID":"/latest/changelog/#v242-2023-09-27","rank":-415},{"content":"Fixes ¶ Fix bug with JSON schema for sequence of discriminated union by @dmontagu in #7647 Fix schema references in discriminated unions by @adriangb in #7646 Fix json schema generation for recursive models by @adriangb in #7653 Fix models_json_schema for generic models by @adriangb in #7654 Fix xfailed test for generic model signatures by @adriangb in #7658","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_40","title":"Changelog - v2.4.2 (2023-09-27) - What's Changed","objectID":"/latest/changelog/#whats-changed_40","rank":-420},{"content":"@austinorr made their first contribution in #7657 @peterHoburg made their first contribution in #7670","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_15","title":"Changelog - v2.4.2 (2023-09-27) - New Contributors","objectID":"/latest/changelog/#new-contributors_15","rank":-425},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v241-2023-09-26","title":"Changelog - v2.4.1 (2023-09-26)","objectID":"/latest/changelog/#v241-2023-09-26","rank":-430},{"content":"Packaging ¶ Update pydantic-core to 2.10.1 by @davidhewitt in #7633 Fixes ¶ Serialize unsubstituted type vars as Any by @adriangb in #7606 Remove schema building caches by @adriangb in #7624 Fix an issue where JSON schema extras weren't JSON encoded by @dmontagu in #7625","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_41","title":"Changelog - v2.4.1 (2023-09-26) - What's Changed","objectID":"/latest/changelog/#whats-changed_41","rank":-435},{"content":"GitHub release","pageID":"Changelog","abs_url":"/latest/changelog/#v240-2023-09-22","title":"Changelog - v2.4.0 (2023-09-22)","objectID":"/latest/changelog/#v240-2023-09-22","rank":-440},{"content":"Packaging ¶ Update pydantic-core to 2.10.0 by @samuelcolvin in #7542 New Features ¶ Add Base64Url types by @dmontagu in #7286 Implement optional number to str coercion by @lig in #7508 Allow access to field_name and data in all validators if there is data and a field name by @samuelcolvin in #7542 Add BaseModel.model_validate_strings and TypeAdapter.validate_strings by @hramezani in #7552 Add Pydantic plugins experimental implementation by @lig @samuelcolvin and @Kludex in #6820 Changes ¶ Do not override model_post_init in subclass with private attrs by @Viicos in #7302 Make fields with defaults not required in the serialization schema by default by @dmontagu in #7275 Mark Extra as deprecated by @disrupted in #7299 Make EncodedStr a dataclass by @Kludex in #7396 Move annotated_handlers to be public by @samuelcolvin in #7569 Performance ¶ Simplify flattening and inlining of CoreSchema by @adriangb in #7523 Remove unused copies in CoreSchema walking by @adriangb in #7528 Add caches for collecting definitions and invalid schemas from a CoreSchema by @adriangb in #7527 Eagerly resolve discriminated unions and cache cases where we can't by @adriangb in #7529 Replace dict.get and dict.setdefault with more verbose versions in CoreSchema building hot paths by @adriangb in #7536 Cache invalid CoreSchema discovery by @adriangb in #7535 Allow disabling CoreSchema validation for faster startup times by @adriangb in #7565 Fixes ¶ Fix config detection for TypedDict from grandparent classes by @dmontagu in #7272 Fix hash function generation for frozen models with unusual MRO by @dmontagu in #7274 Make strict config overridable in field for Path by @hramezani in #7281 Use ser_json_<timedelta|bytes> on default in GenerateJsonSchema by @Kludex in #7269 Adding a check that alias is validated as an identifier for Python by @andree0 in #7319 Raise an error when computed field overrides field by @sydney-runkle in #7346 Fix applying SkipValidation to referenced schemas by @adriangb in #7381 Enforce behavior of private attributes having double leading underscore by @lig in #7265 Standardize __get_pydantic_core_schema__ signature by @hramezani in #7415 Fix generic dataclass fields mutation bug (when using TypeAdapter ) by @sydney-runkle in #7435 Fix TypeError on model_validator in wrap mode by @pmmmwh in #7496 Improve enum error message by @hramezani in #7506 Make repr work for instances that failed initialization when handling ValidationError s by @dmontagu in #7439 Fixed a regular expression denial of service issue by limiting whitespaces by @prodigysml in #7360 Fix handling of UUID values having UUID.version=None by @lig in #7566 Fix __iter__ returning private cached_property info by @sydney-runkle in #7570 Improvements to version info message by @samuelcolvin in #7594","pageID":"Changelog","abs_url":"/latest/changelog/#whats-changed_42","title":"Changelog - v2.4.0 (2023-09-22) - What's Changed","objectID":"/latest/changelog/#whats-changed_42","rank":-445},{"content":"@15498th made their first contribution in #7238 @GabrielCappelli made their first contribution in #7213 @tobni made their first contribution in #7184 @redruin1 made their first contribution in #7282 @FacerAin made their first contribution in #7288 @acdha made their first contribution in #7297 @andree0 made their first contribution in #7319 @gordonhart made their first contribution in #7375 @pmmmwh made their first contribution in #7496 @disrupted made their first contribution in #7299 @prodigysml made their first contribution in #7360","pageID":"Changelog","abs_url":"/latest/changelog/#new-contributors_16","title":"Changelog - v2.4.0 (2023-09-22) - New Contributors","objectID":"/latest/changelog/#new-contributors_16","rank":-450},{"content":"GitHub release 🔥 Remove orphaned changes file from repo by @lig in #7168 Add copy button on documentation by @Kludex in #7190 Fix docs on JSON type by @Kludex in #7189 Update mypy 1.5.0 to 1.5.1 in CI by @hramezani in #7191 fix download links badge by @samuelcolvin in #7200 add 2.2.1 to changelog by @samuelcolvin in #7212 Make ModelWrapValidator protocols generic by @dmontagu in #7154 Correct Field(..., exclude: bool) docs by @samuelcolvin in #7214 Make shadowing attributes a warning instead of an error by @adriangb in #7193 Document Base64Str and Base64Bytes by @Kludex in #7192 Fix config.defer_build for serialization first cases by @samuelcolvin in #7024 clean Model docstrings in JSON Schema by @samuelcolvin in #7210 fix #7228 (typo): docs in validators.md to correct validate_default kwarg by @lmmx in #7229 ✅ Implement tzinfo.fromutc method for TzInfo in pydantic-core by @lig in #7019 Support __get_validators__ by @hramezani in #7197","pageID":"Changelog","abs_url":"/latest/changelog/#v230-2023-08-23","title":"Changelog - v2.3.0 (2023-08-23)","objectID":"/latest/changelog/#v230-2023-08-23","rank":-455},{"content":"GitHub release Make xfail ing test for root model extra stop xfail ing by @dmontagu in #6937 Optimize recursion detection by stopping on the second visit for the same object by @mciucu in #7160 fix link in docs by @tlambert03 in #7166 Replace MiMalloc w/ default allocator by @adriangb in pydantic/pydantic-core#900 Bump pydantic-core to 2.6.1 and prepare 2.2.1 release by @adriangb in #7176","pageID":"Changelog","abs_url":"/latest/changelog/#v221-2023-08-18","title":"Changelog - v2.2.1 (2023-08-18)","objectID":"/latest/changelog/#v221-2023-08-18","rank":-460},{"content":"GitHub release Split \"pipx install\" setup command into two commands on the documentation site by @nomadmtb in #6869 Deprecate Field.include by @hramezani in #6852 Fix typo in default factory error msg by @hramezani in #6880 Simplify handling of typing.Annotated in GenerateSchema by @dmontagu in #6887 Re-enable fastapi tests in CI by @dmontagu in #6883 Make it harder to hit collisions with json schema defrefs by @dmontagu in #6566 Cleaner error for invalid input to Path fields by @samuelcolvin in #6903 support Coordinate Type by @yezz123 in #6906 Fix ForwardRef wrapper for py 3.10.0 (shim until bpo-45166) by @randomir in #6919 Fix misbehavior related to copying of RootModel by @dmontagu in #6918 Fix issue with recursion error caused by ParamSpec by @dmontagu in #6923 Add section about Constrained classes to the Migration Guide by @Kludex in #6924 Use main branch for badge links by @Viicos in #6925 Add test for v1/v2 Annotated discrepancy by @carlbordum in #6926 Make the v1 mypy plugin work with both v1 and v2 by @dmontagu in #6921 Fix issue where generic models couldn't be parametrized with BaseModel by @dmontagu in #6933 Remove xfail for discriminated union with alias by @dmontagu in #6938 add field_serializer to computed_field by @andresliszt in #6965 Use union_schema with Type[Union[...]] by @JeanArhancet in #6952 Fix inherited typeddict attributes / config by @adriangb in #6981 fix dataclass annotated before validator called twice by @davidhewitt in #6998 Update test-fastapi deselected tests by @hramezani in #7014 Fix validator doc format by @hramezani in #7015 Fix typo in docstring of model_json_schema by @AdamVinch-Federated in #7032 remove unused \"type ignores\" with pyright by @samuelcolvin in #7026 Add benchmark representing FastAPI startup time by @adriangb in #7030 Fix json_encoders for Enum subclasses by @adriangb in #7029 Update docstring of ser_json_bytes regarding base64 encoding by @Viicos in #7052 Allow @validate_call to work on async methods by @adriangb in #7046 Fix: mypy error with Settings and SettingsConfigDict by @JeanArhancet in #7002 Fix some typos (repeated words and it's/its) by @eumiro in #7063 Fix the typo in docstring by @harunyasar in #7062 Docs: Fix broken URL in the pydantic-settings package recommendation by @swetjen in #6995 Handle constraints being applied to schemas that don't accept it by @adriangb in #6951 Replace almost_equal_floats with math.isclose by @eumiro in #7082 bump pydantic-core to 2.5.0 by @davidhewitt in #7077 Add short_version and use it in links by @hramezani in #7115 📝 Add usage link to RootModel by @Kludex in #7113 Revert \"Fix default port for mongosrv DSNs (#6827)\" by @Kludex in #7116 Clarify validate_default and _Unset handling in usage docs and migration guide by @benbenbang in #6950 Tweak documentation of Field.exclude by @Viicos in #7086 Do not require validate_assignment to use Field.frozen by @Viicos in #7103 tweaks to _core_utils by @samuelcolvin in #7040 Make DefaultDict working with set by @hramezani in #7126 Don't always require typing.Generic as a base for partially parametrized models by @dmontagu in #7119 Fix issue with JSON schema incorrectly using parent class core schema by @dmontagu in #7020 Fix xfailed test related to TypedDict and alias_generator by @dmontagu in #6940 Improve error message for NameEmail by @dmontagu in #6939 Fix generic computed fields by @dmontagu in #6988 Reflect namedtuple default values during validation by @dmontagu in #7144 Update dependencies, fix pydantic-core usage, fix CI issues by @dmontagu in #7150 Add mypy 1.5.0 by @hramezani in #7118 Handle non-json native enum values by @adriangb in #7056 document round_trip in Json type documentation  by @jc-louis in #7137 Relax signature checks to better support builtins and C extension functions as validators by @adriangb in #7101 add union_mode='left_to_right' by @davidhewitt in #7151 Include an error message hint for inherited ordering by @yvalencia91 in #7124 Fix one docs link and resolve some warnings for two others by @dmontagu in #7153 Include Field extra keys name in warning by @hramezani in #7136","pageID":"Changelog","abs_url":"/latest/changelog/#v220-2023-08-17","title":"Changelog - v2.2.0 (2023-08-17)","objectID":"/latest/changelog/#v220-2023-08-17","rank":-465},{"content":"GitHub release Skip FieldInfo merging when unnecessary by @dmontagu in #6862","pageID":"Changelog","abs_url":"/latest/changelog/#v211-2023-07-25","title":"Changelog - v2.1.1 (2023-07-25)","objectID":"/latest/changelog/#v211-2023-07-25","rank":-470},{"content":"GitHub release Add StringConstraints for use as Annotated metadata by @adriangb in #6605 Try to fix intermittently failing CI by @adriangb in #6683 Remove redundant example of optional vs default. by @ehiggs-deliverect in #6676 Docs update by @samuelcolvin in #6692 Remove the Validate always section in validator docs by @adriangb in #6679 Fix recursion error in json schema generation by @adriangb in #6720 Fix incorrect subclass check for secretstr by @AlexVndnblcke in #6730 update pdm / pdm lockfile to 2.8.0 by @davidhewitt in #6714 unpin pdm on more CI jobs by @davidhewitt in #6755 improve source locations for auxiliary packages in docs by @davidhewitt in #6749 Assume builtins don't accept an info argument by @adriangb in #6754 Fix bug where calling help(BaseModelSubclass) raises errors by @hramezani in #6758 Fix mypy plugin handling of @model_validator(mode=\"after\") by @ljodal in #6753 update pydantic-core to 2.3.1 by @davidhewitt in #6756 Mypy plugin for settings by @hramezani in #6760 Use contentSchema keyword for JSON schema by @dmontagu in #6715 fast-path checking finite decimals by @davidhewitt in #6769 Docs update by @samuelcolvin in #6771 Improve json schema doc by @hramezani in #6772 Update validator docs by @adriangb in #6695 Fix typehint for wrap validator by @dmontagu in #6788 🐛 Fix validation warning for unions of Literal and other type by @lig in #6628 Update documentation for generics support in V2 by @tpdorsey in #6685 add pydantic-core build info to version_info() by @samuelcolvin in #6785 Fix pydantic dataclasses that use slots with default values by @dmontagu in #6796 Fix inheritance of hash function for frozen models by @dmontagu in #6789 ✨ Add SkipJsonSchema annotation by @Kludex in #6653 Error if an invalid field name is used with Field by @dmontagu in #6797 Add GenericModel to MOVED_IN_V2 by @adriangb in #6776 Remove unused code from docs/usage/types/custom.md by @hramezani in #6803 Fix float -> Decimal coercion precision loss by @adriangb in #6810 remove email validation from the north star benchmark by @davidhewitt in #6816 Fix link to mypy by @progsmile in #6824 Improve initialization hooks example by @hramezani in #6822 Fix default port for mongosrv DSNs by @dmontagu in #6827 Improve API documentation, in particular more links between usage and API docs by @samuelcolvin in #6780 update pydantic-core to 2.4.0 by @davidhewitt in #6831 Fix annotated_types.MaxLen validator for custom sequence types by @ImogenBits in #6809 Update V1 by @hramezani in #6833 Make it so callable JSON schema extra works by @dmontagu in #6798 Fix serialization issue with InstanceOf by @dmontagu in #6829 Add back support for json_encoders by @adriangb in #6811 Update field annotations when building the schema by @dmontagu in #6838 Use WeakValueDictionary to fix generic memory leak by @dmontagu in #6681 Add config.defer_build to optionally make model building lazy by @samuelcolvin in #6823 delegate UUID serialization to pydantic-core by @davidhewitt in #6850 Update json_encoders docs by @adriangb in #6848 Fix error message for staticmethod / classmethod order with validate_call by @dmontagu in #6686 Improve documentation for Config by @samuelcolvin in #6847 Update serialization doc to mention Field.exclude takes priority over call-time include/exclude by @hramezani in #6851 Allow customizing core schema generation by making GenerateSchema public by @adriangb in #6737","pageID":"Changelog","abs_url":"/latest/changelog/#v210-2023-07-25","title":"Changelog - v2.1.0 (2023-07-25)","objectID":"/latest/changelog/#v210-2023-07-25","rank":-475},{"content":"GitHub release Mention PyObject (v1) moving to ImportString (v2) in migration doc by @slafs in #6456 Fix release-tweet CI by @Kludex in #6461 Revise the section on required / optional / nullable fields. by @ybressler in #6468 Warn if a type hint is not in fact a type by @adriangb in #6479 Replace TransformSchema with GetPydanticSchema by @dmontagu in #6484 Fix the un-hashability of various annotation types, for use in caching generic containers by @dmontagu in #6480 PYD-164: Rework custom types docs by @adriangb in #6490 Fix ci by @adriangb in #6507 Fix forward ref in generic by @adriangb in #6511 Fix generation of serialization JSON schemas for core_schema.ChainSchema by @dmontagu in #6515 Document the change in Field.alias behavior in Pydantic V2 by @hramezani in #6508 Give better error message attempting to compute the json schema of a model with undefined fields by @dmontagu in #6519 Document alias_priority by @tpdorsey in #6520 Add redirect for types documentation by @tpdorsey in #6513 Allow updating docs without release by @samuelcolvin in #6551 Ensure docs tests always run in the right folder by @dmontagu in #6487 Defer evaluation of return type hints for serializer functions by @dmontagu in #6516 Disable E501 from Ruff and rely on just Black by @adriangb in #6552 Update JSON Schema documentation for V2 by @tpdorsey in #6492 Add documentation of cyclic reference handling by @dmontagu in #6493 Remove the need for change files by @samuelcolvin in #6556 add \"north star\" benchmark by @davidhewitt in #6547 Update Dataclasses docs by @tpdorsey in #6470 ♻️ Use different error message on v1 redirects by @Kludex in #6595 ⬆ Upgrade pydantic-core to v2.2.0 by @lig in #6589 Fix serialization for IPvAny by @dmontagu in #6572 Improve CI by using PDM instead of pip to install typing-extensions by @adriangb in #6602 Add enum error type docs  by @lig in #6603 🐛 Fix max_length for unicode strings by @lig in #6559 Add documentation for accessing features via pydantic.v1 by @tpdorsey in #6604 Include extra when iterating over a model by @adriangb in #6562 Fix typing of model_validator by @adriangb in #6514 Touch up Decimal validator by @adriangb in #6327 Fix various docstrings using fixed pytest-examples by @dmontagu in #6607 Handle function validators in a discriminated union by @dmontagu in #6570 Review json_schema.md by @tpdorsey in #6608 Make validate_call work on basemodel methods by @dmontagu in #6569 add test for big int json serde by @davidhewitt in #6614 Fix pydantic dataclass problem with dataclasses.field default_factory by @hramezani in #6616 Fixed mypy type inference for TypeAdapter by @zakstucke in #6617 Make it work to use None as a generic parameter by @dmontagu in #6609 Make it work to use $ref as an alias by @dmontagu in #6568 add note to migration guide about changes to AnyUrl etc by @davidhewitt in #6618 🐛 Support defining json_schema_extra on RootModel using Field by @lig in #6622 Update pre-commit to prevent commits to main branch on accident by @dmontagu in #6636 Fix PDM CI for python 3.7 on MacOS/windows by @dmontagu in #6627 Produce more accurate signatures for pydantic dataclasses by @dmontagu in #6633 Updates to Url types for Pydantic V2 by @tpdorsey in #6638 Fix list markdown in transform docstring by @StefanBRas in #6649 simplify slots_dataclass construction to appease mypy by @davidhewitt in #6639 Update TypedDict schema generation docstring by @adriangb in #6651 Detect and lint-error for prints by @dmontagu in #6655 Add xfailing test for pydantic-core PR 766 by @dmontagu in #6641 Ignore unrecognized fields from dataclasses metadata by @dmontagu in #6634 Make non-existent class getattr a mypy error by @dmontagu in #6658 Update pydantic-core to 2.3.0 by @hramezani in #6648 Use OrderedDict from typing_extensions by @dmontagu in #6664 Fix typehint for JSON schema extra callable by @dmontagu in #6659","pageID":"Changelog","abs_url":"/latest/changelog/#v203-2023-07-05","title":"Changelog - v2.0.3 (2023-07-05)","objectID":"/latest/changelog/#v203-2023-07-05","rank":-480},{"content":"GitHub release Fix bug where round-trip pickling/unpickling a RootModel would change the value of __dict__ , #6457 by @dmontagu Allow single-item discriminated unions, #6405 by @dmontagu Fix issue with union parsing of enums, #6440 by @dmontagu Docs: Fixed constr documentation, renamed old regex to new pattern , #6452 by @miili Change GenerateJsonSchema.generate_definitions signature, #6436 by @dmontagu See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v202-2023-07-05","title":"Changelog - v2.0.2 (2023-07-05)","objectID":"/latest/changelog/#v202-2023-07-05","rank":-485},{"content":"GitHub release First patch release of Pydantic V2 Extra fields added via setattr (i.e. m.some_extra_field = 'extra_value' )\n  are added to .model_extra if model_config extra='allowed' . Fixed #6333 , #6365 by @aaraney Automatically unpack JSON schema '$ref' for custom types, #6343 by @adriangb Fix tagged unions multiple processing in submodels, #6340 by @suharnikov See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v201-2023-07-04","title":"Changelog - v2.0.1 (2023-07-04)","objectID":"/latest/changelog/#v201-2023-07-04","rank":-490},{"content":"GitHub release Pydantic V2 is here! See this post for more details.","pageID":"Changelog","abs_url":"/latest/changelog/#v20-2023-06-30","title":"Changelog - v2.0 (2023-06-30)","objectID":"/latest/changelog/#v20-2023-06-30","rank":-495},{"content":"Third beta pre-release of Pydantic V2 See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v20b3-2023-06-16","title":"Changelog - v2.0b3 (2023-06-16)","objectID":"/latest/changelog/#v20b3-2023-06-16","rank":-500},{"content":"Add from_attributes runtime flag to TypeAdapter.validate_python and BaseModel.model_validate . See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v20b2-2023-06-03","title":"Changelog - v2.0b2 (2023-06-03)","objectID":"/latest/changelog/#v20b2-2023-06-03","rank":-505},{"content":"First beta pre-release of Pydantic V2 See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v20b1-2023-06-01","title":"Changelog - v2.0b1 (2023-06-01)","objectID":"/latest/changelog/#v20b1-2023-06-01","rank":-510},{"content":"Fourth pre-release of Pydantic V2 See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v20a4-2023-05-05","title":"Changelog - v2.0a4 (2023-05-05)","objectID":"/latest/changelog/#v20a4-2023-05-05","rank":-515},{"content":"Third pre-release of Pydantic V2 See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v20a3-2023-04-20","title":"Changelog - v2.0a3 (2023-04-20)","objectID":"/latest/changelog/#v20a3-2023-04-20","rank":-520},{"content":"Second pre-release of Pydantic V2 See the full changelog here","pageID":"Changelog","abs_url":"/latest/changelog/#v20a2-2023-04-12","title":"Changelog - v2.0a2 (2023-04-12)","objectID":"/latest/changelog/#v20a2-2023-04-12","rank":-525},{"content":"First pre-release of Pydantic V2! See this post for more details.","pageID":"Changelog","abs_url":"/latest/changelog/#v20a1-2023-04-03","title":"Changelog - v2.0a1 (2023-04-03)","objectID":"/latest/changelog/#v20a1-2023-04-03","rank":-530},{"content":"Fix compatibility with ForwardRef._evaluate and Python < 3.12.4 by @griels in https://github.com/pydantic/pydantic/pull/11232","pageID":"Changelog","abs_url":"/latest/changelog/#v11021-2025-01-10","title":"Changelog - v1.10.21 (2025-01-10)","objectID":"/latest/changelog/#v11021-2025-01-10","rank":-535},{"content":"This release provides proper support for Python 3.13, with (Cythonized) wheels published for this version.\nAs a consequence, Cython was updated from 0.29.x to 3.0.x . General maintenance of CI and build ecosystem by @Viicos in https://github.com/pydantic/pydantic/pull/10847 Update Cython to 3.0.x . Properly address Python 3.13 deprecation warnings. Migrate packaging to pyproject.toml , make use of PEP 517 build options. Use build instead of direct setup.py invocations. Update various Github Actions versions. Replace outdated stpmex link in documentation by @jaredenorris in https://github.com/pydantic/pydantic/pull/10997","pageID":"Changelog","abs_url":"/latest/changelog/#v11020-2025-01-07","title":"Changelog - v1.10.20 (2025-01-07)","objectID":"/latest/changelog/#v11020-2025-01-07","rank":-540},{"content":"Add warning when v2 model is nested in v1 model by @sydney-runkle in https://github.com/pydantic/pydantic/pull/10432 Fix deprecation warning in V1 isinstance check by @alicederyn in https://github.com/pydantic/pydantic/pull/10645","pageID":"Changelog","abs_url":"/latest/changelog/#v11019-2024-11-06","title":"Changelog - v1.10.19 (2024-11-06)","objectID":"/latest/changelog/#v11019-2024-11-06","rank":-545},{"content":"Eval type fix in V1 by @sydney-runkle in https://github.com/pydantic/pydantic/pull/9751 Add to_lower_camel to __all__ in utils.py by @sydney-runkle (direct commit) Fix mypy v1 plugin for mypy 1.11 release by @flaeppe in https://github.com/pydantic/pydantic/pull/10139 Fix discriminator key used when discriminator has alias and .schema(by_alias=False) by @exs-dwoodward in https://github.com/pydantic/pydantic/pull/10146","pageID":"Changelog","abs_url":"/latest/changelog/#v11018-2024-08-22","title":"Changelog - v1.10.18 (2024-08-22)","objectID":"/latest/changelog/#v11018-2024-08-22","rank":-550},{"content":"Advertise Python 3.12 for 1.10.x! Part Deux by @vfazio in https://github.com/pydantic/pydantic/pull/9644 Mirrored modules in v1 namespace to fix typing and object resolution in python>3.11 by @exs-dwoodward in https://github.com/pydantic/pydantic/pull/9660 setup: remove upper bound from python_requires by @vfazio in https://github.com/pydantic/pydantic/pull/9685","pageID":"Changelog","abs_url":"/latest/changelog/#v11017-2024-06-20","title":"Changelog - v1.10.17 (2024-06-20)","objectID":"/latest/changelog/#v11017-2024-06-20","rank":-555},{"content":"Specify recursive_guard as kwarg in FutureRef._evaluate by @vfazio in https://github.com/pydantic/pydantic/pull/9612 Fix mypy v1 plugin for upcoming mypy release by @ cdce8p in https://github.com/pydantic/pydantic/pull/9586 Import modules/objects directly from v1 namespace by @exs-dwoodward in https://github.com/pydantic/pydantic/pull/9162","pageID":"Changelog","abs_url":"/latest/changelog/#v11016-2024-06-11","title":"Changelog - v1.10.16 (2024-06-11)","objectID":"/latest/changelog/#v11016-2024-06-11","rank":-560},{"content":"Add pydantic.v1 namespace to Pydantic v1 by @exs-dmiketa in https://github.com/pydantic/pydantic/pull/9042 Relax version of typing-extensions for V1 by @SonOfLilit in https://github.com/pydantic/pydantic/pull/8819 patch fix for mypy by @sydney-runkle in https://github.com/pydantic/pydantic/pull/8765","pageID":"Changelog","abs_url":"/latest/changelog/#v11015-2024-04-03","title":"Changelog - v1.10.15 (2024-04-03)","objectID":"/latest/changelog/#v11015-2024-04-03","rank":-565},{"content":"Update install.md by @dmontagu in #7690 Fix ci to only deploy docs on release by @sydney-runkle in #7740 Ubuntu fixes for V1 by @sydney-runkle in #8540 and #8587 Fix cached_property handling in dataclasses when copied by @rdbisme in #8407","pageID":"Changelog","abs_url":"/latest/changelog/#v11014-2024-01-19","title":"Changelog - v1.10.14 (2024-01-19)","objectID":"/latest/changelog/#v11014-2024-01-19","rank":-570},{"content":"Fix: Add max length check to pydantic.validate_email , #7673 by @hramezani Docs: Fix pip commands to install v1, #6930 by @chbndrhnns","pageID":"Changelog","abs_url":"/latest/changelog/#v11013-2023-09-27","title":"Changelog - v1.10.13 (2023-09-27)","objectID":"/latest/changelog/#v11013-2023-09-27","rank":-575},{"content":"Fixes the maxlen property being dropped on deque validation. Happened only if the deque item has been typed. Changes the _validate_sequence_like func, #6581 by @maciekglowka","pageID":"Changelog","abs_url":"/latest/changelog/#v11012-2023-07-24","title":"Changelog - v1.10.12 (2023-07-24)","objectID":"/latest/changelog/#v11012-2023-07-24","rank":-580},{"content":"Importing create_model in tools.py through relative path instead of absolute path - so that it doesn't import V2 code when copied over to V2 branch, #6361 by @SharathHuddar","pageID":"Changelog","abs_url":"/latest/changelog/#v11011-2023-07-04","title":"Changelog - v1.10.11 (2023-07-04)","objectID":"/latest/changelog/#v11011-2023-07-04","rank":-585},{"content":"Add Pydantic Json field support to settings management, #6250 by @hramezani Fixed literal validator errors for unhashable values, #6188 by @markus1978 Fixed bug with generics receiving forward refs, #6130 by @mark-todd Update install method of FastAPI for internal tests in CI, #6117 by @Kludex","pageID":"Changelog","abs_url":"/latest/changelog/#v11010-2023-06-30","title":"Changelog - v1.10.10 (2023-06-30)","objectID":"/latest/changelog/#v11010-2023-06-30","rank":-590},{"content":"Fix trailing zeros not ignored in Decimal validation, #5968 by @hramezani Fix mypy plugin for v1.4.0, #5928 by @cdce8p Add future and past date hypothesis strategies, #5850 by @bschoenmaeckers Discourage usage of Cython 3 with Pydantic 1.x, #5845 by @lig","pageID":"Changelog","abs_url":"/latest/changelog/#v1109-2023-06-07","title":"Changelog - v1.10.9 (2023-06-07)","objectID":"/latest/changelog/#v1109-2023-06-07","rank":-595},{"content":"Fix a bug in Literal usage with typing-extension==4.6.0 , #5826 by @hramezani This solves the (closed) issue #3849 where aliased fields that use discriminated union fail to validate when the data contains the non-aliased field name, #5736 by @benwah Update email-validator dependency to >=2.0.0post2, #5627 by @adriangb update AnyClassMethod for changes in python/typeshed#9771 , #5505 by @ITProKyle","pageID":"Changelog","abs_url":"/latest/changelog/#v1108-2023-05-23","title":"Changelog - v1.10.8 (2023-05-23)","objectID":"/latest/changelog/#v1108-2023-05-23","rank":-600},{"content":"Fix creating schema from model using ConstrainedStr with regex as dict key, #5223 by @matejetz Address bug in mypy plugin caused by explicit_package_bases=True, #5191 by @dmontagu Add implicit defaults in the mypy plugin for Field with no default argument, #5190 by @dmontagu Fix schema generated for Enum values used as Literals in discriminated unions, #5188 by @javibookline Fix mypy failures caused by the pydantic mypy plugin when users define from_orm in their own classes, #5187 by @dmontagu Fix InitVar usage with pydantic dataclasses, mypy version 1.1.1 and the custom mypy plugin, #5162 by @cdce8p","pageID":"Changelog","abs_url":"/latest/changelog/#v1107-2023-03-22","title":"Changelog - v1.10.7 (2023-03-22)","objectID":"/latest/changelog/#v1107-2023-03-22","rank":-605},{"content":"Implement logic to support creating validators from non standard callables by using defaults to identify them and unwrapping functools.partial and functools.partialmethod when checking the signature, #5126 by @JensHeinrich Fix mypy plugin for v1.1.1, and fix dataclass_transform decorator for pydantic dataclasses, #5111 by @cdce8p Raise ValidationError , not ConfigError , when a discriminator value is unhashable, #4773 by @kurtmckee","pageID":"Changelog","abs_url":"/latest/changelog/#v1106-2023-03-08","title":"Changelog - v1.10.6 (2023-03-08)","objectID":"/latest/changelog/#v1106-2023-03-08","rank":-610},{"content":"Fix broken parametrized bases handling with GenericModel s with complex sets of models, #5052 by @MarkusSintonen Invalidate mypy cache if plugin config changes, #5007 by @cdce8p Fix RecursionError when deep-copying dataclass types wrapped by pydantic, #4949 by @mbillingr Fix X | Y union syntax breaking GenericModel , #4146 by @thenx Switch coverage badge to show coverage for this branch/release, #5060 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v1105-2023-02-15","title":"Changelog - v1.10.5 (2023-02-15)","objectID":"/latest/changelog/#v1105-2023-02-15","rank":-615},{"content":"Change dependency to typing-extensions>=4.2.0 , #4885 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v1104-2022-12-30","title":"Changelog - v1.10.4 (2022-12-30)","objectID":"/latest/changelog/#v1104-2022-12-30","rank":-620},{"content":"NOTE: v1.10.3 was \"yanked\" from PyPI due to #4885 which is fixed in v1.10.4 fix parsing of custom root models, #4883 by @gou177 fix: use dataclass proxy for frozen or empty dataclasses, #4878 by @PrettyWood Fix schema and schema_json on models where a model instance is a one of default values, #4781 by @Bobronium Add Jina AI to sponsors on docs index page, #4767 by @samuelcolvin fix: support assignment on DataclassProxy , #4695 by @PrettyWood Add postgresql+psycopg as allowed scheme for PostgreDsn to make it usable with SQLAlchemy 2, #4689 by @morian Allow dict schemas to have both patternProperties and additionalProperties , #4641 by @jparise Fixes error passing None for optional lists with unique_items , #4568 by @mfulgo Fix GenericModel with Callable param raising a TypeError , #4551 by @mfulgo Fix field regex with StrictStr type annotation, #4538 by @sisp Correct dataclass_transform keyword argument name from field_descriptors to field_specifiers , #4500 by @samuelcolvin fix: avoid multiple calls of __post_init__ when dataclasses are inherited, #4487 by @PrettyWood Reduce the size of binary wheels, #2276 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v1103-2022-12-29","title":"Changelog - v1.10.3 (2022-12-29)","objectID":"/latest/changelog/#v1103-2022-12-29","rank":-625},{"content":"Revert Change: Revert percent encoding of URL parts which was originally added in #4224 , #4470 by @samuelcolvin Prevent long (length > 4_300 ) strings/bytes as input to int fields, see python/cpython#95778 and CVE-2020-10735 , #1477 by @samuelcolvin fix: dataclass wrapper was not always called, #4477 by @PrettyWood Use tomllib on Python 3.11 when parsing mypy configuration, #4476 by @hauntsaninja Basic fix of GenericModel cache to detect order of arguments in Union models, #4474 by @sveinugu Fix mypy plugin when using bare types like list and dict as default_factory , #4457 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v1102-2022-09-05","title":"Changelog - v1.10.2 (2022-09-05)","objectID":"/latest/changelog/#v1102-2022-09-05","rank":-630},{"content":"Add __hash__ method to pydantic.color.Color class, #4454 by @czaki","pageID":"Changelog","abs_url":"/latest/changelog/#v1101-2022-08-31","title":"Changelog - v1.10.1 (2022-08-31)","objectID":"/latest/changelog/#v1101-2022-08-31","rank":-635},{"content":"Refactor the whole pydantic dataclass decorator to really act like its standard lib equivalent.\n  It hence keeps __eq__ , __hash__ , ... and makes comparison with its non-validated version possible.\n  It also fixes usage of frozen dataclasses in fields and usage of default_factory in nested dataclasses.\n  The support of Config.extra has been added.\n  Finally, config customization directly via a dict is now possible, #2557 by @PrettyWood BREAKING CHANGES: The compiled boolean (whether pydantic is compiled with cython) has been moved from main.py to version.py Now that Config.extra is supported, dataclass ignores by default extra arguments (like BaseModel ) Fix PEP487 __set_name__ protocol in BaseModel for PrivateAttrs, #4407 by @tlambert03 Allow for custom parsing of environment variables via parse_env_var in Config , #4406 by @acmiyaguchi Rename master to main , #4405 by @hramezani Fix StrictStr does not raise ValidationError when max_length is present in Field , #4388 by @hramezani Make SecretStr and SecretBytes hashable, #4387 by @chbndrhnns Fix StrictBytes does not raise ValidationError when max_length is present in Field , #4380 by @JeanArhancet Add support for bare type , #4375 by @hramezani Support Python 3.11, including binaries for 3.11 in PyPI, #4374 by @samuelcolvin Add support for re.Pattern , #4366 by @hramezani Fix __post_init_post_parse__ is incorrectly passed keyword arguments when no __post_init__ is defined, #4361 by @hramezani Fix implicitly importing ForwardRef and Callable from pydantic.typing instead of typing and also expose MappingIntStrAny , #4358 by @aminalaee remove Any types from the dataclass decorator so it can be used with the disallow_any_expr mypy option, #4356 by @DetachHead moved repo to pydantic/pydantic , #4348 by @yezz123 fix \"extra fields not permitted\" error when dataclass with Extra.forbid is validated multiple times, #4343 by @detachhead Add Python 3.9 and 3.10 examples to docs, #4339 by @Bobronium Discriminated union models now use oneOf instead of anyOf when generating OpenAPI schema definitions, #4335 by @MaxwellPayne Allow type checkers to infer inner type of Json type. Json[list[str]] will be now inferred as list[str] , Json[Any] should be used instead of plain Json .\n  Runtime behaviour is not changed, #4332 by @Bobronium Allow empty string aliases by using a alias is not None check, rather than bool(alias) , #4253 by @sergeytsaplin Update ForwardRef s in Field.outer_type_ , #4249 by @JacobHayes The use of __dataclass_transform__ has been replaced by typing_extensions.dataclass_transform , which is the preferred way to mark pydantic models as a dataclass under PEP 681 , #4241 by @multimeric Use parent model's Config when validating nested NamedTuple fields, #4219 by @synek Update BaseModel.construct to work with aliased Fields, #4192 by @kylebamos Catch certain raised errors in smart_deepcopy and revert to deepcopy if so, #4184 by @coneybeare Add Config.anystr_upper and to_upper kwarg to constr and conbytes, #4165 by @satheler Fix JSON schema for set and frozenset when they include default values, #4155 by @aminalaee Teach the mypy plugin that methods decorated by @validator are classmethods, #4102 by @DMRobertson Improve mypy plugin's ability to detect required fields, #4086 by @richardxia Support fields of type Type[] in schema, #4051 by @aminalaee Add default value in JSON Schema when const=True , #4031 by @aminalaee Adds reserved word check to signature generation logic, #4011 by @strue36 Fix Json strategy failure for the complex nested field, #4005 by @sergiosim Add JSON-compatible float constraint allow_inf_nan , #3994 by @tiangolo Remove undefined behaviour when env_prefix had characters in common with env_nested_delimiter , #3975 by @arsenron Support generics model with create_model , #3945 by @hot123s allow submodels to overwrite extra field info, #3934 by @PrettyWood Document and test structural pattern matching ( PEP 636 ) on BaseModel , #3920 by @irgolic Fix incorrect deserialization of python timedelta object to ISO 8601 for negative time deltas.\n  Minus was serialized in incorrect place (\"P-1DT23H59M59.888735S\" instead of correct \"-P1DT23H59M59.888735S\"), #3899 by @07pepa Fix validation of discriminated union fields with an alias when passing a model instance, #3846 by @chornsby Add a CockroachDsn type to validate CockroachDB connection strings. The type\n  supports the following schemes: cockroachdb , cockroachdb+psycopg2 and cockroachdb+asyncpg , #3839 by @blubber Fix MyPy plugin to not override pre-existing __init__ method in models, #3824 by @patrick91 Fix mypy version checking, #3783 by @KotlinIsland support overwriting dunder attributes of BaseModel instances, #3777 by @PrettyWood Added ConstrainedDate and condate , #3740 by @hottwaj Support kw_only in dataclasses, #3670 by @detachhead Add comparison method for Color class, #3646 by @aminalaee Drop support for python3.6, associated cleanup, #3605 by @samuelcolvin created new function to_lower_camel() for \"non pascal case\" camel case, #3463 by @schlerp Add checks to default and default_factory arguments in Mypy plugin, #3430 by @klaa97 fix mangling of inspect.signature for BaseModel , #3413 by @fix-inspect-signature Adds the SecretField abstract class so that all the current and future secret fields like SecretStr and SecretBytes will derive from it, #3409 by @expobrain Support multi hosts validation in PostgresDsn , #3337 by @rglsk Fix parsing of very small numeric timedelta values, #3315 by @samuelcolvin Update SecretsSettingsSource to respect config.case_sensitive , #3273 by @JeanArhancet Add MongoDB network data source name (DSN) schema, #3229 by @snosratiershad Add support for multiple dotenv files, #3222 by @rekyungmin Raise an explicit ConfigError when multiple fields are incorrectly set for a single validator, #3215 by @SunsetOrange Allow ellipsis on Field s inside Annotated for TypedDicts required, #3133 by @ezegomez Catch overflow errors in int_validator , #3112 by @ojii Adds a __rich_repr__ method to Representation class which enables pretty printing with Rich , #3099 by @willmcgugan Add percent encoding in AnyUrl and descendent types, #3061 by @FaresAhmedb validate_arguments decorator now supports alias , #3019 by @MAD-py Avoid __dict__ and __weakref__ attributes in AnyUrl and IP address fields, #2890 by @nuno-andre Add ability to use Final in a field type annotation, #2766 by @uriyyo Update requirement to typing_extensions>=4.1.0 to guarantee dataclass_transform is available, #4424 by @commonism Add Explosion and AWS to main sponsors, #4413 by @samuelcolvin Update documentation for copy_on_model_validation to reflect recent changes, #4369 by @samuelcolvin Runtime warning if __slots__ is passed to create_model , __slots__ is then ignored, #4432 by @samuelcolvin Add type hints to BaseSettings.Config to avoid mypy errors, also correct mypy version compatibility notice in docs, #4450 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v1100-2022-08-30","title":"Changelog - v1.10.0 (2022-08-30)","objectID":"/latest/changelog/#v1100-2022-08-30","rank":-640},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v1100b1-2022-08-24","title":"Changelog - v1.10.0b1 (2022-08-24)","objectID":"/latest/changelog/#v1100b1-2022-08-24","rank":-645},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v1100a2-2022-08-24","title":"Changelog - v1.10.0a2 (2022-08-24)","objectID":"/latest/changelog/#v1100a2-2022-08-24","rank":-650},{"content":"Pre-release, see the GitHub release for details.","pageID":"Changelog","abs_url":"/latest/changelog/#v1100a1-2022-08-22","title":"Changelog - v1.10.0a1 (2022-08-22)","objectID":"/latest/changelog/#v1100a1-2022-08-22","rank":-655},{"content":"Revert Breaking Change : v1.9.1 introduced a breaking change where model fields were\ndeep copied by default, this release reverts the default behaviour to match v1.9.0 and before,\nwhile also allow deep-copy behaviour via copy_on_model_validation = 'deep' . See #4092 for more information. Allow for shallow copies of model fields, Config.copy_on_model_validation is now a str which must be 'none' , 'deep' , or 'shallow' corresponding to not copying, deep copy & shallow copy; default 'shallow' , #4093 by @timkpaine","pageID":"Changelog","abs_url":"/latest/changelog/#v192-2022-08-11","title":"Changelog - v1.9.2 (2022-08-11)","objectID":"/latest/changelog/#v192-2022-08-11","rank":-660},{"content":"Thank you to pydantic's sponsors: @tiangolo , @stellargraph , @JonasKs , @grillazz , @Mazyod , @kevinalh , @chdsbd , @povilasb , @povilasb , @jina-ai , @mainframeindustries , @robusta-dev , @SendCloud , @rszamszur , @jodal , @hardbyte , @corleyma , @daddycocoaman , @Rehket , @jokull , @reillysiemens , @westonsteimel , @primer-io , @koxudaxi , @browniebroke , @stradivari96 , @adriangb , @kamalgill , @jqueguiner , @dev-zero , @datarootsio , @RedCarpetUp for their kind support. Limit the size of generics._generic_types_cache and generics._assigned_parameters to avoid unlimited increase in memory usage, #4083 by @samuelcolvin Add Jupyverse and FPS as Jupyter projects using pydantic, #4082 by @davidbrochart Speedup __isinstancecheck__ on pydantic models when the type is not a model, may also avoid memory \"leaks\", #4081 by @samuelcolvin Fix in-place modification of FieldInfo that caused problems with PEP 593 type aliases, #4067 by @adriangb Add support for autocomplete in VS Code via __dataclass_transform__ when using pydantic.dataclasses.dataclass , #4006 by @giuliano-oliveira Remove benchmarks from codebase and docs, #3973 by @samuelcolvin Typing checking with pyright in CI, improve docs on vscode/pylance/pyright, #3972 by @samuelcolvin Fix nested Python dataclass schema regression, #3819 by @himbeles Update documentation about lazy evaluation of sources for Settings, #3806 by @garyd203 Prevent subclasses of bytes being converted to bytes, #3706 by @samuelcolvin Fixed \"error checking inheritance of\" when using PEP585 and PEP604 type hints, #3681 by @aleksul Allow self referencing ClassVar s in models, #3679 by @samuelcolvin Breaking Change, see #4106 : Fix issue with self-referencing dataclass, #3675 by @uriyyo Include non-standard port numbers in rendered URLs, #3652 by @dolfinus Config.copy_on_model_validation does a deep copy and not a shallow one, #3641 by @PrettyWood fix: clarify that discriminated unions do not support singletons, #3636 by @tommilligan Add read_text(encoding='utf-8') for setup.py , #3625 by @hswong3i Fix JSON Schema generation for Discriminated Unions within lists, #3608 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v191-2022-05-19","title":"Changelog - v1.9.1 (2022-05-19)","objectID":"/latest/changelog/#v191-2022-05-19","rank":-665},{"content":"Thank you to pydantic's sponsors: @sthagen , @timdrijvers , @toinbis , @koxudaxi , @ginomempin , @primer-io , @and-semakin , @westonsteimel , @reillysiemens , @es3n1n , @jokull , @JonasKs , @Rehket , @corleyma , @daddycocoaman , @hardbyte , @datarootsio , @jodal , @aminalaee , @rafsaf , @jqueguiner , @chdsbd , @kevinalh , @Mazyod , @grillazz , @JonasKs , @simw , @leynier , @xfenix for their kind support.","pageID":"Changelog","abs_url":"/latest/changelog/#v190-2021-12-31","title":"Changelog - v1.9.0 (2021-12-31)","objectID":"/latest/changelog/#v190-2021-12-31","rank":-670},{"content":"add Python 3.10 support, #2885 by @PrettyWood Discriminated unions , #619 by @PrettyWood Config.smart_union for better union logic , #2092 by @PrettyWood Binaries for Macos M1 CPUs, #3498 by @samuelcolvin Complex types can be set via nested environment variables , e.g. foo___bar , #3159 by @Air-Mark add a dark mode to pydantic documentation, #2913 by @gbdlin Add support for autocomplete in VS Code via __dataclass_transform__ , #2721 by @tiangolo Add \"exclude\" as a field parameter so that it can be configured using model config, #660 by @daviskirk","pageID":"Changelog","abs_url":"/latest/changelog/#highlights","title":"Changelog - v1.9.0 (2021-12-31) - Highlights","objectID":"/latest/changelog/#highlights","rank":-675},{"content":"Apply update_forward_refs to Config.json_encodes prevent name clashes in types defined via strings, #3583 by @samuelcolvin Extend pydantic's mypy plugin to support mypy versions 0.910 , 0.920 , 0.921 & 0.930 , #3573 & #3594 by @PrettyWood , @christianbundy , @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v190-2021-12-31-changes","title":"Changelog - v1.9.0 (2021-12-31) - v1.9.0 (2021-12-31) Changes","objectID":"/latest/changelog/#v190-2021-12-31-changes","rank":-680},{"content":"support generic models with discriminated union, #3551 by @PrettyWood keep old behaviour of json() by default, #3542 by @PrettyWood Removed typing-only __root__ attribute from BaseModel , #3540 by @layday Build Python 3.10 wheels, #3539 by @mbachry Fix display of extra fields with model __repr__ , #3234 by @cocolman models copied via Config.copy_on_model_validation always have all fields, #3201 by @PrettyWood nested ORM from nested dictionaries, #3182 by @PrettyWood fix link to discriminated union section by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#v190a2-2021-12-24-changes","title":"Changelog - v1.9.0 (2021-12-31) - v1.9.0a2 (2021-12-24) Changes","objectID":"/latest/changelog/#v190a2-2021-12-24-changes","rank":-685},{"content":"Add support for Decimal -specific validation configurations in Field() , additionally to using condecimal() ,\n  to allow better support from editors and tooling, #3507 by @tiangolo Add arm64 binaries suitable for MacOS with an M1 CPU to PyPI, #3498 by @samuelcolvin Fix issue where None was considered invalid when using a Union type containing Any or object , #3444 by @tharradine When generating field schema, pass optional field argument (of type pydantic.fields.ModelField ) to __modify_schema__() if present, #3434 by @jasujm Fix issue when pydantic fail to parse typing.ClassVar string type annotation, #3401 by @uriyyo Mention Python >= 3.9.2 as an alternative to typing_extensions.TypedDict , #3374 by @BvB93 Changed the validator method name in the Custom Errors example to more accurately describe what the validator is doing; changed from name_must_contain_space to value_must_equal_bar , #3327 by @michaelrios28 Add AmqpDsn class, #3254 by @kludex Always use Enum value as default in generated JSON schema, #3190 by @joaommartins Add support for Mypy 0.920, #3175 by @christianbundy validate_arguments now supports extra customization (used to always be Extra.forbid ), #3161 by @PrettyWood Complex types can be set by nested environment variables, #3159 by @Air-Mark Fix mypy plugin to collect fields based on pydantic.utils.is_valid_field so that it ignores untyped private variables, #3146 by @hi-ogawa fix validate_arguments issue with Config.validate_all , #3135 by @PrettyWood avoid dict coercion when using dict subclasses as field type, #3122 by @PrettyWood add support for object type, #3062 by @PrettyWood Updates pydantic dataclasses to keep _special properties on parent classes, #3043 by @zulrang Add a TypedDict class for error objects, #3038 by @matthewhughes934 Fix support for using a subclass of an annotation as a default, #3018 by @JacobHayes make create_model_from_typeddict mypy compliant, #3008 by @PrettyWood Make multiple inheritance work when using PrivateAttr , #2989 by @hmvp Parse environment variables as JSON, if they have a Union type with a complex subfield, #2936 by @cbartz Prevent StrictStr permitting Enum values where the enum inherits from str , #2929 by @samuelcolvin Make SecretsSettingsSource parse values being assigned to fields of complex types when sourced from a secrets file,\n  just as when sourced from environment variables, #2917 by @davidmreed add a dark mode to pydantic documentation, #2913 by @gbdlin Make pydantic-mypy plugin compatible with pyproject.toml configuration, consistent with mypy changes.\n  See the doc for more information, #2908 by @jrwalk add Python 3.10 support, #2885 by @PrettyWood Correctly parse generic models with Json[T] , #2860 by @geekingfrog Update contrib docs re: Python version to use for building docs, #2856 by @paxcodes Clarify documentation about pydantic 's support for custom validation and strict type checking,\n  despite pydantic being primarily a parsing library, #2855 by @paxcodes Fix schema generation for Deque fields, #2810 by @sergejkozin fix an edge case when mixing constraints and Literal , #2794 by @PrettyWood Fix postponed annotation resolution for NamedTuple and TypedDict when they're used directly as the type of fields\n  within Pydantic models, #2760 by @jameysharp Fix bug when mypy plugin fails on construct method call for BaseSettings derived classes, #2753 by @uriyyo Add function overloading for a pydantic.create_model function, #2748 by @uriyyo Fix mypy plugin issue with self field declaration, #2743 by @uriyyo The colon at the end of the line \"The fields which were supplied when user was initialised:\" suggests that the code following it is related.\n  Changed it to a period, #2733 by @krisaoe Renamed variable schema to schema_ to avoid shadowing of global variable name, #2724 by @shahriyarr Add support for autocomplete in VS Code via __dataclass_transform__ , #2721 by @tiangolo add missing type annotations in BaseConfig and handle max_length = 0 , #2719 by @PrettyWood Change orm_mode checking to allow recursive ORM mode parsing with dicts, #2718 by @nuno-andre Add episode 313 of the Talk Python To Me podcast, where Michael Kennedy and Samuel Colvin discuss Pydantic, to the docs, #2712 by @RatulMaharaj fix JSON schema generation when a field is of type NamedTuple and has a default value, #2707 by @PrettyWood Enum fields now properly support extra kwargs in schema generation, #2697 by @sammchardy Breaking Change, see #3780 : Make serialization of referenced pydantic models possible, #2650 by @PrettyWood Add uniqueItems option to ConstrainedList , #2618 by @nuno-andre Try to evaluate forward refs automatically at model creation, #2588 by @uriyyo Switch docs preview and coverage display to use smokeshow , #2580 by @samuelcolvin Add __version__ attribute to pydantic module, #2572 by @paxcodes Add postgresql+asyncpg , postgresql+pg8000 , postgresql+psycopg2 , postgresql+psycopg2cffi , postgresql+py-postgresql and postgresql+pygresql schemes for PostgresDsn , #2567 by @postgres-asyncpg Enable the Hypothesis plugin to generate a constrained decimal when the decimal_places argument is specified, #2524 by @cwe5590 Allow collections.abc.Callable to be used as type in Python 3.9, #2519 by @daviskirk Documentation update how to custom compile pydantic when using pip install, small change in setup.py to allow for custom CFLAGS when compiling, #2517 by @peterroelants remove side effect of default_factory to run it only once even if Config.validate_all is set, #2515 by @PrettyWood Add lookahead to ip regexes for AnyUrl hosts. This allows urls with DNS labels\n  looking like IPs to validate as they are perfectly valid host names, #2512 by @sbv-csis Set minItems and maxItems in generated JSON schema for fixed-length tuples, #2497 by @PrettyWood Add strict argument to conbytes , #2489 by @koxudaxi Support user defined generic field types in generic models, #2465 by @daviskirk Add an example and a short explanation of subclassing GetterDict to docs, #2463 by @nuno-andre add KafkaDsn type, HttpUrl now has default port 80 for http and 443 for https, #2447 by @MihanixA Add PastDate and FutureDate types, #2425 by @Kludex Support generating schema for Generic fields with subtypes, #2375 by @maximberg fix(encoder): serialize NameEmail to str, #2341 by @alecgerona add Config.smart_union to prevent coercion in Union if possible, see the doc for more information, #2092 by @PrettyWood Add ability to use typing.Counter as a model field type, #2060 by @uriyyo Add parameterised subclasses to __bases__ when constructing new parameterised classes, so that A <: B => A[int] <: B[int] , #2007 by @diabolo-dan Create FileUrl type that allows URLs that conform to RFC 8089 .\n  Add host_required parameter, which is True by default ( AnyUrl and subclasses), False in RedisDsn , FileUrl , #1983 by @vgerak add confrozenset() , analogous to conset() and conlist() , #1897 by @PrettyWood stop calling parent class root_validator if overridden, #1895 by @PrettyWood Add repr (defaults to True ) parameter to Field , to hide it from the default representation of the BaseModel , #1831 by @fnep Accept empty query/fragment URL parts, #1807 by @xavier","pageID":"Changelog","abs_url":"/latest/changelog/#v190a1-2021-12-18-changes","title":"Changelog - v1.9.0 (2021-12-31) - v1.9.0a1 (2021-12-18) Changes","objectID":"/latest/changelog/#v190a1-2021-12-18-changes","rank":-690},{"content":"Warning A security vulnerability, level \"moderate\" is fixed in v1.8.2. Please upgrade ASAP .\nSee security advisory CVE-2021-29510 Security fix: Fix date and datetime parsing so passing either 'infinity' or float('inf') (or their negative values) does not cause an infinite loop,\n  see security advisory CVE-2021-29510 fix schema generation with Enum by generating a valid name, #2575 by @PrettyWood fix JSON schema generation with a Literal of an enum member, #2536 by @PrettyWood Fix bug with configurations declarations that are passed as\n  keyword arguments during class creation, #2532 by @uriyyo Allow passing json_encoders in class kwargs, #2521 by @layday support arbitrary types with custom __eq__ , #2483 by @PrettyWood support Annotated in validate_arguments and in generic models with Python 3.9, #2483 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#v182-2021-05-11","title":"Changelog - v1.8.2 (2021-05-11)","objectID":"/latest/changelog/#v182-2021-05-11","rank":-695},{"content":"Bug fixes for regressions and new features from v1.8 allow elements of Config.field to update elements of a Field , #2461 by @samuelcolvin fix validation with a BaseModel field and a custom root type, #2449 by @PrettyWood expose Pattern encoder to fastapi , #2444 by @PrettyWood enable the Hypothesis plugin to generate a constrained float when the multiple_of argument is specified, #2442 by @tobi-lipede-oodle Avoid RecursionError when using some types like Enum or Literal with generic models, #2436 by @PrettyWood do not overwrite declared __hash__ in subclasses of a model, #2422 by @PrettyWood fix mypy complaints on Path and UUID related custom types, #2418 by @PrettyWood Support properly variable length tuples of compound types, #2416 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#v181-2021-03-03","title":"Changelog - v1.8.1 (2021-03-03)","objectID":"/latest/changelog/#v181-2021-03-03","rank":-700},{"content":"Thank you to pydantic's sponsors: @jorgecarleitao , @BCarley , @chdsbd , @tiangolo , @matin , @linusg , @kevinalh , @koxudaxi , @timdrijvers , @mkeen , @meadsteve , @ginomempin , @primer-io , @and-semakin , @tomthorogood , @AjitZK , @westonsteimel , @Mazyod , @christippett , @CarlosDomingues , @Kludex , @r-m-n for their kind support.","pageID":"Changelog","abs_url":"/latest/changelog/#v18-2021-02-26","title":"Changelog - v1.8 (2021-02-26)","objectID":"/latest/changelog/#v18-2021-02-26","rank":-705},{"content":"Hypothesis plugin for testing, #2097 by @Zac-HD support for NamedTuple and TypedDict , #2216 by @PrettyWood Support Annotated hints on model fields , #2147 by @JacobHayes frozen parameter on Config to allow models to be hashed, #1880 by @rhuille","pageID":"Changelog","abs_url":"/latest/changelog/#highlights_1","title":"Changelog - v1.8 (2021-02-26) - Highlights","objectID":"/latest/changelog/#highlights_1","rank":-710},{"content":"Breaking Change , remove old deprecation aliases from v1, #2415 by @samuelcolvin : remove notes on migrating to v1 in docs remove Schema which was replaced by Field remove Config.case_insensitive which was replaced by Config.case_sensitive (default False ) remove Config.allow_population_by_alias which was replaced by Config.allow_population_by_field_name remove model.fields which was replaced by model.__fields__ remove model.to_string() which was replaced by str(model) remove model.__values__ which was replaced by model.__dict__ Breaking Change: always validate only first sublevel items with each_item .\n  There were indeed some edge cases with some compound types where the validated items were the last sublevel ones, #1933 by @PrettyWood Update docs extensions to fix local syntax highlighting, #2400 by @daviskirk fix: allow utils.lenient_issubclass to handle typing.GenericAlias objects like list[str] in Python >= 3.9, #2399 by @daviskirk Improve field declaration for pydantic dataclass by allowing the usage of pydantic Field or 'metadata' kwarg of dataclasses.field , #2384 by @PrettyWood Making typing-extensions a required dependency, #2368 by @samuelcolvin Make resolve_annotations more lenient, allowing for missing modules, #2363 by @samuelcolvin Allow configuring models through class kwargs, #2356 by @Bobronium Prevent Mapping subclasses from always being coerced to dict , #2325 by @ofek fix: allow None for type Optional[conset / conlist] , #2320 by @PrettyWood Support empty tuple type, #2318 by @PrettyWood fix: python_requires metadata to require >=3.6.1, #2306 by @hukkinj1 Properly encode Decimal with, or without any decimal places, #2293 by @hultner fix: update __fields_set__ in BaseModel.copy(update=…) , #2290 by @PrettyWood fix: keep order of fields with BaseModel.construct() , #2281 by @PrettyWood Support generating schema for Generic fields, #2262 by @maximberg Fix validate_decorator so **kwargs doesn't exclude values when the keyword\n  has the same name as the *args or **kwargs names, #2251 by @cybojenix Prevent overriding positional arguments with keyword arguments in validate_arguments , as per behaviour with native functions, #2249 by @cybojenix add documentation for con* type functions, #2242 by @tayoogunbiyi Support custom root type (aka __root__ ) when using parse_obj() with nested models, #2238 by @PrettyWood Support custom root type (aka __root__ ) with from_orm() , #2237 by @PrettyWood ensure cythonized functions are left untouched when creating models, based on #1944 by @kollmats , #2228 by @samuelcolvin Resolve forward refs for stdlib dataclasses converted into pydantic ones, #2220 by @PrettyWood Add support for NamedTuple and TypedDict types.\n  Those two types are now handled and validated when used inside BaseModel or pydantic dataclass .\n  Two utils are also added create_model_from_namedtuple and create_model_from_typeddict , #2216 by @PrettyWood Do not ignore annotated fields when type is Union[Type[...], ...] , #2213 by @PrettyWood Raise a user-friendly TypeError when a root_validator does not return a dict (e.g. None ), #2209 by @masalim2 Add a FrozenSet[str] type annotation to the allowed_schemes argument on the strict_url field type, #2198 by @Midnighter add allow_mutation constraint to Field , #2195 by @sblack-usu Allow Field with a default_factory to be used as an argument to a function\n  decorated with validate_arguments , #2176 by @thomascobb Allow non-existent secrets directory by only issuing a warning, #2175 by @davidolrik fix URL regex to parse fragment without query string, #2168 by @andrewmwhite fix: ensure to always return one of the values in Literal field type, #2166 by @PrettyWood Support typing.Annotated hints on model fields. A Field may now be set in the type hint with Annotated[..., Field(...) ; all other annotations are ignored but still visible with get_type_hints(..., include_extras=True) , #2147 by @JacobHayes Added StrictBytes type as well as strict=False option to ConstrainedBytes , #2136 by @rlizzo added Config.anystr_lower and to_lower kwarg to constr and conbytes , #2134 by @tayoogunbiyi Support plain typing.Tuple type, #2132 by @PrettyWood Add a bound method validate to functions decorated with validate_arguments to validate parameters without actually calling the function, #2127 by @PrettyWood Add the ability to customize settings sources (add / disable / change priority order), #2107 by @kozlek Fix mypy complaints about most custom pydantic types, #2098 by @PrettyWood Add a Hypothesis plugin for easier property-based testing with Pydantic's custom types - usage details here , #2097 by @Zac-HD add validator for None , NoneType or Literal[None] , #2095 by @PrettyWood Handle properly fields of type Callable with a default value, #2094 by @PrettyWood Updated create_model return type annotation to return type which inherits from __base__ argument, #2071 by @uriyyo Add merged json_encoders inheritance, #2064 by @art049 allow overwriting ClassVar s in sub-models without having to re-annotate them, #2061 by @layday add default encoder for Pattern type, #2045 by @PrettyWood Add NonNegativeInt , NonPositiveInt , NonNegativeFloat , NonPositiveFloat , #1975 by @mdavis-xyz Use % for percentage in string format of colors, #1960 by @EdwardBetts Fixed issue causing KeyError to be raised when building schema from multiple BaseModel with the same names declared in separate classes, #1912 by @JSextonn Add rediss (Redis over SSL) protocol to RedisDsn Allow URLs without user part (e.g., rediss://:pass@localhost ), #1911 by @TrDex Add a new frozen boolean parameter to Config (default: False ).\n  Setting frozen=True does everything that allow_mutation=False does, and also generates a __hash__() method for the model. This makes instances of the model potentially hashable if all the attributes are hashable, #1880 by @rhuille fix schema generation with multiple Enums having the same name, #1857 by @PrettyWood Added support for 13/19 digits VISA credit cards in PaymentCardNumber type, #1416 by @AlexanderSov fix: prevent RecursionError while using recursive GenericModel s, #1370 by @xppt use enum for typing.Literal in JSON schema, #1350 by @PrettyWood Fix: some recursive models did not require update_forward_refs and silently behaved incorrectly, #1201 by @PrettyWood Fix bug where generic models with fields where the typevar is nested in another type a: List[T] are considered to be concrete. This allows these models to be subclassed and composed as expected, #947 by @daviskirk Add Config.copy_on_model_validation flag. When set to False , pydantic will keep models used as fields\n  untouched on validation instead of reconstructing (copying) them, #265 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#changes_12","title":"Changelog - v1.8 (2021-02-26) - Changes","objectID":"/latest/changelog/#changes_12","rank":-715},{"content":"Security fix: Fix date and datetime parsing so passing either 'infinity' or float('inf') (or their negative values) does not cause an infinite loop,\n  See security advisory CVE-2021-29510","pageID":"Changelog","abs_url":"/latest/changelog/#v174-2021-05-11","title":"Changelog - v1.7.4 (2021-05-11)","objectID":"/latest/changelog/#v174-2021-05-11","rank":-720},{"content":"Thank you to pydantic's sponsors: @timdrijvers , @BCarley , @chdsbd , @tiangolo , @matin , @linusg , @kevinalh , @jorgecarleitao , @koxudaxi , @primer-api , @mkeen , @meadsteve for their kind support. fix: set right default value for required (optional) fields, #2142 by @PrettyWood fix: support underscore_attrs_are_private with generic models, #2138 by @PrettyWood fix: update all modified field values in root_validator when validate_assignment is on, #2116 by @PrettyWood Allow pickling of pydantic.dataclasses.dataclass dynamically created from a built-in dataclasses.dataclass , #2111 by @aimestereo Fix a regression where Enum fields would not propagate keyword arguments to the schema, #2109 by @bm424 Ignore __doc__ as private attribute when Config.underscore_attrs_are_private is set, #2090 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#v173-2020-11-30","title":"Changelog - v1.7.3 (2020-11-30)","objectID":"/latest/changelog/#v173-2020-11-30","rank":-725},{"content":"fix slow GenericModel concrete model creation, allow GenericModel concrete name reusing in module, #2078 by @Bobronium keep the order of the fields when validate_assignment is set, #2073 by @PrettyWood forward all the params of the stdlib dataclass when converted into pydantic dataclass , #2065 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#v172-2020-11-01","title":"Changelog - v1.7.2 (2020-11-01)","objectID":"/latest/changelog/#v172-2020-11-01","rank":-730},{"content":"Thank you to pydantic's sponsors: @timdrijvers , @BCarley , @chdsbd , @tiangolo , @matin , @linusg , @kevinalh , @jorgecarleitao , @koxudaxi , @primer-api , @mkeen for their kind support. fix annotation of validate_arguments when passing configuration as argument, #2055 by @layday Fix mypy assignment error when using PrivateAttr , #2048 by @aphedges fix underscore_attrs_are_private causing TypeError when overriding __init__ , #2047 by @samuelcolvin Fixed regression introduced in v1.7 involving exception handling in field validators when validate_assignment=True , #2044 by @johnsabath fix: pydantic dataclass can inherit from stdlib dataclass and Config.arbitrary_types_allowed is supported, #2042 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#v171-2020-10-28","title":"Changelog - v1.7.1 (2020-10-28)","objectID":"/latest/changelog/#v171-2020-10-28","rank":-735},{"content":"Thank you to pydantic's sponsors: @timdrijvers , @BCarley , @chdsbd , @tiangolo , @matin , @linusg , @kevinalh , @jorgecarleitao , @koxudaxi , @primer-api for their kind support.","pageID":"Changelog","abs_url":"/latest/changelog/#v17-2020-10-26","title":"Changelog - v1.7 (2020-10-26)","objectID":"/latest/changelog/#v17-2020-10-26","rank":-740},{"content":"Python 3.9 support, thanks @PrettyWood Private model attributes , thanks @Bobronium \"secrets files\" support in BaseSettings , thanks @mdgilene convert stdlib dataclasses to pydantic dataclasses and use stdlib dataclasses in models , thanks @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#highlights_2","title":"Changelog - v1.7 (2020-10-26) - Highlights","objectID":"/latest/changelog/#highlights_2","rank":-745},{"content":"Breaking Change: remove __field_defaults__ , add default_factory support with BaseModel.construct .\n  Use .get_default() method on fields in __fields__ attribute instead, #1732 by @PrettyWood Rearrange CI to run linting as a separate job, split install recipes for different tasks, #2020 by @samuelcolvin Allows subclasses of generic models to make some, or all, of the superclass's type parameters concrete, while\n  also defining new type parameters in the subclass, #2005 by @choogeboom Call validator with the correct values parameter type in BaseModel.__setattr__ ,\n  when validate_assignment = True in model config, #1999 by @me-ransh Force fields.Undefined to be a singleton object, fixing inherited generic model schemas, #1981 by @daviskirk Include tests in source distributions, #1976 by @sbraz Add ability to use min_length/max_length constraints with secret types, #1974 by @uriyyo Also check root_validators when validate_assignment is on, #1971 by @PrettyWood Fix const validators not running when custom validators are present, #1957 by @hmvp add deque to field types, #1935 by @wozniakty add basic support for Python 3.9, #1832 by @PrettyWood Fix typo in the anchor of exporting_models.md#modelcopy and incorrect description, #1821 by @KimMachineGun Added ability for BaseSettings to read \"secret files\", #1820 by @mdgilene add parse_raw_as utility function, #1812 by @PrettyWood Support home directory relative paths for dotenv files (e.g. ~/.env ), #1803 by @PrettyWood Clarify documentation for parse_file to show that the argument\n  should be a file path not a file-like object, #1794 by @mdavis-xyz Fix false positive from mypy plugin when a class nested within a BaseModel is named Model , #1770 by @selimb add basic support of Pattern type in schema generation, #1767 by @PrettyWood Support custom title, description and default in schema of enums, #1748 by @PrettyWood Properly represent Literal Enums when use_enum_values is True, #1747 by @noelevans Allows timezone information to be added to strings to be formatted as time objects. Permitted formats are Z for UTC\n  or an offset for absolute positive or negative time shifts. Or the timezone data can be omitted, #1744 by @noelevans Add stub __init__ with Python 3.6 signature for ForwardRef , #1738 by @sirtelemak Fix behaviour with forward refs and optional fields in nested models, #1736 by @PrettyWood add Enum and IntEnum as valid types for fields, #1735 by @PrettyWood Change default value of __module__ argument of create_model from None to 'pydantic.main' .\n  Set reference of created concrete model to it's module to allow pickling (not applied to models created in\n  functions), #1686 by @Bobronium Add private attributes support, #1679 by @Bobronium add config to @validate_arguments , #1663 by @samuelcolvin Allow descendant Settings models to override env variable names for the fields defined in parent Settings models with env in their Config . Previously only env_prefix configuration option was applicable, #1561 by @ojomio Support ref_template when creating schema $ref s, #1479 by @kilo59 Add a __call__ stub to PyObject so that mypy will know that it is callable, #1352 by @brianmaissy pydantic.dataclasses.dataclass decorator now supports built-in dataclasses.dataclass .\n  It is hence possible to convert an existing dataclass easily to add Pydantic validation.\n  Moreover nested dataclasses are also supported, #744 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#changes_13","title":"Changelog - v1.7 (2020-10-26) - Changes","objectID":"/latest/changelog/#changes_13","rank":-750},{"content":"Security fix: Fix date and datetime parsing so passing either 'infinity' or float('inf') (or their negative values) does not cause an infinite loop,\n  See security advisory CVE-2021-29510","pageID":"Changelog","abs_url":"/latest/changelog/#v162-2021-05-11","title":"Changelog - v1.6.2 (2021-05-11)","objectID":"/latest/changelog/#v162-2021-05-11","rank":-755},{"content":"fix validation and parsing of nested models with default_factory , #1710 by @PrettyWood","pageID":"Changelog","abs_url":"/latest/changelog/#v161-2020-07-15","title":"Changelog - v1.6.1 (2020-07-15)","objectID":"/latest/changelog/#v161-2020-07-15","rank":-760},{"content":"Thank you to pydantic's sponsors: @matin , @tiangolo , @chdsbd , @jorgecarleitao , and 1 anonymous sponsor for their kind support. Modify validators for conlist and conset to not have always=True , #1682 by @samuelcolvin add port check to AnyUrl (can't exceed 65536) ports are 16 unsigned bits: 0 <= port <= 2**16-1 src: rfc793 header format , #1654 by @flapili Document default regex anchoring semantics, #1648 by @yurikhan Use chain.from_iterable in class_validators.py. This is a faster and more idiomatic way of using itertools.chain .\n  Instead of computing all the items in the iterable and storing them in memory, they are computed one-by-one and never\n  stored as a huge list. This can save on both runtime and memory space, #1642 by @cool-RR Add conset() , analogous to conlist() , #1623 by @patrickkwang make Pydantic errors (un)pickable, #1616 by @PrettyWood Allow custom encoding for dotenv files, #1615 by @PrettyWood Ensure SchemaExtraCallable is always defined to get type hints on BaseConfig, #1614 by @PrettyWood Update datetime parser to support negative timestamps, #1600 by @mlbiche Update mypy, remove AnyType alias for Type[Any] , #1598 by @samuelcolvin Adjust handling of root validators so that errors are aggregated from all failing root validators, instead of reporting on only the first root validator to fail, #1586 by @beezee Make __modify_schema__ on Enums apply to the enum schema rather than fields that use the enum, #1581 by @therefromhere Fix behavior of __all__ key when used in conjunction with index keys in advanced include/exclude of fields that are sequences, #1579 by @xspirus Subclass validators do not run when referencing a List field defined in a parent class when each_item=True . Added an example to the docs illustrating this, #1566 by @samueldeklund change schema.field_class_to_schema to support frozenset in schema, #1557 by @wangpeibao Call __modify_schema__ only for the field schema, #1552 by @PrettyWood Move the assignment of field.validate_always in fields.py so the always parameter of validators work on inheritance, #1545 by @dcHHH Added support for UUID instantiation through 16 byte strings such as b'\\x12\\x34\\x56\\x78' * 4 . This was done to support BINARY(16) columns in sqlalchemy, #1541 by @shawnwall Add a test assertion that default_factory can return a singleton, #1523 by @therefromhere Add NameEmail.__eq__ so duplicate NameEmail instances are evaluated as equal, #1514 by @stephen-bunn Add datamodel-code-generator link in pydantic document site, #1500 by @koxudaxi Added a \"Discussion of Pydantic\" section to the documentation, with a link to \"Pydantic Introduction\" video by Alexander Hultnér, #1499 by @hultner Avoid some side effects of default_factory by calling it only once\n  if possible and by not setting a default value in the schema, #1491 by @PrettyWood Added docs about dumping dataclasses to JSON, #1487 by @mikegrima Make BaseModel.__signature__ class-only, so getting __signature__ from model instance will raise AttributeError , #1466 by @Bobronium include 'format': 'password' in the schema for secret types, #1424 by @atheuz Modify schema constraints on ConstrainedFloat so that exclusiveMinimum and\n  minimum are not included in the schema if they are equal to -math.inf and exclusiveMaximum and maximum are not included if they are equal to math.inf , #1417 by @vdwees Squash internal __root__ dicts in .dict() (and, by extension, in .json() ), #1414 by @patrickkwang Move const validator to post-validators so it validates the parsed value, #1410 by @selimb Fix model validation to handle nested literals, e.g. Literal['foo', Literal['bar']] , #1364 by @DBCerigo Remove user_required = True from RedisDsn , neither user nor password are required, #1275 by @samuelcolvin Remove extra allOf from schema for fields with Union and custom Field , #1209 by @mostaphaRoudsari Updates OpenAPI schema generation to output all enums as separate models.\n  Instead of inlining the enum values in the model schema, models now use a $ref property to point to the enum definition, #1173 by @calvinwyoung","pageID":"Changelog","abs_url":"/latest/changelog/#v16-2020-07-11","title":"Changelog - v1.6 (2020-07-11)","objectID":"/latest/changelog/#v16-2020-07-11","rank":-765},{"content":"Signature generation with extra: allow never uses a field name, #1418 by @prettywood Avoid mutating Field default value, #1412 by @prettywood","pageID":"Changelog","abs_url":"/latest/changelog/#v151-2020-04-23","title":"Changelog - v1.5.1 (2020-04-23)","objectID":"/latest/changelog/#v151-2020-04-23","rank":-770},{"content":"Make includes/excludes arguments for .dict() , ._iter() , ..., immutable, #1404 by @AlexECX Always use a field's real name with includes/excludes in model._iter() , regardless of by_alias , #1397 by @AlexECX Update constr regex example to include start and end lines, #1396 by @lmcnearney Confirm that shallow model.copy() does make a shallow copy of attributes, #1383 by @samuelcolvin Renaming model_name argument of main.create_model() to __model_name to allow using model_name as a field name, #1367 by @kittipatv Replace raising of exception to silent passing  for non-Var attributes in mypy plugin, #1345 by @b0g3r Remove typing_extensions dependency for Python 3.8, #1342 by @prettywood Make SecretStr and SecretBytes initialization idempotent, #1330 by @atheuz document making secret types dumpable using the json method, #1328 by @atheuz Move all testing and build to github actions, add windows and macos binaries,\n  thank you @StephenBrown2 for much help, #1326 by @samuelcolvin fix card number length check in PaymentCardNumber , PaymentCardBrand now inherits from str , #1317 by @samuelcolvin Have BaseModel inherit from Representation to make mypy happy when overriding __str__ , #1310 by @FuegoFro Allow None as input to all optional list fields, #1307 by @prettywood Add datetime field to default_factory example, #1301 by @StephenBrown2 Allow subclasses of known types to be encoded with superclass encoder, #1291 by @StephenBrown2 Exclude exported fields from all elements of a list/tuple of submodels/dicts with '__all__' , #1286 by @masalim2 Add pydantic.color.Color objects as available input for Color fields, #1258 by @leosussan In examples, type nullable fields as Optional , so that these are valid mypy annotations, #1248 by @kokes Make pattern_validator() accept pre-compiled Pattern objects. Fix str_validator() return type to str , #1237 by @adamgreg Document how to manage Generics and inheritance, #1229 by @esadruhn update_forward_refs() method of BaseModel now copies __dict__ of class module instead of modifying it, #1228 by @paul-ilyin Support instance methods and class methods with @validate_arguments , #1222 by @samuelcolvin Add default_factory argument to Field to create a dynamic default value by passing a zero-argument callable, #1210 by @prettywood add support for NewType of List , Optional , etc, #1207 by @Kazy fix mypy signature for root_validator , #1192 by @samuelcolvin Fixed parsing of nested 'custom root type' models, #1190 by @Shados Add validate_arguments function decorator which checks the arguments to a function matches type annotations, #1179 by @samuelcolvin Add __signature__ to models, #1034 by @Bobronium Refactor ._iter() method, 10x speed boost for dict(model) , #1017 by @Bobronium","pageID":"Changelog","abs_url":"/latest/changelog/#v15-2020-04-18","title":"Changelog - v1.5 (2020-04-18)","objectID":"/latest/changelog/#v15-2020-04-18","rank":-775},{"content":"Breaking Change: alias precedence logic changed so aliases on a field always take priority over\n  an alias from alias_generator to avoid buggy/unexpected behaviour,\n  see here for details, #1178 by @samuelcolvin Add support for unicode and punycode in TLDs, #1182 by @jamescurtin Fix cls argument in validators during assignment, #1172 by @samuelcolvin completing Luhn algorithm for PaymentCardNumber , #1166 by @cuencandres add support for generics that implement __get_validators__ like a custom data type, #1159 by @tiangolo add support for infinite generators with Iterable , #1152 by @tiangolo fix url_regex to accept schemas with + , - and . after the first character, #1142 by @samuelcolvin move version_info() to version.py , suggest its use in issues, #1138 by @samuelcolvin Improve pydantic import time by roughly 50% by deferring some module loading and regex compilation, #1127 by @samuelcolvin Fix EmailStr and NameEmail to accept instances of themselves in cython, #1126 by @koxudaxi Pass model class to the Config.schema_extra callable, #1125 by @therefromhere Fix regex for username and password in URLs, #1115 by @samuelcolvin Add support for nested generic models, #1104 by @dmontagu add __all__ to __init__.py to prevent \"implicit reexport\" errors from mypy, #1072 by @samuelcolvin Add support for using \"dotenv\" files with BaseSettings , #1011 by @acnebs","pageID":"Changelog","abs_url":"/latest/changelog/#v14-2020-01-24","title":"Changelog - v1.4 (2020-01-24)","objectID":"/latest/changelog/#v14-2020-01-24","rank":-780},{"content":"Change schema and schema_model to handle dataclasses by using their __pydantic_model__ feature, #792 by @aviramha Added option for root_validator to be skipped if values validation fails using keyword skip_on_failure=True , #1049 by @aviramha Allow Config.schema_extra to be a callable so that the generated schema can be post-processed, #1054 by @selimb Update mypy to version 0.750, #1057 by @dmontagu Trick Cython into allowing str subclassing, #1061 by @skewty Prevent type attributes being added to schema unless the attribute __schema_attributes__ is True , #1064 by @samuelcolvin Change BaseModel.parse_file to use Config.json_loads , #1067 by @kierandarcy Fix for optional Json fields, #1073 by @volker48 Change the default number of threads used when compiling with cython to one,\n  allow override via the CYTHON_NTHREADS environment variable, #1074 by @samuelcolvin Run FastAPI tests during Pydantic's CI tests, #1075 by @tiangolo My mypy strictness constraints, and associated tweaks to type annotations, #1077 by @samuelcolvin Add __eq__ to SecretStr and SecretBytes to allow \"value equals\", #1079 by @sbv-trueenergy Fix schema generation for nested None case, #1088 by @lutostag Consistent checks for sequence like objects, #1090 by @samuelcolvin Fix Config inheritance on BaseSettings when used with env_prefix , #1091 by @samuelcolvin Fix for __modify_schema__ when it conflicted with field_class_to_schema* , #1102 by @samuelcolvin docs: Fix explanation of case sensitive environment variable names when populating BaseSettings subclass attributes, #1105 by @tribals Rename django-rest-framework benchmark in documentation, #1119 by @frankie567","pageID":"Changelog","abs_url":"/latest/changelog/#v13-2019-12-21","title":"Changelog - v1.3 (2019-12-21)","objectID":"/latest/changelog/#v13-2019-12-21","rank":-785},{"content":"Possible Breaking Change: Add support for required Optional with name: Optional[AnyType] = Field(...) and refactor ModelField creation to preserve required parameter value, #1031 by @tiangolo ;\n  see here for details Add benchmarks for cattrs , #513 by @sebastianmika Add exclude_none option to dict() and friends, #587 by @niknetniko Add benchmarks for valideer , #670 by @gsakkis Add parse_obj_as and parse_file_as functions for ad-hoc parsing of data into arbitrary pydantic-compatible types, #934 by @dmontagu Add allow_reuse argument to validators, thus allowing validator reuse, #940 by @dmontagu Add support for mapping types for custom root models, #958 by @dmontagu Mypy plugin support for dataclasses, #966 by @koxudaxi Add support for dataclasses default factory, #968 by @ahirner Add a ByteSize type for converting byte string ( 1GB ) to plain bytes, #977 by @dgasmith Fix mypy complaint about @root_validator(pre=True) , #984 by @samuelcolvin Add manylinux binaries for Python 3.8 to pypi, also support manylinux2010, #994 by @samuelcolvin Adds ByteSize conversion to another unit, #995 by @dgasmith Fix __str__ and __repr__ inheritance for models, #1022 by @samuelcolvin add testimonials section to docs, #1025 by @sullivancolin Add support for typing.Literal for Python 3.8, #1026 by @dmontagu","pageID":"Changelog","abs_url":"/latest/changelog/#v12-2019-11-28","title":"Changelog - v1.2 (2019-11-28)","objectID":"/latest/changelog/#v12-2019-11-28","rank":-790},{"content":"Fix bug where use of complex fields on sub-models could cause fields to be incorrectly configured, #1015 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v111-2019-11-20","title":"Changelog - v1.1.1 (2019-11-20)","objectID":"/latest/changelog/#v111-2019-11-20","rank":-795},{"content":"Add a mypy plugin for type checking BaseModel.__init__ and more, #722 by @dmontagu Change return type typehint for GenericModel.__class_getitem__ to prevent PyCharm warnings, #936 by @dmontagu Fix usage of Any to allow None , also support TypeVar thus allowing use of un-parameterised collection types\n  e.g. Dict and List , #962 by @samuelcolvin Set FieldInfo on subfields to fix schema generation for complex nested types, #965 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v11-2019-11-07","title":"Changelog - v1.1 (2019-11-07)","objectID":"/latest/changelog/#v11-2019-11-07","rank":-800},{"content":"Breaking Change: deprecate the Model.fields property, use Model.__fields__ instead, #883 by @samuelcolvin Breaking Change: Change the precedence of aliases so child model aliases override parent aliases,\n  including using alias_generator , #904 by @samuelcolvin Breaking change: Rename skip_defaults to exclude_unset , and add ability to exclude actual defaults, #915 by @dmontagu Add **kwargs to pydantic.main.ModelMetaclass.__new__ so __init_subclass__ can take custom parameters on extended BaseModel classes, #867 by @retnikt Fix field of a type that has a default value, #880 by @koxudaxi Use FutureWarning instead of DeprecationWarning when alias instead of env is used for settings models, #881 by @samuelcolvin Fix issue with BaseSettings inheritance and alias getting set to None , #882 by @samuelcolvin Modify __repr__ and __str__ methods to be consistent across all public classes, add __pretty__ to support\n  python-devtools, #884 by @samuelcolvin deprecation warning for case_insensitive on BaseSettings config, #885 by @samuelcolvin For BaseSettings merge environment variables and in-code values recursively, as long as they create a valid object\n  when merged together, to allow splitting init arguments, #888 by @idmitrievsky change secret types example, #890 by @ashears Change the signature of Model.construct() to be more user-friendly, document construct() usage, #898 by @samuelcolvin Add example for the construct() method, #907 by @ashears Improve use of Field constraints on complex types, raise an error if constraints are not enforceable,\n  also support tuples with an ellipsis Tuple[X, ...] , Sequence and FrozenSet in schema, #909 by @samuelcolvin update docs for bool missing valid value, #911 by @trim21 Better str / repr logic for ModelField , #912 by @samuelcolvin Fix ConstrainedList , update schema generation to reflect min_items and max_items Field() arguments, #917 by @samuelcolvin Allow abstracts sets (eg. dict keys) in the include and exclude arguments of dict() , #921 by @samuelcolvin Fix JSON serialization errors on ValidationError.json() by using pydantic_encoder , #922 by @samuelcolvin Clarify usage of remove_untouched , improve error message for types with no validators, #926 by @retnikt","pageID":"Changelog","abs_url":"/latest/changelog/#v10-2019-10-23","title":"Changelog - v1.0 (2019-10-23)","objectID":"/latest/changelog/#v10-2019-10-23","rank":-805},{"content":"Mark StrictBool typecheck as bool to allow for default values without mypy errors, #690 by @dmontagu Transfer the documentation build from sphinx to mkdocs, re-write much of the documentation, #856 by @samuelcolvin Add support for custom naming schemes for GenericModel subclasses, #859 by @dmontagu Add if TYPE_CHECKING: to the excluded lines for test coverage, #874 by @dmontagu Rename allow_population_by_alias to allow_population_by_field_name , remove unnecessary warning about it, #875 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v10b2-2019-10-07","title":"Changelog - v1.0b2 (2019-10-07)","objectID":"/latest/changelog/#v10b2-2019-10-07","rank":-810},{"content":"Breaking Change: rename Schema to Field , make it a function to placate mypy, #577 by @samuelcolvin Breaking Change: modify parsing behavior for bool , #617 by @dmontagu Breaking Change: get_validators is no longer recognised, use __get_validators__ . Config.ignore_extra and Config.allow_extra are no longer recognised, use Config.extra , #720 by @samuelcolvin Breaking Change: modify default config settings for BaseSettings ; case_insensitive renamed to case_sensitive ,\n  default changed to case_sensitive = False , env_prefix default changed to '' - e.g. no prefix, #721 by @dmontagu Breaking change: Implement root_validator and rename root errors from __obj__ to __root__ , #729 by @samuelcolvin Breaking Change: alter the behaviour of dict(model) so that sub-models are nolonger\n  converted to dictionaries, #733 by @samuelcolvin Breaking change: Added initvars support to post_init_post_parse , #748 by @Raphael-C-Almeida Breaking Change: Make BaseModel.json() only serialize the __root__ key for models with custom root, #752 by @dmontagu Breaking Change: complete rewrite of URL parsing logic, #755 by @samuelcolvin Breaking Change: preserve superclass annotations for field-determination when not provided in subclass, #757 by @dmontagu Breaking Change: BaseSettings now uses the special env settings to define which environment variables to\n  read, not aliases, #847 by @samuelcolvin add support for assert statements inside validators, #653 by @abdusco Update documentation to specify the use of pydantic.dataclasses.dataclass and subclassing pydantic.BaseModel , #710 by @maddosaurus Allow custom JSON decoding and encoding via json_loads and json_dumps Config properties, #714 by @samuelcolvin make all annotated fields occur in the order declared, #715 by @dmontagu use pytest to test mypy integration, #735 by @dmontagu add __repr__ method to ErrorWrapper , #738 by @samuelcolvin Added support for FrozenSet members in dataclasses, and a better error when attempting to use types from the typing module that are not supported by Pydantic, #745 by @djpetti add documentation for Pycharm Plugin, #750 by @koxudaxi fix broken examples in the docs, #753 by @dmontagu moving typing related objects into pydantic.typing , #761 by @samuelcolvin Minor performance improvements to ErrorWrapper , ValidationError and datetime parsing, #763 by @samuelcolvin Improvements to datetime / date / time / timedelta types: more descriptive errors,\n  change errors to value_error not type_error , support bytes, #766 by @samuelcolvin fix error messages for Literal types with multiple allowed values, #770 by @dmontagu Improved auto-generated title field in JSON schema by converting underscore to space, #772 by @skewty support mypy --no-implicit-reexport for dataclasses, also respect --no-implicit-reexport in pydantic itself, #783 by @samuelcolvin add the PaymentCardNumber type, #790 by @matin Fix const validations for lists, #794 by @hmvp Set additionalProperties to false in schema for models with extra fields disallowed, #796 by @Code0x58 EmailStr validation method now returns local part case-sensitive per RFC 5321, #798 by @henriklindgren Added ability to validate strictness to ConstrainedFloat , ConstrainedInt and ConstrainedStr and added StrictFloat and StrictInt classes, #799 by @DerRidda Improve handling of None and Optional , replace whole with each_item (inverse meaning, default False )\n  on validators, #803 by @samuelcolvin add support for Type[T] type hints, #807 by @timonbimon Performance improvements from removing change_exceptions , change how pydantic error are constructed, #819 by @samuelcolvin Fix the error message arising when a BaseModel -type model field causes a ValidationError during parsing, #820 by @dmontagu allow getter_dict on Config , modify GetterDict to be more like a Mapping object and thus easier to work with, #821 by @samuelcolvin Only check TypeVar param on base GenericModel class, #842 by @zpencerq rename Model._schema_cache -> Model.__schema_cache__ , Model._json_encoder -> Model.__json_encoder__ , Model._custom_root_type -> Model.__custom_root_type__ , #851 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v10b1-2019-10-01","title":"Changelog - v1.0b1 (2019-10-01)","objectID":"/latest/changelog/#v10b1-2019-10-01","rank":-815},{"content":"(Docs are available here ) fix __post_init__ usage with dataclass inheritance, fix #739 by @samuelcolvin fix required fields validation on GenericModels classes, #742 by @amitbl fix defining custom Schema on GenericModel fields, #754 by @amitbl","pageID":"Changelog","abs_url":"/latest/changelog/#v0322-2019-08-17","title":"Changelog - v0.32.2 (2019-08-17)","objectID":"/latest/changelog/#v0322-2019-08-17","rank":-820},{"content":"do not validate extra fields when validate_assignment is on, #724 by @YaraslauZhylko","pageID":"Changelog","abs_url":"/latest/changelog/#v0321-2019-08-08","title":"Changelog - v0.32.1 (2019-08-08)","objectID":"/latest/changelog/#v0321-2019-08-08","rank":-825},{"content":"add model name to ValidationError error message, #676 by @dmontagu breaking change : remove __getattr__ and rename __values__ to __dict__ on BaseModel ,\n  deprecation warning on use __values__ attr, attributes access speed increased up to 14 times, #712 by @Bobronium support ForwardRef (without self-referencing annotations) in Python 3.6, #706 by @koxudaxi implement schema_extra in Config sub-class, #663 by @tiangolo","pageID":"Changelog","abs_url":"/latest/changelog/#v032-2019-08-06","title":"Changelog - v0.32 (2019-08-06)","objectID":"/latest/changelog/#v032-2019-08-06","rank":-830},{"content":"fix json generation for EnumError , #697 by @dmontagu update numerous dependencies","pageID":"Changelog","abs_url":"/latest/changelog/#v0311-2019-07-31","title":"Changelog - v0.31.1 (2019-07-31)","objectID":"/latest/changelog/#v0311-2019-07-31","rank":-835},{"content":"better support for floating point multiple_of values, #652 by @justindujardin fix schema generation for NewType and Literal , #649 by @dmontagu fix alias_generator and field config conflict, #645 by @gmetzker and #658 by @Bobronium more detailed message for EnumError , #673 by @dmontagu add advanced exclude support for dict , json and copy , #648 by @Bobronium fix bug in GenericModel for models with concrete parameterized fields, #672 by @dmontagu add documentation for Literal type, #651 by @dmontagu add Config.keep_untouched for custom descriptors support, #679 by @Bobronium use inspect.cleandoc internally to get model description, #657 by @tiangolo add Color to schema generation, by @euri10 add documentation for Literal type, #651 by @dmontagu","pageID":"Changelog","abs_url":"/latest/changelog/#v031-2019-07-24","title":"Changelog - v0.31 (2019-07-24)","objectID":"/latest/changelog/#v031-2019-07-24","rank":-840},{"content":"fix so nested classes which inherit and change __init__ are correctly processed while still allowing self as a\n  parameter, #644 by @lnaden and @dgasmith","pageID":"Changelog","abs_url":"/latest/changelog/#v0301-2019-07-15","title":"Changelog - v0.30.1 (2019-07-15)","objectID":"/latest/changelog/#v0301-2019-07-15","rank":-845},{"content":"enforce single quotes in code, #612 by @samuelcolvin fix infinite recursion with dataclass inheritance and __post_init__ , #606 by @Hanaasagi fix default values for GenericModel , #610 by @dmontagu clarify that self-referencing models require Python 3.7+, #616 by @vlcinsky fix truncate for types, #611 by @dmontagu add alias_generator support, #622 by @Bobronium fix unparameterized generic type schema generation, #625 by @dmontagu fix schema generation with multiple/circular references to the same model, #621 by @tiangolo and @wongpat support custom root types, #628 by @koxudaxi support self as a field name in parse_obj , #632 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v030-2019-07-07","title":"Changelog - v0.30 (2019-07-07)","objectID":"/latest/changelog/#v030-2019-07-07","rank":-850},{"content":"support dataclasses.InitVar, #592 by @pfrederiks Updated documentation to elucidate the usage of Union when defining multiple types under an attribute's\n  annotation and showcase how the type-order can affect marshalling of provided values, #594 by @somada141 add conlist type, #583 by @hmvp add support for generics, #595 by @dmontagu","pageID":"Changelog","abs_url":"/latest/changelog/#v029-2019-06-19","title":"Changelog - v0.29 (2019-06-19)","objectID":"/latest/changelog/#v029-2019-06-19","rank":-855},{"content":"fix support for JSON Schema generation when using models with circular references in Python 3.7, #572 by @tiangolo support __post_init_post_parse__ on dataclasses, #567 by @sevaho allow dumping dataclasses to JSON, #575 by @samuelcolvin and @DanielOberg ORM mode, #562 by @samuelcolvin fix pydantic.compiled on ipython, #573 by @dmontagu and @samuelcolvin add StrictBool type, #579 by @cazgp","pageID":"Changelog","abs_url":"/latest/changelog/#v028-2019-06-06","title":"Changelog - v0.28 (2019-06-06)","objectID":"/latest/changelog/#v028-2019-06-06","rank":-860},{"content":"breaking change _pydantic_post_init to execute dataclass' original __post_init__ before\n  validation, #560 by @HeavenVolkoff fix handling of generic types without specified parameters, #550 by @dmontagu breaking change (maybe): this is the first release compiled with cython , see the docs and please\n  submit an issue if you run into problems","pageID":"Changelog","abs_url":"/latest/changelog/#v027-2019-05-30","title":"Changelog - v0.27 (2019-05-30)","objectID":"/latest/changelog/#v027-2019-05-30","rank":-865},{"content":"fix JSON Schema for list , tuple , and set , #540 by @tiangolo compiling with cython, manylinux binaries, some other performance improvements, #548 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0270a1-2019-05-26","title":"Changelog - v0.27.0a1 (2019-05-26)","objectID":"/latest/changelog/#v0270a1-2019-05-26","rank":-870},{"content":"fix to schema generation for IPvAnyAddress , IPvAnyInterface , IPvAnyNetwork #498 by @pilosus fix variable length tuples support, #495 by @pilosus fix return type hint for create_model , #526 by @dmontagu Breaking Change: fix .dict(skip_keys=True) skipping values set via alias (this involves changing validate_model() to always returns Tuple[Dict[str, Any], Set[str], Optional[ValidationError]] ), #517 by @sommd fix to schema generation for IPv4Address , IPv6Address , IPv4Interface , IPv6Interface , IPv4Network , IPv6Network #532 by @euri10 add Color type, #504 by @pilosus and @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v026-2019-05-22","title":"Changelog - v0.26 (2019-05-22)","objectID":"/latest/changelog/#v026-2019-05-22","rank":-875},{"content":"Improve documentation on self-referencing models and annotations, #487 by @theenglishway fix .dict() with extra keys, #490 by @JaewonKim support const keyword in Schema , #434 by @Sean1708","pageID":"Changelog","abs_url":"/latest/changelog/#v025-2019-05-05","title":"Changelog - v0.25 (2019-05-05)","objectID":"/latest/changelog/#v025-2019-05-05","rank":-880},{"content":"fix handling ForwardRef in sub-types, like Union , #464 by @tiangolo fix secret serialization, #465 by @atheuz Support custom validators for dataclasses, #454 by @primal100 fix parse_obj to cope with dict-like objects, #472 by @samuelcolvin fix to schema generation in nested dataclass-based models, #474 by @NoAnyLove fix json for Path , FilePath , and DirectoryPath objects, #473 by @mikegoodspeed","pageID":"Changelog","abs_url":"/latest/changelog/#v024-2019-04-23","title":"Changelog - v0.24 (2019-04-23)","objectID":"/latest/changelog/#v024-2019-04-23","rank":-885},{"content":"improve documentation for contributing section, #441 by @pilosus improve README.rst to include essential information about the package, #446 by @pilosus IntEnum support, #444 by @potykion fix PyObject callable value, #409 by @pilosus fix black deprecation warnings after update, #451 by @pilosus fix ForwardRef collection bug, #450 by @tigerwings Support specialized ClassVars , #455 by @tyrylu fix JSON serialization for ipaddress types, #333 by @pilosus add SecretStr and SecretBytes types, #452 by @atheuz","pageID":"Changelog","abs_url":"/latest/changelog/#v023-2019-04-04","title":"Changelog - v0.23 (2019-04-04)","objectID":"/latest/changelog/#v023-2019-04-04","rank":-890},{"content":"add IPv{4,6,Any}Network and IPv{4,6,Any}Interface types from ipaddress stdlib, #333 by @pilosus add docs for datetime types, #386 by @pilosus fix to schema generation in dataclass-based models, #408 by @pilosus fix path in nested models, #437 by @kataev add Sequence support, #304 by @pilosus","pageID":"Changelog","abs_url":"/latest/changelog/#v022-2019-03-29","title":"Changelog - v0.22 (2019-03-29)","objectID":"/latest/changelog/#v022-2019-03-29","rank":-895},{"content":"fix typo in NoneIsNotAllowedError message, #414 by @YaraslauZhylko add IPvAnyAddress , IPv4Address and IPv6Address types, #333 by @pilosus","pageID":"Changelog","abs_url":"/latest/changelog/#v0210-2019-03-15","title":"Changelog - v0.21.0 (2019-03-15)","objectID":"/latest/changelog/#v0210-2019-03-15","rank":-900},{"content":"fix type hints of parse_obj and similar methods, #405 by @erosennin fix submodel validation, #403 by @samuelcolvin correct type hints for ValidationError.json , #406 by @layday","pageID":"Changelog","abs_url":"/latest/changelog/#v0201-2019-02-26","title":"Changelog - v0.20.1 (2019-02-26)","objectID":"/latest/changelog/#v0201-2019-02-26","rank":-905},{"content":"fix tests for Python 3.8, #396 by @samuelcolvin Adds fields to the dir method for autocompletion in interactive sessions, #398 by @dgasmith support ForwardRef (and therefore from __future__ import annotations ) with dataclasses, #397 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0200-2019-02-18","title":"Changelog - v0.20.0 (2019-02-18)","objectID":"/latest/changelog/#v0200-2019-02-18","rank":-910},{"content":"breaking change (maybe): more sophisticated argument parsing for validators, any subset of values , config and field is now permitted, eg. (cls, value, field) ,\n  however the variadic key word argument (\" **kwargs \") must be called kwargs , #388 by @samuelcolvin breaking change : Adds skip_defaults argument to BaseModel.dict() to allow skipping of fields that\n  were not explicitly set, signature of Model.construct() changed, #389 by @dgasmith add py.typed marker file for PEP-561 support, #391 by @je-l Fix extra behaviour for multiple inheritance/mix-ins, #394 by @YaraslauZhylko","pageID":"Changelog","abs_url":"/latest/changelog/#v0200a1-2019-02-13","title":"Changelog - v0.20.0a1 (2019-02-13)","objectID":"/latest/changelog/#v0200a1-2019-02-13","rank":-915},{"content":"Support Callable type hint, fix #279 by @proofit404 Fix schema for fields with validator decorator, fix #375 by @tiangolo Add multiple_of constraint to ConstrainedDecimal , ConstrainedFloat , ConstrainedInt and their related types condecimal , confloat , and conint #371 , thanks @StephenBrown2 Deprecated ignore_extra and allow_extra Config fields in favor of extra , #352 by @liiight Add type annotations to all functions, test fully with mypy, #373 by @samuelcolvin fix for 'missing' error with validate_all or validate_always , #381 by @samuelcolvin Change the second/millisecond watershed for date/datetime parsing to 2e10 , #385 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0190-2019-02-04","title":"Changelog - v0.19.0 (2019-02-04)","objectID":"/latest/changelog/#v0190-2019-02-04","rank":-920},{"content":"Fix to schema generation with Optional fields, fix #361 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0182-2019-01-22","title":"Changelog - v0.18.2 (2019-01-22)","objectID":"/latest/changelog/#v0182-2019-01-22","rank":-925},{"content":"add ConstrainedBytes and conbytes types, #315 @Gr1N adding MANIFEST.in to include license in package .tar.gz , #358 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0181-2019-01-17","title":"Changelog - v0.18.1 (2019-01-17)","objectID":"/latest/changelog/#v0181-2019-01-17","rank":-930},{"content":"breaking change : don't call validators on keys of dictionaries, #254 by @samuelcolvin Fix validators with always=True when the default is None or the type is optional, also prevent whole validators being called for sub-fields, fix #132 by @samuelcolvin improve documentation for settings priority and allow it to be easily changed, #343 by @samuelcolvin fix ignore_extra=False and allow_population_by_alias=True , fix #257 by @samuelcolvin breaking change : Set BaseConfig attributes min_anystr_length and max_anystr_length to None by default, fix #349 in #350 by @tiangolo add support for postponed annotations, #348 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0180-2019-01-13","title":"Changelog - v0.18.0 (2019-01-13)","objectID":"/latest/changelog/#v0180-2019-01-13","rank":-935},{"content":"fix schema for timedelta as number, #325 by @tiangolo prevent validators being called repeatedly after inheritance, #327 by @samuelcolvin prevent duplicate validator check in ipython, fix #312 by @samuelcolvin add \"Using Pydantic\" section to docs, #323 by @tiangolo & #326 by @samuelcolvin fix schema generation for fields annotated as : dict , : list , : tuple and : set , #330 & #335 by @nkonin add support for constrained strings as dict keys in schema, #332 by @tiangolo support for passing Config class in dataclasses decorator, #276 by @jarekkar ( breaking change : this supersedes the validate_assignment argument with config ) support for nested dataclasses, #334 by @samuelcolvin better errors when getting an ImportError with PyObject , #309 by @samuelcolvin rename get_validators to __get_validators__ , deprecation warning on use of old name, #338 by @samuelcolvin support ClassVar by excluding such attributes from fields, #184 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0170-2018-12-27","title":"Changelog - v0.17.0 (2018-12-27)","objectID":"/latest/changelog/#v0170-2018-12-27","rank":-940},{"content":"fix create_model to correctly use the passed __config__ , #320 by @hugoduncan","pageID":"Changelog","abs_url":"/latest/changelog/#v0161-2018-12-10","title":"Changelog - v0.16.1 (2018-12-10)","objectID":"/latest/changelog/#v0161-2018-12-10","rank":-945},{"content":"breaking change : refactor schema generation to be compatible with JSON Schema and OpenAPI specs, #308 by @tiangolo add schema to schema module to generate top-level schemas from base models, #308 by @tiangolo add additional fields to Schema class to declare validation for str and numeric values, #311 by @tiangolo rename _schema to schema on fields, #318 by @samuelcolvin add case_insensitive option to BaseSettings Config , #277 by @jasonkuhrt","pageID":"Changelog","abs_url":"/latest/changelog/#v0160-2018-12-03","title":"Changelog - v0.16.0 (2018-12-03)","objectID":"/latest/changelog/#v0160-2018-12-03","rank":-950},{"content":"move codebase to use black, #287 by @samuelcolvin fix alias use in settings, #286 by @jasonkuhrt and @samuelcolvin fix datetime parsing in parse_date , #298 by @samuelcolvin allow dataclass inheritance, fix #293 by @samuelcolvin fix PyObject = None , fix #305 by @samuelcolvin allow Pattern type, fix #303 by @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0150-2018-11-18","title":"Changelog - v0.15.0 (2018-11-18)","objectID":"/latest/changelog/#v0150-2018-11-18","rank":-955},{"content":"dataclasses decorator, #269 by @Gaunt and @samuelcolvin","pageID":"Changelog","abs_url":"/latest/changelog/#v0140-2018-10-02","title":"Changelog - v0.14.0 (2018-10-02)","objectID":"/latest/changelog/#v0140-2018-10-02","rank":-960},{"content":"fix issue where int_validator doesn't cast a bool to an int #264 by @nphyatt add deep copy support for BaseModel.copy() #249 , @gangefors","pageID":"Changelog","abs_url":"/latest/changelog/#v0131-2018-09-21","title":"Changelog - v0.13.1 (2018-09-21)","objectID":"/latest/changelog/#v0131-2018-09-21","rank":-965},{"content":"raise an exception if a field's name shadows an existing BaseModel attribute #242 add UrlStr and urlstr types #236 timedelta json encoding ISO8601 and total seconds, custom json encoders #247 , by @cfkanesan and @samuelcolvin allow timedelta objects as values for properties of type timedelta (matches datetime etc. behavior) #247","pageID":"Changelog","abs_url":"/latest/changelog/#v0130-2018-08-25","title":"Changelog - v0.13.0 (2018-08-25)","objectID":"/latest/changelog/#v0130-2018-08-25","rank":-970},{"content":"fix schema generation for fields defined using typing.Any #237","pageID":"Changelog","abs_url":"/latest/changelog/#v0121-2018-07-31","title":"Changelog - v0.12.1 (2018-07-31)","objectID":"/latest/changelog/#v0121-2018-07-31","rank":-975},{"content":"add by_alias argument in .dict() and .json() model methods #205 add Json type support #214 support tuples #227 major improvements and changes to schema #213","pageID":"Changelog","abs_url":"/latest/changelog/#v0120-2018-07-31","title":"Changelog - v0.12.0 (2018-07-31)","objectID":"/latest/changelog/#v0120-2018-07-31","rank":-980},{"content":"add NewType support #115 fix list , set & tuple validation #225 separate out validate_model method, allow errors to be returned along with valid values #221","pageID":"Changelog","abs_url":"/latest/changelog/#v0112-2018-07-05","title":"Changelog - v0.11.2 (2018-07-05)","objectID":"/latest/changelog/#v0112-2018-07-05","rank":-985},{"content":"support Python 3.7 #216 , thanks @layday Allow arbitrary types in model #209 , thanks @oldPadavan","pageID":"Changelog","abs_url":"/latest/changelog/#v0111-2018-07-02","title":"Changelog - v0.11.1 (2018-07-02)","objectID":"/latest/changelog/#v0111-2018-07-02","rank":-990},{"content":"make list , tuple and set types stricter #86 breaking change : remove msgpack parsing #201 add FilePath and DirectoryPath types #10 model schema generation #190 JSON serialization of models and schemas #133","pageID":"Changelog","abs_url":"/latest/changelog/#v0110-2018-06-28","title":"Changelog - v0.11.0 (2018-06-28)","objectID":"/latest/changelog/#v0110-2018-06-28","rank":-995},{"content":"add Config.allow_population_by_alias #160 , thanks @bendemaree breaking change : new errors format #179 , thanks @Gr1N breaking change : removed Config.min_number_size and Config.max_number_size #183 , thanks @Gr1N breaking change : correct behaviour of lt and gt arguments to conint etc. #188 for the old behaviour use le and ge #194 , thanks @jaheba added error context and ability to redefine error message templates using Config.error_msg_templates #183 ,\n  thanks @Gr1N fix typo in validator exception #150 copy defaults to model values, so different models don't share objects #154","pageID":"Changelog","abs_url":"/latest/changelog/#v0100-2018-06-11","title":"Changelog - v0.10.0 (2018-06-11)","objectID":"/latest/changelog/#v0100-2018-06-11","rank":-1000},{"content":"allow custom get_field_config on config classes #159 add UUID1 , UUID3 , UUID4 and UUID5 types #167 , thanks @Gr1N modify some inconsistent docstrings and annotations #173 , thanks @YannLuo fix type annotations for exotic types #171 , thanks @Gr1N Reuse type validators in exotic types #171 scheduled monthly requirements updates #168 add Decimal , ConstrainedDecimal and condecimal types #170 , thanks @Gr1N","pageID":"Changelog","abs_url":"/latest/changelog/#v091-2018-05-10","title":"Changelog - v0.9.1 (2018-05-10)","objectID":"/latest/changelog/#v091-2018-05-10","rank":-1005},{"content":"tweak email-validator import error message #145 fix parse error of parse_date() and parse_datetime() when input is 0 #144 , thanks @YannLuo add Config.anystr_strip_whitespace and strip_whitespace kwarg to constr ,\n  by default values is False #163 , thanks @Gr1N add ConstrainedFloat , confloat , PositiveFloat and NegativeFloat types #166 , thanks @Gr1N","pageID":"Changelog","abs_url":"/latest/changelog/#v090-2018-04-28","title":"Changelog - v0.9.0 (2018-04-28)","objectID":"/latest/changelog/#v090-2018-04-28","rank":-1010},{"content":"fix type annotation for inherit_config #139 breaking change : check for invalid field names in validators #140 validate attributes of parent models #141 breaking change : email validation now uses email-validator #142","pageID":"Changelog","abs_url":"/latest/changelog/#v080-2018-03-25","title":"Changelog - v0.8.0 (2018-03-25)","objectID":"/latest/changelog/#v080-2018-03-25","rank":-1015},{"content":"fix bug with create_model modifying the base class","pageID":"Changelog","abs_url":"/latest/changelog/#v071-2018-02-07","title":"Changelog - v0.7.1 (2018-02-07)","objectID":"/latest/changelog/#v071-2018-02-07","rank":-1020},{"content":"added compatibility with abstract base classes (ABCs) #123 add create_model method #113 #125 breaking change : rename .config to .__config__ on a model breaking change : remove deprecated .values() on a model, use .dict() instead remove use of OrderedDict and use simple dict #126 add Config.use_enum_values #127 add wildcard validators of the form @validate('*') #128","pageID":"Changelog","abs_url":"/latest/changelog/#v070-2018-02-06","title":"Changelog - v0.7.0 (2018-02-06)","objectID":"/latest/changelog/#v070-2018-02-06","rank":-1025},{"content":"allow Python date and times objects #122","pageID":"Changelog","abs_url":"/latest/changelog/#v064-2018-02-01","title":"Changelog - v0.6.4 (2018-02-01)","objectID":"/latest/changelog/#v064-2018-02-01","rank":-1030},{"content":"fix direct install without README.rst present","pageID":"Changelog","abs_url":"/latest/changelog/#v063-2017-11-26","title":"Changelog - v0.6.3 (2017-11-26)","objectID":"/latest/changelog/#v063-2017-11-26","rank":-1035},{"content":"errors for invalid validator use safer check for complex models in Settings","pageID":"Changelog","abs_url":"/latest/changelog/#v062-2017-11-13","title":"Changelog - v0.6.2 (2017-11-13)","objectID":"/latest/changelog/#v062-2017-11-13","rank":-1040},{"content":"prevent duplicate validators, #101 add always kwarg to validators, #102","pageID":"Changelog","abs_url":"/latest/changelog/#v061-2017-11-08","title":"Changelog - v0.6.1 (2017-11-08)","objectID":"/latest/changelog/#v061-2017-11-08","rank":-1045},{"content":"assignment validation #94 , thanks petroswork! JSON in environment variables for complex types, #96 add validator decorators for complex validation, #97 depreciate values(...) and replace with .dict(...) , #99","pageID":"Changelog","abs_url":"/latest/changelog/#v060-2017-11-07","title":"Changelog - v0.6.0 (2017-11-07)","objectID":"/latest/changelog/#v060-2017-11-07","rank":-1050},{"content":"add UUID validation #89 remove index and track from error object (json) if they're null #90 improve the error text when a list is provided rather than a dict #90 add benchmarks table to docs #91","pageID":"Changelog","abs_url":"/latest/changelog/#v050-2017-10-23","title":"Changelog - v0.5.0 (2017-10-23)","objectID":"/latest/changelog/#v050-2017-10-23","rank":-1055},{"content":"show length in string validation error fix aliases in config during inheritance #55 simplify error display use unicode ellipsis in truncate add parse_obj , parse_raw and parse_file helper functions #58 switch annotation only fields to come first in fields list not last","pageID":"Changelog","abs_url":"/latest/changelog/#v040-2017-07-08","title":"Changelog - v0.4.0 (2017-07-08)","objectID":"/latest/changelog/#v040-2017-07-08","rank":-1060},{"content":"immutable models via config.allow_mutation = False , associated cleanup and performance improvement #44 immutable helper methods construct() and copy() #53 allow pickling of models #53 setattr is removed as __setattr__ is now intelligent #44 raise_exception removed, Models now always raise exceptions #44 instance method validators removed django-restful-framework benchmarks added #47 fix inheritance bug #49 make str type stricter so list, dict etc are not coerced to strings. #52 add StrictStr which only always strings as input #52","pageID":"Changelog","abs_url":"/latest/changelog/#v030-2017-06-21","title":"Changelog - v0.3.0 (2017-06-21)","objectID":"/latest/changelog/#v030-2017-06-21","rank":-1065},{"content":"pypi and travis together messed up the deploy of v0.2 this should fix it","pageID":"Changelog","abs_url":"/latest/changelog/#v021-2017-06-07","title":"Changelog - v0.2.1 (2017-06-07)","objectID":"/latest/changelog/#v021-2017-06-07","rank":-1070},{"content":"breaking change : values() on a model is now a method not a property,\n  takes include and exclude arguments allow annotation only fields to support mypy add pretty to_string(pretty=True) method for models","pageID":"Changelog","abs_url":"/latest/changelog/#v020-2017-06-07","title":"Changelog - v0.2.0 (2017-06-07)","objectID":"/latest/changelog/#v020-2017-06-07","rank":-1075},{"content":"add docs add history","pageID":"Changelog","abs_url":"/latest/changelog/#v010-2017-06-03","title":"Changelog - v0.1.0 (2017-06-03)","objectID":"/latest/changelog/#v010-2017-06-03","rank":-1080},{"content":"We'd love you to contribute to Pydantic!","pageID":"Contributing","abs_url":"/latest/contributing/#Contributing","title":"Contributing","objectID":"/latest/contributing/#Contributing","rank":100},{"content":"Questions, feature requests and bug reports are all welcome as discussions or issues . However, to report a security vulnerability, please see our security policy . To make it as simple as possible for us to help you, please include the output of the following call in your issue: python -c \"import pydantic.version; print(pydantic.version.version_info())\" If you're using Pydantic prior to v2.0 please use: python -c \"import pydantic.utils; print(pydantic.utils.version_info())\" Please try to always include the above unless you're unable to install Pydantic or know it's not relevant\nto your question or feature request.","pageID":"Contributing","abs_url":"/latest/contributing/#issues","title":"Contributing - Issues","objectID":"/latest/contributing/#issues","rank":95},{"content":"It should be extremely simple to get started and create a Pull Request.\nPydantic is released regularly so you should see your improvements release in a matter of days or weeks 🚀. Unless your change is trivial (typo, docs tweak etc.), please create an issue to discuss the change before\ncreating a pull request. Pydantic V1 is in maintenance mode Pydantic v1 is in maintenance mode, meaning that only bug fixes and security fixes will be accepted.\nNew features should be targeted at Pydantic v2. To submit a fix to Pydantic v1, use the 1.10.X-fixes as a target branch. If you're looking for something to get your teeth into, check out the \"help wanted\" label on github. To make contributing as easy and fast as possible, you'll want to run tests and linting locally. Luckily,\nPydantic has few dependencies, doesn't require compiling and tests don't need access to databases, etc.\nBecause of this, setting up and running the tests should be very simple. Tip tl;dr : use make format to fix formatting, make to run tests and linting and make docs to build the docs.","pageID":"Contributing","abs_url":"/latest/contributing/#pull-requests","title":"Contributing - Pull Requests","objectID":"/latest/contributing/#pull-requests","rank":90},{"content":"You'll need the following prerequisites: Any Python version between Python 3.9 and 3.12 uv or other virtual environment tool git make","pageID":"Contributing","abs_url":"/latest/contributing/#prerequisites","title":"Contributing - Pull Requests - Prerequisites","objectID":"/latest/contributing/#prerequisites","rank":85},{"content":"Fork the repository on GitHub and clone your fork locally. # Clone your fork and cd into the repo directory git clone git@github.com:<your username>/pydantic.git cd pydantic # Install UV and pre-commit # We use pipx here, for other options see: # https://docs.astral.sh/uv/getting-started/installation/ # https://pre-commit.com/#install # To get pipx itself: # https://pypa.github.io/pipx/ pipx install uv\npipx install pre-commit # Install pydantic, dependencies, test dependencies and doc dependencies make install","pageID":"Contributing","abs_url":"/latest/contributing/#installation-and-setup","title":"Contributing - Pull Requests - Installation and setup","objectID":"/latest/contributing/#installation-and-setup","rank":80},{"content":"Create a new branch for your changes. # Checkout a new branch and make your changes git checkout -b my-new-feature-branch # Make your changes...","pageID":"Contributing","abs_url":"/latest/contributing/#check-out-a-new-branch-and-make-your-changes","title":"Contributing - Pull Requests - Check out a new branch and make your changes","objectID":"/latest/contributing/#check-out-a-new-branch-and-make-your-changes","rank":75},{"content":"Run tests and linting locally to make sure everything is working as expected. # Run automated code formatting and linting make format # Pydantic uses ruff, an awesome Python linter written in rust # https://github.com/astral-sh/ruff # Run tests and linting make # There are a few sub-commands in Makefile like `test`, `testcov` and `lint` # which you might want to use, but generally just `make` should be all you need. # You can run `make help` to see more options.","pageID":"Contributing","abs_url":"/latest/contributing/#run-tests-and-linting","title":"Contributing - Pull Requests - Run tests and linting","objectID":"/latest/contributing/#run-tests-and-linting","rank":70},{"content":"If you've made any changes to the documentation (including changes to function signatures, class definitions, or docstrings that will appear in the API documentation), make sure it builds successfully. We use mkdocs-material[imaging] to support social previews.\nYou can find directions on how to install the required dependencies here . # Build documentation make docs # If you have changed the documentation, make sure it builds successfully. # You can also use `uv run mkdocs serve` to serve the documentation at localhost:8000 If this isn't working due to issues with the imaging plugin, try commenting out the social plugin line in mkdocs.yml and running make docs again. Updating the documentation ¶ We push a new version of the documentation with each minor release, and we push to a dev path with each commit to main . If you're updating the documentation out of cycle with a minor release and want your changes to be reflected on latest ,\ndo the following: Open a PR against main with your docs changes Once the PR is merged, checkout the docs-update branch. This branch should be up to date with the latest patch release.\nFor example, if the latest release is v2.9.2 , you should make sure docs-update is up to date with the v2.9.2 tag. Checkout a new branch from docs-update and cherry-pick your changes onto this branch. Push your changes and open a PR against docs-update . Once the PR is merged, the new docs will be built and deployed. Note Maintainer shortcut - as a maintainer, you can skip the second PR and just cherry pick directly onto the docs-update branch.","pageID":"Contributing","abs_url":"/latest/contributing/#build-documentation","title":"Contributing - Pull Requests - Build documentation","objectID":"/latest/contributing/#build-documentation","rank":65},{"content":"Commit your changes, push your branch to GitHub, and create a pull request. Please follow the pull request template and fill in as much information as possible. Link to any relevant issues and include a description of your changes. When your pull request is ready for review, add a comment with the message \"please review\" and we'll take a look as soon as we can.","pageID":"Contributing","abs_url":"/latest/contributing/#commit-and-push-your-changes","title":"Contributing - Pull Requests - Commit and push your changes","objectID":"/latest/contributing/#commit-and-push-your-changes","rank":60},{"content":"Documentation is written in Markdown and built using Material for MkDocs . API documentation is build from docstrings using mkdocstrings .","pageID":"Contributing","abs_url":"/latest/contributing/#documentation-style","title":"Contributing - Documentation style","objectID":"/latest/contributing/#documentation-style","rank":55},{"content":"When contributing to Pydantic, please make sure that all code is well documented. The following should be documented using properly formatted docstrings: Modules Class definitions Function definitions Module-level variables Pydantic uses Google-style docstrings formatted according to PEP 257 guidelines. (See Example Google Style Python Docstrings for further examples.) pydocstyle is used for linting docstrings. You can run make format to check your docstrings. Where this is a conflict between Google-style docstrings and pydocstyle linting, follow the pydocstyle linting hints. Class attributes and function arguments should be documented in the format \"name: description.\" When applicable, a return type should be documented with just a description. Types are inferred from the signature. class Foo:\n    \"\"\"A class docstring.\n\n    Attributes:\n        bar: A description of bar. Defaults to \"bar\".\n    \"\"\"\n\n    bar: str = 'bar' def bar(self, baz: int) -> str:\n    \"\"\"A function docstring.\n\n    Args:\n        baz: A description of `baz`.\n\n    Returns:\n        A description of the return value.\n    \"\"\"\n\n    return 'bar' You may include example code in docstrings. This code should be complete, self-contained, and runnable. Docstring examples are tested, so make sure they are correct and complete. See  for an example. Class and instance attributes Class attributes should be documented in the class docstring. Instance attributes should be documented as \"Args\" in the __init__ docstring.","pageID":"Contributing","abs_url":"/latest/contributing/#code-documentation","title":"Contributing - Documentation style - Code documentation","objectID":"/latest/contributing/#code-documentation","rank":50},{"content":"In general, documentation should be written in a friendly, approachable style. It should be easy to read and understand, and should be as concise as possible while still being complete. Code examples are encouraged, but should be kept short and simple. However, every code example should be complete, self-contained, and runnable. (If you're not sure how to do this, ask for help!) We prefer print output to naked asserts, but if you're testing something that doesn't have a useful print output, asserts are fine. Pydantic's unit test will test all code examples in the documentation, so it's important that they are correct and complete. When adding a new code example, use the following to test examples and update their formatting and output: # Run tests and update code examples pytest tests/test_docs.py --update-examples","pageID":"Contributing","abs_url":"/latest/contributing/#documentation-style_1","title":"Contributing - Documentation style - Documentation Style","objectID":"/latest/contributing/#documentation-style_1","rank":45},{"content":"If you're working with pydantic and pydantic-core , you might find it helpful to debug Python and Rust code together.\nHere's a quick guide on how to do that. This tutorial is done in VSCode, but you can use similar steps in other IDEs.","pageID":"Contributing","abs_url":"/latest/contributing/#debugging-python-and-rust","title":"Contributing - Debugging Python and Rust","objectID":"/latest/contributing/#debugging-python-and-rust","rank":40},{"content":"Pydantic has a badge that you can use to show that your project uses Pydantic. You can use this badge in your README.md :","pageID":"Contributing","abs_url":"/latest/contributing/#badges","title":"Contributing - Badges","objectID":"/latest/contributing/#badges","rank":35},{"content":"[ ![Pydantic v1 ]( https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v1.json )](https://pydantic.dev)\n\n[ ![Pydantic v2 ]( https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json )](https://pydantic.dev)","pageID":"Contributing","abs_url":"/latest/contributing/#with-markdown","title":"Contributing - Badges - With Markdown","objectID":"/latest/contributing/#with-markdown","rank":30},{"content":".. image :: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v1.json :target: https://pydantic.dev :alt: Pydantic .. image :: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json :target: https://pydantic.dev :alt: Pydantic","pageID":"Contributing","abs_url":"/latest/contributing/#with-restructuredtext","title":"Contributing - Badges - With reStructuredText","objectID":"/latest/contributing/#with-restructuredtext","rank":25},{"content":"< a href = \"https://pydantic.dev\" >< img src = \"https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v1.json\" alt = \"Pydantic Version 1\" style = \"max-width:100%;\" ></ a > < a href = \"https://pydantic.dev\" >< img src = \"https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json\" alt = \"Pydantic Version 2\" style = \"max-width:100%;\" ></ a >","pageID":"Contributing","abs_url":"/latest/contributing/#with-html","title":"Contributing - Badges - With HTML","objectID":"/latest/contributing/#with-html","rank":20},{"content":"To be able to identify regressions early during development, Pydantic runs tests on various third-party projects\nusing Pydantic. We consider adding support for testing new open source projects (that rely heavily on Pydantic) if your said project matches some of the following criteria: The project is actively maintained. The project makes use of Pydantic internals (e.g. relying on the  metaclass, typing utilities). The project is popular enough (although small projects can still be included depending on how Pydantic is being used). The project CI is simple enough to be ported into Pydantic's testing workflow. If your project meets some of these criteria, you can open feature request to discuss the inclusion of your project.","pageID":"Contributing","abs_url":"/latest/contributing/#adding-your-library-as-part-of-pydantics-third-party-test-suite","title":"Contributing - Adding your library as part of Pydantic's third party test suite","objectID":"/latest/contributing/#adding-your-library-as-part-of-pydantics-third-party-test-suite","rank":15},{"content":"If you need help getting started with Pydantic or with advanced usage, the following sources may be useful.","pageID":"Help with Pydantic","abs_url":"/latest/help_with_pydantic/#getting-help-with-pydantic","title":"Help with Pydantic","objectID":"/latest/help_with_pydantic/#getting-help-with-pydantic","rank":100},{"content":"The usage documentation is the most complete guide on how to use Pydantic.","pageID":"Help with Pydantic","abs_url":"/latest/help_with_pydantic/#usage-documentation","title":"Help with Pydantic - Usage Documentation","objectID":"/latest/help_with_pydantic/#usage-documentation","rank":95},{"content":"The API documentation gives reference docs for all public Pydantic APIs.","pageID":"Help with Pydantic","abs_url":"/latest/help_with_pydantic/#api-documentation","title":"Help with Pydantic - API Documentation","objectID":"/latest/help_with_pydantic/#api-documentation","rank":90},{"content":"GitHub discussions are useful for asking questions, your question and the answer will help everyone.","pageID":"Help with Pydantic","abs_url":"/latest/help_with_pydantic/#github-discussions","title":"Help with Pydantic - GitHub Discussions","objectID":"/latest/help_with_pydantic/#github-discussions","rank":85},{"content":"Use the pydantic tag on Stack Overflow to ask questions, note this is not always monitored by the core Pydantic team.","pageID":"Help with Pydantic","abs_url":"/latest/help_with_pydantic/#stack-overflow","title":"Help with Pydantic - Stack Overflow","objectID":"/latest/help_with_pydantic/#stack-overflow","rank":80},{"content":"Youtube has lots of useful videos on Pydantic . In particular Marcelo Trylesinski's video \"Pydantic V1 to V2 - The Migration\" has helped people a lot when migrating from Pydantic V1 to V2.","pageID":"Help with Pydantic","abs_url":"/latest/help_with_pydantic/#youtube","title":"Help with Pydantic - YouTube","objectID":"/latest/help_with_pydantic/#youtube","rank":75},{"content":"Installation is as simple as: pip uv pip install pydantic uv add pydantic Pydantic has a few dependencies: pydantic-core : Core validation logic for Pydantic written in Rust. typing-extensions : Backport of the standard library  module. annotated-types : Reusable constraint types to use with . If you've got Python 3.9+ and pip installed, you're good to go. Pydantic is also available on conda under the conda-forge channel: conda install pydantic -c conda-forge","pageID":"Installation","abs_url":"/latest/install/#Installation","title":"Installation","objectID":"/latest/install/#Installation","rank":100},{"content":"Pydantic has the following optional dependencies: email : Email validation provided by the email-validator package. timezone : Fallback IANA time zone database provided by the tzdata package. To install optional dependencies along with Pydantic: pip uv # with the `email` extra: pip install 'pydantic[email]' # or with `email` and `timezone` extras: pip install 'pydantic[email,timezone]' # with the `email` extra: uv add 'pydantic[email]' # or with `email` and `timezone` extras: uv add 'pydantic[email,timezone]' Of course, you can also install requirements manually with pip install email-validator tzdata .","pageID":"Installation","abs_url":"/latest/install/#optional-dependencies","title":"Installation - Optional dependencies","objectID":"/latest/install/#optional-dependencies","rank":95},{"content":"And if you prefer to install Pydantic directly from the repository: pip uv pip install 'git+https://github.com/pydantic/pydantic@main' # or with `email` and `timezone` extras: pip install 'git+https://github.com/pydantic/pydantic@main#egg=pydantic[email,timezone]' uv add 'git+https://github.com/pydantic/pydantic@main' # or with `email` and `timezone` extras: uv add 'git+https://github.com/pydantic/pydantic@main#egg=pydantic[email,timezone]'","pageID":"Installation","abs_url":"/latest/install/#install-from-repository","title":"Installation - Install from repository","objectID":"/latest/install/#install-from-repository","rank":90},{"content":"Pydantic V2 introduces a number of changes to the API, including some breaking changes. This page provides a guide highlighting the most\nimportant changes to help you migrate your code from Pydantic V1 to Pydantic V2.","pageID":"Migration Guide","abs_url":"/latest/migration/#Migration Guide","title":"Migration Guide","objectID":"/latest/migration/#Migration Guide","rank":100},{"content":"Pydantic V2 is now the current production release of Pydantic.\nYou can install Pydantic V2 from PyPI: pip install -U pydantic If you encounter any issues, please create an issue in GitHub using\nthe bug V2 label. This will help us to actively monitor and track errors, and to continue to improve the library's\nperformance. If you need to use latest Pydantic V1 for any reason, see the Continue using Pydantic V1 features section below for details on installation and imports from pydantic.v1 .","pageID":"Migration Guide","abs_url":"/latest/migration/#install-pydantic-v2","title":"Migration Guide - Install Pydantic V2","objectID":"/latest/migration/#install-pydantic-v2","rank":95},{"content":"We have created a tool to help you migrate your code. This tool is still in beta, but we hope it will help you to\nmigrate your code more quickly. You can install the tool from PyPI: pip install bump-pydantic The usage is simple. If your project structure is: * repo_folder\n    * my_package\n        * <python source files> ... Then you'll want to do: cd /path/to/repo_folder\nbump-pydantic my_package See more about it on the Bump Pydantic repository.","pageID":"Migration Guide","abs_url":"/latest/migration/#code-transformation-tool","title":"Migration Guide - Code transformation tool","objectID":"/latest/migration/#code-transformation-tool","rank":90},{"content":"Pydantic V1 is still available when you need it, though we recommend migrating to\nPydantic V2 for its improvements and new features. If you need to use latest Pydantic V1, you can install it with: pip install \"pydantic==1.*\" The Pydantic V2 package also continues to provide access to the Pydantic V1 API\nby importing through pydantic.v1 . For example, you can use the BaseModel class from Pydantic V1 instead of the\nPydantic V2 pydantic.BaseModel class: from pydantic.v1 import BaseModel You can also import functions that have been removed from Pydantic V2, such as lenient_isinstance : from pydantic.v1.utils import lenient_isinstance Pydantic V1 documentation is available at https://docs.pydantic.dev/1.10/ .","pageID":"Migration Guide","abs_url":"/latest/migration/#continue-using-pydantic-v1-features","title":"Migration Guide - Continue using Pydantic V1 features","objectID":"/latest/migration/#continue-using-pydantic-v1-features","rank":85},{"content":"As of pydantic>=1.10.17 , the pydantic.v1 namespace can be used within V1.\nThis makes it easier to migrate to V2, which also supports the pydantic.v1 namespace. In order to unpin a pydantic<2 dependency and continue using V1\nfeatures, take the following steps: Replace pydantic<2 with pydantic>=1.10.17 Find and replace all occurrences of: from pydantic. import with: from pydantic.v1. import Here's how you can import pydantic 's v1 features based on your version of pydantic : pydantic>=1.10.17,<3 pydantic<3 As of v1.10.17 the .v1 namespace is available in V1, allowing imports as below: from pydantic.v1.fields import ModelField All versions of Pydantic V1 and V2 support the following import pattern, in case you don't\nknow which version of Pydantic you are using: try:\n    from pydantic.v1.fields import ModelField\nexcept ImportError:\n    from pydantic.fields import ModelField Note When importing modules using pydantic>=1.10.17,<2 with the .v1 namespace\nthese modules will not be the same module as the same import without the .v1 namespace, but the symbols imported will be. For example pydantic.v1.fields is not pydantic.fields but pydantic.v1.fields.ModelField is pydantic.fields.ModelField . Luckily, this is not likely to be relevant\nin the vast majority of cases. It's just an unfortunate consequence of providing a smoother migration experience.","pageID":"Migration Guide","abs_url":"/latest/migration/#using-pydantic-v1-features-in-a-v1v2-environment","title":"Migration Guide - Continue using Pydantic V1 features - Using Pydantic v1 features in a v1/v2 environment","objectID":"/latest/migration/#using-pydantic-v1-features-in-a-v1v2-environment","rank":80},{"content":"The following sections provide details on the most important changes in Pydantic V2.","pageID":"Migration Guide","abs_url":"/latest/migration/#migration-guide","title":"Migration Guide - Migration guide","objectID":"/latest/migration/#migration-guide","rank":75},{"content":"Various method names have been changed; all non-deprecated BaseModel methods now have names matching either the\nformat model_.* or __.*pydantic.*__ . Where possible, we have retained the deprecated methods with their old names\nto help ease migration, but calling them will emit DeprecationWarning s. Pydantic V1 Pydantic V2 __fields__ model_fields __private_attributes__ __pydantic_private__ __validators__ __pydantic_validator__ construct() model_construct() copy() model_copy() dict() model_dump() json_schema() model_json_schema() json() model_dump_json() parse_obj() model_validate() update_forward_refs() model_rebuild() Some of the built-in data-loading functionality has been slated for removal. In particular, parse_raw and parse_file are now deprecated. In Pydantic V2, model_validate_json works like parse_raw . Otherwise, you should load the data and then pass it to model_validate . The from_orm method has been deprecated; you can now just use model_validate (equivalent to parse_obj from\n  Pydantic V1) to achieve something similar, as long as you've set from_attributes=True in the model config. The __eq__ method has changed for models. Models can only be equal to other BaseModel instances. For two model instances to be equal, they must have the same: Type (or, in the case of generic models, non-parametrized generic origin type) Field values Extra values (only relevant when model_config['extra'] == 'allow' ) Private attribute values; models with different values of private attributes are no longer equal. Models are no longer equal to the dicts containing their data. Non-generic models of different types are never equal. Generic models with different origin types are never equal. We don't require exact type equality so that,\n    for example, instances of MyGenericModel[Any] could be equal to instances of MyGenericModel[int] . We have replaced the use of the __root__ field to specify a \"custom root model\" with a new type called RootModel which is intended to replace the functionality of\n    using a field called __root__ in Pydantic V1. Note, RootModel types no longer support the arbitrary_types_allowed config setting. See this issue comment for an explanation. We have significantly expanded Pydantic's capabilities related to customizing serialization. In particular, we have\n    added the @field_serializer , @model_serializer , and @computed_field decorators, which each address various\n    shortcomings from Pydantic V1. See Custom serializers for the usage docs of these new decorators. Due to performance overhead and implementation complexity, we have now deprecated support for specifying json_encoders in the model config. This functionality was originally added for the purpose of achieving custom\n    serialization logic, and we think the new serialization decorators are a better choice in most common scenarios. We have changed the behavior related to serializing subclasses of models when they occur as nested fields in a parent\n  model. In V1, we would always include all fields from the subclass instance. In V2, when we dump a model, we only\n  include the fields that are defined on the annotated type of the field. This helps prevent some accidental security\n  bugs. You can read more about this (including how to opt out of this behavior) in the relevant section of the model exporting docs. GetterDict has been removed as it was just an implementation detail of orm_mode , which has been removed. In many cases, arguments passed to the constructor will be copied in order to perform validation and, where necessary, coercion.\n  This is notable in the case of passing mutable objects as arguments to a constructor.\n  You can see an example + more detail here . The .json() method is deprecated, and attempting to use this deprecated method with arguments such as indent or ensure_ascii may lead to confusing errors. For best results, switch to V2's equivalent, model_dump_json() .\nIf you'd still like to use said arguments, you can use this workaround . JSON serialization of non-string key values is generally done with str(key) , leading to some changes in behavior such as the following: from typing import Optional\n\nfrom pydantic import BaseModel as V2BaseModel\nfrom pydantic.v1 import BaseModel as V1BaseModel\n\n\nclass V1Model(V1BaseModel):\n    a: dict[Optional[str], int]\n\n\nclass V2Model(V2BaseModel):\n    a: dict[Optional[str], int]\n\n\nv1_model = V1Model(a={None: 123})\nv2_model = V2Model(a={None: 123})\n\n# V1\nprint(v1_model.json())\n#> {\"a\": {\"null\": 123}}\n\n# V2\nprint(v2_model.model_dump_json())\n#> {\"a\":{\"None\":123}} model_dump_json() results are compacted in order to save space, and don't always exactly match that of json.dumps() output.\nThat being said, you can easily modify the separators used in json.dumps() results in order to align the two outputs: import json\n\nfrom pydantic import BaseModel as V2BaseModel\nfrom pydantic.v1 import BaseModel as V1BaseModel\n\n\nclass V1Model(V1BaseModel):\n    a: list[str]\n\n\nclass V2Model(V2BaseModel):\n    a: list[str]\n\n\nv1_model = V1Model(a=['fancy', 'sushi'])\nv2_model = V2Model(a=['fancy', 'sushi'])\n\n# V1\nprint(v1_model.json())\n#> {\"a\": [\"fancy\", \"sushi\"]}\n\n# V2\nprint(v2_model.model_dump_json())\n#> {\"a\":[\"fancy\",\"sushi\"]}\n\n# Plain json.dumps\nprint(json.dumps(v2_model.model_dump()))\n#> {\"a\": [\"fancy\", \"sushi\"]}\n\n# Modified json.dumps\nprint(json.dumps(v2_model.model_dump(), separators=(',', ':')))\n#> {\"a\":[\"fancy\",\"sushi\"]}","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-pydanticbasemodel","title":"Migration Guide - Migration guide - Changes to pydantic.BaseModel","objectID":"/latest/migration/#changes-to-pydanticbasemodel","rank":70},{"content":"The pydantic.generics.GenericModel class is no longer necessary, and has been removed. Instead, you can now\ncreate generic BaseModel subclasses by just adding Generic as a parent class on a BaseModel subclass directly.\nThis looks like class MyGenericModel(BaseModel, Generic[T]): ... . Mixing of V1 and V2 models is not supported which means that type parameters of such generic BaseModel (V2)\ncannot be V1 models. While it may not raise an error, we strongly advise against using parametrized generics in isinstance checks. For example, you should not do isinstance(my_model, MyGenericModel[int]) .\n    However, it is fine to do isinstance(my_model, MyGenericModel) . (Note that for standard generics, it would raise\n    an error to do a subclass check with a parameterized generic.) If you need to perform isinstance checks against parametrized generics, you can do this by subclassing the\n    parametrized generic class. This looks like class MyIntModel(MyGenericModel[int]): ... and isinstance(my_model, MyIntModel) . Find more information in the Generic models documentation.","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-pydanticgenericsgenericmodel","title":"Migration Guide - Migration guide - Changes to pydantic.generics.GenericModel","objectID":"/latest/migration/#changes-to-pydanticgenericsgenericmodel","rank":65},{"content":"Field no longer supports arbitrary keyword arguments to be added to the JSON schema. Instead, any extra\ndata you want to add to the JSON schema should be passed as a dictionary to the json_schema_extra keyword argument. In Pydantic V1, the alias property returns the field's name when no alias is set.\nIn Pydantic V2, this behavior has changed to return None when no alias is set. The following properties have been removed from or changed in Field : const min_items (use min_length instead) max_items (use max_length instead) unique_items allow_mutation (use frozen instead) regex (use pattern instead) final (use the  type hint instead) Field constraints are no longer automatically pushed down to the parameters of generics.  For example, you can no longer validate every element of a list matches a regex by providing my_list: list[str] = Field(pattern=\".*\") .  Instead, use  to provide an annotation on the str itself: my_list: list[Annotated[str, Field(pattern=\".*\")]]","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-pydanticfield","title":"Migration Guide - Migration guide - Changes to pydantic.Field","objectID":"/latest/migration/#changes-to-pydanticfield","rank":60},{"content":"Pydantic dataclasses continue to be useful for enabling the data validation on standard\ndataclasses without having to subclass BaseModel . Pydantic V2 introduces the following changes to this dataclass behavior: When used as fields, dataclasses (Pydantic or vanilla) no longer accept tuples as validation inputs; dicts should be\n  used instead. The __post_init__ in Pydantic dataclasses will now be called after validation, rather than before. As a result, the __post_init_post_parse__ method would have become redundant, so has been removed. Pydantic no longer supports extra='allow' for Pydantic dataclasses, where extra fields passed to the initializer would be\n    stored as extra attributes on the dataclass. extra='ignore' is still supported for the purpose of ignoring\n    unexpected fields while parsing data, they just won't be stored on the instance. Pydantic dataclasses no longer have an attribute __pydantic_model__ , and no longer use an underlying BaseModel to perform validation or provide other functionality. To perform validation, generate a JSON schema, or make use of\n    any other functionality that may have required __pydantic_model__ in V1, you should now wrap the dataclass\n    with a  ( discussed more below ) and\n    make use of its methods. In Pydantic V1, if you used a vanilla (i.e., non-Pydantic) dataclass as a field, the config of the parent type would\n    be used as though it was the config for the dataclass itself as well. In Pydantic V2, this is no longer the case. In Pydantic V2, to override the config (like you would with model_config on a BaseModel ),\n    you can use the config parameter on the @dataclass decorator.\n    See Dataclass Config for examples.","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-dataclasses","title":"Migration Guide - Migration guide - Changes to dataclasses","objectID":"/latest/migration/#changes-to-dataclasses","rank":55},{"content":"In Pydantic V2, to specify config on a model, you should set a class attribute called model_config to be a dict\n  with the key/value pairs you want to be used as the config. The Pydantic V1 behavior to create a class called Config in the namespace of the parent BaseModel subclass is now deprecated. When subclassing a model, the model_config attribute is inherited. This is helpful in the case where you'd like to use\na base class with a given configuration for many models. Note, if you inherit from multiple BaseModel subclasses,\nlike class MyModel(Model1, Model2) , the non-default settings in the model_config attribute from the two models\nwill be merged, and for any settings defined in both, those from Model2 will override those from Model1 . The following config settings have been removed: allow_mutation — this has been removed. You should be able to use frozen equivalently (inverse of current use). error_msg_templates fields — this was the source of various bugs, so has been removed.\n  You should be able to use Annotated on fields to modify them as desired. getter_dict — orm_mode has been removed, and this implementation detail is no longer necessary. smart_union - the default union_mode in Pydantic V2 is 'smart' . underscore_attrs_are_private — the Pydantic V2 behavior is now the same as if this was always set\n  to True in Pydantic V1. json_loads json_dumps copy_on_model_validation post_init_call The following config settings have been renamed: allow_population_by_field_name → populate_by_name (or validate_by_name starting in v2.11) anystr_lower → str_to_lower anystr_strip_whitespace → str_strip_whitespace anystr_upper → str_to_upper keep_untouched → ignored_types max_anystr_length → str_max_length min_anystr_length → str_min_length orm_mode → from_attributes schema_extra → json_schema_extra validate_all → validate_default See the  for more details.","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-config","title":"Migration Guide - Migration guide - Changes to config","objectID":"/latest/migration/#changes-to-config","rank":50},{"content":"@validator and @root_validator are deprecated ¶ @validator has been deprecated, and should be replaced with @field_validator , which provides various new features\n    and improvements. The new @field_validator decorator does not have the each_item keyword argument; validators you want to\n    apply to items within a generic container should be added by annotating the type argument. See validators in Annotated metadata for details.\n    This looks like list[Annotated[int, Field(ge=0)]] Even if you keep using the deprecated @validator decorator, you can no longer add the field or config arguments to the signature of validator functions. If you need access to these, you'll need\n    to migrate to @field_validator — see the next section for more details. If you use the always=True keyword argument to a validator function, note that standard validators\n    for the annotated type will also be applied even to defaults, not just the custom validators. For\n    example, despite the fact that the validator below will never error, the following code raises a ValidationError : Note To avoid this, you can use the validate_default argument in the Field function. When set to True , it mimics the behavior of always=True in Pydantic v1. However, the new way of using validate_default is encouraged as it provides more flexibility and control. from pydantic import BaseModel, validator\n\n\nclass Model(BaseModel):\n    x: str = 1\n\n    @validator('x', always=True)\n    @classmethod\n    def validate_x(cls, v):\n        return v\n\n\nModel() @root_validator has been deprecated, and should be replaced with @model_validator , which also provides new features and improvements. Under some circumstances (such as assignment when model_config['validate_assignment'] is True ),\n    the @model_validator decorator will receive an instance of the model, not a dict of values. You may\n    need to be careful to handle this case. Even if you keep using the deprecated @root_validator decorator, due to refactors in validation logic,\n    you can no longer run with skip_on_failure=False (which is the default value of this keyword argument,\n    so must be set explicitly to True ). Changes to @validator 's allowed signatures ¶ In Pydantic V1, functions wrapped by @validator could receive keyword arguments with metadata about what was\nbeing validated. Some of these arguments have been removed from @field_validator in Pydantic V2: config : Pydantic V2's config is now a dictionary instead of a class, which means this argument is no longer\n    backwards compatible. If you need to access the configuration you should migrate to @field_validator and use info.config . field : this argument used to be a ModelField object, which was a quasi-internal class that no longer exists\n    in Pydantic V2. Most of this information can still be accessed by using the field name from info.field_name to index into cls.model_fields from pydantic import BaseModel, ValidationInfo, field_validator\n\n\nclass Model(BaseModel):\n    x: int\n\n    @field_validator('x')\n    def val_x(cls, v: int, info: ValidationInfo) -> int:\n        assert info.config is not None\n        print(info.config.get('title'))\n        #> Model\n        print(cls.model_fields[info.field_name].is_required())\n        #> True\n        return v\n\n\nModel(x=1) TypeError is no longer converted to ValidationError in validators ¶ Previously, when raising a TypeError within a validator function, that error would be wrapped into a ValidationError and, in some cases (such as with FastAPI), these errors might be displayed to end users. This led to a variety of\nundesirable behavior — for example, calling a function with the wrong signature might produce a user-facing ValidationError . However, in Pydantic V2, when a TypeError is raised in a validator, it is no longer converted into a ValidationError : import pytest\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Model(BaseModel):\n    x: int\n\n    @field_validator('x')\n    def val_x(cls, v: int) -> int:\n        return str.lower(v)  # raises a TypeError\n\n\nwith pytest.raises(TypeError):\n    Model(x=1) This applies to all validation decorators. Validator behavior changes ¶ Pydantic V2 includes some changes to type coercion. For example: coercing int , float , and Decimal values to strings is now optional and disabled by default, see\n  . iterable of pairs is no longer coerced to a dict. See the Conversion table for details on Pydantic V2 type coercion defaults. The allow_reuse keyword argument is no longer necessary ¶ Previously, Pydantic tracked \"reused\" functions in decorators as this was a common source of mistakes.\nWe did this by comparing the function's fully qualified name (module name + function name), which could result in false\npositives. The allow_reuse keyword argument could be used to disable this when it was intentional. Our approach to detecting repeatedly defined functions has been overhauled to only error for redefinition within a\nsingle class, reducing false positives and bringing the behavior more in line with the errors that type checkers\nand linters would give for defining a method with the same name multiple times in a single class definition. In nearly all cases, if you were using allow_reuse=True , you should be able to simply delete that keyword argument and\nhave things keep working as expected. @validate_arguments has been renamed to @validate_call ¶ In Pydantic V2, the @validate_arguments decorator has been renamed to @validate_call . In Pydantic V1, the decorated function had various attributes added, such as raw_function , and validate (which could be used to validate arguments without actually calling the decorated function). Due to limited use of\nthese attributes, and performance-oriented changes in implementation, we have not preserved this functionality in @validate_call .","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-validators","title":"Migration Guide - Migration guide - Changes to validators","objectID":"/latest/migration/#changes-to-validators","rank":45},{"content":"In Pydantic V1 we made great efforts to preserve the types of all field inputs for generic collections when they were\nproper subtypes of the field annotations. For example, given the annotation Mapping[str, int] if you passed in a collection.Counter() you'd get a collection.Counter() as the value. Supporting this behavior in V2 would have negative performance implications for the general case\n(we'd have to check types every time) and would add a lot of complexity to validation. Further, even in V1 this behavior\nwas inconsistent and partially broken: it did not work for many types ( str , UUID , etc.), and for generic\ncollections it's impossible to re-build the original input correctly without a lot of special casing\n(consider ChainMap ; rebuilding the input is necessary because we need to replace values after validation, e.g.\nif coercing strings to ints). In Pydantic V2 we no longer attempt to preserve the input type in all cases; instead, we only promise that the output\ntype will match the type annotations. Going back to the Mapping example, we promise the output will be a valid Mapping , and in practice it will be a\nplain dict : from collections.abc import Mapping\n\nfrom pydantic import TypeAdapter\n\n\nclass MyDict(dict):\n    pass\n\n\nta = TypeAdapter(Mapping[str, int])\nv = ta.validate_python(MyDict())\nprint(type(v))\n#> If you want the output type to be a specific type, consider annotating it as such or implementing a custom validator: from collections.abc import Mapping\nfrom typing import Annotated, Any, TypeVar\n\nfrom pydantic import (\n    TypeAdapter,\n    ValidationInfo,\n    ValidatorFunctionWrapHandler,\n    WrapValidator,\n)\n\n\ndef restore_input_type(\n    value: Any, handler: ValidatorFunctionWrapHandler, _info: ValidationInfo\n) -> Any:\n    return type(value)(handler(value))\n\n\nT = TypeVar('T')\nPreserveType = Annotated[T, WrapValidator(restore_input_type)]\n\n\nta = TypeAdapter(PreserveType[Mapping[str, int]])\n\n\nclass MyDict(dict):\n    pass\n\n\nv = ta.validate_python(MyDict())\nassert type(v) is MyDict While we don't promise to preserve input types everywhere, we do preserve them for subclasses of BaseModel ,\nand for dataclasses: import pydantic.dataclasses\nfrom pydantic import BaseModel\n\n\nclass InnerModel(BaseModel):\n    x: int\n\n\nclass OuterModel(BaseModel):\n    inner: InnerModel\n\n\nclass SubInnerModel(InnerModel):\n    y: int\n\n\nm = OuterModel(inner=SubInnerModel(x=1, y=2))\nprint(m)\n#> inner=SubInnerModel(x=1, y=2)\n\n\n@pydantic.dataclasses.dataclass\nclass InnerDataclass:\n    x: int\n\n\n@pydantic.dataclasses.dataclass\nclass SubInnerDataclass(InnerDataclass):\n    y: int\n\n\n@pydantic.dataclasses.dataclass\nclass OuterDataclass:\n    inner: InnerDataclass\n\n\nd = OuterDataclass(inner=SubInnerDataclass(x=1, y=2))\nprint(d)\n#> OuterDataclass(inner=SubInnerDataclass(x=1, y=2))","pageID":"Migration Guide","abs_url":"/latest/migration/#input-types-are-not-preserved","title":"Migration Guide - Migration guide - Input types are not preserved","objectID":"/latest/migration/#input-types-are-not-preserved","rank":40},{"content":"Dicts ¶ Iterables of pairs (which include empty iterables) no longer pass validation for fields of type dict . Unions ¶ While union types will still attempt validation of each choice from left to right, they now preserve the type of the\ninput whenever possible, even if the correct type is not the first choice for which the input would pass validation.\nAs a demonstration, consider the following example: Python 3.9 and above Python 3.10 and above from typing import Union\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    x: Union[int, str]\n\n\nprint(Model(x='1'))\n#> x='1' from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    x: int | str\n\n\nprint(Model(x='1'))\n#> x='1' In Pydantic V1, the printed result would have been x=1 , since the value would pass validation as an int .\nIn Pydantic V2, we recognize that the value is an instance of one of the cases and short-circuit the standard union validation. To revert to the non-short-circuiting left-to-right behavior of V1, annotate the union with Field(union_mode='left_to_right') .\nSee Union Mode for more details. Required, optional, and nullable fields ¶ Pydantic V2 changes some of the logic for specifying whether a field annotated as Optional is required\n(i.e., has no default value) or not (i.e., has a default value of None or any other value of the corresponding type), and now more closely matches the\nbehavior of dataclasses . Similarly, fields annotated as Any no longer have a default value of None . The following table describes the behavior of field annotations in V2: State Field Definition Required, cannot be None f1: str Not required, cannot be None , is 'abc' by default f2: str = 'abc' Required, can be None f3: Optional[str] Not required, can be None , is None by default f4: Optional[str] = None Not required, can be None , is 'abc' by default f5: Optional[str] = 'abc' Required, can be any type (including None ) f6: Any Not required, can be any type (including None ) f7: Any = None Note A field annotated as typing.Optional[T] will be required, and will allow for a value of None .\nIt does not mean that the field has a default value of None . (This is a breaking change from V1.) Note Any default value if provided makes a field not required. Here is a code example demonstrating the above: Python 3.9 and above Python 3.10 and above from typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Foo(BaseModel):\n    f1: str  # required, cannot be None\n    f2: Optional[str]  # required, can be None - same as str | None\n    f3: Optional[str] = None  # not required, can be None\n    f4: str = 'Foobar'  # not required, but cannot be None\n\n\ntry:\n    Foo(f1=None, f2=None, f4='b')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Foo\n    f1\n      Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    \"\"\" from pydantic import BaseModel, ValidationError\n\n\nclass Foo(BaseModel):\n    f1: str  # required, cannot be None\n    f2: str | None  # required, can be None - same as str | None\n    f3: str | None = None  # not required, can be None\n    f4: str = 'Foobar'  # not required, but cannot be None\n\n\ntry:\n    Foo(f1=None, f2=None, f4='b')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Foo\n    f1\n      Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    \"\"\" Patterns / regex on strings ¶ Pydantic V1 used Python's regex library. Pydantic V2 uses the Rust regex crate .\nThis crate is not just a \"Rust version of regular expressions\", it's a completely different approach to regular expressions.\nIn particular, it promises linear time searching of strings in exchange for dropping a couple of features (namely look arounds and backreferences).\nWe believe this is a tradeoff worth making, in particular because Pydantic is used to validate untrusted input where ensuring things don't accidentally run in exponential time depending on the untrusted input is important.\nOn the flipside, for anyone not using these features complex regex validation should be orders of magnitude faster because it's done in Rust and in linear time. If you still want to use Python's regex library, you can use the regex_engine config setting.","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-handling-of-standard-types","title":"Migration Guide - Migration guide - Changes to Handling of Standard Types","objectID":"/latest/migration/#changes-to-handling-of-standard-types","rank":35},{"content":"In V1, whenever a field was annotated as int , any float value would be accepted, which could lead to a potential data\nloss if the float value contains a non-zero decimal part. In V2, type conversion from floats to integers is only allowed\nif the decimal part is zero: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n\nprint(Model(x=10.0))\n#> x=10\ntry:\n    Model(x=10.2)\nexcept ValidationError as err:\n    print(err)\n    \"\"\"\n    1 validation error for Model\n    x\n      Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=10.2, input_type=float]\n    \"\"\"","pageID":"Migration Guide","abs_url":"/latest/migration/#type-conversion-from-floats-to-integers","title":"Migration Guide - Migration guide - Type conversion from floats to integers","objectID":"/latest/migration/#type-conversion-from-floats-to-integers","rank":30},{"content":"Pydantic V1 had weak support for validating or serializing non- BaseModel types. To work with them, you had to either create a \"root\" model or use the utility functions in pydantic.tools (namely, parse_obj_as and schema_of ). In Pydantic V2 this is a lot easier: the  class lets you create an object\nwith methods for validating, serializing, and producing JSON schemas for arbitrary types.\nThis serves as a complete replacement for parse_obj_as and schema_of (which are now deprecated),\nand also covers some of the use cases of \"root\" models. ( RootModel , discussed above , covers the others.) from pydantic import TypeAdapter\n\nadapter = TypeAdapter(list[int])\nassert adapter.validate_python(['1', '2', '3']) == [1, 2, 3]\nprint(adapter.json_schema())\n#> {'items': {'type': 'integer'}, 'type': 'array'} Due to limitations of inferring generic types with common type checkers, to get proper typing in some scenarios, you\nmay need to explicitly specify the generic parameter: from pydantic import TypeAdapter\n\nadapter = TypeAdapter[str | int](str | int)\n... See Type Adapter for more information.","pageID":"Migration Guide","abs_url":"/latest/migration/#introduction-of-typeadapter","title":"Migration Guide - Migration guide - Introduction of TypeAdapter","objectID":"/latest/migration/#introduction-of-typeadapter","rank":25},{"content":"We have completely overhauled the way custom types are defined in pydantic. We have exposed hooks for generating both pydantic-core and JSON schemas, allowing you to get all the performance\nbenefits of Pydantic V2 even when using your own custom types. We have also introduced ways to use  to add custom validation to your own types. The main changes are: __get_validators__ should be replaced with __get_pydantic_core_schema__ .\n  See Custom Data Types for more information. __modify_schema__ becomes __get_pydantic_json_schema__ .\n  See JSON Schema Customization for more information. Additionally, you can use  to modify or provide the __get_pydantic_core_schema__ and __get_pydantic_json_schema__ functions of a type by annotating it, rather than modifying the type itself.\nThis provides a powerful and flexible mechanism for integrating third-party types with Pydantic, and in some cases\nmay help you remove hacks from Pydantic V1 introduced to work around the limitations for custom types. See Custom Data Types for more information.","pageID":"Migration Guide","abs_url":"/latest/migration/#defining-custom-types","title":"Migration Guide - Migration guide - Defining custom types","objectID":"/latest/migration/#defining-custom-types","rank":20},{"content":"We received many requests over the years to make changes to the JSON schemas that pydantic generates. In Pydantic V2, we have tried to address many of the common requests: The JSON schema for Optional fields now indicates that the value null is allowed. The Decimal type is now exposed in JSON schema (and serialized) as a string. The JSON schema no longer preserves namedtuples as namedtuples. The JSON schema we generate by default now targets draft 2020-12 (with some OpenAPI extensions). When they differ, you can now specify if you want the JSON schema representing the inputs to validation,\n    or the outputs from serialization. However, there have been many reasonable requests over the years for changes which we have not chosen to implement. In Pydantic V1, even if you were willing to implement changes yourself, it was very difficult because the JSON schema\ngeneration process involved various recursive function calls; to override one, you'd have to copy and modify the whole\nimplementation. In Pydantic V2, one of our design goals was to make it easier to customize JSON schema generation. To this end, we have\nintroduced the class GenerateJsonSchema ,\nwhich implements the translation of a type's pydantic-core schema into\na JSON schema. By design, this class breaks the JSON schema generation process into smaller methods that can be\neasily overridden in subclasses to modify the \"global\" approach to generating JSON schema. The various methods that can be used to produce JSON schema (such as BaseModel.model_json_schema or TypeAdapter.json_schema ) accept a keyword argument schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema ,\nand you can pass your custom subclass to these methods in order to use your own approach to generating JSON schema. Hopefully this means that if you disagree with any of the choices we've made, or if you are reliant on behaviors in\nPydantic V1 that have changed in Pydantic V2, you can use a custom schema_generator , modifying the GenerateJsonSchema class as necessary for your application.","pageID":"Migration Guide","abs_url":"/latest/migration/#changes-to-json-schema-generation","title":"Migration Guide - Migration guide - Changes to JSON schema generation","objectID":"/latest/migration/#changes-to-json-schema-generation","rank":15},{"content":"BaseSettings , the base object for Pydantic settings management , has been moved to a separate package, pydantic-settings . Also, the parse_env_var classmethod has been removed. So, you need to customise settings sources to have your own parsing function.","pageID":"Migration Guide","abs_url":"/latest/migration/#basesettings-has-moved-to-pydantic-settings","title":"Migration Guide - Migration guide - BaseSettings has moved to pydantic-settings","objectID":"/latest/migration/#basesettings-has-moved-to-pydantic-settings","rank":10},{"content":"The following special-use types have been moved to the Pydantic Extra Types package,\nwhich may be installed separately if needed. Color Types Payment Card Numbers","pageID":"Migration Guide","abs_url":"/latest/migration/#color-and-payment-card-numbers-moved-to-pydantic-extra-types","title":"Migration Guide - Migration guide - Color and Payment Card Numbers moved to pydantic-extra-types","objectID":"/latest/migration/#color-and-payment-card-numbers-moved-to-pydantic-extra-types","rank":5},{"content":"In Pydantic V1 the  type inherited from str , and all the other Url and Dsn types inherited from these. In Pydantic V2 these types are built on two new Url and MultiHostUrl classes using Annotated . Inheriting from str had upsides and downsides, and for V2 we decided it would be better to remove this. To use these\ntypes in APIs which expect str you'll now need to convert them (with str(url) ). Pydantic V2 uses Rust's Url crate for URL validation.\nSome of the URL validation differs slightly from the previous behavior in V1.\nOne notable difference is that the new Url types append slashes to the validated version if no path is included,\neven if a slash is not specified in the argument to a Url type constructor. See the example below for this behavior: from pydantic import AnyUrl\n\nassert str(AnyUrl(url='https://google.com')) == 'https://google.com/'\nassert str(AnyUrl(url='https://google.com/')) == 'https://google.com/'\nassert str(AnyUrl(url='https://google.com/api')) == 'https://google.com/api'\nassert str(AnyUrl(url='https://google.com/api/')) == 'https://google.com/api/' If you still want to use the old behavior without the appended slash, take a look at this solution .","pageID":"Migration Guide","abs_url":"/latest/migration/#url-and-dsn-types-in-pydanticnetworks-no-longer-inherit-from-str","title":"Migration Guide - Migration guide - Url and Dsn types in pydantic.networks no longer inherit from str","objectID":"/latest/migration/#url-and-dsn-types-in-pydanticnetworks-no-longer-inherit-from-str","rank":0},{"content":"The Constrained* classes were removed , and you should replace them by Annotated[<type>, Field(...)] , for example: from pydantic import BaseModel, ConstrainedInt\n\n\nclass MyInt(ConstrainedInt):\n    ge = 0\n\n\nclass Model(BaseModel):\n    x: MyInt ...becomes: from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nMyInt = Annotated[int, Field(ge=0)]\n\n\nclass Model(BaseModel):\n    x: MyInt Read more about it in the Composing types via Annotated docs. For ConstrainedStr you can use  instead.","pageID":"Migration Guide","abs_url":"/latest/migration/#constrained-types","title":"Migration Guide - Migration guide - Constrained types","objectID":"/latest/migration/#constrained-types","rank":-5},{"content":"Pydantic V2 contains a mypy plugin in pydantic.mypy . When using V1 features the pydantic.v1.mypy plugin might need to also be enabled. To configure the mypy plugins: mypy.ini pyproject.toml [mypy] plugins = pydantic.mypy, pydantic.v1.mypy # include `.v1.mypy` if required. [tool.mypy] plugins = [ \"pydantic.mypy\" , \"pydantic.v1.mypy\" , # include `.v1.mypy` if required. ]","pageID":"Migration Guide","abs_url":"/latest/migration/#mypy-plugins","title":"Migration Guide - Migration guide - Mypy plugins","objectID":"/latest/migration/#mypy-plugins","rank":-10},{"content":"Dropped support for email-validator<2.0.0 . Make sure to update\n  using pip install -U email-validator .","pageID":"Migration Guide","abs_url":"/latest/migration/#other-changes","title":"Migration Guide - Other changes","objectID":"/latest/migration/#other-changes","rank":-15},{"content":"Pydantic V1 Pydantic V2 pydantic.BaseSettings pydantic_settings.BaseSettings pydantic.color pydantic.types.PaymentCardBrand pydantic_extra_types.PaymentCardBrand pydantic.types.PaymentCardNumber pydantic_extra_types.PaymentCardNumber pydantic.utils.version_info pydantic.error_wrappers.ValidationError pydantic.utils.to_camel pydantic.utils.to_lower_camel pydantic.PyObject","pageID":"Migration Guide","abs_url":"/latest/migration/#moved-in-pydantic-v2","title":"Migration Guide - Moved in Pydantic V2","objectID":"/latest/migration/#moved-in-pydantic-v2","rank":-20},{"content":"Pydantic V1 Pydantic V2 pydantic.tools.schema_of pydantic.deprecated.tools.schema_of pydantic.tools.parse_obj_as pydantic.deprecated.tools.parse_obj_as pydantic.tools.schema_json_of pydantic.deprecated.tools.schema_json_of pydantic.json.pydantic_encoder pydantic.deprecated.json.pydantic_encoder pydantic.validate_arguments pydantic.deprecated.decorator.validate_arguments pydantic.json.custom_pydantic_encoder pydantic.deprecated.json.custom_pydantic_encoder pydantic.json.ENCODERS_BY_TYPE pydantic.deprecated.json.ENCODERS_BY_TYPE pydantic.json.timedelta_isoformat pydantic.deprecated.json.timedelta_isoformat pydantic.decorator.validate_arguments pydantic.deprecated.decorator.validate_arguments pydantic.class_validators.validator pydantic.deprecated.class_validators.validator pydantic.class_validators.root_validator pydantic.deprecated.class_validators.root_validator pydantic.utils.deep_update pydantic.v1.utils.deep_update pydantic.utils.GetterDict pydantic.v1.utils.GetterDict pydantic.utils.lenient_issubclass pydantic.v1.utils.lenient_issubclass pydantic.utils.lenient_isinstance pydantic.v1.utils.lenient_isinstance pydantic.utils.is_valid_field pydantic.v1.utils.is_valid_field pydantic.utils.update_not_none pydantic.v1.utils.update_not_none pydantic.utils.import_string pydantic.v1.utils.import_string pydantic.utils.Representation pydantic.v1.utils.Representation pydantic.utils.ROOT_KEY pydantic.v1.utils.ROOT_KEY pydantic.utils.smart_deepcopy pydantic.v1.utils.smart_deepcopy pydantic.utils.sequence_like pydantic.v1.utils.sequence_like","pageID":"Migration Guide","abs_url":"/latest/migration/#deprecated-and-moved-in-pydantic-v2","title":"Migration Guide - Deprecated and moved in Pydantic V2","objectID":"/latest/migration/#deprecated-and-moved-in-pydantic-v2","rank":-25},{"content":"pydantic.ConstrainedBytes pydantic.ConstrainedDate pydantic.ConstrainedDecimal pydantic.ConstrainedFloat pydantic.ConstrainedFrozenSet pydantic.ConstrainedInt pydantic.ConstrainedList pydantic.ConstrainedSet pydantic.ConstrainedStr pydantic.JsonWrapper pydantic.NoneBytes This was an alias to None | bytes . pydantic.NoneStr This was an alias to None | str . pydantic.NoneStrBytes This was an alias to None | str | bytes . pydantic.Protocol pydantic.Required pydantic.StrBytes This was an alias to str | bytes . pydantic.compiled pydantic.config.get_config pydantic.config.inherit_config pydantic.config.prepare_config pydantic.create_model_from_namedtuple pydantic.create_model_from_typeddict pydantic.dataclasses.create_pydantic_model_from_dataclass pydantic.dataclasses.make_dataclass_validator pydantic.dataclasses.set_validation pydantic.datetime_parse.parse_date pydantic.datetime_parse.parse_time pydantic.datetime_parse.parse_datetime pydantic.datetime_parse.parse_duration pydantic.error_wrappers.ErrorWrapper pydantic.errors.AnyStrMaxLengthError pydantic.errors.AnyStrMinLengthError pydantic.errors.ArbitraryTypeError pydantic.errors.BoolError pydantic.errors.BytesError pydantic.errors.CallableError pydantic.errors.ClassError pydantic.errors.ColorError pydantic.errors.ConfigError pydantic.errors.DataclassTypeError pydantic.errors.DateError pydantic.errors.DateNotInTheFutureError pydantic.errors.DateNotInThePastError pydantic.errors.DateTimeError pydantic.errors.DecimalError pydantic.errors.DecimalIsNotFiniteError pydantic.errors.DecimalMaxDigitsError pydantic.errors.DecimalMaxPlacesError pydantic.errors.DecimalWholeDigitsError pydantic.errors.DictError pydantic.errors.DurationError pydantic.errors.EmailError pydantic.errors.EnumError pydantic.errors.EnumMemberError pydantic.errors.ExtraError pydantic.errors.FloatError pydantic.errors.FrozenSetError pydantic.errors.FrozenSetMaxLengthError pydantic.errors.FrozenSetMinLengthError pydantic.errors.HashableError pydantic.errors.IPv4AddressError pydantic.errors.IPv4InterfaceError pydantic.errors.IPv4NetworkError pydantic.errors.IPv6AddressError pydantic.errors.IPv6InterfaceError pydantic.errors.IPv6NetworkError pydantic.errors.IPvAnyAddressError pydantic.errors.IPvAnyInterfaceError pydantic.errors.IPvAnyNetworkError pydantic.errors.IntEnumError pydantic.errors.IntegerError pydantic.errors.InvalidByteSize pydantic.errors.InvalidByteSizeUnit pydantic.errors.InvalidDiscriminator pydantic.errors.InvalidLengthForBrand pydantic.errors.JsonError pydantic.errors.JsonTypeError pydantic.errors.ListError pydantic.errors.ListMaxLengthError pydantic.errors.ListMinLengthError pydantic.errors.ListUniqueItemsError pydantic.errors.LuhnValidationError pydantic.errors.MissingDiscriminator pydantic.errors.MissingError pydantic.errors.NoneIsAllowedError pydantic.errors.NoneIsNotAllowedError pydantic.errors.NotDigitError pydantic.errors.NotNoneError pydantic.errors.NumberNotGeError pydantic.errors.NumberNotGtError pydantic.errors.NumberNotLeError pydantic.errors.NumberNotLtError pydantic.errors.NumberNotMultipleError pydantic.errors.PathError pydantic.errors.PathNotADirectoryError pydantic.errors.PathNotAFileError pydantic.errors.PathNotExistsError pydantic.errors.PatternError pydantic.errors.PyObjectError pydantic.errors.PydanticTypeError pydantic.errors.PydanticValueError pydantic.errors.SequenceError pydantic.errors.SetError pydantic.errors.SetMaxLengthError pydantic.errors.SetMinLengthError pydantic.errors.StrError pydantic.errors.StrRegexError pydantic.errors.StrictBoolError pydantic.errors.SubclassError pydantic.errors.TimeError pydantic.errors.TupleError pydantic.errors.TupleLengthError pydantic.errors.UUIDError pydantic.errors.UUIDVersionError pydantic.errors.UrlError pydantic.errors.UrlExtraError pydantic.errors.UrlHostError pydantic.errors.UrlHostTldError pydantic.errors.UrlPortError pydantic.errors.UrlSchemeError pydantic.errors.UrlSchemePermittedError pydantic.errors.UrlUserInfoError pydantic.errors.WrongConstantError pydantic.main.validate_model pydantic.networks.stricturl pydantic.parse_file_as pydantic.parse_raw_as pydantic.stricturl pydantic.tools.parse_file_as pydantic.tools.parse_raw_as pydantic.types.JsonWrapper pydantic.types.NoneBytes pydantic.types.NoneStr pydantic.types.NoneStrBytes pydantic.types.PyObject pydantic.types.StrBytes pydantic.typing.evaluate_forwardref pydantic.typing.AbstractSetIntStr pydantic.typing.AnyCallable pydantic.typing.AnyClassMethod pydantic.typing.CallableGenerator pydantic.typing.DictAny pydantic.typing.DictIntStrAny pydantic.typing.DictStrAny pydantic.typing.IntStr pydantic.typing.ListStr pydantic.typing.MappingIntStrAny pydantic.typing.NoArgAnyCallable pydantic.typing.NoneType pydantic.typing.ReprArgs pydantic.typing.SetStr pydantic.typing.StrPath pydantic.typing.TupleGenerator pydantic.typing.WithArgsTypes pydantic.typing.all_literal_values pydantic.typing.display_as_type pydantic.typing.get_all_type_hints pydantic.typing.get_args pydantic.typing.get_origin pydantic.typing.get_sub_types pydantic.typing.is_callable_type pydantic.typing.is_classvar pydantic.typing.is_finalvar pydantic.typing.is_literal_type pydantic.typing.is_namedtuple pydantic.typing.is_new_type pydantic.typing.is_none_type pydantic.typing.is_typeddict pydantic.typing.is_typeddict_special pydantic.typing.is_union pydantic.typing.new_type_supertype pydantic.typing.resolve_annotations pydantic.typing.typing_base pydantic.typing.update_field_forward_refs pydantic.typing.update_model_forward_refs pydantic.utils.ClassAttribute pydantic.utils.DUNDER_ATTRIBUTES pydantic.utils.PyObjectStr pydantic.utils.ValueItems pydantic.utils.almost_equal_floats pydantic.utils.get_discriminator_alias_and_values pydantic.utils.get_model pydantic.utils.get_unique_discriminator_alias pydantic.utils.in_ipython pydantic.utils.is_valid_identifier pydantic.utils.path_type pydantic.utils.validate_field_name pydantic.validate_model","pageID":"Migration Guide","abs_url":"/latest/migration/#removed-in-pydantic-v2","title":"Migration Guide - Removed in Pydantic V2","objectID":"/latest/migration/#removed-in-pydantic-v2","rank":-30},{"content":"Pydantic has an amazing community of contributors, reviewers, and experts that help propel the project forward.\nHere, we celebrate those people and their contributions.","pageID":"Pydantic People","abs_url":"/latest/pydantic_people/#pydantic-people","title":"Pydantic People","objectID":"/latest/pydantic_people/#pydantic-people","rank":100},{"content":"These are the current maintainers of the Pydantic repository. Feel free to tag us if you have questions, review requests, or feature requests for which you'd like feedback! @adriangb @hramezani @samuelcolvin @dmontagu @davidhewitt @alexmojaki @Viicos @Kludex","pageID":"Pydantic People","abs_url":"/latest/pydantic_people/#maintainers","title":"Pydantic People - Maintainers","objectID":"/latest/pydantic_people/#maintainers","rank":95},{"content":"These are the users that have helped others the most with questions in GitHub through all time . @PrettyWood Questions replied: 143 @uriyyo Questions replied: 96 @sydney-runkle Questions replied: 38 @lesnik512 Questions replied: 21 @harunyasar Questions replied: 17 @nymous Questions replied: 13 @janas-adam Questions replied: 12 @ybressler Questions replied: None","pageID":"Pydantic People","abs_url":"/latest/pydantic_people/#experts","title":"Pydantic People - Experts","objectID":"/latest/pydantic_people/#experts","rank":90},{"content":"These are the users that have helped others the most with questions in GitHub during the last month. @janas-adam Questions replied: 3","pageID":"Pydantic People","abs_url":"/latest/pydantic_people/#most-active-users-last-month","title":"Pydantic People - Experts - Most active users last month","objectID":"/latest/pydantic_people/#most-active-users-last-month","rank":85},{"content":"These are the users that have created the most pull requests that have been merged . @sydney-runkle Contributions: 382 @PrettyWood Contributions: 122 @dependabot-preview Contributions: 75 @tpdorsey Contributions: 71 @lig Contributions: 49 @pyup-bot Contributions: 46 @tiangolo Contributions: 22 @Bobronium Contributions: 19 @Gr1N Contributions: 17 @misrasaurabh1 Contributions: 16 @uriyyo Contributions: 15 @pilosus Contributions: 12 @yezz123 Contributions: 12 @kc0506 Contributions: 12 @StephenBrown2 Contributions: 10 @koxudaxi Contributions: 9 @cdce8p Contributions: 9 @aminalaee Contributions: 8 @NeevCohen Contributions: 8 @layday Contributions: 7 @daviskirk Contributions: 7 @tlambert03 Contributions: 7 @dgasmith Contributions: 6 @Atheuz Contributions: 6 @AdolfoVillalobos Contributions: 6 @nuno-andre Contributions: 5 @ofek Contributions: 5 @mschoettle Contributions: 5 @karta9821 Contributions: 5 @hmvp Contributions: 4 @retnikt Contributions: 4 @therefromhere Contributions: 4 @JeanArhancet Contributions: 4 @commonism Contributions: 4 @MarkusSintonen Contributions: 4 @JensHeinrich Contributions: 4 @mgorny Contributions: 4 @ornariece Contributions: 4 @rx-dwoodward Contributions: 4 @dAIsySHEng1 Contributions: 4","pageID":"Pydantic People","abs_url":"/latest/pydantic_people/#top-contributors","title":"Pydantic People - Top contributors","objectID":"/latest/pydantic_people/#top-contributors","rank":80},{"content":"These are the users that have reviewed the most Pull Requests from others, assisting with code quality, documentation, bug fixes, feature requests, etc. @sydney-runkle Reviews: 691 @PrettyWood Reviews: 211 @lig Reviews: 103 @tpdorsey Reviews: 77 @hyperlint-ai Reviews: 57 @tiangolo Reviews: 44 @DouweM Reviews: 32 @Bobronium Reviews: 27 @Gr1N Reviews: 17 @StephenBrown2 Reviews: 17 @MarkusSintonen Reviews: 16 @ybressler Reviews: 15 @uriyyo Reviews: 11 @koxudaxi Reviews: 10 @daviskirk Reviews: 10 @yezz123 Reviews: 10 @Zac-HD Reviews: 8 @layday Reviews: 7 @kc0506 Reviews: 7 @pilosus Reviews: 6 @Kilo59 Reviews: 6 @JeanArhancet Reviews: 6 @tlambert03 Reviews: 5 @christianbundy Reviews: 5 @nix010 Reviews: 5 @karta9821 Reviews: 5 @graingert Reviews: 4 @hmvp Reviews: 4 @wozniakty Reviews: 4 @nuno-andre Reviews: 4 @antdking Reviews: 4 @dimaqq Reviews: 4 @DetachHead Reviews: 4 @JensHeinrich Reviews: 4","pageID":"Pydantic People","abs_url":"/latest/pydantic_people/#top-reviewers","title":"Pydantic People - Top Reviewers","objectID":"/latest/pydantic_people/#top-reviewers","rank":75},{"content":"The data displayed above is calculated monthly via the Github GraphQL API. The source code for this script is located here .\nMany thanks to Sebastián Ramírez for the script from which we based this logic. Depending on changing conditions, the thresholds for the different categories of contributors may change in the future.","pageID":"Pydantic People","abs_url":"/latest/pydantic_people/#about-the-data","title":"Pydantic People - About the data","objectID":"/latest/pydantic_people/#about-the-data","rank":70},{"content":"First of all, we recognize that the transitions from Pydantic V1 to V2 has been and will be painful for some users.\nWe're sorry about this pain , it was an unfortunate but necessary step to correct design mistakes of V1. There will not be another breaking change of this magnitude!","pageID":"Version Policy","abs_url":"/latest/version-policy/#Version Policy","title":"Version Policy","objectID":"/latest/version-policy/#Version Policy","rank":100},{"content":"Active development of V1 has already stopped, however critical bug fixes and security vulnerabilities will be fixed in V1 until\nthe release of Pydantic V3.","pageID":"Version Policy","abs_url":"/latest/version-policy/#pydantic-v1","title":"Version Policy - Pydantic V1","objectID":"/latest/version-policy/#pydantic-v1","rank":95},{"content":"We will not intentionally make breaking changes in minor releases of V2. Functionality marked as deprecated will not be removed until the next major V3 release. Of course, some apparently safe changes and bug fixes will inevitably break some users' code — obligatory link to xkcd . The following changes will NOT be considered breaking changes, and may occur in minor releases: Bug fixes that may result in existing code breaking, provided that such code was relying on undocumented features/constructs. Changing the format of JSON Schema references . Changing the msg , ctx , and loc fields of  exceptions. type will not change — if you're programmatically parsing error messages, you should use type . Adding new keys to  exceptions — e.g. we intend to add line_number and column_number to errors when validating JSON once we migrate to a new JSON parser. Adding new  errors. Changing how __repr__ behaves, even of public classes. In all cases we will aim to minimize churn and do so only when justified by the increase of quality of Pydantic for users.","pageID":"Version Policy","abs_url":"/latest/version-policy/#pydantic-v2","title":"Version Policy - Pydantic V2","objectID":"/latest/version-policy/#pydantic-v2","rank":90},{"content":"We expect to make new major releases roughly once a year going forward, although as mentioned above, any associated breaking changes should be trivial to fix compared to the V1-to-V2 transition.","pageID":"Version Policy","abs_url":"/latest/version-policy/#pydantic-v3-and-beyond","title":"Version Policy - Pydantic V3 and beyond","objectID":"/latest/version-policy/#pydantic-v3-and-beyond","rank":85},{"content":"At Pydantic, we like to move quickly and innovate! To that end, we may introduce experimental features in minor releases. Usage Documentation To learn more about our current experimental features, see the experimental features documentation . Please keep in mind, experimental features are active works in progress. If these features are successful, they'll eventually become part of Pydantic. If unsuccessful, said features will be removed with little notice. While in its experimental phase, a feature's API and behaviors may not be stable, and it's very possible that changes made to the feature will not be backward-compatible.","pageID":"Version Policy","abs_url":"/latest/version-policy/#experimental-features","title":"Version Policy - Experimental Features","objectID":"/latest/version-policy/#experimental-features","rank":80},{"content":"We use one of the following naming conventions to indicate that a feature is experimental: The feature is located in the experimental module. In this case, you can access the feature like this: from pydantic.experimental import feature_name The feature is located in the main module, but prefixed with experimental_ . This case occurs when we add a new field, argument, or method to an existing data structure already within the main pydantic module. New features with these naming conventions are subject to change or removal, and we are looking for feedback and suggestions before making them a permanent part of Pydantic. See the feedback section for more information.","pageID":"Version Policy","abs_url":"/latest/version-policy/#naming-conventions","title":"Version Policy - Experimental Features - Naming Conventions","objectID":"/latest/version-policy/#naming-conventions","rank":75},{"content":"When you import an experimental feature from the experimental module, you'll see a warning message that the feature is experimental. You can disable this warning with the following: import warnings\n\nfrom pydantic import PydanticExperimentalWarning\n\nwarnings.filterwarnings('ignore', category=PydanticExperimentalWarning)","pageID":"Version Policy","abs_url":"/latest/version-policy/#importing-experimental-features","title":"Version Policy - Experimental Features - Importing Experimental Features","objectID":"/latest/version-policy/#importing-experimental-features","rank":70},{"content":"A new feature is added, either in the experimental module or with the experimental_ prefix. The behavior is often modified during patch/minor releases, with potential API/behavior changes. If the feature is successful, we promote it to Pydantic with the following steps: a. If it was in the experimental module, the feature is cloned to Pydantic's main module. The original experimental feature still remains in the experimental module, but it will show a warning when used. If the feature was already in the main Pydantic module, we create a copy of the feature without the experimental_ prefix, so the feature exists with both the official and experimental names. A deprecation warning is attached to the experimental version. b. At some point, the code of the experimental feature is removed, but there will still be a stub of the feature that provides an error message with appropriate instructions. c. As a last step, the experimental version of the feature is entirely removed from the codebase. If the feature is unsuccessful or unpopular, it's removed with little notice. A stub will remain in the location of the deprecated feature with an error message. Thanks to streamlit for the inspiration for the lifecycle and naming conventions of our new experimental feature patterns.","pageID":"Version Policy","abs_url":"/latest/version-policy/#lifecycle-of-experimental-features","title":"Version Policy - Experimental Features - Lifecycle of Experimental Features","objectID":"/latest/version-policy/#lifecycle-of-experimental-features","rank":65},{"content":"Pydantic will drop support for a Python version when the following conditions are met: The Python version has reached its expected end of life . less than 5% of downloads of the most recent minor release are using that version.","pageID":"Version Policy","abs_url":"/latest/version-policy/#support-for-python-versions","title":"Version Policy - Support for Python versions","objectID":"/latest/version-policy/#support-for-python-versions","rank":60},{"content":"Today, Pydantic is downloaded many times a month and used by some of the largest and most recognisable organisations in the world. It's hard to know why so many people have adopted Pydantic since its inception six years ago, but here are a few guesses.","pageID":"Why use Pydantic","abs_url":"/latest/why/#why-use-pydantic-validation","title":"Why use Pydantic","objectID":"/latest/why/#why-use-pydantic-validation","rank":100},{"content":"The schema that Pydantic validates against is generally defined by Python type hints . Type hints are great for this since, if you're writing modern Python, you already know how to use them.\nUsing type hints also means that Pydantic integrates well with static typing tools\n(like mypy and Pyright )\nand IDEs (like PyCharm and VSCode ). Learn more See the documentation on supported types .","pageID":"Why use Pydantic","abs_url":"/latest/why/#type-hints","title":"Why use Pydantic - Type hints powering schema validation","objectID":"/latest/why/#type-hints","rank":95},{"content":"Pydantic's core validation logic is implemented in a separate package ( pydantic-core ),\nwhere validation for most types is implemented in Rust. As a result, Pydantic is among the fastest data validation libraries for Python. Unlike other performance-centric libraries written in compiled languages, Pydantic also has excellent support for customizing validation via functional validators . Learn more Samuel Colvin's talk at PyCon 2023 explains how pydantic-core works and how it integrates with Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#performance","title":"Why use Pydantic - Performance","objectID":"/latest/why/#performance","rank":90},{"content":"Pydantic provides functionality to serialize model in three ways: To a Python dict made up of the associated Python objects. To a Python dict made up only of \"jsonable\" types. To a JSON string. In all three modes, the output can be customized by excluding specific fields, excluding unset fields, excluding default values, and excluding None values. Learn more See the documentation on serialization .","pageID":"Why use Pydantic","abs_url":"/latest/why/#serialization","title":"Why use Pydantic - Serialization","objectID":"/latest/why/#serialization","rank":85},{"content":"A JSON Schema can be generated for any Pydantic schema — allowing self-documenting APIs and integration with a wide variety of tools which support the JSON Schema format. Pydantic is compliant with the latest version of JSON Schema specification\n( 2020-12 ), which\nis compatible with OpenAPI 3.1 . Learn more See the documentation on JSON Schema .","pageID":"Why use Pydantic","abs_url":"/latest/why/#json-schema","title":"Why use Pydantic - JSON Schema","objectID":"/latest/why/#json-schema","rank":80},{"content":"By default, Pydantic is tolerant to common incorrect types and coerces data to the right type —\ne.g. a numeric string passed to an int field will be parsed as an int . Pydantic also has as strict mode , where types are not coerced and a\nvalidation error is raised unless the input data exactly matches the expected schema. But strict mode would be pretty useless when validating JSON data since JSON doesn't have types matching\nmany common Python types like ,  or . To solve this, Pydantic can parse and validate JSON in one step. This allows sensible data conversion\n(e.g. when parsing strings into  objects). Since the JSON parsing is\nimplemented in Rust, it's also very performant. Learn more See the documentation on strict mode .","pageID":"Why use Pydantic","abs_url":"/latest/why/#strict-lax","title":"Why use Pydantic - Strict mode and data coercion","objectID":"/latest/why/#strict-lax","rank":75},{"content":"Pydantic provides four ways to create schemas and perform validation and serialization: BaseModel — Pydantic's own super class with many common utilities available via instance methods. Pydantic dataclasses — a wrapper around standard dataclasses with additional validation performed. — a general way to adapt any type for validation and serialization.\n   This allows types like TypedDict and NamedTuple to be validated as well as simple types (like  or ) — all types supported\n   can be used with . validate_call — a decorator to perform validation when calling a function.","pageID":"Why use Pydantic","abs_url":"/latest/why/#dataclasses-typeddict-more","title":"Why use Pydantic - Dataclasses, TypedDicts, and more","objectID":"/latest/why/#dataclasses-typeddict-more","rank":70},{"content":"Functional validators and serializers, as well as a powerful protocol for custom types, means the way Pydantic operates can be customized on a per-field or per-type basis. Learn more See the documentation on validators , custom serializers ,\nand custom types .","pageID":"Why use Pydantic","abs_url":"/latest/why/#customisation","title":"Why use Pydantic - Customisation","objectID":"/latest/why/#customisation","rank":65},{"content":"At the time of writing there are 466,400 repositories on GitHub and 8,119 packages on PyPI that depend on Pydantic. Some notable libraries that depend on Pydantic: huggingface/transformers 138,570 stars hwchase17/langchain 99,542 stars tiangolo/fastapi 80,497 stars apache/airflow 38,577 stars lm-sys/FastChat 37,650 stars microsoft/DeepSpeed 36,521 stars OpenBB-finance/OpenBBTerminal 35,971 stars gradio-app/gradio 35,740 stars ray-project/ray 35,176 stars pola-rs/polars 31,698 stars Lightning-AI/lightning 28,902 stars mindsdb/mindsdb 27,141 stars embedchain/embedchain 24,379 stars pynecone-io/reflex 21,558 stars heartexlabs/label-studio 20,571 stars Sanster/lama-cleaner 20,313 stars mlflow/mlflow 19,393 stars RasaHQ/rasa 19,337 stars spotDL/spotify-downloader 18,604 stars chroma-core/chroma 17,393 stars airbytehq/airbyte 17,120 stars openai/evals 15,437 stars tiangolo/sqlmodel 15,127 stars ydataai/ydata-profiling 12,687 stars pyodide/pyodide 12,653 stars dagster-io/dagster 12,440 stars PaddlePaddle/PaddleNLP 12,312 stars matrix-org/synapse 11,857 stars lucidrains/DALLE2-pytorch 11,207 stars great-expectations/great_expectations 10,164 stars modin-project/modin 10,002 stars aws/serverless-application-model 9,402 stars sqlfluff/sqlfluff 8,535 stars replicate/cog 8,344 stars autogluon/autogluon 8,326 stars lucidrains/imagen-pytorch 8,164 stars brycedrennan/imaginAIry 8,050 stars vitalik/django-ninja 7,685 stars NVlabs/SPADE 7,632 stars bridgecrewio/checkov 7,340 stars bentoml/BentoML 7,322 stars skypilot-org/skypilot 7,113 stars apache/iceberg 6,853 stars deeppavlov/DeepPavlov 6,777 stars PrefectHQ/marvin 5,454 stars NVIDIA/NeMo-Guardrails 4,383 stars microsoft/FLAML 4,035 stars jina-ai/discoart 3,846 stars docarray/docarray 3,007 stars aws-powertools/powertools-lambda-python 2,980 stars roman-right/beanie 2,172 stars art049/odmantic 1,096 stars More libraries using Pydantic can be found at Kludex/awesome-pydantic .","pageID":"Why use Pydantic","abs_url":"/latest/why/#ecosystem","title":"Why use Pydantic - Ecosystem","objectID":"/latest/why/#ecosystem","rank":60},{"content":"Some notable companies and organisations using Pydantic together with comments on why/how we know they're using Pydantic. The organisations below are included because they match one or more of the following criteria: Using Pydantic as a dependency in a public repository. Referring traffic to the Pydantic documentation site from an organization-internal domain — specific referrers are not included since they're generally not in the public domain. Direct communication between the Pydantic team and engineers employed by the organization about usage of Pydantic within the organization. We've included some extra detail where appropriate and already in the public domain.","pageID":"Why use Pydantic","abs_url":"/latest/why/#using-pydantic","title":"Why use Pydantic - Organisations using Pydantic","objectID":"/latest/why/#using-pydantic","rank":55},{"content":"adobe/dy-sql uses Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-adobe","title":"Why use Pydantic - Organisations using Pydantic - Adobe","objectID":"/latest/why/#org-adobe","rank":50},{"content":"powertools-lambda-python awslabs/gluonts AWS sponsored Samuel Colvin $5,000 to work on Pydantic in 2022","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-amazon","title":"Why use Pydantic - Organisations using Pydantic - Amazon and AWS","objectID":"/latest/why/#org-amazon","rank":45},{"content":"anthropics/anthropic-sdk-python uses Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-anthropic","title":"Why use Pydantic - Organisations using Pydantic - Anthropic","objectID":"/latest/why/#org-anthropic","rank":40},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-apple","title":"Why use Pydantic - Organisations using Pydantic - Apple","objectID":"/latest/why/#org-apple","rank":35},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-asml","title":"Why use Pydantic - Organisations using Pydantic - ASML","objectID":"/latest/why/#org-asml","rank":30},{"content":"Multiple repos in the AstraZeneca GitHub org depend on Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-astrazeneca","title":"Why use Pydantic - Organisations using Pydantic - AstraZeneca","objectID":"/latest/why/#org-astrazeneca","rank":25},{"content":"Pydantic is listed in their report of Open Source Used In RADKit . cisco/webex-assistant-sdk","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-cisco","title":"Why use Pydantic - Organisations using Pydantic - Cisco Systems","objectID":"/latest/why/#org-cisco","rank":20},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-comcast","title":"Why use Pydantic - Organisations using Pydantic - Comcast","objectID":"/latest/why/#org-comcast","rank":15},{"content":"Extensive use of Pydantic in DataDog/integrations-core and other repos Communication with engineers from Datadog about how they use Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-datadog","title":"Why use Pydantic - Organisations using Pydantic - Datadog","objectID":"/latest/why/#org-datadog","rank":10},{"content":"Multiple repos in the facebookresearch GitHub org depend on Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-facebook","title":"Why use Pydantic - Organisations using Pydantic - Facebook","objectID":"/latest/why/#org-facebook","rank":5},{"content":"GitHub sponsored Pydantic $750 in 2022","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-github","title":"Why use Pydantic - Organisations using Pydantic - GitHub","objectID":"/latest/why/#org-github","rank":0},{"content":"Extensive use of Pydantic in google/turbinia and other repos.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-google","title":"Why use Pydantic - Organisations using Pydantic - Google","objectID":"/latest/why/#org-google","rank":-5},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-hsbc","title":"Why use Pydantic - Organisations using Pydantic - HSBC","objectID":"/latest/why/#org-hsbc","rank":-10},{"content":"Multiple repos in the IBM GitHub org depend on Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-ibm","title":"Why use Pydantic - Organisations using Pydantic - IBM","objectID":"/latest/why/#org-ibm","rank":-15},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-intel","title":"Why use Pydantic - Organisations using Pydantic - Intel","objectID":"/latest/why/#org-intel","rank":-20},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-intuit","title":"Why use Pydantic - Organisations using Pydantic - Intuit","objectID":"/latest/why/#org-intuit","rank":-25},{"content":"Tweet explaining how the IPCC use Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-ipcc","title":"Why use Pydantic - Organisations using Pydantic - Intergovernmental Panel on Climate Change","objectID":"/latest/why/#org-ipcc","rank":-30},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-jpmorgan","title":"Why use Pydantic - Organisations using Pydantic - JPMorgan","objectID":"/latest/why/#org-jpmorgan","rank":-35},{"content":"The developers of the Jupyter notebook are using Pydantic for subprojects Through the FastAPI-based Jupyter server Jupyverse FPS 's configuration management.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-jupyter","title":"Why use Pydantic - Organisations using Pydantic - Jupyter","objectID":"/latest/why/#org-jupyter","rank":-40},{"content":"DeepSpeed deep learning optimisation library uses Pydantic extensively Multiple repos in the microsoft GitHub org depend on Pydantic, in particular their Pydantic is also used in the Azure GitHub org Comments on GitHub show Microsoft engineers using Pydantic as part of Windows and Office","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-microsoft","title":"Why use Pydantic - Organisations using Pydantic - Microsoft","objectID":"/latest/why/#org-microsoft","rank":-45},{"content":"Multiple repos in the MolSSI GitHub org depend on Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-molssi","title":"Why use Pydantic - Organisations using Pydantic - Molecular Science Software Institute","objectID":"/latest/why/#org-molssi","rank":-50},{"content":"Multiple repos in the NASA GitHub org depend on Pydantic. NASA are also using Pydantic via FastAPI in their JWST project to process images from the James Webb Space Telescope,\nsee this tweet .","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-nasa","title":"Why use Pydantic - Organisations using Pydantic - NASA","objectID":"/latest/why/#org-nasa","rank":-55},{"content":"Multiple repos in the Netflix GitHub org depend on Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-netflix","title":"Why use Pydantic - Organisations using Pydantic - Netflix","objectID":"/latest/why/#org-netflix","rank":-60},{"content":"The nsacyber/WALKOFF repo depends on Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-nsa","title":"Why use Pydantic - Organisations using Pydantic - NSA","objectID":"/latest/why/#org-nsa","rank":-65},{"content":"Multiple repositories in the NVIDIA GitHub org depend on Pydantic. Their \"Omniverse Services\" depends on Pydantic according to their documentation .","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-nvidia","title":"Why use Pydantic - Organisations using Pydantic - NVIDIA","objectID":"/latest/why/#org-nvidia","rank":-70},{"content":"OpenAI use Pydantic for their ChatCompletions API, as per this discussion on GitHub. Anecdotally, OpenAI use Pydantic extensively for their internal services.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-openai","title":"Why use Pydantic - Organisations using Pydantic - OpenAI","objectID":"/latest/why/#org-openai","rank":-75},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-oracle","title":"Why use Pydantic - Organisations using Pydantic - Oracle","objectID":"/latest/why/#org-oracle","rank":-80},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-palantir","title":"Why use Pydantic - Organisations using Pydantic - Palantir","objectID":"/latest/why/#org-palantir","rank":-85},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-qualcomm","title":"Why use Pydantic - Organisations using Pydantic - Qualcomm","objectID":"/latest/why/#org-qualcomm","rank":-90},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-redhat","title":"Why use Pydantic - Organisations using Pydantic - Red Hat","objectID":"/latest/why/#org-redhat","rank":-95},{"content":"Anecdotally, all internal services at Revolut are built with FastAPI and therefore Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-revolut","title":"Why use Pydantic - Organisations using Pydantic - Revolut","objectID":"/latest/why/#org-revolut","rank":-100},{"content":"The robusta-dev/robusta repo depends on Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-robusta","title":"Why use Pydantic - Organisations using Pydantic - Robusta","objectID":"/latest/why/#org-robusta","rank":-105},{"content":"Salesforce sponsored Samuel Colvin $10,000 to work on Pydantic in 2022.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-salesforce","title":"Why use Pydantic - Organisations using Pydantic - Salesforce","objectID":"/latest/why/#org-salesforce","rank":-110},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-starbucks","title":"Why use Pydantic - Organisations using Pydantic - Starbucks","objectID":"/latest/why/#org-starbucks","rank":-115},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-ti","title":"Why use Pydantic - Organisations using Pydantic - Texas Instruments","objectID":"/latest/why/#org-ti","rank":-120},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-twilio","title":"Why use Pydantic - Organisations using Pydantic - Twilio","objectID":"/latest/why/#org-twilio","rank":-125},{"content":"Twitter's the-algorithm repo where they open sourced their recommendation engine uses Pydantic.","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-twitter","title":"Why use Pydantic - Organisations using Pydantic - Twitter","objectID":"/latest/why/#org-twitter","rank":-130},{"content":"(Based on the criteria described above)","pageID":"Why use Pydantic","abs_url":"/latest/why/#org-ukhomeoffice","title":"Why use Pydantic - Organisations using Pydantic - UK Home Office","objectID":"/latest/why/#org-ukhomeoffice","rank":-135},{"content":"Support for alias configurations. AliasPath dataclass ¶ AliasPath(first_arg: , *args:  | ) Usage Documentation AliasPath and AliasChoices A data class used by validation_alias as a convenience to create aliases. Attributes: Name Type Description [ | ] A list of string or integer aliases. convert_to_aliases ¶ convert_to_aliases() -> [ | ] Converts arguments to a list of string or integer aliases. Returns: Type Description [ | ] The list of aliases. search_dict_for_path ¶ search_dict_for_path(d: ) -> Searches a dictionary for the path specified by the alias. Returns: Type Description The value at the specified path, or PydanticUndefined if the path is not found. AliasChoices dataclass ¶ AliasChoices(\n    first_choice:  | , *choices:  | \n) Usage Documentation AliasPath and AliasChoices A data class used by validation_alias as a convenience to create aliases. Attributes: Name Type Description [ | ] A list containing a string or AliasPath . convert_to_aliases ¶ convert_to_aliases() -> [[ | ]] Converts arguments to a list of lists containing string or integer aliases. Returns: Type Description [[ | ]] The list of aliases. AliasGenerator dataclass ¶ AliasGenerator(\n    alias: [[], ] | None = None,\n    validation_alias: (\n        [[],  |  | ]\n        | None\n    ) = None,\n    serialization_alias: [[], ] | None = None,\n) Usage Documentation Using an AliasGenerator A data class used by alias_generator as a convenience to create various aliases. Attributes: Name Type Description [[], ] | None A callable that takes a field name and returns an alias for it. [[],  |  | ] | None A callable that takes a field name and returns a validation alias for it. [[], ] | None A callable that takes a field name and returns a serialization alias for it. generate_aliases ¶ generate_aliases(\n    field_name: ,\n) -> [\n     | None,\n     |  |  | None,\n     | None,\n] Generate alias , validation_alias , and serialization_alias for a field. Returns: Type Description [ | None,  |  |  | None,  | None] A tuple of three aliases - validation, alias, and serialization.","pageID":"Aliases","abs_url":"/latest/api/aliases/#Aliases","title":"Aliases","objectID":"/latest/api/aliases/#Aliases","rank":100},{"content":"AliasPath(first_arg: , *args:  | ) Usage Documentation AliasPath and AliasChoices A data class used by validation_alias as a convenience to create aliases. Attributes: Name Type Description [ | ] A list of string or integer aliases. convert_to_aliases ¶ convert_to_aliases() -> [ | ] Converts arguments to a list of string or integer aliases. Returns: Type Description [ | ] The list of aliases. search_dict_for_path ¶ search_dict_for_path(d: ) -> Searches a dictionary for the path specified by the alias. Returns: Type Description The value at the specified path, or PydanticUndefined if the path is not found.","pageID":"Aliases","abs_url":"/latest/api/aliases/#pydantic.aliases.AliasPath","title":"Aliases - AliasPath  dataclass","objectID":"/latest/api/aliases/#pydantic.aliases.AliasPath","rank":95},{"content":"convert_to_aliases() -> [ | ] Converts arguments to a list of string or integer aliases. Returns: Type Description [ | ] The list of aliases.","pageID":"Aliases","abs_url":"/latest/api/aliases/#pydantic.aliases.AliasPath.convert_to_aliases","title":"Aliases - AliasPath  dataclass - convert_to_aliases","objectID":"/latest/api/aliases/#pydantic.aliases.AliasPath.convert_to_aliases","rank":90},{"content":"search_dict_for_path(d: ) -> Searches a dictionary for the path specified by the alias. Returns: Type Description The value at the specified path, or PydanticUndefined if the path is not found.","pageID":"Aliases","abs_url":"/latest/api/aliases/#pydantic.aliases.AliasPath.search_dict_for_path","title":"Aliases - AliasPath  dataclass - search_dict_for_path","objectID":"/latest/api/aliases/#pydantic.aliases.AliasPath.search_dict_for_path","rank":85},{"content":"AliasChoices(\n    first_choice:  | , *choices:  | \n) Usage Documentation AliasPath and AliasChoices A data class used by validation_alias as a convenience to create aliases. Attributes: Name Type Description [ | ] A list containing a string or AliasPath . convert_to_aliases ¶ convert_to_aliases() -> [[ | ]] Converts arguments to a list of lists containing string or integer aliases. Returns: Type Description [[ | ]] The list of aliases.","pageID":"Aliases","abs_url":"/latest/api/aliases/#pydantic.aliases.AliasChoices","title":"Aliases - AliasChoices  dataclass","objectID":"/latest/api/aliases/#pydantic.aliases.AliasChoices","rank":80},{"content":"convert_to_aliases() -> [[ | ]] Converts arguments to a list of lists containing string or integer aliases. Returns: Type Description [[ | ]] The list of aliases.","pageID":"Aliases","abs_url":"/latest/api/aliases/#pydantic.aliases.AliasChoices.convert_to_aliases","title":"Aliases - AliasChoices  dataclass - convert_to_aliases","objectID":"/latest/api/aliases/#pydantic.aliases.AliasChoices.convert_to_aliases","rank":75},{"content":"AliasGenerator(\n    alias: [[], ] | None = None,\n    validation_alias: (\n        [[],  |  | ]\n        | None\n    ) = None,\n    serialization_alias: [[], ] | None = None,\n) Usage Documentation Using an AliasGenerator A data class used by alias_generator as a convenience to create various aliases. Attributes: Name Type Description [[], ] | None A callable that takes a field name and returns an alias for it. [[],  |  | ] | None A callable that takes a field name and returns a validation alias for it. [[], ] | None A callable that takes a field name and returns a serialization alias for it. generate_aliases ¶ generate_aliases(\n    field_name: ,\n) -> [\n     | None,\n     |  |  | None,\n     | None,\n] Generate alias , validation_alias , and serialization_alias for a field. Returns: Type Description [ | None,  |  |  | None,  | None] A tuple of three aliases - validation, alias, and serialization.","pageID":"Aliases","abs_url":"/latest/api/aliases/#pydantic.aliases.AliasGenerator","title":"Aliases - AliasGenerator  dataclass","objectID":"/latest/api/aliases/#pydantic.aliases.AliasGenerator","rank":70},{"content":"generate_aliases(\n    field_name: ,\n) -> [\n     | None,\n     |  |  | None,\n     | None,\n] Generate alias , validation_alias , and serialization_alias for a field. Returns: Type Description [ | None,  |  |  | None,  | None] A tuple of three aliases - validation, alias, and serialization.","pageID":"Aliases","abs_url":"/latest/api/aliases/#pydantic.aliases.AliasGenerator.generate_aliases","title":"Aliases - AliasGenerator  dataclass - generate_aliases","objectID":"/latest/api/aliases/#pydantic.aliases.AliasGenerator.generate_aliases","rank":65},{"content":"Type annotations to use with __get_pydantic_core_schema__ and __get_pydantic_json_schema__ . GetJsonSchemaHandler ¶ Handler to call into the next JSON schema generation function. Attributes: Name Type Description Json schema mode, can be validation or serialization . resolve_ref_schema ¶ resolve_ref_schema(\n    maybe_ref_json_schema: ,\n) -> Get the real schema for a {\"$ref\": ...} schema.\nIf the schema given is not a $ref schema, it will be returned as is.\nThis means you don't have to check before calling this function. Parameters: Name Type Description Default maybe_ref_json_schema A JsonSchemaValue which may be a $ref schema. required Raises: Type Description If the ref is not found. Returns: Name Type Description JsonSchemaValue A JsonSchemaValue that has no $ref . GetCoreSchemaHandler ¶ Handler to call into the next CoreSchema schema generation function. field_name property ¶ field_name:  | None Get the name of the closest field to this validator. generate_schema ¶ generate_schema(source_type: ) -> Generate a schema unrelated to the current context.\nUse this function if e.g. you are handling schema generation for a sequence\nand want to generate a schema for its items.\nOtherwise, you may end up doing something like applying a min_length constraint\nthat was intended for the sequence itself to its items! Parameters: Name Type Description Default source_type The input type. required Returns: Name Type Description CoreSchema The pydantic-core CoreSchema generated. resolve_ref_schema ¶ resolve_ref_schema(\n    maybe_ref_schema: ,\n) -> Get the real schema for a definition-ref schema.\nIf the schema given is not a definition-ref schema, it will be returned as is.\nThis means you don't have to check before calling this function. Parameters: Name Type Description Default maybe_ref_schema A CoreSchema , ref -based or not. required Raises: Type Description If the ref is not found. Returns: Type Description A concrete CoreSchema .","pageID":"Annotated Handlers","abs_url":"/latest/api/annotated_handlers/#Annotated Handlers","title":"Annotated Handlers","objectID":"/latest/api/annotated_handlers/#Annotated Handlers","rank":100},{"content":"Handler to call into the next JSON schema generation function. Attributes: Name Type Description Json schema mode, can be validation or serialization . resolve_ref_schema ¶ resolve_ref_schema(\n    maybe_ref_json_schema: ,\n) -> Get the real schema for a {\"$ref\": ...} schema.\nIf the schema given is not a $ref schema, it will be returned as is.\nThis means you don't have to check before calling this function. Parameters: Name Type Description Default maybe_ref_json_schema A JsonSchemaValue which may be a $ref schema. required Raises: Type Description If the ref is not found. Returns: Name Type Description JsonSchemaValue A JsonSchemaValue that has no $ref .","pageID":"Annotated Handlers","abs_url":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetJsonSchemaHandler","title":"Annotated Handlers - GetJsonSchemaHandler","objectID":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetJsonSchemaHandler","rank":95},{"content":"resolve_ref_schema(\n    maybe_ref_json_schema: ,\n) -> Get the real schema for a {\"$ref\": ...} schema.\nIf the schema given is not a $ref schema, it will be returned as is.\nThis means you don't have to check before calling this function. Parameters: Name Type Description Default maybe_ref_json_schema A JsonSchemaValue which may be a $ref schema. required Raises: Type Description If the ref is not found. Returns: Name Type Description JsonSchemaValue A JsonSchemaValue that has no $ref .","pageID":"Annotated Handlers","abs_url":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetJsonSchemaHandler.resolve_ref_schema","title":"Annotated Handlers - GetJsonSchemaHandler - resolve_ref_schema","objectID":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetJsonSchemaHandler.resolve_ref_schema","rank":90},{"content":"Handler to call into the next CoreSchema schema generation function. field_name property ¶ field_name:  | None Get the name of the closest field to this validator. generate_schema ¶ generate_schema(source_type: ) -> Generate a schema unrelated to the current context.\nUse this function if e.g. you are handling schema generation for a sequence\nand want to generate a schema for its items.\nOtherwise, you may end up doing something like applying a min_length constraint\nthat was intended for the sequence itself to its items! Parameters: Name Type Description Default source_type The input type. required Returns: Name Type Description CoreSchema The pydantic-core CoreSchema generated. resolve_ref_schema ¶ resolve_ref_schema(\n    maybe_ref_schema: ,\n) -> Get the real schema for a definition-ref schema.\nIf the schema given is not a definition-ref schema, it will be returned as is.\nThis means you don't have to check before calling this function. Parameters: Name Type Description Default maybe_ref_schema A CoreSchema , ref -based or not. required Raises: Type Description If the ref is not found. Returns: Type Description A concrete CoreSchema .","pageID":"Annotated Handlers","abs_url":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler","title":"Annotated Handlers - GetCoreSchemaHandler","objectID":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler","rank":85},{"content":"field_name:  | None Get the name of the closest field to this validator.","pageID":"Annotated Handlers","abs_url":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler.field_name","title":"Annotated Handlers - GetCoreSchemaHandler - field_name  property","objectID":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler.field_name","rank":80},{"content":"generate_schema(source_type: ) -> Generate a schema unrelated to the current context.\nUse this function if e.g. you are handling schema generation for a sequence\nand want to generate a schema for its items.\nOtherwise, you may end up doing something like applying a min_length constraint\nthat was intended for the sequence itself to its items! Parameters: Name Type Description Default source_type The input type. required Returns: Name Type Description CoreSchema The pydantic-core CoreSchema generated.","pageID":"Annotated Handlers","abs_url":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler.generate_schema","title":"Annotated Handlers - GetCoreSchemaHandler - generate_schema","objectID":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler.generate_schema","rank":75},{"content":"resolve_ref_schema(\n    maybe_ref_schema: ,\n) -> Get the real schema for a definition-ref schema.\nIf the schema given is not a definition-ref schema, it will be returned as is.\nThis means you don't have to check before calling this function. Parameters: Name Type Description Default maybe_ref_schema A CoreSchema , ref -based or not. required Raises: Type Description If the ref is not found. Returns: Type Description A concrete CoreSchema .","pageID":"Annotated Handlers","abs_url":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler.resolve_ref_schema","title":"Annotated Handlers - GetCoreSchemaHandler - resolve_ref_schema","objectID":"/latest/api/annotated_handlers/#pydantic.annotated_handlers.GetCoreSchemaHandler.resolve_ref_schema","rank":70},{"content":"Pydantic models are simply classes which inherit from BaseModel and define fields as annotated attributes. pydantic.BaseModel ¶ Usage Documentation Models A base class for creating Pydantic models. Attributes: Name Type Description [] The names of the class variables defined on the model. [, ] Metadata about the private attributes of the model. The synthesized __init__ of the model. Whether model building is completed, or if there are still undefined fields. The core schema of the model. Whether the model has a custom __init__ function. Metadata containing the decorators defined on the model.\nThis replaces Model.__validators__ and Model.__root_validators__ from Pydantic V1. Metadata for generic models; contains data used for a similar purpose to args , origin , parameters in typing-module generics. May eventually be replaced by these. [, ] | None Parent namespace of the model, used for automatic rebuilding of models. None | ['model_post_init'] The name of the post-init method for the model, if defined. Whether the model is a . The pydantic-core SchemaSerializer used to dump instances of the model. | The pydantic-core SchemaValidator used to validate instances of the model. [, ] A dictionary of field names and their corresponding  objects. [, ] A dictionary of computed field names and their corresponding  objects. [, ] | None A dictionary containing extra values, if \nis set to 'allow' . [] The names of fields explicitly set during instantiation. [, ] | None Values of private attributes set on the model instance. __init__ ¶ __init__(**data: ) -> None Raises  if the input data cannot be\nvalidated to form a valid model. self is explicitly positional-only to allow self as a field name. model_config class-attribute ¶ model_config:  = () Configuration for the model, should be a dictionary conforming to . model_fields classmethod ¶ model_fields() -> [, ] A mapping of field names to their respective  instances. Warning Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.\nInstead, you should access this attribute from the model class. model_computed_fields classmethod ¶ model_computed_fields() -> [, ] A mapping of computed field names to their respective  instances. Warning Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.\nInstead, you should access this attribute from the model class. __pydantic_core_schema__ class-attribute ¶ __pydantic_core_schema__: The core schema of the model. model_extra property ¶ model_extra: [, ] | None Get extra fields set during validation. Returns: Type Description [, ] | None A dictionary of extra fields, or None if config.extra is not set to \"allow\" . model_fields_set property ¶ model_fields_set: [] Returns the set of fields that have been explicitly set on this model instance. Returns: Type Description [] A set of strings representing the fields that have been set,\ni.e. that were not filled from defaults. model_construct classmethod ¶ model_construct(\n    _fields_set: [] | None = None, **values: \n) -> Creates a new instance of the Model class with validated data. Creates a new model setting __dict__ and __pydantic_fields_set__ from trusted or pre-validated data.\nDefault values are respected, but no other validation is performed. Note model_construct() generally respects the model_config.extra setting on the provided model.\nThat is, if model_config.extra == 'allow' , then all extra passed values are added to the model instance's __dict__ and __pydantic_extra__ fields. If model_config.extra == 'ignore' (the default), then all extra passed values are ignored.\nBecause no validation is performed with a call to model_construct() , having model_config.extra == 'forbid' does not result in\nan error if extra values are passed, but they will be ignored. Parameters: Name Type Description Default _fields_set [] | None A set of field names that were originally explicitly set during instantiation. If provided,\nthis is directly used for the  attribute.\nOtherwise, the field names from the values argument will be used. None values Trusted or pre-validated data dictionary. {} Returns: Type Description A new instance of the Model class with validated data. model_copy ¶ model_copy(\n    *,\n    update: [, ] | None = None,\n    deep:  = False\n) -> Usage Documentation model_copy Returns a copy of the model. Note The underlying instance's  attribute is copied. This\nmight have unexpected side effects if you store anything in it, on top of the model\nfields (e.g. the value of ). Parameters: Name Type Description Default update [, ] | None Values to change/add in the new model. Note: the data is not validated\nbefore creating the new model. You should trust this data. None deep Set to True to make a deep copy of the model. False Returns: Type Description New model instance. model_dump ¶ model_dump(\n    *,\n    mode: [\"json\", \"python\"] |  = \"python\",\n    include:  | None = None,\n    exclude:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False\n) -> [, ] Usage Documentation model_dump Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. Parameters: Name Type Description Default mode ['json', 'python'] | The mode in which to_python should run.\nIf mode is 'json', the output will only contain JSON serializable types.\nIf mode is 'python', the output may contain non-JSON-serializable Python objects. 'python' include | None A set of fields to include in the output. None exclude | None A set of fields to exclude from the output. None context | None Additional context to pass to the serializer. None by_alias | None Whether to use the field's alias in the dictionary key if defined. None exclude_unset Whether to exclude fields that have not been explicitly set. False exclude_defaults Whether to exclude fields that are set to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip If True, dumped values should be valid as input for non-idempotent types such as Json[T]. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False Returns: Type Description [, ] A dictionary representation of the model. model_dump_json ¶ model_dump_json(\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False\n) -> Usage Documentation model_dump_json Generates a JSON representation of the model using Pydantic's to_json method. Parameters: Name Type Description Default indent | None Indentation to use in the JSON output. If None is passed, the output will be compact. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None Field(s) to include in the JSON output. None exclude | None Field(s) to exclude from the JSON output. None context | None Additional context to pass to the serializer. None by_alias | None Whether to serialize using field aliases. None exclude_unset Whether to exclude fields that have not been explicitly set. False exclude_defaults Whether to exclude fields that are set to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip If True, dumped values should be valid as input for non-idempotent types such as Json[T]. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False Returns: Type Description A JSON string representation of the model. model_json_schema classmethod ¶ model_json_schema(\n    by_alias:  = True,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n    mode:  = \"validation\",\n) -> [, ] Generates a JSON schema for a model class. Parameters: Name Type Description Default by_alias Whether to use attribute aliases or not. True ref_template The reference template. schema_generator [] To override the logic used to generate the JSON schema, as a subclass of GenerateJsonSchema with your desired modifications mode The mode in which to generate the schema. 'validation' Returns: Type Description [, ] The JSON schema for the given model class. model_parametrized_name classmethod ¶ model_parametrized_name(\n    params: [[], ...]\n) -> Compute the class name for parametrizations of generic classes. This method can be overridden to achieve a custom naming scheme for generic BaseModels. Parameters: Name Type Description Default params [[], ...] Tuple of types of the class. Given a generic class Model with 2 type variables and a concrete model Model[str, int] ,\nthe value (str, int) would be passed to params . required Returns: Type Description String representing the new class where params are passed to cls as type variables. Raises: Type Description Raised when trying to generate concrete names for non-generic models. model_post_init ¶ model_post_init(context: ) -> None Override this method to perform additional initialization after __init__ and model_construct .\nThis is useful if you want to do some validation that requires the entire model to be initialized. model_rebuild classmethod ¶ model_rebuild(\n    *,\n    force:  = False,\n    raise_errors:  = True,\n    _parent_namespace_depth:  = 2,\n    _types_namespace:  | None = None\n) ->  | None Try to rebuild the pydantic-core schema for the model. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\nthe initial attempt to build the schema, and automatic rebuilding fails. Parameters: Name Type Description Default force Whether to force the rebuilding of the model schema, defaults to False . False raise_errors Whether to raise errors, defaults to True . True _parent_namespace_depth The depth level of the parent namespace, defaults to 2. 2 _types_namespace | None The types namespace, defaults to None . None Returns: Type Description | None Returns None if the schema is already \"complete\" and rebuilding was not required. | None If rebuilding was required, returns True if rebuilding was successful, otherwise False . model_validate classmethod ¶ model_validate(\n    obj: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a pydantic model instance. Parameters: Name Type Description Default obj The object to validate. required strict | None Whether to enforce types strictly. None from_attributes | None Whether to extract data from object attributes. None context | None Additional context to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If the object could not be validated. Returns: Type Description The validated model instance. model_validate_json classmethod ¶ model_validate_json(\n    json_data:  |  | ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Usage Documentation JSON Parsing Validate the given JSON data against the Pydantic model. Parameters: Name Type Description Default json_data |  | The JSON data to validate. required strict | None Whether to enforce types strictly. None context | None Extra variables to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated Pydantic model. Raises: Type Description If json_data is not a JSON string or the object could not be validated. model_validate_strings classmethod ¶ model_validate_strings(\n    obj: ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate the given object with string data against the Pydantic model. Parameters: Name Type Description Default obj The object containing string data to validate. required strict | None Whether to enforce types strictly. None context | None Extra variables to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated Pydantic model. pydantic.create_model ¶ create_model(\n    model_name: ,\n    /,\n    *,\n    __config__:  | None = None,\n    __doc__:  | None = None,\n    __base__: None = None,\n    __module__:  = ,\n    __validators__: (\n        [, [..., ]] | None\n    ) = None,\n    __cls_kwargs__: [, ] | None = None,\n    **field_definitions:  | [, ],\n) -> [] create_model(\n    model_name: ,\n    /,\n    *,\n    __config__:  | None = None,\n    __doc__:  | None = None,\n    __base__: [] | [[], ...],\n    __module__:  = ,\n    __validators__: (\n        [, [..., ]] | None\n    ) = None,\n    __cls_kwargs__: [, ] | None = None,\n    **field_definitions:  | [, ],\n) -> [] create_model(\n    model_name: ,\n    /,\n    *,\n    __config__:  | None = None,\n    __doc__:  | None = None,\n    __base__: (\n        [] | [[], ...] | None\n    ) = None,\n    __module__:  | None = None,\n    __validators__: (\n        [, [..., ]] | None\n    ) = None,\n    __cls_kwargs__: [, ] | None = None,\n    **field_definitions:  | [, ],\n) -> [] Usage Documentation Dynamic Model Creation Dynamically creates and returns a new Pydantic model, in other words, create_model dynamically creates a\nsubclass of . Parameters: Name Type Description Default model_name The name of the newly created model. required __config__ | None The configuration of the new model. None __doc__ | None The docstring of the new model. None __base__ [] | [[], ...] | None The base class or classes for the new model. None __module__ | None The name of the module that the model belongs to;\nif None , the value is taken from sys._getframe(1) None __validators__ [, [..., ]] | None A dictionary of methods that validate fields. The keys are the names of the validation methods to\nbe added to the model, and the values are the validation methods themselves. You can read more about functional\nvalidators here . None __cls_kwargs__ [, ] | None A dictionary of keyword arguments for class creation, such as metaclass . None **field_definitions | [, ] Field definitions of the new model. Either: a single element, representing the type annotation of the field. a two-tuple, the first element being the type and the second element the assigned value\n  (either a default or the  function). {} Returns: Type Description [] The new . Raises: Type Description If __base__ and __config__ are both passed.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#BaseModel","title":"BaseModel","objectID":"/latest/api/base_model/#BaseModel","rank":100},{"content":"Usage Documentation Models A base class for creating Pydantic models. Attributes: Name Type Description [] The names of the class variables defined on the model. [, ] Metadata about the private attributes of the model. The synthesized __init__ of the model. Whether model building is completed, or if there are still undefined fields. The core schema of the model. Whether the model has a custom __init__ function. Metadata containing the decorators defined on the model.\nThis replaces Model.__validators__ and Model.__root_validators__ from Pydantic V1. Metadata for generic models; contains data used for a similar purpose to args , origin , parameters in typing-module generics. May eventually be replaced by these. [, ] | None Parent namespace of the model, used for automatic rebuilding of models. None | ['model_post_init'] The name of the post-init method for the model, if defined. Whether the model is a . The pydantic-core SchemaSerializer used to dump instances of the model. | The pydantic-core SchemaValidator used to validate instances of the model. [, ] A dictionary of field names and their corresponding  objects. [, ] A dictionary of computed field names and their corresponding  objects. [, ] | None A dictionary containing extra values, if \nis set to 'allow' . [] The names of fields explicitly set during instantiation. [, ] | None Values of private attributes set on the model instance. __init__ ¶ __init__(**data: ) -> None Raises  if the input data cannot be\nvalidated to form a valid model. self is explicitly positional-only to allow self as a field name. model_config class-attribute ¶ model_config:  = () Configuration for the model, should be a dictionary conforming to . model_fields classmethod ¶ model_fields() -> [, ] A mapping of field names to their respective  instances. Warning Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.\nInstead, you should access this attribute from the model class. model_computed_fields classmethod ¶ model_computed_fields() -> [, ] A mapping of computed field names to their respective  instances. Warning Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.\nInstead, you should access this attribute from the model class. __pydantic_core_schema__ class-attribute ¶ __pydantic_core_schema__: The core schema of the model. model_extra property ¶ model_extra: [, ] | None Get extra fields set during validation. Returns: Type Description [, ] | None A dictionary of extra fields, or None if config.extra is not set to \"allow\" . model_fields_set property ¶ model_fields_set: [] Returns the set of fields that have been explicitly set on this model instance. Returns: Type Description [] A set of strings representing the fields that have been set,\ni.e. that were not filled from defaults. model_construct classmethod ¶ model_construct(\n    _fields_set: [] | None = None, **values: \n) -> Creates a new instance of the Model class with validated data. Creates a new model setting __dict__ and __pydantic_fields_set__ from trusted or pre-validated data.\nDefault values are respected, but no other validation is performed. Note model_construct() generally respects the model_config.extra setting on the provided model.\nThat is, if model_config.extra == 'allow' , then all extra passed values are added to the model instance's __dict__ and __pydantic_extra__ fields. If model_config.extra == 'ignore' (the default), then all extra passed values are ignored.\nBecause no validation is performed with a call to model_construct() , having model_config.extra == 'forbid' does not result in\nan error if extra values are passed, but they will be ignored. Parameters: Name Type Description Default _fields_set [] | None A set of field names that were originally explicitly set during instantiation. If provided,\nthis is directly used for the  attribute.\nOtherwise, the field names from the values argument will be used. None values Trusted or pre-validated data dictionary. {} Returns: Type Description A new instance of the Model class with validated data. model_copy ¶ model_copy(\n    *,\n    update: [, ] | None = None,\n    deep:  = False\n) -> Usage Documentation model_copy Returns a copy of the model. Note The underlying instance's  attribute is copied. This\nmight have unexpected side effects if you store anything in it, on top of the model\nfields (e.g. the value of ). Parameters: Name Type Description Default update [, ] | None Values to change/add in the new model. Note: the data is not validated\nbefore creating the new model. You should trust this data. None deep Set to True to make a deep copy of the model. False Returns: Type Description New model instance. model_dump ¶ model_dump(\n    *,\n    mode: [\"json\", \"python\"] |  = \"python\",\n    include:  | None = None,\n    exclude:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False\n) -> [, ] Usage Documentation model_dump Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. Parameters: Name Type Description Default mode ['json', 'python'] | The mode in which to_python should run.\nIf mode is 'json', the output will only contain JSON serializable types.\nIf mode is 'python', the output may contain non-JSON-serializable Python objects. 'python' include | None A set of fields to include in the output. None exclude | None A set of fields to exclude from the output. None context | None Additional context to pass to the serializer. None by_alias | None Whether to use the field's alias in the dictionary key if defined. None exclude_unset Whether to exclude fields that have not been explicitly set. False exclude_defaults Whether to exclude fields that are set to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip If True, dumped values should be valid as input for non-idempotent types such as Json[T]. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False Returns: Type Description [, ] A dictionary representation of the model. model_dump_json ¶ model_dump_json(\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False\n) -> Usage Documentation model_dump_json Generates a JSON representation of the model using Pydantic's to_json method. Parameters: Name Type Description Default indent | None Indentation to use in the JSON output. If None is passed, the output will be compact. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None Field(s) to include in the JSON output. None exclude | None Field(s) to exclude from the JSON output. None context | None Additional context to pass to the serializer. None by_alias | None Whether to serialize using field aliases. None exclude_unset Whether to exclude fields that have not been explicitly set. False exclude_defaults Whether to exclude fields that are set to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip If True, dumped values should be valid as input for non-idempotent types such as Json[T]. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False Returns: Type Description A JSON string representation of the model. model_json_schema classmethod ¶ model_json_schema(\n    by_alias:  = True,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n    mode:  = \"validation\",\n) -> [, ] Generates a JSON schema for a model class. Parameters: Name Type Description Default by_alias Whether to use attribute aliases or not. True ref_template The reference template. schema_generator [] To override the logic used to generate the JSON schema, as a subclass of GenerateJsonSchema with your desired modifications mode The mode in which to generate the schema. 'validation' Returns: Type Description [, ] The JSON schema for the given model class. model_parametrized_name classmethod ¶ model_parametrized_name(\n    params: [[], ...]\n) -> Compute the class name for parametrizations of generic classes. This method can be overridden to achieve a custom naming scheme for generic BaseModels. Parameters: Name Type Description Default params [[], ...] Tuple of types of the class. Given a generic class Model with 2 type variables and a concrete model Model[str, int] ,\nthe value (str, int) would be passed to params . required Returns: Type Description String representing the new class where params are passed to cls as type variables. Raises: Type Description Raised when trying to generate concrete names for non-generic models. model_post_init ¶ model_post_init(context: ) -> None Override this method to perform additional initialization after __init__ and model_construct .\nThis is useful if you want to do some validation that requires the entire model to be initialized. model_rebuild classmethod ¶ model_rebuild(\n    *,\n    force:  = False,\n    raise_errors:  = True,\n    _parent_namespace_depth:  = 2,\n    _types_namespace:  | None = None\n) ->  | None Try to rebuild the pydantic-core schema for the model. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\nthe initial attempt to build the schema, and automatic rebuilding fails. Parameters: Name Type Description Default force Whether to force the rebuilding of the model schema, defaults to False . False raise_errors Whether to raise errors, defaults to True . True _parent_namespace_depth The depth level of the parent namespace, defaults to 2. 2 _types_namespace | None The types namespace, defaults to None . None Returns: Type Description | None Returns None if the schema is already \"complete\" and rebuilding was not required. | None If rebuilding was required, returns True if rebuilding was successful, otherwise False . model_validate classmethod ¶ model_validate(\n    obj: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a pydantic model instance. Parameters: Name Type Description Default obj The object to validate. required strict | None Whether to enforce types strictly. None from_attributes | None Whether to extract data from object attributes. None context | None Additional context to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If the object could not be validated. Returns: Type Description The validated model instance. model_validate_json classmethod ¶ model_validate_json(\n    json_data:  |  | ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Usage Documentation JSON Parsing Validate the given JSON data against the Pydantic model. Parameters: Name Type Description Default json_data |  | The JSON data to validate. required strict | None Whether to enforce types strictly. None context | None Extra variables to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated Pydantic model. Raises: Type Description If json_data is not a JSON string or the object could not be validated. model_validate_strings classmethod ¶ model_validate_strings(\n    obj: ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate the given object with string data against the Pydantic model. Parameters: Name Type Description Default obj The object containing string data to validate. required strict | None Whether to enforce types strictly. None context | None Extra variables to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated Pydantic model.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel","title":"BaseModel - pydantic.BaseModel","objectID":"/latest/api/base_model/#pydantic.BaseModel","rank":95},{"content":"__init__(**data: ) -> None Raises  if the input data cannot be\nvalidated to form a valid model. self is explicitly positional-only to allow self as a field name.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.__init__","title":"BaseModel - pydantic.BaseModel - __init__","objectID":"/latest/api/base_model/#pydantic.BaseModel.__init__","rank":90},{"content":"model_config:  = () Configuration for the model, should be a dictionary conforming to .","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_config","title":"BaseModel - pydantic.BaseModel - model_config  class-attribute","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_config","rank":85},{"content":"model_fields() -> [, ] A mapping of field names to their respective  instances. Warning Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.\nInstead, you should access this attribute from the model class.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_fields","title":"BaseModel - pydantic.BaseModel - model_fields  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_fields","rank":80},{"content":"model_computed_fields() -> [, ] A mapping of computed field names to their respective  instances. Warning Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.\nInstead, you should access this attribute from the model class.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_computed_fields","title":"BaseModel - pydantic.BaseModel - model_computed_fields  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_computed_fields","rank":75},{"content":"__pydantic_core_schema__: The core schema of the model.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.__pydantic_core_schema__","title":"BaseModel - pydantic.BaseModel - __pydantic_core_schema__  class-attribute","objectID":"/latest/api/base_model/#pydantic.BaseModel.__pydantic_core_schema__","rank":70},{"content":"model_extra: [, ] | None Get extra fields set during validation. Returns: Type Description [, ] | None A dictionary of extra fields, or None if config.extra is not set to \"allow\" .","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_extra","title":"BaseModel - pydantic.BaseModel - model_extra  property","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_extra","rank":65},{"content":"model_fields_set: [] Returns the set of fields that have been explicitly set on this model instance. Returns: Type Description [] A set of strings representing the fields that have been set,\ni.e. that were not filled from defaults.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_fields_set","title":"BaseModel - pydantic.BaseModel - model_fields_set  property","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_fields_set","rank":60},{"content":"model_construct(\n    _fields_set: [] | None = None, **values: \n) -> Creates a new instance of the Model class with validated data. Creates a new model setting __dict__ and __pydantic_fields_set__ from trusted or pre-validated data.\nDefault values are respected, but no other validation is performed. Note model_construct() generally respects the model_config.extra setting on the provided model.\nThat is, if model_config.extra == 'allow' , then all extra passed values are added to the model instance's __dict__ and __pydantic_extra__ fields. If model_config.extra == 'ignore' (the default), then all extra passed values are ignored.\nBecause no validation is performed with a call to model_construct() , having model_config.extra == 'forbid' does not result in\nan error if extra values are passed, but they will be ignored. Parameters: Name Type Description Default _fields_set [] | None A set of field names that were originally explicitly set during instantiation. If provided,\nthis is directly used for the  attribute.\nOtherwise, the field names from the values argument will be used. None values Trusted or pre-validated data dictionary. {} Returns: Type Description A new instance of the Model class with validated data.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_construct","title":"BaseModel - pydantic.BaseModel - model_construct  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_construct","rank":55},{"content":"model_copy(\n    *,\n    update: [, ] | None = None,\n    deep:  = False\n) -> Usage Documentation model_copy Returns a copy of the model. Note The underlying instance's  attribute is copied. This\nmight have unexpected side effects if you store anything in it, on top of the model\nfields (e.g. the value of ). Parameters: Name Type Description Default update [, ] | None Values to change/add in the new model. Note: the data is not validated\nbefore creating the new model. You should trust this data. None deep Set to True to make a deep copy of the model. False Returns: Type Description New model instance.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_copy","title":"BaseModel - pydantic.BaseModel - model_copy","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_copy","rank":50},{"content":"model_dump(\n    *,\n    mode: [\"json\", \"python\"] |  = \"python\",\n    include:  | None = None,\n    exclude:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False\n) -> [, ] Usage Documentation model_dump Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. Parameters: Name Type Description Default mode ['json', 'python'] | The mode in which to_python should run.\nIf mode is 'json', the output will only contain JSON serializable types.\nIf mode is 'python', the output may contain non-JSON-serializable Python objects. 'python' include | None A set of fields to include in the output. None exclude | None A set of fields to exclude from the output. None context | None Additional context to pass to the serializer. None by_alias | None Whether to use the field's alias in the dictionary key if defined. None exclude_unset Whether to exclude fields that have not been explicitly set. False exclude_defaults Whether to exclude fields that are set to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip If True, dumped values should be valid as input for non-idempotent types such as Json[T]. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False Returns: Type Description [, ] A dictionary representation of the model.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_dump","title":"BaseModel - pydantic.BaseModel - model_dump","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_dump","rank":45},{"content":"model_dump_json(\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False\n) -> Usage Documentation model_dump_json Generates a JSON representation of the model using Pydantic's to_json method. Parameters: Name Type Description Default indent | None Indentation to use in the JSON output. If None is passed, the output will be compact. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None Field(s) to include in the JSON output. None exclude | None Field(s) to exclude from the JSON output. None context | None Additional context to pass to the serializer. None by_alias | None Whether to serialize using field aliases. None exclude_unset Whether to exclude fields that have not been explicitly set. False exclude_defaults Whether to exclude fields that are set to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip If True, dumped values should be valid as input for non-idempotent types such as Json[T]. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False Returns: Type Description A JSON string representation of the model.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_dump_json","title":"BaseModel - pydantic.BaseModel - model_dump_json","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_dump_json","rank":40},{"content":"model_json_schema(\n    by_alias:  = True,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n    mode:  = \"validation\",\n) -> [, ] Generates a JSON schema for a model class. Parameters: Name Type Description Default by_alias Whether to use attribute aliases or not. True ref_template The reference template. schema_generator [] To override the logic used to generate the JSON schema, as a subclass of GenerateJsonSchema with your desired modifications mode The mode in which to generate the schema. 'validation' Returns: Type Description [, ] The JSON schema for the given model class.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_json_schema","title":"BaseModel - pydantic.BaseModel - model_json_schema  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_json_schema","rank":35},{"content":"model_parametrized_name(\n    params: [[], ...]\n) -> Compute the class name for parametrizations of generic classes. This method can be overridden to achieve a custom naming scheme for generic BaseModels. Parameters: Name Type Description Default params [[], ...] Tuple of types of the class. Given a generic class Model with 2 type variables and a concrete model Model[str, int] ,\nthe value (str, int) would be passed to params . required Returns: Type Description String representing the new class where params are passed to cls as type variables. Raises: Type Description Raised when trying to generate concrete names for non-generic models.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_parametrized_name","title":"BaseModel - pydantic.BaseModel - model_parametrized_name  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_parametrized_name","rank":30},{"content":"model_post_init(context: ) -> None Override this method to perform additional initialization after __init__ and model_construct .\nThis is useful if you want to do some validation that requires the entire model to be initialized.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_post_init","title":"BaseModel - pydantic.BaseModel - model_post_init","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_post_init","rank":25},{"content":"model_rebuild(\n    *,\n    force:  = False,\n    raise_errors:  = True,\n    _parent_namespace_depth:  = 2,\n    _types_namespace:  | None = None\n) ->  | None Try to rebuild the pydantic-core schema for the model. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\nthe initial attempt to build the schema, and automatic rebuilding fails. Parameters: Name Type Description Default force Whether to force the rebuilding of the model schema, defaults to False . False raise_errors Whether to raise errors, defaults to True . True _parent_namespace_depth The depth level of the parent namespace, defaults to 2. 2 _types_namespace | None The types namespace, defaults to None . None Returns: Type Description | None Returns None if the schema is already \"complete\" and rebuilding was not required. | None If rebuilding was required, returns True if rebuilding was successful, otherwise False .","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_rebuild","title":"BaseModel - pydantic.BaseModel - model_rebuild  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_rebuild","rank":20},{"content":"model_validate(\n    obj: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a pydantic model instance. Parameters: Name Type Description Default obj The object to validate. required strict | None Whether to enforce types strictly. None from_attributes | None Whether to extract data from object attributes. None context | None Additional context to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If the object could not be validated. Returns: Type Description The validated model instance.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_validate","title":"BaseModel - pydantic.BaseModel - model_validate  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_validate","rank":15},{"content":"model_validate_json(\n    json_data:  |  | ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Usage Documentation JSON Parsing Validate the given JSON data against the Pydantic model. Parameters: Name Type Description Default json_data |  | The JSON data to validate. required strict | None Whether to enforce types strictly. None context | None Extra variables to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated Pydantic model. Raises: Type Description If json_data is not a JSON string or the object could not be validated.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_validate_json","title":"BaseModel - pydantic.BaseModel - model_validate_json  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_validate_json","rank":10},{"content":"model_validate_strings(\n    obj: ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate the given object with string data against the Pydantic model. Parameters: Name Type Description Default obj The object containing string data to validate. required strict | None Whether to enforce types strictly. None context | None Extra variables to pass to the validator. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated Pydantic model.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.BaseModel.model_validate_strings","title":"BaseModel - pydantic.BaseModel - model_validate_strings  classmethod","objectID":"/latest/api/base_model/#pydantic.BaseModel.model_validate_strings","rank":5},{"content":"create_model(\n    model_name: ,\n    /,\n    *,\n    __config__:  | None = None,\n    __doc__:  | None = None,\n    __base__: None = None,\n    __module__:  = ,\n    __validators__: (\n        [, [..., ]] | None\n    ) = None,\n    __cls_kwargs__: [, ] | None = None,\n    **field_definitions:  | [, ],\n) -> [] create_model(\n    model_name: ,\n    /,\n    *,\n    __config__:  | None = None,\n    __doc__:  | None = None,\n    __base__: [] | [[], ...],\n    __module__:  = ,\n    __validators__: (\n        [, [..., ]] | None\n    ) = None,\n    __cls_kwargs__: [, ] | None = None,\n    **field_definitions:  | [, ],\n) -> [] create_model(\n    model_name: ,\n    /,\n    *,\n    __config__:  | None = None,\n    __doc__:  | None = None,\n    __base__: (\n        [] | [[], ...] | None\n    ) = None,\n    __module__:  | None = None,\n    __validators__: (\n        [, [..., ]] | None\n    ) = None,\n    __cls_kwargs__: [, ] | None = None,\n    **field_definitions:  | [, ],\n) -> [] Usage Documentation Dynamic Model Creation Dynamically creates and returns a new Pydantic model, in other words, create_model dynamically creates a\nsubclass of . Parameters: Name Type Description Default model_name The name of the newly created model. required __config__ | None The configuration of the new model. None __doc__ | None The docstring of the new model. None __base__ [] | [[], ...] | None The base class or classes for the new model. None __module__ | None The name of the module that the model belongs to;\nif None , the value is taken from sys._getframe(1) None __validators__ [, [..., ]] | None A dictionary of methods that validate fields. The keys are the names of the validation methods to\nbe added to the model, and the values are the validation methods themselves. You can read more about functional\nvalidators here . None __cls_kwargs__ [, ] | None A dictionary of keyword arguments for class creation, such as metaclass . None **field_definitions | [, ] Field definitions of the new model. Either: a single element, representing the type annotation of the field. a two-tuple, the first element being the type and the second element the assigned value\n  (either a default or the  function). {} Returns: Type Description [] The new . Raises: Type Description If __base__ and __config__ are both passed.","pageID":"BaseModel","abs_url":"/latest/api/base_model/#pydantic.create_model","title":"BaseModel - pydantic.create_model","objectID":"/latest/api/base_model/#pydantic.create_model","rank":0},{"content":"Configuration for Pydantic models. ConfigDict ¶ Bases: A TypedDict for configuring Pydantic behaviour. title instance-attribute ¶ title:  | None The title for the generated JSON schema, defaults to the model's name model_title_generator instance-attribute ¶ model_title_generator: [[], ] | None A callable that takes a model class and returns the title for it. Defaults to None . field_title_generator instance-attribute ¶ field_title_generator: (\n    [[,  | ], ]\n    | None\n) A callable that takes a field's name and info and returns title for it. Defaults to None . str_to_lower instance-attribute ¶ str_to_lower: Whether to convert all characters to lowercase for str types. Defaults to False . str_to_upper instance-attribute ¶ str_to_upper: Whether to convert all characters to uppercase for str types. Defaults to False . str_strip_whitespace instance-attribute ¶ str_strip_whitespace: Whether to strip leading and trailing whitespace for str types. str_min_length instance-attribute ¶ str_min_length: The minimum length for str types. Defaults to None . str_max_length instance-attribute ¶ str_max_length:  | None The maximum length for str types. Defaults to None . extra instance-attribute ¶ extra:  | None Whether to ignore, allow, or forbid extra data during model initialization. Defaults to 'ignore' . Three configuration values are available: 'ignore' : Providing extra data is ignored (the default): from pydantic import BaseModel, ConfigDict\n\nclass User(BaseModel):\n    model_config = ConfigDict(extra='ignore')  # (1)!\n\n    name: str\n\nuser = User(name='John Doe', age=20)  # (2)!\nprint(user)\n#> name='John Doe' This is the default behaviour. The age argument is ignored. 'forbid' : Providing extra data is not permitted, and a \n  will be raised if this is the case: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='forbid')\n\n\ntry:\n    Model(x=1, y='a')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    y\n      Extra inputs are not permitted [type=extra_forbidden, input_value='a', input_type=str]\n    \"\"\" 'allow' : Providing extra data is allowed and stored in the __pydantic_extra__ dictionary attribute: from pydantic import BaseModel, ConfigDict\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\nm = Model(x=1, y='a')\nassert m.__pydantic_extra__ == {'y': 'a'} By default, no validation will be applied to these extra items, but you can set a type for the values by overriding\n  the type annotation for __pydantic_extra__ : from pydantic import BaseModel, ConfigDict, Field, ValidationError\n\n\nclass Model(BaseModel):\n    __pydantic_extra__: dict[str, int] = Field(init=False)  # (1)!\n\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\ntry:\n    Model(x=1, y='a')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    y\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\"\n\nm = Model(x=1, y='2')\nassert m.x == 1\nassert m.y == 2\nassert m.model_dump() == {'x': 1, 'y': 2}\nassert m.__pydantic_extra__ == {'y': 2} The = Field(init=False) does not have any effect at runtime, but prevents the __pydantic_extra__ field from\n   being included as a parameter to the model's __init__ method by type checkers. frozen instance-attribute ¶ frozen: Whether models are faux-immutable, i.e. whether __setattr__ is allowed, and also generates\na __hash__() method for the model. This makes instances of the model potentially hashable if all the\nattributes are hashable. Defaults to False . populate_by_name instance-attribute ¶ populate_by_name: Whether an aliased field may be populated by its name as given by the model\nattribute, as well as the alias. Defaults to False . Warning populate_by_name usage is not recommended in v2.11+ and will be deprecated in v3.\nInstead, you should use the  configuration setting. When validate_by_name=True and validate_by_alias=True , this is strictly equivalent to the\nprevious behavior of populate_by_name=True . In v2.11, we also introduced a  setting that introduces more fine grained\ncontrol for validation behavior. Here's how you might go about using the new settings to achieve the same behavior: from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(validate_by_name=True, validate_by_alias=True)\n\n    my_field: str = Field(alias='my_alias')  # (1)!\n\nm = Model(my_alias='foo')  # (2)!\nprint(m)\n#> my_field='foo'\n\nm = Model(my_field='foo')  # (3)!\nprint(m)\n#> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model is populated by the alias 'my_alias' . The model is populated by the attribute name 'my_field' . use_enum_values instance-attribute ¶ use_enum_values: Whether to populate models with the value property of enums, rather than the raw enum.\nThis may be useful if you want to serialize model.model_dump() later. Defaults to False . Note If you have an Optional[Enum] value that you set a default for, you need to use validate_default=True for said Field to ensure that the use_enum_values flag takes effect on the default, as extracting an\nenum's value occurs during validation, not serialization. from enum import Enum\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field\n\nclass SomeEnum(Enum):\n    FOO = 'foo'\n    BAR = 'bar'\n    BAZ = 'baz'\n\nclass SomeModel(BaseModel):\n    model_config = ConfigDict(use_enum_values=True)\n\n    some_enum: SomeEnum\n    another_enum: Optional[SomeEnum] = Field(\n        default=SomeEnum.FOO, validate_default=True\n    )\n\nmodel1 = SomeModel(some_enum=SomeEnum.BAR)\nprint(model1.model_dump())\n#> {'some_enum': 'bar', 'another_enum': 'foo'}\n\nmodel2 = SomeModel(some_enum=SomeEnum.BAR, another_enum=SomeEnum.BAZ)\nprint(model2.model_dump())\n#> {'some_enum': 'bar', 'another_enum': 'baz'} validate_assignment instance-attribute ¶ validate_assignment: Whether to validate the data when the model is changed. Defaults to False . The default behavior of Pydantic is to validate the data when the model is created. In case the user changes the data after the model is created, the model is not revalidated. from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n\nuser = User(name='John Doe')  # (1)!\nprint(user)\n#> name='John Doe'\nuser.name = 123  # (1)!\nprint(user)\n#> name=123 The validation happens only when the model is created. The validation does not happen when the data is changed. In case you want to revalidate the model when the data is changed, you can use validate_assignment=True : from pydantic import BaseModel, ValidationError\n\nclass User(BaseModel, validate_assignment=True):  # (1)!\n    name: str\n\nuser = User(name='John Doe')  # (2)!\nprint(user)\n#> name='John Doe'\ntry:\n    user.name = 123  # (3)!\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for User\n    name\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    ''' You can either use class keyword arguments, or model_config to set validate_assignment=True . The validation happens when the model is created. The validation also happens when the data is changed. arbitrary_types_allowed instance-attribute ¶ arbitrary_types_allowed: Whether arbitrary types are allowed for field types. Defaults to False . from pydantic import BaseModel, ConfigDict, ValidationError\n\n# This is not a pydantic model, it's an arbitrary class\nclass Pet:\n    def __init__(self, name: str):\n        self.name = name\n\nclass Model(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    pet: Pet\n    owner: str\n\npet = Pet(name='Hedwig')\n# A simple check of instance type is used to validate the data\nmodel = Model(owner='Harry', pet=pet)\nprint(model)\n#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\nprint(model.pet)\n#> <__main__.Pet object at 0x0123456789ab>\nprint(model.pet.name)\n#> Hedwig\nprint(type(model.pet))\n#> try:\n    # If the value is not an instance of the type, it's invalid\n    Model(owner='Harry', pet='Hedwig')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    pet\n      Input should be an instance of Pet [type=is_instance_of, input_value='Hedwig', input_type=str]\n    '''\n\n# Nothing in the instance of the arbitrary type is checked\n# Here name probably should have been a str, but it's not validated\npet2 = Pet(name=42)\nmodel2 = Model(owner='Harry', pet=pet2)\nprint(model2)\n#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\nprint(model2.pet)\n#> <__main__.Pet object at 0x0123456789ab>\nprint(model2.pet.name)\n#> 42\nprint(type(model2.pet))\n#> from_attributes instance-attribute ¶ from_attributes: Whether to build models and look up discriminators of tagged unions using python object attributes. loc_by_alias instance-attribute ¶ loc_by_alias: Whether to use the actual key provided in the data (e.g. alias) for error loc s rather than the field's name. Defaults to True . alias_generator instance-attribute ¶ alias_generator: (\n    [[], ] |  | None\n) A callable that takes a field name and returns an alias for it\nor an instance of . Defaults to None . When using a callable, the alias generator is used for both validation and serialization.\nIf you want to use different alias generators for validation and serialization, you can use\n instead. If data source field names do not match your code style (e. g. CamelCase fields),\nyou can automatically generate aliases using alias_generator . Here's an example with\na basic callable: from pydantic import BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_pascal\n\nclass Voice(BaseModel):\n    model_config = ConfigDict(alias_generator=to_pascal)\n\n    name: str\n    language_code: str\n\nvoice = Voice(Name='Filiz', LanguageCode='tr-TR')\nprint(voice.language_code)\n#> tr-TR\nprint(voice.model_dump(by_alias=True))\n#> {'Name': 'Filiz', 'LanguageCode': 'tr-TR'} If you want to use different alias generators for validation and serialization, you can use\n. from pydantic import AliasGenerator, BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_camel, to_pascal\n\nclass Athlete(BaseModel):\n    first_name: str\n    last_name: str\n    sport: str\n\n    model_config = ConfigDict(\n        alias_generator=AliasGenerator(\n            validation_alias=to_camel,\n            serialization_alias=to_pascal,\n        )\n    )\n\nathlete = Athlete(firstName='John', lastName='Doe', sport='track')\nprint(athlete.model_dump(by_alias=True))\n#> {'FirstName': 'John', 'LastName': 'Doe', 'Sport': 'track'} ignored_types instance-attribute ¶ ignored_types: [, ...] A tuple of types that may occur as values of class attributes without annotations. This is\ntypically used for custom descriptors (classes that behave like property ). If an attribute is set on a\nclass without an annotation and has a type that is not in this tuple (or otherwise recognized by pydantic ), an error will be raised. Defaults to () . allow_inf_nan instance-attribute ¶ allow_inf_nan: Whether to allow infinity ( +inf an -inf ) and NaN values to float and decimal fields. Defaults to True . json_schema_extra instance-attribute ¶ json_schema_extra:  |  | None A dict or callable to provide extra JSON schema properties. Defaults to None . json_encoders instance-attribute ¶ json_encoders: [[], ] | None A dict of custom JSON encoders for specific types. Defaults to None . Deprecated This config option is a carryover from v1.\nWe originally planned to remove it in v2 but didn't have a 1:1 replacement so we are keeping it for now.\nIt is still deprecated and will likely be removed in the future. strict instance-attribute ¶ strict: (new in V2) If True , strict validation is applied to all fields on the model. By default, Pydantic attempts to coerce values to the correct type, when possible. There are situations in which you may want to disable this behavior, and instead raise an error if a value's type\ndoes not match the field's type annotation. To configure strict mode for all fields on a model, you can set strict=True on the model. from pydantic import BaseModel, ConfigDict\n\nclass Model(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n    name: str\n    age: int See Strict Mode for more details. See the Conversion Table for more details on how Pydantic converts data in both\nstrict and lax modes. revalidate_instances instance-attribute ¶ revalidate_instances: [\n    \"always\", \"never\", \"subclass-instances\"\n] When and how to revalidate models and dataclasses during validation. Accepts the string\nvalues of 'never' , 'always' and 'subclass-instances' . Defaults to 'never' . 'never' will not revalidate models and dataclasses during validation 'always' will revalidate models and dataclasses during validation 'subclass-instances' will revalidate models and dataclasses during validation if the instance is a\n    subclass of the model or dataclass By default, model and dataclass instances are not revalidated during validation. from pydantic import BaseModel\n\nclass User(BaseModel, revalidate_instances='never'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]  # (2)!\nt = Transaction(user=my_user)  # (3)!\nprint(t)\n#> user=User(hobbies=[1])\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)\n#> user=SubUser(hobbies=['scuba diving'], sins=['lying']) revalidate_instances is set to 'never' by **default. The assignment is not validated, unless you set validate_assignment to True in the model's config. Since revalidate_instances is set to never , this is not revalidated. If you want to revalidate instances during validation, you can set revalidate_instances to 'always' in the model's config. from pydantic import BaseModel, ValidationError\n\nclass User(BaseModel, revalidate_instances='always'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]\ntry:\n    t = Transaction(user=my_user)  # (2)!\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Transaction\n    user.hobbies.0\n      Input should be a valid string [type=string_type, input_value=1, input_type=int]\n    '''\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)  # (3)!\n#> user=User(hobbies=['scuba diving']) revalidate_instances is set to 'always' . The model is revalidated, since revalidate_instances is set to 'always' . Using 'never' we would have gotten user=SubUser(hobbies=['scuba diving'], sins=['lying']) . It's also possible to set revalidate_instances to 'subclass-instances' to only revalidate instances\nof subclasses of the model. from pydantic import BaseModel\n\nclass User(BaseModel, revalidate_instances='subclass-instances'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]\nt = Transaction(user=my_user)  # (2)!\nprint(t)\n#> user=User(hobbies=[1])\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)  # (3)!\n#> user=User(hobbies=['scuba diving']) revalidate_instances is set to 'subclass-instances' . This is not revalidated, since my_user is not a subclass of User . Using 'never' we would have gotten user=SubUser(hobbies=['scuba diving'], sins=['lying']) . ser_json_timedelta instance-attribute ¶ ser_json_timedelta: ['iso8601', 'float'] The format of JSON serialized timedeltas. Accepts the string values of 'iso8601' and 'float' . Defaults to 'iso8601' . 'iso8601' will serialize timedeltas to ISO 8601 durations. 'float' will serialize timedeltas to the total number of seconds. ser_json_bytes instance-attribute ¶ ser_json_bytes: ['utf8', 'base64', 'hex'] The encoding of JSON serialized bytes. Defaults to 'utf8' .\nSet equal to val_json_bytes to get back an equal value after serialization round trip. 'utf8' will serialize bytes to UTF-8 strings. 'base64' will serialize bytes to URL safe base64 strings. 'hex' will serialize bytes to hexadecimal strings. val_json_bytes instance-attribute ¶ val_json_bytes: ['utf8', 'base64', 'hex'] The encoding of JSON serialized bytes to decode. Defaults to 'utf8' .\nSet equal to ser_json_bytes to get back an equal value after serialization round trip. 'utf8' will deserialize UTF-8 strings to bytes. 'base64' will deserialize URL safe base64 strings to bytes. 'hex' will deserialize hexadecimal strings to bytes. ser_json_inf_nan instance-attribute ¶ ser_json_inf_nan: ['null', 'constants', 'strings'] The encoding of JSON serialized infinity and NaN float values. Defaults to 'null' . 'null' will serialize infinity and NaN values as null . 'constants' will serialize infinity and NaN values as Infinity and NaN . 'strings' will serialize infinity as string \"Infinity\" and NaN as string \"NaN\" . validate_default instance-attribute ¶ validate_default: Whether to validate default values during validation. Defaults to False . validate_return instance-attribute ¶ validate_return: Whether to validate the return value from call validators. Defaults to False . protected_namespaces instance-attribute ¶ protected_namespaces: [ | [], ...] A tuple of strings and/or patterns that prevent models from having fields with names that conflict with them.\nFor strings, we match on a prefix basis. Ex, if 'dog' is in the protected namespace, 'dog_name' will be protected.\nFor patterns, we match on the entire field name. Ex, if re.compile(r'^dog$') is in the protected namespace, 'dog' will be protected, but 'dog_name' will not be.\nDefaults to ('model_validate', 'model_dump',) . The reason we've selected these is to prevent collisions with other validation / dumping formats\nin the future - ex, model_validate_{some_newly_supported_format} . Before v2.10, Pydantic used ('model_',) as the default value for this setting to\nprevent collisions between model attributes and BaseModel 's own methods. This was changed\nin v2.10 given feedback that this restriction was limiting in AI and data science contexts,\nwhere it is common to have fields with names like model_id , model_input , model_output , etc. For more details, see https://github.com/pydantic/pydantic/issues/10315. import warnings\n\nfrom pydantic import BaseModel\n\nwarnings.filterwarnings('error')  # Raise warnings as errors\n\ntry:\n\n    class Model(BaseModel):\n        model_dump_something: str\n\nexcept UserWarning as e:\n    print(e)\n    '''\n    Field 'model_dump_something' in 'Model' conflicts with protected namespace 'model_dump'.\n\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('model_validate',).\n    ''' You can customize this behavior using the protected_namespaces setting: import re\nimport warnings\n\nfrom pydantic import BaseModel, ConfigDict\n\nwith warnings.catch_warnings(record=True) as caught_warnings:\n    warnings.simplefilter('always')  # Catch all warnings\n\n    class Model(BaseModel):\n        safe_field: str\n        also_protect_field: str\n        protect_this: str\n\n        model_config = ConfigDict(\n            protected_namespaces=(\n                'protect_me_',\n                'also_protect_',\n                re.compile('^protect_this$'),\n            )\n        )\n\nfor warning in caught_warnings:\n    print(f'{warning.message}')\n    '''\n    Field 'also_protect_field' in 'Model' conflicts with protected namespace 'also_protect_'.\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('protect_me_', re.compile('^protect_this$'))`.\n\n    Field 'protect_this' in 'Model' conflicts with protected namespace 're.compile('^protect_this$')'.\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('protect_me_', 'also_protect_')`.\n    ''' While Pydantic will only emit a warning when an item is in a protected namespace but does not actually have a collision,\nan error is raised if there is an actual collision with an existing attribute: from pydantic import BaseModel, ConfigDict\n\ntry:\n\n    class Model(BaseModel):\n        model_validate: str\n\n        model_config = ConfigDict(protected_namespaces=('model_',))\n\nexcept ValueError as e:\n    print(e)\n    '''\n    Field 'model_validate' conflicts with member > of protected namespace 'model_'.\n    ''' hide_input_in_errors instance-attribute ¶ hide_input_in_errors: Whether to hide inputs when printing errors. Defaults to False . Pydantic shows the input value and type when it raises ValidationError during the validation. from pydantic import BaseModel, ValidationError\n\nclass Model(BaseModel):\n    a: str\n\ntry:\n    Model(a=123)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    a\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    ''' You can hide the input value and type by setting the hide_input_in_errors config to True . from pydantic import BaseModel, ConfigDict, ValidationError\n\nclass Model(BaseModel):\n    a: str\n    model_config = ConfigDict(hide_input_in_errors=True)\n\ntry:\n    Model(a=123)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    a\n      Input should be a valid string [type=string_type]\n    ''' defer_build instance-attribute ¶ defer_build: Whether to defer model validator and serializer construction until the first model validation. Defaults to False. This can be useful to avoid the overhead of building models which are only\nused nested within other models, or when you want to manually define type namespace via\n. Since v2.10, this setting also applies to pydantic dataclasses and TypeAdapter instances. plugin_settings instance-attribute ¶ plugin_settings: [, ] | None A dict of settings for plugins. Defaults to None . schema_generator instance-attribute ¶ schema_generator: [] | None Warning schema_generator is deprecated in v2.10. Prior to v2.10, this setting was advertised as highly subject to change.\nIt's possible that this interface may once again become public once the internal core schema generation\nAPI is more stable, but that will likely come after significant performance improvements have been made. json_schema_serialization_defaults_required instance-attribute ¶ json_schema_serialization_defaults_required: Whether fields with default values should be marked as required in the serialization schema. Defaults to False . This ensures that the serialization schema will reflect the fact a field with a default will always be present\nwhen serializing the model, even though it is not required for validation. However, there are scenarios where this may be undesirable — in particular, if you want to share the schema\nbetween validation and serialization, and don't mind fields with defaults being marked as not required during\nserialization. See #7209 for more details. from pydantic import BaseModel, ConfigDict\n\nclass Model(BaseModel):\n    a: str = 'a'\n\n    model_config = ConfigDict(json_schema_serialization_defaults_required=True)\n\nprint(Model.model_json_schema(mode='validation'))\n'''\n{\n    'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n    'title': 'Model',\n    'type': 'object',\n}\n'''\nprint(Model.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n    'required': ['a'],\n    'title': 'Model',\n    'type': 'object',\n}\n''' json_schema_mode_override instance-attribute ¶ json_schema_mode_override: [\n    \"validation\", \"serialization\", None\n] If not None , the specified mode will be used to generate the JSON schema regardless of what mode was passed to\nthe function call. Defaults to None . This provides a way to force the JSON schema generation to reflect a specific mode, e.g., to always use the\nvalidation schema. It can be useful when using frameworks (such as FastAPI) that may generate different schemas for validation\nand serialization that must both be referenced from the same schema; when this happens, we automatically append -Input to the definition reference for the validation schema and -Output to the definition reference for the\nserialization schema. By specifying a json_schema_mode_override though, this prevents the conflict between\nthe validation and serialization schemas (since both will use the specified schema), and so prevents the suffixes\nfrom being added to the definition references. from pydantic import BaseModel, ConfigDict, Json\n\nclass Model(BaseModel):\n    a: Json[int]  # requires a string to validate, but will dump an int\n\nprint(Model.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {'a': {'title': 'A', 'type': 'integer'}},\n    'required': ['a'],\n    'title': 'Model',\n    'type': 'object',\n}\n'''\n\nclass ForceInputModel(Model):\n    # the following ensures that even with mode='serialization', we\n    # will get the schema that would be generated for validation.\n    model_config = ConfigDict(json_schema_mode_override='validation')\n\nprint(ForceInputModel.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {\n        'a': {\n            'contentMediaType': 'application/json',\n            'contentSchema': {'type': 'integer'},\n            'title': 'A',\n            'type': 'string',\n        }\n    },\n    'required': ['a'],\n    'title': 'ForceInputModel',\n    'type': 'object',\n}\n''' coerce_numbers_to_str instance-attribute ¶ coerce_numbers_to_str: If True , enables automatic coercion of any Number type to str in \"lax\" (non-strict) mode. Defaults to False . Pydantic doesn't allow number types ( int , float , Decimal ) to be coerced as type str by default. from decimal import Decimal\n\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\nclass Model(BaseModel):\n    value: str\n\ntry:\n    print(Model(value=42))\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    value\n      Input should be a valid string [type=string_type, input_value=42, input_type=int]\n    '''\n\nclass Model(BaseModel):\n    model_config = ConfigDict(coerce_numbers_to_str=True)\n\n    value: str\n\nrepr(Model(value=42).value)\n#> \"42\"\nrepr(Model(value=42.13).value)\n#> \"42.13\"\nrepr(Model(value=Decimal('42.13')).value)\n#> \"42.13\" regex_engine instance-attribute ¶ regex_engine: ['rust-regex', 'python-re'] The regex engine to be used for pattern validation.\nDefaults to 'rust-regex' . rust-regex uses the regex Rust crate,\n  which is non-backtracking and therefore more DDoS resistant, but does not support all regex features. python-re use the re module,\n  which supports all regex features, but may be slower. Note If you use a compiled regex pattern, the python-re engine will be used regardless of this setting.\nThis is so that flags such as re.IGNORECASE are respected. from pydantic import BaseModel, ConfigDict, Field, ValidationError\n\nclass Model(BaseModel):\n    model_config = ConfigDict(regex_engine='python-re')\n\n    value: str = Field(pattern=r'^abc(?=def)')\n\nprint(Model(value='abcdef').value)\n#> abcdef\n\ntry:\n    print(Model(value='abxyzcdef'))\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    value\n      String should match pattern '^abc(?=def)' [type=string_pattern_mismatch, input_value='abxyzcdef', input_type=str]\n    ''' validation_error_cause instance-attribute ¶ validation_error_cause: If True , Python exceptions that were part of a validation failure will be shown as an exception group as a cause. Can be useful for debugging. Defaults to False . use_attribute_docstrings instance-attribute ¶ use_attribute_docstrings: Whether docstrings of attributes (bare string literals immediately following the attribute declaration)\nshould be used for field descriptions. Defaults to False . Available in Pydantic v2.7+. from pydantic import BaseModel, ConfigDict, Field\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(use_attribute_docstrings=True)\n\n    x: str\n    \"\"\"\n    Example of an attribute docstring\n    \"\"\"\n\n    y: int = Field(description=\"Description in Field\")\n    \"\"\"\n    Description in Field overrides attribute docstring\n    \"\"\"\n\n\nprint(Model.model_fields[\"x\"].description)\n# > Example of an attribute docstring\nprint(Model.model_fields[\"y\"].description)\n# > Description in Field This requires the source code of the class to be available at runtime. Usage with TypedDict and stdlib dataclasses Due to current limitations, attribute docstrings detection may not work as expected when using\n and stdlib dataclasses, in particular when: inheritance is being used. multiple classes have the same name in the same source file (unless Python 3.13 or greater is used). cache_strings instance-attribute ¶ cache_strings:  | ['all', 'keys', 'none'] Whether to cache strings to avoid constructing new Python objects. Defaults to True. Enabling this setting should significantly improve validation performance while increasing memory usage slightly. True or 'all' (the default): cache all strings 'keys' : cache only dictionary keys False or 'none' : no caching Note True or 'all' is required to cache strings during general validation because\nvalidators don't know if they're in a key or a value. Tip If repeated strings are rare, it's recommended to use 'keys' or 'none' to reduce memory usage,\nas the performance difference is minimal if repeated strings are rare. validate_by_alias instance-attribute ¶ validate_by_alias: Whether an aliased field may be populated by its alias. Defaults to True . Note In v2.11, validate_by_alias was introduced in conjunction with \nto empower users with more fine grained validation control. In <v2.11, disabling validation by alias was not possible. Here's an example of disabling validation by alias: from pydantic import BaseModel , ConfigDict , Field class Model ( BaseModel ): model_config = ConfigDict ( validate_by_name = True , validate_by_alias = False ) my_field : str = Field ( validation_alias = 'my_alias' ) # (1)! m = Model ( my_field = 'foo' ) # (2)! print ( m ) #> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model can only be populated by the attribute name 'my_field' . Warning You cannot set both validate_by_alias and validate_by_name to False .\nThis would make it impossible to populate an attribute. See usage errors for an example. If you set validate_by_alias to False , under the hood, Pydantic dynamically sets validate_by_name to True to ensure that validation can still occur. validate_by_name instance-attribute ¶ validate_by_name: Whether an aliased field may be populated by its name as given by the model\nattribute. Defaults to False . Note In v2.0-v2.10, the populate_by_name configuration setting was used to specify\nwhether or not a field could be populated by its name and alias. In v2.11, validate_by_name was introduced in conjunction with \nto empower users with more fine grained validation behavior control. from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(validate_by_name=True, validate_by_alias=True)\n\n    my_field: str = Field(validation_alias='my_alias')  # (1)!\n\nm = Model(my_alias='foo')  # (2)!\nprint(m)\n#> my_field='foo'\n\nm = Model(my_field='foo')  # (3)!\nprint(m)\n#> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model is populated by the alias 'my_alias' . The model is populated by the attribute name 'my_field' . Warning You cannot set both validate_by_alias and validate_by_name to False .\nThis would make it impossible to populate an attribute. See usage errors for an example. serialize_by_alias instance-attribute ¶ serialize_by_alias: Whether an aliased field should be serialized by its alias. Defaults to False . Note: In v2.11, serialize_by_alias was introduced to address the popular request for consistency with alias behavior for validation and serialization settings.\nIn v3, the default value is expected to change to True for consistency with the validation default. from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(serialize_by_alias=True)\n\n    my_field: str = Field(serialization_alias='my_alias')  # (1)!\n\nm = Model(my_field='foo')\nprint(m.model_dump())  # (2)!\n#> {'my_alias': 'foo'} The field 'my_field' has an alias 'my_alias' . The model is serialized using the alias 'my_alias' for the 'my_field' attribute. with_config ¶ with_config(\n    *, config: \n) -> [[], ] with_config(\n    config: ,\n) -> [[], ] with_config(\n    **config: [],\n) -> [[], ] with_config(\n    config:  | None = None, /, **kwargs: \n) -> [[], ] Usage Documentation Configuration with other types A convenience decorator to set a Pydantic configuration on a TypedDict or a dataclass from the standard library. Although the configuration can be set using the __pydantic_config__ attribute, it does not play well with type checkers,\nespecially with TypedDict . Usage from typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, with_config\n\n@with_config(ConfigDict(str_to_lower=True))\nclass TD(TypedDict):\n    x: str\n\nta = TypeAdapter(TD)\n\nprint(ta.validate_python({'x': 'ABC'}))\n#> {'x': 'abc'} ExtraValues module-attribute ¶ ExtraValues = ['allow', 'ignore', 'forbid'] pydantic.alias_generators ¶ Alias generators for converting between different capitalization conventions. to_pascal ¶ to_pascal(snake: ) -> Convert a snake_case string to PascalCase. Parameters: Name Type Description Default snake The string to convert. required Returns: Type Description The PascalCase string. to_camel ¶ to_camel(snake: ) -> Convert a snake_case string to camelCase. Parameters: Name Type Description Default snake The string to convert. required Returns: Type Description The converted camelCase string. to_snake ¶ to_snake(camel: ) -> Convert a PascalCase, camelCase, or kebab-case string to snake_case. Parameters: Name Type Description Default camel The string to convert. required Returns: Type Description The converted string in snake_case.","pageID":"Configuration","abs_url":"/latest/api/config/#Configuration","title":"Configuration","objectID":"/latest/api/config/#Configuration","rank":100},{"content":"Bases: A TypedDict for configuring Pydantic behaviour. title instance-attribute ¶ title:  | None The title for the generated JSON schema, defaults to the model's name model_title_generator instance-attribute ¶ model_title_generator: [[], ] | None A callable that takes a model class and returns the title for it. Defaults to None . field_title_generator instance-attribute ¶ field_title_generator: (\n    [[,  | ], ]\n    | None\n) A callable that takes a field's name and info and returns title for it. Defaults to None . str_to_lower instance-attribute ¶ str_to_lower: Whether to convert all characters to lowercase for str types. Defaults to False . str_to_upper instance-attribute ¶ str_to_upper: Whether to convert all characters to uppercase for str types. Defaults to False . str_strip_whitespace instance-attribute ¶ str_strip_whitespace: Whether to strip leading and trailing whitespace for str types. str_min_length instance-attribute ¶ str_min_length: The minimum length for str types. Defaults to None . str_max_length instance-attribute ¶ str_max_length:  | None The maximum length for str types. Defaults to None . extra instance-attribute ¶ extra:  | None Whether to ignore, allow, or forbid extra data during model initialization. Defaults to 'ignore' . Three configuration values are available: 'ignore' : Providing extra data is ignored (the default): from pydantic import BaseModel, ConfigDict\n\nclass User(BaseModel):\n    model_config = ConfigDict(extra='ignore')  # (1)!\n\n    name: str\n\nuser = User(name='John Doe', age=20)  # (2)!\nprint(user)\n#> name='John Doe' This is the default behaviour. The age argument is ignored. 'forbid' : Providing extra data is not permitted, and a \n  will be raised if this is the case: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='forbid')\n\n\ntry:\n    Model(x=1, y='a')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    y\n      Extra inputs are not permitted [type=extra_forbidden, input_value='a', input_type=str]\n    \"\"\" 'allow' : Providing extra data is allowed and stored in the __pydantic_extra__ dictionary attribute: from pydantic import BaseModel, ConfigDict\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\nm = Model(x=1, y='a')\nassert m.__pydantic_extra__ == {'y': 'a'} By default, no validation will be applied to these extra items, but you can set a type for the values by overriding\n  the type annotation for __pydantic_extra__ : from pydantic import BaseModel, ConfigDict, Field, ValidationError\n\n\nclass Model(BaseModel):\n    __pydantic_extra__: dict[str, int] = Field(init=False)  # (1)!\n\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\ntry:\n    Model(x=1, y='a')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    y\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\"\n\nm = Model(x=1, y='2')\nassert m.x == 1\nassert m.y == 2\nassert m.model_dump() == {'x': 1, 'y': 2}\nassert m.__pydantic_extra__ == {'y': 2} The = Field(init=False) does not have any effect at runtime, but prevents the __pydantic_extra__ field from\n   being included as a parameter to the model's __init__ method by type checkers. frozen instance-attribute ¶ frozen: Whether models are faux-immutable, i.e. whether __setattr__ is allowed, and also generates\na __hash__() method for the model. This makes instances of the model potentially hashable if all the\nattributes are hashable. Defaults to False . populate_by_name instance-attribute ¶ populate_by_name: Whether an aliased field may be populated by its name as given by the model\nattribute, as well as the alias. Defaults to False . Warning populate_by_name usage is not recommended in v2.11+ and will be deprecated in v3.\nInstead, you should use the  configuration setting. When validate_by_name=True and validate_by_alias=True , this is strictly equivalent to the\nprevious behavior of populate_by_name=True . In v2.11, we also introduced a  setting that introduces more fine grained\ncontrol for validation behavior. Here's how you might go about using the new settings to achieve the same behavior: from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(validate_by_name=True, validate_by_alias=True)\n\n    my_field: str = Field(alias='my_alias')  # (1)!\n\nm = Model(my_alias='foo')  # (2)!\nprint(m)\n#> my_field='foo'\n\nm = Model(my_field='foo')  # (3)!\nprint(m)\n#> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model is populated by the alias 'my_alias' . The model is populated by the attribute name 'my_field' . use_enum_values instance-attribute ¶ use_enum_values: Whether to populate models with the value property of enums, rather than the raw enum.\nThis may be useful if you want to serialize model.model_dump() later. Defaults to False . Note If you have an Optional[Enum] value that you set a default for, you need to use validate_default=True for said Field to ensure that the use_enum_values flag takes effect on the default, as extracting an\nenum's value occurs during validation, not serialization. from enum import Enum\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field\n\nclass SomeEnum(Enum):\n    FOO = 'foo'\n    BAR = 'bar'\n    BAZ = 'baz'\n\nclass SomeModel(BaseModel):\n    model_config = ConfigDict(use_enum_values=True)\n\n    some_enum: SomeEnum\n    another_enum: Optional[SomeEnum] = Field(\n        default=SomeEnum.FOO, validate_default=True\n    )\n\nmodel1 = SomeModel(some_enum=SomeEnum.BAR)\nprint(model1.model_dump())\n#> {'some_enum': 'bar', 'another_enum': 'foo'}\n\nmodel2 = SomeModel(some_enum=SomeEnum.BAR, another_enum=SomeEnum.BAZ)\nprint(model2.model_dump())\n#> {'some_enum': 'bar', 'another_enum': 'baz'} validate_assignment instance-attribute ¶ validate_assignment: Whether to validate the data when the model is changed. Defaults to False . The default behavior of Pydantic is to validate the data when the model is created. In case the user changes the data after the model is created, the model is not revalidated. from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n\nuser = User(name='John Doe')  # (1)!\nprint(user)\n#> name='John Doe'\nuser.name = 123  # (1)!\nprint(user)\n#> name=123 The validation happens only when the model is created. The validation does not happen when the data is changed. In case you want to revalidate the model when the data is changed, you can use validate_assignment=True : from pydantic import BaseModel, ValidationError\n\nclass User(BaseModel, validate_assignment=True):  # (1)!\n    name: str\n\nuser = User(name='John Doe')  # (2)!\nprint(user)\n#> name='John Doe'\ntry:\n    user.name = 123  # (3)!\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for User\n    name\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    ''' You can either use class keyword arguments, or model_config to set validate_assignment=True . The validation happens when the model is created. The validation also happens when the data is changed. arbitrary_types_allowed instance-attribute ¶ arbitrary_types_allowed: Whether arbitrary types are allowed for field types. Defaults to False . from pydantic import BaseModel, ConfigDict, ValidationError\n\n# This is not a pydantic model, it's an arbitrary class\nclass Pet:\n    def __init__(self, name: str):\n        self.name = name\n\nclass Model(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    pet: Pet\n    owner: str\n\npet = Pet(name='Hedwig')\n# A simple check of instance type is used to validate the data\nmodel = Model(owner='Harry', pet=pet)\nprint(model)\n#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\nprint(model.pet)\n#> <__main__.Pet object at 0x0123456789ab>\nprint(model.pet.name)\n#> Hedwig\nprint(type(model.pet))\n#> try:\n    # If the value is not an instance of the type, it's invalid\n    Model(owner='Harry', pet='Hedwig')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    pet\n      Input should be an instance of Pet [type=is_instance_of, input_value='Hedwig', input_type=str]\n    '''\n\n# Nothing in the instance of the arbitrary type is checked\n# Here name probably should have been a str, but it's not validated\npet2 = Pet(name=42)\nmodel2 = Model(owner='Harry', pet=pet2)\nprint(model2)\n#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\nprint(model2.pet)\n#> <__main__.Pet object at 0x0123456789ab>\nprint(model2.pet.name)\n#> 42\nprint(type(model2.pet))\n#> from_attributes instance-attribute ¶ from_attributes: Whether to build models and look up discriminators of tagged unions using python object attributes. loc_by_alias instance-attribute ¶ loc_by_alias: Whether to use the actual key provided in the data (e.g. alias) for error loc s rather than the field's name. Defaults to True . alias_generator instance-attribute ¶ alias_generator: (\n    [[], ] |  | None\n) A callable that takes a field name and returns an alias for it\nor an instance of . Defaults to None . When using a callable, the alias generator is used for both validation and serialization.\nIf you want to use different alias generators for validation and serialization, you can use\n instead. If data source field names do not match your code style (e. g. CamelCase fields),\nyou can automatically generate aliases using alias_generator . Here's an example with\na basic callable: from pydantic import BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_pascal\n\nclass Voice(BaseModel):\n    model_config = ConfigDict(alias_generator=to_pascal)\n\n    name: str\n    language_code: str\n\nvoice = Voice(Name='Filiz', LanguageCode='tr-TR')\nprint(voice.language_code)\n#> tr-TR\nprint(voice.model_dump(by_alias=True))\n#> {'Name': 'Filiz', 'LanguageCode': 'tr-TR'} If you want to use different alias generators for validation and serialization, you can use\n. from pydantic import AliasGenerator, BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_camel, to_pascal\n\nclass Athlete(BaseModel):\n    first_name: str\n    last_name: str\n    sport: str\n\n    model_config = ConfigDict(\n        alias_generator=AliasGenerator(\n            validation_alias=to_camel,\n            serialization_alias=to_pascal,\n        )\n    )\n\nathlete = Athlete(firstName='John', lastName='Doe', sport='track')\nprint(athlete.model_dump(by_alias=True))\n#> {'FirstName': 'John', 'LastName': 'Doe', 'Sport': 'track'} ignored_types instance-attribute ¶ ignored_types: [, ...] A tuple of types that may occur as values of class attributes without annotations. This is\ntypically used for custom descriptors (classes that behave like property ). If an attribute is set on a\nclass without an annotation and has a type that is not in this tuple (or otherwise recognized by pydantic ), an error will be raised. Defaults to () . allow_inf_nan instance-attribute ¶ allow_inf_nan: Whether to allow infinity ( +inf an -inf ) and NaN values to float and decimal fields. Defaults to True . json_schema_extra instance-attribute ¶ json_schema_extra:  |  | None A dict or callable to provide extra JSON schema properties. Defaults to None . json_encoders instance-attribute ¶ json_encoders: [[], ] | None A dict of custom JSON encoders for specific types. Defaults to None . Deprecated This config option is a carryover from v1.\nWe originally planned to remove it in v2 but didn't have a 1:1 replacement so we are keeping it for now.\nIt is still deprecated and will likely be removed in the future. strict instance-attribute ¶ strict: (new in V2) If True , strict validation is applied to all fields on the model. By default, Pydantic attempts to coerce values to the correct type, when possible. There are situations in which you may want to disable this behavior, and instead raise an error if a value's type\ndoes not match the field's type annotation. To configure strict mode for all fields on a model, you can set strict=True on the model. from pydantic import BaseModel, ConfigDict\n\nclass Model(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n    name: str\n    age: int See Strict Mode for more details. See the Conversion Table for more details on how Pydantic converts data in both\nstrict and lax modes. revalidate_instances instance-attribute ¶ revalidate_instances: [\n    \"always\", \"never\", \"subclass-instances\"\n] When and how to revalidate models and dataclasses during validation. Accepts the string\nvalues of 'never' , 'always' and 'subclass-instances' . Defaults to 'never' . 'never' will not revalidate models and dataclasses during validation 'always' will revalidate models and dataclasses during validation 'subclass-instances' will revalidate models and dataclasses during validation if the instance is a\n    subclass of the model or dataclass By default, model and dataclass instances are not revalidated during validation. from pydantic import BaseModel\n\nclass User(BaseModel, revalidate_instances='never'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]  # (2)!\nt = Transaction(user=my_user)  # (3)!\nprint(t)\n#> user=User(hobbies=[1])\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)\n#> user=SubUser(hobbies=['scuba diving'], sins=['lying']) revalidate_instances is set to 'never' by **default. The assignment is not validated, unless you set validate_assignment to True in the model's config. Since revalidate_instances is set to never , this is not revalidated. If you want to revalidate instances during validation, you can set revalidate_instances to 'always' in the model's config. from pydantic import BaseModel, ValidationError\n\nclass User(BaseModel, revalidate_instances='always'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]\ntry:\n    t = Transaction(user=my_user)  # (2)!\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Transaction\n    user.hobbies.0\n      Input should be a valid string [type=string_type, input_value=1, input_type=int]\n    '''\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)  # (3)!\n#> user=User(hobbies=['scuba diving']) revalidate_instances is set to 'always' . The model is revalidated, since revalidate_instances is set to 'always' . Using 'never' we would have gotten user=SubUser(hobbies=['scuba diving'], sins=['lying']) . It's also possible to set revalidate_instances to 'subclass-instances' to only revalidate instances\nof subclasses of the model. from pydantic import BaseModel\n\nclass User(BaseModel, revalidate_instances='subclass-instances'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]\nt = Transaction(user=my_user)  # (2)!\nprint(t)\n#> user=User(hobbies=[1])\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)  # (3)!\n#> user=User(hobbies=['scuba diving']) revalidate_instances is set to 'subclass-instances' . This is not revalidated, since my_user is not a subclass of User . Using 'never' we would have gotten user=SubUser(hobbies=['scuba diving'], sins=['lying']) . ser_json_timedelta instance-attribute ¶ ser_json_timedelta: ['iso8601', 'float'] The format of JSON serialized timedeltas. Accepts the string values of 'iso8601' and 'float' . Defaults to 'iso8601' . 'iso8601' will serialize timedeltas to ISO 8601 durations. 'float' will serialize timedeltas to the total number of seconds. ser_json_bytes instance-attribute ¶ ser_json_bytes: ['utf8', 'base64', 'hex'] The encoding of JSON serialized bytes. Defaults to 'utf8' .\nSet equal to val_json_bytes to get back an equal value after serialization round trip. 'utf8' will serialize bytes to UTF-8 strings. 'base64' will serialize bytes to URL safe base64 strings. 'hex' will serialize bytes to hexadecimal strings. val_json_bytes instance-attribute ¶ val_json_bytes: ['utf8', 'base64', 'hex'] The encoding of JSON serialized bytes to decode. Defaults to 'utf8' .\nSet equal to ser_json_bytes to get back an equal value after serialization round trip. 'utf8' will deserialize UTF-8 strings to bytes. 'base64' will deserialize URL safe base64 strings to bytes. 'hex' will deserialize hexadecimal strings to bytes. ser_json_inf_nan instance-attribute ¶ ser_json_inf_nan: ['null', 'constants', 'strings'] The encoding of JSON serialized infinity and NaN float values. Defaults to 'null' . 'null' will serialize infinity and NaN values as null . 'constants' will serialize infinity and NaN values as Infinity and NaN . 'strings' will serialize infinity as string \"Infinity\" and NaN as string \"NaN\" . validate_default instance-attribute ¶ validate_default: Whether to validate default values during validation. Defaults to False . validate_return instance-attribute ¶ validate_return: Whether to validate the return value from call validators. Defaults to False . protected_namespaces instance-attribute ¶ protected_namespaces: [ | [], ...] A tuple of strings and/or patterns that prevent models from having fields with names that conflict with them.\nFor strings, we match on a prefix basis. Ex, if 'dog' is in the protected namespace, 'dog_name' will be protected.\nFor patterns, we match on the entire field name. Ex, if re.compile(r'^dog$') is in the protected namespace, 'dog' will be protected, but 'dog_name' will not be.\nDefaults to ('model_validate', 'model_dump',) . The reason we've selected these is to prevent collisions with other validation / dumping formats\nin the future - ex, model_validate_{some_newly_supported_format} . Before v2.10, Pydantic used ('model_',) as the default value for this setting to\nprevent collisions between model attributes and BaseModel 's own methods. This was changed\nin v2.10 given feedback that this restriction was limiting in AI and data science contexts,\nwhere it is common to have fields with names like model_id , model_input , model_output , etc. For more details, see https://github.com/pydantic/pydantic/issues/10315. import warnings\n\nfrom pydantic import BaseModel\n\nwarnings.filterwarnings('error')  # Raise warnings as errors\n\ntry:\n\n    class Model(BaseModel):\n        model_dump_something: str\n\nexcept UserWarning as e:\n    print(e)\n    '''\n    Field 'model_dump_something' in 'Model' conflicts with protected namespace 'model_dump'.\n\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('model_validate',).\n    ''' You can customize this behavior using the protected_namespaces setting: import re\nimport warnings\n\nfrom pydantic import BaseModel, ConfigDict\n\nwith warnings.catch_warnings(record=True) as caught_warnings:\n    warnings.simplefilter('always')  # Catch all warnings\n\n    class Model(BaseModel):\n        safe_field: str\n        also_protect_field: str\n        protect_this: str\n\n        model_config = ConfigDict(\n            protected_namespaces=(\n                'protect_me_',\n                'also_protect_',\n                re.compile('^protect_this$'),\n            )\n        )\n\nfor warning in caught_warnings:\n    print(f'{warning.message}')\n    '''\n    Field 'also_protect_field' in 'Model' conflicts with protected namespace 'also_protect_'.\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('protect_me_', re.compile('^protect_this$'))`.\n\n    Field 'protect_this' in 'Model' conflicts with protected namespace 're.compile('^protect_this$')'.\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('protect_me_', 'also_protect_')`.\n    ''' While Pydantic will only emit a warning when an item is in a protected namespace but does not actually have a collision,\nan error is raised if there is an actual collision with an existing attribute: from pydantic import BaseModel, ConfigDict\n\ntry:\n\n    class Model(BaseModel):\n        model_validate: str\n\n        model_config = ConfigDict(protected_namespaces=('model_',))\n\nexcept ValueError as e:\n    print(e)\n    '''\n    Field 'model_validate' conflicts with member > of protected namespace 'model_'.\n    ''' hide_input_in_errors instance-attribute ¶ hide_input_in_errors: Whether to hide inputs when printing errors. Defaults to False . Pydantic shows the input value and type when it raises ValidationError during the validation. from pydantic import BaseModel, ValidationError\n\nclass Model(BaseModel):\n    a: str\n\ntry:\n    Model(a=123)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    a\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    ''' You can hide the input value and type by setting the hide_input_in_errors config to True . from pydantic import BaseModel, ConfigDict, ValidationError\n\nclass Model(BaseModel):\n    a: str\n    model_config = ConfigDict(hide_input_in_errors=True)\n\ntry:\n    Model(a=123)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    a\n      Input should be a valid string [type=string_type]\n    ''' defer_build instance-attribute ¶ defer_build: Whether to defer model validator and serializer construction until the first model validation. Defaults to False. This can be useful to avoid the overhead of building models which are only\nused nested within other models, or when you want to manually define type namespace via\n. Since v2.10, this setting also applies to pydantic dataclasses and TypeAdapter instances. plugin_settings instance-attribute ¶ plugin_settings: [, ] | None A dict of settings for plugins. Defaults to None . schema_generator instance-attribute ¶ schema_generator: [] | None Warning schema_generator is deprecated in v2.10. Prior to v2.10, this setting was advertised as highly subject to change.\nIt's possible that this interface may once again become public once the internal core schema generation\nAPI is more stable, but that will likely come after significant performance improvements have been made. json_schema_serialization_defaults_required instance-attribute ¶ json_schema_serialization_defaults_required: Whether fields with default values should be marked as required in the serialization schema. Defaults to False . This ensures that the serialization schema will reflect the fact a field with a default will always be present\nwhen serializing the model, even though it is not required for validation. However, there are scenarios where this may be undesirable — in particular, if you want to share the schema\nbetween validation and serialization, and don't mind fields with defaults being marked as not required during\nserialization. See #7209 for more details. from pydantic import BaseModel, ConfigDict\n\nclass Model(BaseModel):\n    a: str = 'a'\n\n    model_config = ConfigDict(json_schema_serialization_defaults_required=True)\n\nprint(Model.model_json_schema(mode='validation'))\n'''\n{\n    'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n    'title': 'Model',\n    'type': 'object',\n}\n'''\nprint(Model.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n    'required': ['a'],\n    'title': 'Model',\n    'type': 'object',\n}\n''' json_schema_mode_override instance-attribute ¶ json_schema_mode_override: [\n    \"validation\", \"serialization\", None\n] If not None , the specified mode will be used to generate the JSON schema regardless of what mode was passed to\nthe function call. Defaults to None . This provides a way to force the JSON schema generation to reflect a specific mode, e.g., to always use the\nvalidation schema. It can be useful when using frameworks (such as FastAPI) that may generate different schemas for validation\nand serialization that must both be referenced from the same schema; when this happens, we automatically append -Input to the definition reference for the validation schema and -Output to the definition reference for the\nserialization schema. By specifying a json_schema_mode_override though, this prevents the conflict between\nthe validation and serialization schemas (since both will use the specified schema), and so prevents the suffixes\nfrom being added to the definition references. from pydantic import BaseModel, ConfigDict, Json\n\nclass Model(BaseModel):\n    a: Json[int]  # requires a string to validate, but will dump an int\n\nprint(Model.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {'a': {'title': 'A', 'type': 'integer'}},\n    'required': ['a'],\n    'title': 'Model',\n    'type': 'object',\n}\n'''\n\nclass ForceInputModel(Model):\n    # the following ensures that even with mode='serialization', we\n    # will get the schema that would be generated for validation.\n    model_config = ConfigDict(json_schema_mode_override='validation')\n\nprint(ForceInputModel.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {\n        'a': {\n            'contentMediaType': 'application/json',\n            'contentSchema': {'type': 'integer'},\n            'title': 'A',\n            'type': 'string',\n        }\n    },\n    'required': ['a'],\n    'title': 'ForceInputModel',\n    'type': 'object',\n}\n''' coerce_numbers_to_str instance-attribute ¶ coerce_numbers_to_str: If True , enables automatic coercion of any Number type to str in \"lax\" (non-strict) mode. Defaults to False . Pydantic doesn't allow number types ( int , float , Decimal ) to be coerced as type str by default. from decimal import Decimal\n\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\nclass Model(BaseModel):\n    value: str\n\ntry:\n    print(Model(value=42))\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    value\n      Input should be a valid string [type=string_type, input_value=42, input_type=int]\n    '''\n\nclass Model(BaseModel):\n    model_config = ConfigDict(coerce_numbers_to_str=True)\n\n    value: str\n\nrepr(Model(value=42).value)\n#> \"42\"\nrepr(Model(value=42.13).value)\n#> \"42.13\"\nrepr(Model(value=Decimal('42.13')).value)\n#> \"42.13\" regex_engine instance-attribute ¶ regex_engine: ['rust-regex', 'python-re'] The regex engine to be used for pattern validation.\nDefaults to 'rust-regex' . rust-regex uses the regex Rust crate,\n  which is non-backtracking and therefore more DDoS resistant, but does not support all regex features. python-re use the re module,\n  which supports all regex features, but may be slower. Note If you use a compiled regex pattern, the python-re engine will be used regardless of this setting.\nThis is so that flags such as re.IGNORECASE are respected. from pydantic import BaseModel, ConfigDict, Field, ValidationError\n\nclass Model(BaseModel):\n    model_config = ConfigDict(regex_engine='python-re')\n\n    value: str = Field(pattern=r'^abc(?=def)')\n\nprint(Model(value='abcdef').value)\n#> abcdef\n\ntry:\n    print(Model(value='abxyzcdef'))\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    value\n      String should match pattern '^abc(?=def)' [type=string_pattern_mismatch, input_value='abxyzcdef', input_type=str]\n    ''' validation_error_cause instance-attribute ¶ validation_error_cause: If True , Python exceptions that were part of a validation failure will be shown as an exception group as a cause. Can be useful for debugging. Defaults to False . use_attribute_docstrings instance-attribute ¶ use_attribute_docstrings: Whether docstrings of attributes (bare string literals immediately following the attribute declaration)\nshould be used for field descriptions. Defaults to False . Available in Pydantic v2.7+. from pydantic import BaseModel, ConfigDict, Field\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(use_attribute_docstrings=True)\n\n    x: str\n    \"\"\"\n    Example of an attribute docstring\n    \"\"\"\n\n    y: int = Field(description=\"Description in Field\")\n    \"\"\"\n    Description in Field overrides attribute docstring\n    \"\"\"\n\n\nprint(Model.model_fields[\"x\"].description)\n# > Example of an attribute docstring\nprint(Model.model_fields[\"y\"].description)\n# > Description in Field This requires the source code of the class to be available at runtime. Usage with TypedDict and stdlib dataclasses Due to current limitations, attribute docstrings detection may not work as expected when using\n and stdlib dataclasses, in particular when: inheritance is being used. multiple classes have the same name in the same source file (unless Python 3.13 or greater is used). cache_strings instance-attribute ¶ cache_strings:  | ['all', 'keys', 'none'] Whether to cache strings to avoid constructing new Python objects. Defaults to True. Enabling this setting should significantly improve validation performance while increasing memory usage slightly. True or 'all' (the default): cache all strings 'keys' : cache only dictionary keys False or 'none' : no caching Note True or 'all' is required to cache strings during general validation because\nvalidators don't know if they're in a key or a value. Tip If repeated strings are rare, it's recommended to use 'keys' or 'none' to reduce memory usage,\nas the performance difference is minimal if repeated strings are rare. validate_by_alias instance-attribute ¶ validate_by_alias: Whether an aliased field may be populated by its alias. Defaults to True . Note In v2.11, validate_by_alias was introduced in conjunction with \nto empower users with more fine grained validation control. In <v2.11, disabling validation by alias was not possible. Here's an example of disabling validation by alias: from pydantic import BaseModel , ConfigDict , Field class Model ( BaseModel ): model_config = ConfigDict ( validate_by_name = True , validate_by_alias = False ) my_field : str = Field ( validation_alias = 'my_alias' ) # (1)! m = Model ( my_field = 'foo' ) # (2)! print ( m ) #> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model can only be populated by the attribute name 'my_field' . Warning You cannot set both validate_by_alias and validate_by_name to False .\nThis would make it impossible to populate an attribute. See usage errors for an example. If you set validate_by_alias to False , under the hood, Pydantic dynamically sets validate_by_name to True to ensure that validation can still occur. validate_by_name instance-attribute ¶ validate_by_name: Whether an aliased field may be populated by its name as given by the model\nattribute. Defaults to False . Note In v2.0-v2.10, the populate_by_name configuration setting was used to specify\nwhether or not a field could be populated by its name and alias. In v2.11, validate_by_name was introduced in conjunction with \nto empower users with more fine grained validation behavior control. from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(validate_by_name=True, validate_by_alias=True)\n\n    my_field: str = Field(validation_alias='my_alias')  # (1)!\n\nm = Model(my_alias='foo')  # (2)!\nprint(m)\n#> my_field='foo'\n\nm = Model(my_field='foo')  # (3)!\nprint(m)\n#> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model is populated by the alias 'my_alias' . The model is populated by the attribute name 'my_field' . Warning You cannot set both validate_by_alias and validate_by_name to False .\nThis would make it impossible to populate an attribute. See usage errors for an example. serialize_by_alias instance-attribute ¶ serialize_by_alias: Whether an aliased field should be serialized by its alias. Defaults to False . Note: In v2.11, serialize_by_alias was introduced to address the popular request for consistency with alias behavior for validation and serialization settings.\nIn v3, the default value is expected to change to True for consistency with the validation default. from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(serialize_by_alias=True)\n\n    my_field: str = Field(serialization_alias='my_alias')  # (1)!\n\nm = Model(my_field='foo')\nprint(m.model_dump())  # (2)!\n#> {'my_alias': 'foo'} The field 'my_field' has an alias 'my_alias' . The model is serialized using the alias 'my_alias' for the 'my_field' attribute.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict","title":"Configuration - ConfigDict","objectID":"/latest/api/config/#pydantic.config.ConfigDict","rank":95},{"content":"title:  | None The title for the generated JSON schema, defaults to the model's name","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.title","title":"Configuration - ConfigDict - title  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.title","rank":90},{"content":"model_title_generator: [[], ] | None A callable that takes a model class and returns the title for it. Defaults to None .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.model_title_generator","title":"Configuration - ConfigDict - model_title_generator  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.model_title_generator","rank":85},{"content":"field_title_generator: (\n    [[,  | ], ]\n    | None\n) A callable that takes a field's name and info and returns title for it. Defaults to None .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.field_title_generator","title":"Configuration - ConfigDict - field_title_generator  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.field_title_generator","rank":80},{"content":"str_to_lower: Whether to convert all characters to lowercase for str types. Defaults to False .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.str_to_lower","title":"Configuration - ConfigDict - str_to_lower  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.str_to_lower","rank":75},{"content":"str_to_upper: Whether to convert all characters to uppercase for str types. Defaults to False .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.str_to_upper","title":"Configuration - ConfigDict - str_to_upper  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.str_to_upper","rank":70},{"content":"str_strip_whitespace: Whether to strip leading and trailing whitespace for str types.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.str_strip_whitespace","title":"Configuration - ConfigDict - str_strip_whitespace  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.str_strip_whitespace","rank":65},{"content":"str_min_length: The minimum length for str types. Defaults to None .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.str_min_length","title":"Configuration - ConfigDict - str_min_length  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.str_min_length","rank":60},{"content":"str_max_length:  | None The maximum length for str types. Defaults to None .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.str_max_length","title":"Configuration - ConfigDict - str_max_length  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.str_max_length","rank":55},{"content":"extra:  | None Whether to ignore, allow, or forbid extra data during model initialization. Defaults to 'ignore' . Three configuration values are available: 'ignore' : Providing extra data is ignored (the default): from pydantic import BaseModel, ConfigDict\n\nclass User(BaseModel):\n    model_config = ConfigDict(extra='ignore')  # (1)!\n\n    name: str\n\nuser = User(name='John Doe', age=20)  # (2)!\nprint(user)\n#> name='John Doe' This is the default behaviour. The age argument is ignored. 'forbid' : Providing extra data is not permitted, and a \n  will be raised if this is the case: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='forbid')\n\n\ntry:\n    Model(x=1, y='a')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    y\n      Extra inputs are not permitted [type=extra_forbidden, input_value='a', input_type=str]\n    \"\"\" 'allow' : Providing extra data is allowed and stored in the __pydantic_extra__ dictionary attribute: from pydantic import BaseModel, ConfigDict\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\nm = Model(x=1, y='a')\nassert m.__pydantic_extra__ == {'y': 'a'} By default, no validation will be applied to these extra items, but you can set a type for the values by overriding\n  the type annotation for __pydantic_extra__ : from pydantic import BaseModel, ConfigDict, Field, ValidationError\n\n\nclass Model(BaseModel):\n    __pydantic_extra__: dict[str, int] = Field(init=False)  # (1)!\n\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\ntry:\n    Model(x=1, y='a')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    y\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\"\n\nm = Model(x=1, y='2')\nassert m.x == 1\nassert m.y == 2\nassert m.model_dump() == {'x': 1, 'y': 2}\nassert m.__pydantic_extra__ == {'y': 2} The = Field(init=False) does not have any effect at runtime, but prevents the __pydantic_extra__ field from\n   being included as a parameter to the model's __init__ method by type checkers.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.extra","title":"Configuration - ConfigDict - extra  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.extra","rank":50},{"content":"frozen: Whether models are faux-immutable, i.e. whether __setattr__ is allowed, and also generates\na __hash__() method for the model. This makes instances of the model potentially hashable if all the\nattributes are hashable. Defaults to False .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.frozen","title":"Configuration - ConfigDict - frozen  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.frozen","rank":45},{"content":"populate_by_name: Whether an aliased field may be populated by its name as given by the model\nattribute, as well as the alias. Defaults to False . Warning populate_by_name usage is not recommended in v2.11+ and will be deprecated in v3.\nInstead, you should use the  configuration setting. When validate_by_name=True and validate_by_alias=True , this is strictly equivalent to the\nprevious behavior of populate_by_name=True . In v2.11, we also introduced a  setting that introduces more fine grained\ncontrol for validation behavior. Here's how you might go about using the new settings to achieve the same behavior: from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(validate_by_name=True, validate_by_alias=True)\n\n    my_field: str = Field(alias='my_alias')  # (1)!\n\nm = Model(my_alias='foo')  # (2)!\nprint(m)\n#> my_field='foo'\n\nm = Model(my_field='foo')  # (3)!\nprint(m)\n#> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model is populated by the alias 'my_alias' . The model is populated by the attribute name 'my_field' .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.populate_by_name","title":"Configuration - ConfigDict - populate_by_name  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.populate_by_name","rank":40},{"content":"use_enum_values: Whether to populate models with the value property of enums, rather than the raw enum.\nThis may be useful if you want to serialize model.model_dump() later. Defaults to False . Note If you have an Optional[Enum] value that you set a default for, you need to use validate_default=True for said Field to ensure that the use_enum_values flag takes effect on the default, as extracting an\nenum's value occurs during validation, not serialization. from enum import Enum\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field\n\nclass SomeEnum(Enum):\n    FOO = 'foo'\n    BAR = 'bar'\n    BAZ = 'baz'\n\nclass SomeModel(BaseModel):\n    model_config = ConfigDict(use_enum_values=True)\n\n    some_enum: SomeEnum\n    another_enum: Optional[SomeEnum] = Field(\n        default=SomeEnum.FOO, validate_default=True\n    )\n\nmodel1 = SomeModel(some_enum=SomeEnum.BAR)\nprint(model1.model_dump())\n#> {'some_enum': 'bar', 'another_enum': 'foo'}\n\nmodel2 = SomeModel(some_enum=SomeEnum.BAR, another_enum=SomeEnum.BAZ)\nprint(model2.model_dump())\n#> {'some_enum': 'bar', 'another_enum': 'baz'}","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.use_enum_values","title":"Configuration - ConfigDict - use_enum_values  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.use_enum_values","rank":35},{"content":"validate_assignment: Whether to validate the data when the model is changed. Defaults to False . The default behavior of Pydantic is to validate the data when the model is created. In case the user changes the data after the model is created, the model is not revalidated. from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n\nuser = User(name='John Doe')  # (1)!\nprint(user)\n#> name='John Doe'\nuser.name = 123  # (1)!\nprint(user)\n#> name=123 The validation happens only when the model is created. The validation does not happen when the data is changed. In case you want to revalidate the model when the data is changed, you can use validate_assignment=True : from pydantic import BaseModel, ValidationError\n\nclass User(BaseModel, validate_assignment=True):  # (1)!\n    name: str\n\nuser = User(name='John Doe')  # (2)!\nprint(user)\n#> name='John Doe'\ntry:\n    user.name = 123  # (3)!\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for User\n    name\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    ''' You can either use class keyword arguments, or model_config to set validate_assignment=True . The validation happens when the model is created. The validation also happens when the data is changed.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.validate_assignment","title":"Configuration - ConfigDict - validate_assignment  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.validate_assignment","rank":30},{"content":"arbitrary_types_allowed: Whether arbitrary types are allowed for field types. Defaults to False . from pydantic import BaseModel, ConfigDict, ValidationError\n\n# This is not a pydantic model, it's an arbitrary class\nclass Pet:\n    def __init__(self, name: str):\n        self.name = name\n\nclass Model(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    pet: Pet\n    owner: str\n\npet = Pet(name='Hedwig')\n# A simple check of instance type is used to validate the data\nmodel = Model(owner='Harry', pet=pet)\nprint(model)\n#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\nprint(model.pet)\n#> <__main__.Pet object at 0x0123456789ab>\nprint(model.pet.name)\n#> Hedwig\nprint(type(model.pet))\n#> try:\n    # If the value is not an instance of the type, it's invalid\n    Model(owner='Harry', pet='Hedwig')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    pet\n      Input should be an instance of Pet [type=is_instance_of, input_value='Hedwig', input_type=str]\n    '''\n\n# Nothing in the instance of the arbitrary type is checked\n# Here name probably should have been a str, but it's not validated\npet2 = Pet(name=42)\nmodel2 = Model(owner='Harry', pet=pet2)\nprint(model2)\n#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'\nprint(model2.pet)\n#> <__main__.Pet object at 0x0123456789ab>\nprint(model2.pet.name)\n#> 42\nprint(type(model2.pet))\n#>","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.arbitrary_types_allowed","title":"Configuration - ConfigDict - arbitrary_types_allowed  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.arbitrary_types_allowed","rank":25},{"content":"from_attributes: Whether to build models and look up discriminators of tagged unions using python object attributes.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.from_attributes","title":"Configuration - ConfigDict - from_attributes  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.from_attributes","rank":20},{"content":"loc_by_alias: Whether to use the actual key provided in the data (e.g. alias) for error loc s rather than the field's name. Defaults to True .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.loc_by_alias","title":"Configuration - ConfigDict - loc_by_alias  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.loc_by_alias","rank":15},{"content":"alias_generator: (\n    [[], ] |  | None\n) A callable that takes a field name and returns an alias for it\nor an instance of . Defaults to None . When using a callable, the alias generator is used for both validation and serialization.\nIf you want to use different alias generators for validation and serialization, you can use\n instead. If data source field names do not match your code style (e. g. CamelCase fields),\nyou can automatically generate aliases using alias_generator . Here's an example with\na basic callable: from pydantic import BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_pascal\n\nclass Voice(BaseModel):\n    model_config = ConfigDict(alias_generator=to_pascal)\n\n    name: str\n    language_code: str\n\nvoice = Voice(Name='Filiz', LanguageCode='tr-TR')\nprint(voice.language_code)\n#> tr-TR\nprint(voice.model_dump(by_alias=True))\n#> {'Name': 'Filiz', 'LanguageCode': 'tr-TR'} If you want to use different alias generators for validation and serialization, you can use\n. from pydantic import AliasGenerator, BaseModel, ConfigDict\nfrom pydantic.alias_generators import to_camel, to_pascal\n\nclass Athlete(BaseModel):\n    first_name: str\n    last_name: str\n    sport: str\n\n    model_config = ConfigDict(\n        alias_generator=AliasGenerator(\n            validation_alias=to_camel,\n            serialization_alias=to_pascal,\n        )\n    )\n\nathlete = Athlete(firstName='John', lastName='Doe', sport='track')\nprint(athlete.model_dump(by_alias=True))\n#> {'FirstName': 'John', 'LastName': 'Doe', 'Sport': 'track'}","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.alias_generator","title":"Configuration - ConfigDict - alias_generator  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.alias_generator","rank":10},{"content":"ignored_types: [, ...] A tuple of types that may occur as values of class attributes without annotations. This is\ntypically used for custom descriptors (classes that behave like property ). If an attribute is set on a\nclass without an annotation and has a type that is not in this tuple (or otherwise recognized by pydantic ), an error will be raised. Defaults to () .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.ignored_types","title":"Configuration - ConfigDict - ignored_types  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.ignored_types","rank":5},{"content":"allow_inf_nan: Whether to allow infinity ( +inf an -inf ) and NaN values to float and decimal fields. Defaults to True .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.allow_inf_nan","title":"Configuration - ConfigDict - allow_inf_nan  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.allow_inf_nan","rank":0},{"content":"json_schema_extra:  |  | None A dict or callable to provide extra JSON schema properties. Defaults to None .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.json_schema_extra","title":"Configuration - ConfigDict - json_schema_extra  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.json_schema_extra","rank":-5},{"content":"json_encoders: [[], ] | None A dict of custom JSON encoders for specific types. Defaults to None . Deprecated This config option is a carryover from v1.\nWe originally planned to remove it in v2 but didn't have a 1:1 replacement so we are keeping it for now.\nIt is still deprecated and will likely be removed in the future.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.json_encoders","title":"Configuration - ConfigDict - json_encoders  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.json_encoders","rank":-10},{"content":"strict: (new in V2) If True , strict validation is applied to all fields on the model. By default, Pydantic attempts to coerce values to the correct type, when possible. There are situations in which you may want to disable this behavior, and instead raise an error if a value's type\ndoes not match the field's type annotation. To configure strict mode for all fields on a model, you can set strict=True on the model. from pydantic import BaseModel, ConfigDict\n\nclass Model(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n    name: str\n    age: int See Strict Mode for more details. See the Conversion Table for more details on how Pydantic converts data in both\nstrict and lax modes.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.strict","title":"Configuration - ConfigDict - strict  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.strict","rank":-15},{"content":"revalidate_instances: [\n    \"always\", \"never\", \"subclass-instances\"\n] When and how to revalidate models and dataclasses during validation. Accepts the string\nvalues of 'never' , 'always' and 'subclass-instances' . Defaults to 'never' . 'never' will not revalidate models and dataclasses during validation 'always' will revalidate models and dataclasses during validation 'subclass-instances' will revalidate models and dataclasses during validation if the instance is a\n    subclass of the model or dataclass By default, model and dataclass instances are not revalidated during validation. from pydantic import BaseModel\n\nclass User(BaseModel, revalidate_instances='never'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]  # (2)!\nt = Transaction(user=my_user)  # (3)!\nprint(t)\n#> user=User(hobbies=[1])\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)\n#> user=SubUser(hobbies=['scuba diving'], sins=['lying']) revalidate_instances is set to 'never' by **default. The assignment is not validated, unless you set validate_assignment to True in the model's config. Since revalidate_instances is set to never , this is not revalidated. If you want to revalidate instances during validation, you can set revalidate_instances to 'always' in the model's config. from pydantic import BaseModel, ValidationError\n\nclass User(BaseModel, revalidate_instances='always'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]\ntry:\n    t = Transaction(user=my_user)  # (2)!\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Transaction\n    user.hobbies.0\n      Input should be a valid string [type=string_type, input_value=1, input_type=int]\n    '''\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)  # (3)!\n#> user=User(hobbies=['scuba diving']) revalidate_instances is set to 'always' . The model is revalidated, since revalidate_instances is set to 'always' . Using 'never' we would have gotten user=SubUser(hobbies=['scuba diving'], sins=['lying']) . It's also possible to set revalidate_instances to 'subclass-instances' to only revalidate instances\nof subclasses of the model. from pydantic import BaseModel\n\nclass User(BaseModel, revalidate_instances='subclass-instances'):  # (1)!\n    hobbies: list[str]\n\nclass SubUser(User):\n    sins: list[str]\n\nclass Transaction(BaseModel):\n    user: User\n\nmy_user = User(hobbies=['reading'])\nt = Transaction(user=my_user)\nprint(t)\n#> user=User(hobbies=['reading'])\n\nmy_user.hobbies = [1]\nt = Transaction(user=my_user)  # (2)!\nprint(t)\n#> user=User(hobbies=[1])\n\nmy_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\nt = Transaction(user=my_sub_user)\nprint(t)  # (3)!\n#> user=User(hobbies=['scuba diving']) revalidate_instances is set to 'subclass-instances' . This is not revalidated, since my_user is not a subclass of User . Using 'never' we would have gotten user=SubUser(hobbies=['scuba diving'], sins=['lying']) .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.revalidate_instances","title":"Configuration - ConfigDict - revalidate_instances  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.revalidate_instances","rank":-20},{"content":"ser_json_timedelta: ['iso8601', 'float'] The format of JSON serialized timedeltas. Accepts the string values of 'iso8601' and 'float' . Defaults to 'iso8601' . 'iso8601' will serialize timedeltas to ISO 8601 durations. 'float' will serialize timedeltas to the total number of seconds.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.ser_json_timedelta","title":"Configuration - ConfigDict - ser_json_timedelta  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.ser_json_timedelta","rank":-25},{"content":"ser_json_bytes: ['utf8', 'base64', 'hex'] The encoding of JSON serialized bytes. Defaults to 'utf8' .\nSet equal to val_json_bytes to get back an equal value after serialization round trip. 'utf8' will serialize bytes to UTF-8 strings. 'base64' will serialize bytes to URL safe base64 strings. 'hex' will serialize bytes to hexadecimal strings.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.ser_json_bytes","title":"Configuration - ConfigDict - ser_json_bytes  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.ser_json_bytes","rank":-30},{"content":"val_json_bytes: ['utf8', 'base64', 'hex'] The encoding of JSON serialized bytes to decode. Defaults to 'utf8' .\nSet equal to ser_json_bytes to get back an equal value after serialization round trip. 'utf8' will deserialize UTF-8 strings to bytes. 'base64' will deserialize URL safe base64 strings to bytes. 'hex' will deserialize hexadecimal strings to bytes.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.val_json_bytes","title":"Configuration - ConfigDict - val_json_bytes  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.val_json_bytes","rank":-35},{"content":"ser_json_inf_nan: ['null', 'constants', 'strings'] The encoding of JSON serialized infinity and NaN float values. Defaults to 'null' . 'null' will serialize infinity and NaN values as null . 'constants' will serialize infinity and NaN values as Infinity and NaN . 'strings' will serialize infinity as string \"Infinity\" and NaN as string \"NaN\" .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.ser_json_inf_nan","title":"Configuration - ConfigDict - ser_json_inf_nan  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.ser_json_inf_nan","rank":-40},{"content":"validate_default: Whether to validate default values during validation. Defaults to False .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.validate_default","title":"Configuration - ConfigDict - validate_default  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.validate_default","rank":-45},{"content":"validate_return: Whether to validate the return value from call validators. Defaults to False .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.validate_return","title":"Configuration - ConfigDict - validate_return  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.validate_return","rank":-50},{"content":"protected_namespaces: [ | [], ...] A tuple of strings and/or patterns that prevent models from having fields with names that conflict with them.\nFor strings, we match on a prefix basis. Ex, if 'dog' is in the protected namespace, 'dog_name' will be protected.\nFor patterns, we match on the entire field name. Ex, if re.compile(r'^dog$') is in the protected namespace, 'dog' will be protected, but 'dog_name' will not be.\nDefaults to ('model_validate', 'model_dump',) . The reason we've selected these is to prevent collisions with other validation / dumping formats\nin the future - ex, model_validate_{some_newly_supported_format} . Before v2.10, Pydantic used ('model_',) as the default value for this setting to\nprevent collisions between model attributes and BaseModel 's own methods. This was changed\nin v2.10 given feedback that this restriction was limiting in AI and data science contexts,\nwhere it is common to have fields with names like model_id , model_input , model_output , etc. For more details, see https://github.com/pydantic/pydantic/issues/10315. import warnings\n\nfrom pydantic import BaseModel\n\nwarnings.filterwarnings('error')  # Raise warnings as errors\n\ntry:\n\n    class Model(BaseModel):\n        model_dump_something: str\n\nexcept UserWarning as e:\n    print(e)\n    '''\n    Field 'model_dump_something' in 'Model' conflicts with protected namespace 'model_dump'.\n\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('model_validate',).\n    ''' You can customize this behavior using the protected_namespaces setting: import re\nimport warnings\n\nfrom pydantic import BaseModel, ConfigDict\n\nwith warnings.catch_warnings(record=True) as caught_warnings:\n    warnings.simplefilter('always')  # Catch all warnings\n\n    class Model(BaseModel):\n        safe_field: str\n        also_protect_field: str\n        protect_this: str\n\n        model_config = ConfigDict(\n            protected_namespaces=(\n                'protect_me_',\n                'also_protect_',\n                re.compile('^protect_this$'),\n            )\n        )\n\nfor warning in caught_warnings:\n    print(f'{warning.message}')\n    '''\n    Field 'also_protect_field' in 'Model' conflicts with protected namespace 'also_protect_'.\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('protect_me_', re.compile('^protect_this$'))`.\n\n    Field 'protect_this' in 'Model' conflicts with protected namespace 're.compile('^protect_this$')'.\n    You may be able to solve this by setting the 'protected_namespaces' configuration to ('protect_me_', 'also_protect_')`.\n    ''' While Pydantic will only emit a warning when an item is in a protected namespace but does not actually have a collision,\nan error is raised if there is an actual collision with an existing attribute: from pydantic import BaseModel, ConfigDict\n\ntry:\n\n    class Model(BaseModel):\n        model_validate: str\n\n        model_config = ConfigDict(protected_namespaces=('model_',))\n\nexcept ValueError as e:\n    print(e)\n    '''\n    Field 'model_validate' conflicts with member > of protected namespace 'model_'.\n    '''","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.protected_namespaces","title":"Configuration - ConfigDict - protected_namespaces  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.protected_namespaces","rank":-55},{"content":"hide_input_in_errors: Whether to hide inputs when printing errors. Defaults to False . Pydantic shows the input value and type when it raises ValidationError during the validation. from pydantic import BaseModel, ValidationError\n\nclass Model(BaseModel):\n    a: str\n\ntry:\n    Model(a=123)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    a\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    ''' You can hide the input value and type by setting the hide_input_in_errors config to True . from pydantic import BaseModel, ConfigDict, ValidationError\n\nclass Model(BaseModel):\n    a: str\n    model_config = ConfigDict(hide_input_in_errors=True)\n\ntry:\n    Model(a=123)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    a\n      Input should be a valid string [type=string_type]\n    '''","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.hide_input_in_errors","title":"Configuration - ConfigDict - hide_input_in_errors  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.hide_input_in_errors","rank":-60},{"content":"defer_build: Whether to defer model validator and serializer construction until the first model validation. Defaults to False. This can be useful to avoid the overhead of building models which are only\nused nested within other models, or when you want to manually define type namespace via\n. Since v2.10, this setting also applies to pydantic dataclasses and TypeAdapter instances.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.defer_build","title":"Configuration - ConfigDict - defer_build  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.defer_build","rank":-65},{"content":"plugin_settings: [, ] | None A dict of settings for plugins. Defaults to None .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.plugin_settings","title":"Configuration - ConfigDict - plugin_settings  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.plugin_settings","rank":-70},{"content":"schema_generator: [] | None Warning schema_generator is deprecated in v2.10. Prior to v2.10, this setting was advertised as highly subject to change.\nIt's possible that this interface may once again become public once the internal core schema generation\nAPI is more stable, but that will likely come after significant performance improvements have been made.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.schema_generator","title":"Configuration - ConfigDict - schema_generator  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.schema_generator","rank":-75},{"content":"json_schema_serialization_defaults_required: Whether fields with default values should be marked as required in the serialization schema. Defaults to False . This ensures that the serialization schema will reflect the fact a field with a default will always be present\nwhen serializing the model, even though it is not required for validation. However, there are scenarios where this may be undesirable — in particular, if you want to share the schema\nbetween validation and serialization, and don't mind fields with defaults being marked as not required during\nserialization. See #7209 for more details. from pydantic import BaseModel, ConfigDict\n\nclass Model(BaseModel):\n    a: str = 'a'\n\n    model_config = ConfigDict(json_schema_serialization_defaults_required=True)\n\nprint(Model.model_json_schema(mode='validation'))\n'''\n{\n    'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n    'title': 'Model',\n    'type': 'object',\n}\n'''\nprint(Model.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},\n    'required': ['a'],\n    'title': 'Model',\n    'type': 'object',\n}\n'''","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.json_schema_serialization_defaults_required","title":"Configuration - ConfigDict - json_schema_serialization_defaults_required  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.json_schema_serialization_defaults_required","rank":-80},{"content":"json_schema_mode_override: [\n    \"validation\", \"serialization\", None\n] If not None , the specified mode will be used to generate the JSON schema regardless of what mode was passed to\nthe function call. Defaults to None . This provides a way to force the JSON schema generation to reflect a specific mode, e.g., to always use the\nvalidation schema. It can be useful when using frameworks (such as FastAPI) that may generate different schemas for validation\nand serialization that must both be referenced from the same schema; when this happens, we automatically append -Input to the definition reference for the validation schema and -Output to the definition reference for the\nserialization schema. By specifying a json_schema_mode_override though, this prevents the conflict between\nthe validation and serialization schemas (since both will use the specified schema), and so prevents the suffixes\nfrom being added to the definition references. from pydantic import BaseModel, ConfigDict, Json\n\nclass Model(BaseModel):\n    a: Json[int]  # requires a string to validate, but will dump an int\n\nprint(Model.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {'a': {'title': 'A', 'type': 'integer'}},\n    'required': ['a'],\n    'title': 'Model',\n    'type': 'object',\n}\n'''\n\nclass ForceInputModel(Model):\n    # the following ensures that even with mode='serialization', we\n    # will get the schema that would be generated for validation.\n    model_config = ConfigDict(json_schema_mode_override='validation')\n\nprint(ForceInputModel.model_json_schema(mode='serialization'))\n'''\n{\n    'properties': {\n        'a': {\n            'contentMediaType': 'application/json',\n            'contentSchema': {'type': 'integer'},\n            'title': 'A',\n            'type': 'string',\n        }\n    },\n    'required': ['a'],\n    'title': 'ForceInputModel',\n    'type': 'object',\n}\n'''","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.json_schema_mode_override","title":"Configuration - ConfigDict - json_schema_mode_override  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.json_schema_mode_override","rank":-85},{"content":"coerce_numbers_to_str: If True , enables automatic coercion of any Number type to str in \"lax\" (non-strict) mode. Defaults to False . Pydantic doesn't allow number types ( int , float , Decimal ) to be coerced as type str by default. from decimal import Decimal\n\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\nclass Model(BaseModel):\n    value: str\n\ntry:\n    print(Model(value=42))\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    value\n      Input should be a valid string [type=string_type, input_value=42, input_type=int]\n    '''\n\nclass Model(BaseModel):\n    model_config = ConfigDict(coerce_numbers_to_str=True)\n\n    value: str\n\nrepr(Model(value=42).value)\n#> \"42\"\nrepr(Model(value=42.13).value)\n#> \"42.13\"\nrepr(Model(value=Decimal('42.13')).value)\n#> \"42.13\"","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.coerce_numbers_to_str","title":"Configuration - ConfigDict - coerce_numbers_to_str  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.coerce_numbers_to_str","rank":-90},{"content":"regex_engine: ['rust-regex', 'python-re'] The regex engine to be used for pattern validation.\nDefaults to 'rust-regex' . rust-regex uses the regex Rust crate,\n  which is non-backtracking and therefore more DDoS resistant, but does not support all regex features. python-re use the re module,\n  which supports all regex features, but may be slower. Note If you use a compiled regex pattern, the python-re engine will be used regardless of this setting.\nThis is so that flags such as re.IGNORECASE are respected. from pydantic import BaseModel, ConfigDict, Field, ValidationError\n\nclass Model(BaseModel):\n    model_config = ConfigDict(regex_engine='python-re')\n\n    value: str = Field(pattern=r'^abc(?=def)')\n\nprint(Model(value='abcdef').value)\n#> abcdef\n\ntry:\n    print(Model(value='abxyzcdef'))\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    value\n      String should match pattern '^abc(?=def)' [type=string_pattern_mismatch, input_value='abxyzcdef', input_type=str]\n    '''","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.regex_engine","title":"Configuration - ConfigDict - regex_engine  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.regex_engine","rank":-95},{"content":"validation_error_cause: If True , Python exceptions that were part of a validation failure will be shown as an exception group as a cause. Can be useful for debugging. Defaults to False .","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.validation_error_cause","title":"Configuration - ConfigDict - validation_error_cause  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.validation_error_cause","rank":-100},{"content":"use_attribute_docstrings: Whether docstrings of attributes (bare string literals immediately following the attribute declaration)\nshould be used for field descriptions. Defaults to False . Available in Pydantic v2.7+. from pydantic import BaseModel, ConfigDict, Field\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(use_attribute_docstrings=True)\n\n    x: str\n    \"\"\"\n    Example of an attribute docstring\n    \"\"\"\n\n    y: int = Field(description=\"Description in Field\")\n    \"\"\"\n    Description in Field overrides attribute docstring\n    \"\"\"\n\n\nprint(Model.model_fields[\"x\"].description)\n# > Example of an attribute docstring\nprint(Model.model_fields[\"y\"].description)\n# > Description in Field This requires the source code of the class to be available at runtime. Usage with TypedDict and stdlib dataclasses Due to current limitations, attribute docstrings detection may not work as expected when using\n and stdlib dataclasses, in particular when: inheritance is being used. multiple classes have the same name in the same source file (unless Python 3.13 or greater is used).","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.use_attribute_docstrings","title":"Configuration - ConfigDict - use_attribute_docstrings  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.use_attribute_docstrings","rank":-105},{"content":"cache_strings:  | ['all', 'keys', 'none'] Whether to cache strings to avoid constructing new Python objects. Defaults to True. Enabling this setting should significantly improve validation performance while increasing memory usage slightly. True or 'all' (the default): cache all strings 'keys' : cache only dictionary keys False or 'none' : no caching Note True or 'all' is required to cache strings during general validation because\nvalidators don't know if they're in a key or a value. Tip If repeated strings are rare, it's recommended to use 'keys' or 'none' to reduce memory usage,\nas the performance difference is minimal if repeated strings are rare.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.cache_strings","title":"Configuration - ConfigDict - cache_strings  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.cache_strings","rank":-110},{"content":"validate_by_alias: Whether an aliased field may be populated by its alias. Defaults to True . Note In v2.11, validate_by_alias was introduced in conjunction with \nto empower users with more fine grained validation control. In <v2.11, disabling validation by alias was not possible. Here's an example of disabling validation by alias: from pydantic import BaseModel , ConfigDict , Field class Model ( BaseModel ): model_config = ConfigDict ( validate_by_name = True , validate_by_alias = False ) my_field : str = Field ( validation_alias = 'my_alias' ) # (1)! m = Model ( my_field = 'foo' ) # (2)! print ( m ) #> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model can only be populated by the attribute name 'my_field' . Warning You cannot set both validate_by_alias and validate_by_name to False .\nThis would make it impossible to populate an attribute. See usage errors for an example. If you set validate_by_alias to False , under the hood, Pydantic dynamically sets validate_by_name to True to ensure that validation can still occur.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.validate_by_alias","title":"Configuration - ConfigDict - validate_by_alias  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.validate_by_alias","rank":-115},{"content":"validate_by_name: Whether an aliased field may be populated by its name as given by the model\nattribute. Defaults to False . Note In v2.0-v2.10, the populate_by_name configuration setting was used to specify\nwhether or not a field could be populated by its name and alias. In v2.11, validate_by_name was introduced in conjunction with \nto empower users with more fine grained validation behavior control. from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(validate_by_name=True, validate_by_alias=True)\n\n    my_field: str = Field(validation_alias='my_alias')  # (1)!\n\nm = Model(my_alias='foo')  # (2)!\nprint(m)\n#> my_field='foo'\n\nm = Model(my_field='foo')  # (3)!\nprint(m)\n#> my_field='foo' The field 'my_field' has an alias 'my_alias' . The model is populated by the alias 'my_alias' . The model is populated by the attribute name 'my_field' . Warning You cannot set both validate_by_alias and validate_by_name to False .\nThis would make it impossible to populate an attribute. See usage errors for an example.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.validate_by_name","title":"Configuration - ConfigDict - validate_by_name  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.validate_by_name","rank":-120},{"content":"serialize_by_alias: Whether an aliased field should be serialized by its alias. Defaults to False . Note: In v2.11, serialize_by_alias was introduced to address the popular request for consistency with alias behavior for validation and serialization settings.\nIn v3, the default value is expected to change to True for consistency with the validation default. from pydantic import BaseModel, ConfigDict, Field\n\nclass Model(BaseModel):\n    model_config = ConfigDict(serialize_by_alias=True)\n\n    my_field: str = Field(serialization_alias='my_alias')  # (1)!\n\nm = Model(my_field='foo')\nprint(m.model_dump())  # (2)!\n#> {'my_alias': 'foo'} The field 'my_field' has an alias 'my_alias' . The model is serialized using the alias 'my_alias' for the 'my_field' attribute.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ConfigDict.serialize_by_alias","title":"Configuration - ConfigDict - serialize_by_alias  instance-attribute","objectID":"/latest/api/config/#pydantic.config.ConfigDict.serialize_by_alias","rank":-125},{"content":"with_config(\n    *, config: \n) -> [[], ] with_config(\n    config: ,\n) -> [[], ] with_config(\n    **config: [],\n) -> [[], ] with_config(\n    config:  | None = None, /, **kwargs: \n) -> [[], ] Usage Documentation Configuration with other types A convenience decorator to set a Pydantic configuration on a TypedDict or a dataclass from the standard library. Although the configuration can be set using the __pydantic_config__ attribute, it does not play well with type checkers,\nespecially with TypedDict . Usage from typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, with_config\n\n@with_config(ConfigDict(str_to_lower=True))\nclass TD(TypedDict):\n    x: str\n\nta = TypeAdapter(TD)\n\nprint(ta.validate_python({'x': 'ABC'}))\n#> {'x': 'abc'}","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.with_config","title":"Configuration - with_config","objectID":"/latest/api/config/#pydantic.config.with_config","rank":-130},{"content":"ExtraValues = ['allow', 'ignore', 'forbid']","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.config.ExtraValues","title":"Configuration - ExtraValues  module-attribute","objectID":"/latest/api/config/#pydantic.config.ExtraValues","rank":-135},{"content":"Alias generators for converting between different capitalization conventions. to_pascal ¶ to_pascal(snake: ) -> Convert a snake_case string to PascalCase. Parameters: Name Type Description Default snake The string to convert. required Returns: Type Description The PascalCase string. to_camel ¶ to_camel(snake: ) -> Convert a snake_case string to camelCase. Parameters: Name Type Description Default snake The string to convert. required Returns: Type Description The converted camelCase string. to_snake ¶ to_snake(camel: ) -> Convert a PascalCase, camelCase, or kebab-case string to snake_case. Parameters: Name Type Description Default camel The string to convert. required Returns: Type Description The converted string in snake_case.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.alias_generators","title":"Configuration - pydantic.alias_generators","objectID":"/latest/api/config/#pydantic.alias_generators","rank":-140},{"content":"to_pascal(snake: ) -> Convert a snake_case string to PascalCase. Parameters: Name Type Description Default snake The string to convert. required Returns: Type Description The PascalCase string.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.alias_generators.to_pascal","title":"Configuration - pydantic.alias_generators - to_pascal","objectID":"/latest/api/config/#pydantic.alias_generators.to_pascal","rank":-145},{"content":"to_camel(snake: ) -> Convert a snake_case string to camelCase. Parameters: Name Type Description Default snake The string to convert. required Returns: Type Description The converted camelCase string.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.alias_generators.to_camel","title":"Configuration - pydantic.alias_generators - to_camel","objectID":"/latest/api/config/#pydantic.alias_generators.to_camel","rank":-150},{"content":"to_snake(camel: ) -> Convert a PascalCase, camelCase, or kebab-case string to snake_case. Parameters: Name Type Description Default camel The string to convert. required Returns: Type Description The converted string in snake_case.","pageID":"Configuration","abs_url":"/latest/api/config/#pydantic.alias_generators.to_snake","title":"Configuration - pydantic.alias_generators - to_snake","objectID":"/latest/api/config/#pydantic.alias_generators.to_snake","rank":-155},{"content":"Provide an enhanced dataclass that performs validation. dataclass ¶ dataclass(\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  = False,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None,\n    kw_only:  = ...,\n    slots:  = ...\n) -> [[[]], []] dataclass(\n    _cls: [],\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None,\n    kw_only:  = ...,\n    slots:  = ...\n) -> [] dataclass(\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None\n) -> [[[]], []] dataclass(\n    _cls: [],\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None\n) -> [] dataclass(\n    _cls: [] | None = None,\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None,\n    kw_only:  = False,\n    slots:  = False\n) -> (\n    [[[]], []]\n    | []\n) Usage Documentation dataclasses A decorator used to create a Pydantic-enhanced dataclass, similar to the standard Python dataclass ,\nbut with added validation. This function should be used similarly to dataclasses.dataclass . Parameters: Name Type Description Default _cls [] | None The target dataclass . None init [False] Included for signature compatibility with dataclasses.dataclass , and is passed through to dataclasses.dataclass when appropriate. If specified, must be set to False , as pydantic inserts its\nown __init__ function. False repr A boolean indicating whether to include the field in the __repr__ output. True eq Determines if a __eq__ method should be generated for the class. True order Determines if comparison magic methods should be generated, such as __lt__ , but not __eq__ . False unsafe_hash Determines if a __hash__ method should be included in the class, as in dataclasses.dataclass . False frozen | None Determines if the generated class should be a 'frozen' dataclass , which does not allow its\nattributes to be modified after it has been initialized. If not set, the value from the provided config argument will be used (and will default to False otherwise). None config | [] | None The Pydantic config to use for the dataclass . None validate_on_init | None A deprecated parameter included for backwards compatibility; in V2, all Pydantic dataclasses\nare validated on init. None kw_only Determines if __init__ method parameters must be specified by keyword only. Defaults to False . False slots Determines if the generated class should be a 'slots' dataclass , which does not allow the addition of\nnew attributes after instantiation. False Returns: Type Description [[[]], []] | [] A decorator that accepts a class as its argument and returns a Pydantic dataclass . Raises: Type Description Raised if init is not False or validate_on_init is False . rebuild_dataclass ¶ rebuild_dataclass(\n    cls: [],\n    *,\n    force:  = False,\n    raise_errors:  = True,\n    _parent_namespace_depth:  = 2,\n    _types_namespace:  | None = None\n) ->  | None Try to rebuild the pydantic-core schema for the dataclass. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\nthe initial attempt to build the schema, and automatic rebuilding fails. This is analogous to BaseModel.model_rebuild . Parameters: Name Type Description Default cls [] The class to rebuild the pydantic-core schema for. required force Whether to force the rebuilding of the schema, defaults to False . False raise_errors Whether to raise errors, defaults to True . True _parent_namespace_depth The depth level of the parent namespace, defaults to 2. 2 _types_namespace | None The types namespace, defaults to None . None Returns: Type Description | None Returns None if the schema is already \"complete\" and rebuilding was not required. | None If rebuilding was required, returns True if rebuilding was successful, otherwise False . is_pydantic_dataclass ¶ is_pydantic_dataclass(\n    class_: [],\n) -> [[]] Whether a class is a pydantic dataclass. Parameters: Name Type Description Default class_ [] The class. required Returns: Type Description [[]] True if the class is a pydantic dataclass, False otherwise.","pageID":"Pydantic Dataclasses","abs_url":"/latest/api/dataclasses/#Pydantic Dataclasses","title":"Pydantic Dataclasses","objectID":"/latest/api/dataclasses/#Pydantic Dataclasses","rank":100},{"content":"dataclass(\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  = False,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None,\n    kw_only:  = ...,\n    slots:  = ...\n) -> [[[]], []] dataclass(\n    _cls: [],\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None,\n    kw_only:  = ...,\n    slots:  = ...\n) -> [] dataclass(\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None\n) -> [[[]], []] dataclass(\n    _cls: [],\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None\n) -> [] dataclass(\n    _cls: [] | None = None,\n    *,\n    init: [False] = False,\n    repr:  = True,\n    eq:  = True,\n    order:  = False,\n    unsafe_hash:  = False,\n    frozen:  | None = None,\n    config:  | [] | None = None,\n    validate_on_init:  | None = None,\n    kw_only:  = False,\n    slots:  = False\n) -> (\n    [[[]], []]\n    | []\n) Usage Documentation dataclasses A decorator used to create a Pydantic-enhanced dataclass, similar to the standard Python dataclass ,\nbut with added validation. This function should be used similarly to dataclasses.dataclass . Parameters: Name Type Description Default _cls [] | None The target dataclass . None init [False] Included for signature compatibility with dataclasses.dataclass , and is passed through to dataclasses.dataclass when appropriate. If specified, must be set to False , as pydantic inserts its\nown __init__ function. False repr A boolean indicating whether to include the field in the __repr__ output. True eq Determines if a __eq__ method should be generated for the class. True order Determines if comparison magic methods should be generated, such as __lt__ , but not __eq__ . False unsafe_hash Determines if a __hash__ method should be included in the class, as in dataclasses.dataclass . False frozen | None Determines if the generated class should be a 'frozen' dataclass , which does not allow its\nattributes to be modified after it has been initialized. If not set, the value from the provided config argument will be used (and will default to False otherwise). None config | [] | None The Pydantic config to use for the dataclass . None validate_on_init | None A deprecated parameter included for backwards compatibility; in V2, all Pydantic dataclasses\nare validated on init. None kw_only Determines if __init__ method parameters must be specified by keyword only. Defaults to False . False slots Determines if the generated class should be a 'slots' dataclass , which does not allow the addition of\nnew attributes after instantiation. False Returns: Type Description [[[]], []] | [] A decorator that accepts a class as its argument and returns a Pydantic dataclass . Raises: Type Description Raised if init is not False or validate_on_init is False .","pageID":"Pydantic Dataclasses","abs_url":"/latest/api/dataclasses/#pydantic.dataclasses.dataclass","title":"Pydantic Dataclasses - dataclass","objectID":"/latest/api/dataclasses/#pydantic.dataclasses.dataclass","rank":95},{"content":"rebuild_dataclass(\n    cls: [],\n    *,\n    force:  = False,\n    raise_errors:  = True,\n    _parent_namespace_depth:  = 2,\n    _types_namespace:  | None = None\n) ->  | None Try to rebuild the pydantic-core schema for the dataclass. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\nthe initial attempt to build the schema, and automatic rebuilding fails. This is analogous to BaseModel.model_rebuild . Parameters: Name Type Description Default cls [] The class to rebuild the pydantic-core schema for. required force Whether to force the rebuilding of the schema, defaults to False . False raise_errors Whether to raise errors, defaults to True . True _parent_namespace_depth The depth level of the parent namespace, defaults to 2. 2 _types_namespace | None The types namespace, defaults to None . None Returns: Type Description | None Returns None if the schema is already \"complete\" and rebuilding was not required. | None If rebuilding was required, returns True if rebuilding was successful, otherwise False .","pageID":"Pydantic Dataclasses","abs_url":"/latest/api/dataclasses/#pydantic.dataclasses.rebuild_dataclass","title":"Pydantic Dataclasses - rebuild_dataclass","objectID":"/latest/api/dataclasses/#pydantic.dataclasses.rebuild_dataclass","rank":90},{"content":"is_pydantic_dataclass(\n    class_: [],\n) -> [[]] Whether a class is a pydantic dataclass. Parameters: Name Type Description Default class_ [] The class. required Returns: Type Description [[]] True if the class is a pydantic dataclass, False otherwise.","pageID":"Pydantic Dataclasses","abs_url":"/latest/api/dataclasses/#pydantic.dataclasses.is_pydantic_dataclass","title":"Pydantic Dataclasses - is_pydantic_dataclass","objectID":"/latest/api/dataclasses/#pydantic.dataclasses.is_pydantic_dataclass","rank":85},{"content":"Pydantic-specific errors. PydanticErrorMixin ¶ PydanticErrorMixin(\n    message: , *, code:  | None\n) A mixin class for common functionality shared by all Pydantic-specific errors. Attributes: Name Type Description A message describing the error. An optional error code from PydanticErrorCodes enum. PydanticUserError ¶ PydanticUserError(\n    message: , *, code:  | None\n) Bases: , An error raised due to incorrect use of Pydantic. PydanticUndefinedAnnotation ¶ PydanticUndefinedAnnotation(name: , message: ) Bases: , A subclass of NameError raised when handling undefined annotations during CoreSchema generation. Attributes: Name Type Description Name of the error. Description of the error. from_name_error classmethod ¶ from_name_error(name_error: ) -> Convert a NameError to a PydanticUndefinedAnnotation error. Parameters: Name Type Description Default name_error NameError to be converted. required Returns: Type Description Converted PydanticUndefinedAnnotation error. PydanticImportError ¶ PydanticImportError(message: ) Bases: , An error raised when an import fails due to module changes between V1 and V2. Attributes: Name Type Description Description of the error. PydanticSchemaGenerationError ¶ PydanticSchemaGenerationError(message: ) Bases: An error raised during failures to generate a CoreSchema for some type. Attributes: Name Type Description Description of the error. PydanticInvalidForJsonSchema ¶ PydanticInvalidForJsonSchema(message: ) Bases: An error raised during failures to generate a JSON schema for some CoreSchema . Attributes: Name Type Description Description of the error. PydanticForbiddenQualifier ¶ PydanticForbiddenQualifier(\n    qualifier: , annotation: \n) Bases: An error raised if a forbidden type qualifier is found in a type annotation.","pageID":"Errors","abs_url":"/latest/api/errors/#Errors","title":"Errors","objectID":"/latest/api/errors/#Errors","rank":100},{"content":"PydanticErrorMixin(\n    message: , *, code:  | None\n) A mixin class for common functionality shared by all Pydantic-specific errors. Attributes: Name Type Description A message describing the error. An optional error code from PydanticErrorCodes enum.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticErrorMixin","title":"Errors - PydanticErrorMixin","objectID":"/latest/api/errors/#pydantic.errors.PydanticErrorMixin","rank":95},{"content":"PydanticUserError(\n    message: , *, code:  | None\n) Bases: , An error raised due to incorrect use of Pydantic.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticUserError","title":"Errors - PydanticUserError","objectID":"/latest/api/errors/#pydantic.errors.PydanticUserError","rank":90},{"content":"PydanticUndefinedAnnotation(name: , message: ) Bases: , A subclass of NameError raised when handling undefined annotations during CoreSchema generation. Attributes: Name Type Description Name of the error. Description of the error. from_name_error classmethod ¶ from_name_error(name_error: ) -> Convert a NameError to a PydanticUndefinedAnnotation error. Parameters: Name Type Description Default name_error NameError to be converted. required Returns: Type Description Converted PydanticUndefinedAnnotation error.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticUndefinedAnnotation","title":"Errors - PydanticUndefinedAnnotation","objectID":"/latest/api/errors/#pydantic.errors.PydanticUndefinedAnnotation","rank":85},{"content":"from_name_error(name_error: ) -> Convert a NameError to a PydanticUndefinedAnnotation error. Parameters: Name Type Description Default name_error NameError to be converted. required Returns: Type Description Converted PydanticUndefinedAnnotation error.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticUndefinedAnnotation.from_name_error","title":"Errors - PydanticUndefinedAnnotation - from_name_error  classmethod","objectID":"/latest/api/errors/#pydantic.errors.PydanticUndefinedAnnotation.from_name_error","rank":80},{"content":"PydanticImportError(message: ) Bases: , An error raised when an import fails due to module changes between V1 and V2. Attributes: Name Type Description Description of the error.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticImportError","title":"Errors - PydanticImportError","objectID":"/latest/api/errors/#pydantic.errors.PydanticImportError","rank":75},{"content":"PydanticSchemaGenerationError(message: ) Bases: An error raised during failures to generate a CoreSchema for some type. Attributes: Name Type Description Description of the error.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticSchemaGenerationError","title":"Errors - PydanticSchemaGenerationError","objectID":"/latest/api/errors/#pydantic.errors.PydanticSchemaGenerationError","rank":70},{"content":"PydanticInvalidForJsonSchema(message: ) Bases: An error raised during failures to generate a JSON schema for some CoreSchema . Attributes: Name Type Description Description of the error.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticInvalidForJsonSchema","title":"Errors - PydanticInvalidForJsonSchema","objectID":"/latest/api/errors/#pydantic.errors.PydanticInvalidForJsonSchema","rank":65},{"content":"PydanticForbiddenQualifier(\n    qualifier: , annotation: \n) Bases: An error raised if a forbidden type qualifier is found in a type annotation.","pageID":"Errors","abs_url":"/latest/api/errors/#pydantic.errors.PydanticForbiddenQualifier","title":"Errors - PydanticForbiddenQualifier","objectID":"/latest/api/errors/#pydantic.errors.PydanticForbiddenQualifier","rank":60},{"content":"","pageID":"Experimental","abs_url":"/latest/api/experimental/#experimental-api","title":"Experimental","objectID":"/latest/api/experimental/#experimental-api","rank":100},{"content":"Experimental pipeline API functionality. Be careful with this API, it's subject to change. _Pipeline dataclass ¶ _Pipeline(_steps: [, ...]) Bases: [, ] Abstract representation of a chain of validation, transformation, and parsing steps. transform ¶ transform(\n    func: [[], ]\n) -> [, ] Transform the output of the previous step. If used as the first step in a pipeline, the type of the field is used.\nThat is, the transformation is applied to after the value is parsed to the field's type. validate_as ¶ validate_as(\n    tp: [], *, strict:  = ...\n) -> [, ] validate_as(\n    tp: , *, strict:  = ...\n) -> [, ] validate_as(\n    tp: [] | ,\n    *,\n    strict:  = False\n) -> [, ] Validate / parse the input into a new type. If no type is provided, the type of the field is used. Types are parsed in Pydantic's lax mode by default,\nbut you can enable strict mode by passing strict=True . validate_as_deferred ¶ validate_as_deferred(\n    func: [[], []]\n) -> [, ] Parse the input into a new type, deferring resolution of the type until the current class\nis fully defined. This is useful when you need to reference the class in it's own type annotations. constrain ¶ constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: [],\n) -> [, ] constrain(constraint: ) -> Constrain a value to meet a certain condition. We support most conditions from annotated_types , as well as regular expressions. Most of the time you'll be calling a shortcut method like gt , lt , len , etc\nso you don't need to call this directly. predicate ¶ predicate(\n    func: [[], ]\n) -> [, ] Constrain a value to meet a certain predicate. gt ¶ gt(gt: ) -> [, ] Constrain a value to be greater than a certain value. lt ¶ lt(lt: ) -> [, ] Constrain a value to be less than a certain value. ge ¶ ge(ge: ) -> [, ] Constrain a value to be greater than or equal to a certain value. le ¶ le(le: ) -> [, ] Constrain a value to be less than or equal to a certain value. len ¶ len(\n    min_len: , max_len:  | None = None\n) -> [, ] Constrain a value to have a certain length. multiple_of ¶ multiple_of(\n    multiple_of: ,\n) -> [, ] multiple_of(\n    multiple_of: ,\n) -> [, ] multiple_of(multiple_of: ) -> [, ] Constrain a value to be a multiple of a certain number. eq ¶ eq(value: ) -> [, ] Constrain a value to be equal to a certain value. not_eq ¶ not_eq(value: ) -> [, ] Constrain a value to not be equal to a certain value. in_ ¶ in_(values: []) -> [, ] Constrain a value to be in a certain set. not_in ¶ not_in(values: []) -> [, ] Constrain a value to not be in a certain set. otherwise ¶ otherwise(\n    other: [, ]\n) -> [ | ,  | ] Combine two validation chains, returning the result of the first chain if it succeeds, and the second chain if it fails. then ¶ then(\n    other: [, ]\n) -> [, ] Pipe the result of one validation chain into another.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pipeline-api","title":"Experimental - Pipeline API","objectID":"/latest/api/experimental/#pipeline-api","rank":95},{"content":"_Pipeline(_steps: [, ...]) Bases: [, ] Abstract representation of a chain of validation, transformation, and parsing steps. transform ¶ transform(\n    func: [[], ]\n) -> [, ] Transform the output of the previous step. If used as the first step in a pipeline, the type of the field is used.\nThat is, the transformation is applied to after the value is parsed to the field's type. validate_as ¶ validate_as(\n    tp: [], *, strict:  = ...\n) -> [, ] validate_as(\n    tp: , *, strict:  = ...\n) -> [, ] validate_as(\n    tp: [] | ,\n    *,\n    strict:  = False\n) -> [, ] Validate / parse the input into a new type. If no type is provided, the type of the field is used. Types are parsed in Pydantic's lax mode by default,\nbut you can enable strict mode by passing strict=True . validate_as_deferred ¶ validate_as_deferred(\n    func: [[], []]\n) -> [, ] Parse the input into a new type, deferring resolution of the type until the current class\nis fully defined. This is useful when you need to reference the class in it's own type annotations. constrain ¶ constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: [],\n) -> [, ] constrain(constraint: ) -> Constrain a value to meet a certain condition. We support most conditions from annotated_types , as well as regular expressions. Most of the time you'll be calling a shortcut method like gt , lt , len , etc\nso you don't need to call this directly. predicate ¶ predicate(\n    func: [[], ]\n) -> [, ] Constrain a value to meet a certain predicate. gt ¶ gt(gt: ) -> [, ] Constrain a value to be greater than a certain value. lt ¶ lt(lt: ) -> [, ] Constrain a value to be less than a certain value. ge ¶ ge(ge: ) -> [, ] Constrain a value to be greater than or equal to a certain value. le ¶ le(le: ) -> [, ] Constrain a value to be less than or equal to a certain value. len ¶ len(\n    min_len: , max_len:  | None = None\n) -> [, ] Constrain a value to have a certain length. multiple_of ¶ multiple_of(\n    multiple_of: ,\n) -> [, ] multiple_of(\n    multiple_of: ,\n) -> [, ] multiple_of(multiple_of: ) -> [, ] Constrain a value to be a multiple of a certain number. eq ¶ eq(value: ) -> [, ] Constrain a value to be equal to a certain value. not_eq ¶ not_eq(value: ) -> [, ] Constrain a value to not be equal to a certain value. in_ ¶ in_(values: []) -> [, ] Constrain a value to be in a certain set. not_in ¶ not_in(values: []) -> [, ] Constrain a value to not be in a certain set. otherwise ¶ otherwise(\n    other: [, ]\n) -> [ | ,  | ] Combine two validation chains, returning the result of the first chain if it succeeds, and the second chain if it fails. then ¶ then(\n    other: [, ]\n) -> [, ] Pipe the result of one validation chain into another.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline","title":"Experimental - _Pipeline  dataclass","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline","rank":90},{"content":"transform(\n    func: [[], ]\n) -> [, ] Transform the output of the previous step. If used as the first step in a pipeline, the type of the field is used.\nThat is, the transformation is applied to after the value is parsed to the field's type.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.transform","title":"Experimental - _Pipeline  dataclass - transform","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.transform","rank":85},{"content":"validate_as(\n    tp: [], *, strict:  = ...\n) -> [, ] validate_as(\n    tp: , *, strict:  = ...\n) -> [, ] validate_as(\n    tp: [] | ,\n    *,\n    strict:  = False\n) -> [, ] Validate / parse the input into a new type. If no type is provided, the type of the field is used. Types are parsed in Pydantic's lax mode by default,\nbut you can enable strict mode by passing strict=True .","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.validate_as","title":"Experimental - _Pipeline  dataclass - validate_as","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.validate_as","rank":80},{"content":"validate_as_deferred(\n    func: [[], []]\n) -> [, ] Parse the input into a new type, deferring resolution of the type until the current class\nis fully defined. This is useful when you need to reference the class in it's own type annotations.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.validate_as_deferred","title":"Experimental - _Pipeline  dataclass - validate_as_deferred","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.validate_as_deferred","rank":75},{"content":"constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: ,\n) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(constraint: ) -> [, ] constrain(\n    constraint: [],\n) -> [, ] constrain(constraint: ) -> Constrain a value to meet a certain condition. We support most conditions from annotated_types , as well as regular expressions. Most of the time you'll be calling a shortcut method like gt , lt , len , etc\nso you don't need to call this directly.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.constrain","title":"Experimental - _Pipeline  dataclass - constrain","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.constrain","rank":70},{"content":"predicate(\n    func: [[], ]\n) -> [, ] Constrain a value to meet a certain predicate.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.predicate","title":"Experimental - _Pipeline  dataclass - predicate","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.predicate","rank":65},{"content":"gt(gt: ) -> [, ] Constrain a value to be greater than a certain value.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.gt","title":"Experimental - _Pipeline  dataclass - gt","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.gt","rank":60},{"content":"lt(lt: ) -> [, ] Constrain a value to be less than a certain value.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.lt","title":"Experimental - _Pipeline  dataclass - lt","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.lt","rank":55},{"content":"ge(ge: ) -> [, ] Constrain a value to be greater than or equal to a certain value.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.ge","title":"Experimental - _Pipeline  dataclass - ge","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.ge","rank":50},{"content":"le(le: ) -> [, ] Constrain a value to be less than or equal to a certain value.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.le","title":"Experimental - _Pipeline  dataclass - le","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.le","rank":45},{"content":"len(\n    min_len: , max_len:  | None = None\n) -> [, ] Constrain a value to have a certain length.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.len","title":"Experimental - _Pipeline  dataclass - len","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.len","rank":40},{"content":"multiple_of(\n    multiple_of: ,\n) -> [, ] multiple_of(\n    multiple_of: ,\n) -> [, ] multiple_of(multiple_of: ) -> [, ] Constrain a value to be a multiple of a certain number.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.multiple_of","title":"Experimental - _Pipeline  dataclass - multiple_of","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.multiple_of","rank":35},{"content":"eq(value: ) -> [, ] Constrain a value to be equal to a certain value.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.eq","title":"Experimental - _Pipeline  dataclass - eq","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.eq","rank":30},{"content":"not_eq(value: ) -> [, ] Constrain a value to not be equal to a certain value.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.not_eq","title":"Experimental - _Pipeline  dataclass - not_eq","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.not_eq","rank":25},{"content":"in_(values: []) -> [, ] Constrain a value to be in a certain set.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.in_","title":"Experimental - _Pipeline  dataclass - in_","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.in_","rank":20},{"content":"not_in(values: []) -> [, ] Constrain a value to not be in a certain set.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.not_in","title":"Experimental - _Pipeline  dataclass - not_in","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.not_in","rank":15},{"content":"otherwise(\n    other: [, ]\n) -> [ | ,  | ] Combine two validation chains, returning the result of the first chain if it succeeds, and the second chain if it fails.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.otherwise","title":"Experimental - _Pipeline  dataclass - otherwise","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.otherwise","rank":10},{"content":"then(\n    other: [, ]\n) -> [, ] Pipe the result of one validation chain into another.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.then","title":"Experimental - _Pipeline  dataclass - then","objectID":"/latest/api/experimental/#pydantic.experimental.pipeline._Pipeline.then","rank":5},{"content":"Experimental module exposing a function to generate a core schema that validates callable arguments. generate_arguments_schema ¶ generate_arguments_schema(\n    func: [..., ],\n    schema_type: [\n        \"arguments\", \"arguments-v3\"\n    ] = \"arguments-v3\",\n    parameters_callback: (\n        [[, , ], [\"skip\"] | None]\n        | None\n    ) = None,\n    config:  | None = None,\n) -> Generate the schema for the arguments of a function. Parameters: Name Type Description Default func [..., ] The function to generate the schema for. required schema_type ['arguments', 'arguments-v3'] The type of schema to generate. 'arguments-v3' parameters_callback [[, , ], ['skip'] | None] | None A callable that will be invoked for each parameter. The callback\nshould take three required arguments: the index, the name and the type annotation\n(or  if not annotated) of the parameter.\nThe callback can optionally return 'skip' , so that the parameter gets excluded\nfrom the resulting schema. None config | None The configuration to use. None Returns: Type Description The generated schema.","pageID":"Experimental","abs_url":"/latest/api/experimental/#arguments-schema-api","title":"Experimental - Arguments schema API","objectID":"/latest/api/experimental/#arguments-schema-api","rank":0},{"content":"generate_arguments_schema(\n    func: [..., ],\n    schema_type: [\n        \"arguments\", \"arguments-v3\"\n    ] = \"arguments-v3\",\n    parameters_callback: (\n        [[, , ], [\"skip\"] | None]\n        | None\n    ) = None,\n    config:  | None = None,\n) -> Generate the schema for the arguments of a function. Parameters: Name Type Description Default func [..., ] The function to generate the schema for. required schema_type ['arguments', 'arguments-v3'] The type of schema to generate. 'arguments-v3' parameters_callback [[, , ], ['skip'] | None] | None A callable that will be invoked for each parameter. The callback\nshould take three required arguments: the index, the name and the type annotation\n(or  if not annotated) of the parameter.\nThe callback can optionally return 'skip' , so that the parameter gets excluded\nfrom the resulting schema. None config | None The configuration to use. None Returns: Type Description The generated schema.","pageID":"Experimental","abs_url":"/latest/api/experimental/#pydantic.experimental.arguments_schema.generate_arguments_schema","title":"Experimental - generate_arguments_schema","objectID":"/latest/api/experimental/#pydantic.experimental.arguments_schema.generate_arguments_schema","rank":-5},{"content":"Defining fields on models. Field ¶ Field(\n    default: ,\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default:  | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    default: ,\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [True],\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    default: ,\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [False] = ...,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    *,\n    default_factory: (\n        [[], ] | [[[, ]], ]\n    ),\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [True],\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    *,\n    default_factory: (\n        [[], ] | [[[, ]], ]\n    ),\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [False] | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default:  | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    default:  = ,\n    *,\n    default_factory: (\n        [[], ]\n        | [[[, ]], ]\n        | None\n    ) = ,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default:  | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Usage Documentation Fields Create a field for objects that can be configured. Used to provide extra information about a field, either for the model schema or complex validation. Some arguments\napply only to number fields ( int , float , Decimal ) and some apply only to str . Parameters: Name Type Description Default default Default value if the field is not set. default_factory [[], ] | [[[, ]], ] | None A callable to generate the default value. The callable can either take 0 arguments\n(in which case it is called as is) or a single argument containing the already validated data. alias | None The name to use for the attribute when validating or serializing by alias.\nThis is often used for things like converting between snake and camel case. alias_priority | None Priority of the alias. This affects whether an alias generator is used. validation_alias |  |  | None Like alias , but only affects validation, not serialization. serialization_alias | None Like alias , but only affects serialization, not validation. title | None Human-readable title. field_title_generator [[, ], ] | None A callable that takes a field name and returns title for it. description | None Human-readable description. examples [] | None Example values for this field. exclude | None Whether to exclude the field from the model serialization. exclude_if [[], ] | None A callable that determines whether to exclude a field during serialization based on its value. discriminator |  | None Field name or Discriminator for discriminating the type in a tagged union. deprecated |  |  | None A deprecation message, an instance of warnings.deprecated or the typing_extensions.deprecated backport,\nor a boolean. If True , a default deprecation message will be emitted when accessing the field. json_schema_extra | [[], None] | None A dict or callable to provide extra JSON schema properties. frozen | None Whether the field is frozen. If true, attempts to change the value on an instance will raise an error. validate_default | None If True , apply validation to the default value every time you create an instance.\nOtherwise, for performance reasons, the default value of the field is trusted and not validated. repr A boolean indicating whether to include the field in the __repr__ output. init | None Whether the field should be included in the constructor of the dataclass.\n(Only applies to dataclasses.) init_var | None Whether the field should only be included in the constructor of the dataclass.\n(Only applies to dataclasses.) kw_only | None Whether the field should be a keyword-only argument in the constructor of the dataclass.\n(Only applies to dataclasses.) coerce_numbers_to_str | None Whether to enable coercion of any Number type to str (not applicable in strict mode). strict | None If True , strict validation is applied to the field.\nSee Strict Mode for details. gt | None Greater than. If set, value must be greater than this. Only applicable to numbers. ge | None Greater than or equal. If set, value must be greater than or equal to this. Only applicable to numbers. lt | None Less than. If set, value must be less than this. Only applicable to numbers. le | None Less than or equal. If set, value must be less than or equal to this. Only applicable to numbers. multiple_of | None Value must be a multiple of this. Only applicable to numbers. min_length | None Minimum length for iterables. max_length | None Maximum length for iterables. pattern | [] | None Pattern for strings (a regular expression). allow_inf_nan | None Allow inf , -inf , nan . Only applicable to float and  numbers. max_digits | None Maximum number of allow digits for strings. decimal_places | None Maximum number of decimal places allowed for numbers. union_mode ['smart', 'left_to_right'] The strategy to apply when validating a union. Can be smart (the default), or left_to_right .\nSee Union Mode for details. fail_fast | None If True , validation will stop on the first error. If False , all validation errors will be collected.\nThis option can be applied only to iterable types (list, tuple, set, and frozenset). extra [] (Deprecated) Extra fields that will be included in the JSON schema. Warning The extra kwargs is deprecated. Use json_schema_extra instead. {} Returns: Type Description A new . The return annotation is Any so Field can be used on\ntype-annotated fields without causing a type error. FieldInfo ¶ FieldInfo(**kwargs: []) Bases: This class holds information about a field. FieldInfo is used for any field definition regardless of whether the \nfunction is explicitly used. Warning You generally shouldn't be creating FieldInfo directly, you'll only need to use it when accessing .model_fields internals. Attributes: Name Type Description [] | None The type annotation of the field. The default value of the field. [[], ] | [[[, ]], ] | None A callable to generate the default value. The callable can either take 0 arguments\n(in which case it is called as is) or a single argument containing the already validated data. | None The alias name of the field. | None The priority of the field's alias. |  |  | None The validation alias of the field. | None The serialization alias of the field. | None The title of the field. [[, ], ] | None A callable that takes a field name and returns title for it. | None The description of the field. [] | None List of examples of the field. | None Whether to exclude the field from the model serialization. [[], ] | None A callable that determines whether to exclude a field during serialization based on its value. |  | None Field name or Discriminator for discriminating the type in a tagged union. |  |  | None A deprecation message, an instance of warnings.deprecated or the typing_extensions.deprecated backport,\nor a boolean. If True , a default deprecation message will be emitted when accessing the field. | [[], None] | None A dict or callable to provide extra JSON schema properties. | None Whether the field is frozen. | None Whether to validate the default value of the field. Whether to include the field in representation of the model. | None Whether the field should be included in the constructor of the dataclass. | None Whether the field should only be included in the constructor of the dataclass, and not stored. | None Whether the field should be a keyword-only argument in the constructor of the dataclass. [] List of metadata constraints. See the signature of pydantic.fields.Field for more details about the expected arguments. from_field staticmethod ¶ from_field(\n    default:  = ,\n    **kwargs: []\n) -> Create a new FieldInfo object with the Field function. Parameters: Name Type Description Default default The default value for the field. Defaults to Undefined. **kwargs [] Additional arguments dictionary. {} Raises: Type Description If 'annotation' is passed as a keyword argument. Returns: Type Description A new FieldInfo object with the given parameters. from_annotation staticmethod ¶ from_annotation(\n    annotation: [],\n    *,\n    _source:  = \n) -> Creates a FieldInfo instance from a bare annotation. This function is used internally to create a FieldInfo from a bare annotation like this: import pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: int  # <-- like this We also account for the case where the annotation can be an instance of Annotated and where\none of the (not first) arguments in Annotated is an instance of FieldInfo , e.g.: from typing import Annotated\n\nimport annotated_types\n\nimport pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: Annotated[int, annotated_types.Gt(42)]\n    bar: Annotated[int, pydantic.Field(gt=42)] Parameters: Name Type Description Default annotation [] An annotation object. required Returns: Type Description An instance of the field metadata. from_annotated_attribute staticmethod ¶ from_annotated_attribute(\n    annotation: [],\n    default: ,\n    *,\n    _source:  = \n) -> Create FieldInfo from an annotation with a default value. This is used in cases like the following: from typing import Annotated\n\nimport annotated_types\n\nimport pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: int = 4  # <-- like this\n    bar: Annotated[int, annotated_types.Gt(4)] = 4  # <-- or this\n    spam: Annotated[int, pydantic.Field(gt=4)] = 4  # <-- or this Parameters: Name Type Description Default annotation [] The type annotation of the field. required default The default value of the field. required Returns: Type Description A field object with the passed values. merge_field_infos staticmethod ¶ merge_field_infos(\n    *field_infos: , **overrides: \n) -> Merge FieldInfo instances keeping only explicitly set attributes. Later FieldInfo instances override earlier ones. Returns: Name Type Description FieldInfo A merged FieldInfo instance. deprecation_message property ¶ deprecation_message:  | None The deprecation message to be emitted, or None if not set. default_factory_takes_validated_data property ¶ default_factory_takes_validated_data:  | None Whether the provided default factory callable has a validated data parameter. Returns None if no default factory is set. get_default ¶ get_default(\n    *,\n    call_default_factory: [True],\n    validated_data: [, ] | None = None\n) -> get_default(\n    *, call_default_factory: [False] = ...\n) -> get_default(\n    *,\n    call_default_factory:  = False,\n    validated_data: [, ] | None = None\n) -> Get the default value. We expose an option for whether to call the default_factory (if present), as calling it may\nresult in side effects that we want to avoid. However, there are times when it really should\nbe called (namely, when instantiating a model via model_construct ). Parameters: Name Type Description Default call_default_factory Whether to call the default factory or not. False validated_data [, ] | None The already validated data to be passed to the default factory. None Returns: Type Description The default value, calling the default factory if requested or None if not set. is_required ¶ is_required() -> Check if the field is required (i.e., does not have a default value or factory). Returns: Type Description True if the field is required, False otherwise. rebuild_annotation ¶ rebuild_annotation() -> Attempts to rebuild the original annotation for use in function signatures. If metadata is present, it adds it to the original annotation using Annotated . Otherwise, it returns the original annotation as-is. Note that because the metadata has been flattened, the original annotation\nmay not be reconstructed exactly as originally provided, e.g. if the original\ntype had unrecognized annotations, or was annotated with a call to pydantic.Field . Returns: Type Description The rebuilt annotation. apply_typevars_map ¶ apply_typevars_map(\n    typevars_map: [, ] | None,\n    globalns:  | None = None,\n    localns:  | None = None,\n) -> None Apply a typevars_map to the annotation. This method is used when analyzing parametrized generic types to replace typevars with their concrete types. This method applies the typevars_map to the annotation in place. Parameters: Name Type Description Default typevars_map [, ] | None A dictionary mapping type variables to their concrete types. required globalns | None The globals namespace to use during type annotation evaluation. None localns | None The locals namespace to use during type annotation evaluation. None PrivateAttr ¶ PrivateAttr(\n    default: , *, init: [False] = False\n) -> PrivateAttr(\n    *,\n    default_factory: [[], ],\n    init: [False] = False\n) -> PrivateAttr(*, init: [False] = False) -> PrivateAttr(\n    default:  = ,\n    *,\n    default_factory: [[], ] | None = None,\n    init: [False] = False\n) -> Usage Documentation Private Model Attributes Indicates that an attribute is intended for private use and not handled during normal validation/serialization. Private attributes are not validated by Pydantic, so it's up to you to ensure they are used in a type-safe manner. Private attributes are stored in __private_attributes__ on the model. Parameters: Name Type Description Default default The attribute's default value. Defaults to Undefined. default_factory [[], ] | None Callable that will be\ncalled when a default value is needed for this attribute.\nIf both default and default_factory are set, an error will be raised. None init [False] Whether the attribute should be included in the constructor of the dataclass. Always False . False Returns: Type Description An instance of  class. Raises: Type Description If both default and default_factory are set. ModelPrivateAttr ¶ ModelPrivateAttr(\n    default:  = ,\n    *,\n    default_factory: [[], ] | None = None\n) Bases: A descriptor for private attributes in class models. Warning You generally shouldn't be creating ModelPrivateAttr instances directly, instead use pydantic.fields.PrivateAttr . (This is similar to FieldInfo vs. Field .) Attributes: Name Type Description The default value of the attribute if not provided. A callable function that generates the default value of the\nattribute if not provided. get_default ¶ get_default() -> Retrieve the default value of the object. If self.default_factory is None , the method will return a deep copy of the self.default object. If self.default_factory is not None , it will call self.default_factory and return the value returned. Returns: Type Description The default value of the object. computed_field ¶ computed_field(func: ) -> computed_field(\n    *,\n    alias:  | None = None,\n    alias_priority:  | None = None,\n    title:  | None = None,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = None,\n    description:  | None = None,\n    deprecated:  |  |  | None = None,\n    examples: [] | None = None,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = None,\n    repr:  = True,\n    return_type:  = \n) -> [[], ] computed_field(\n    func:  | None = None,\n    /,\n    *,\n    alias:  | None = None,\n    alias_priority:  | None = None,\n    title:  | None = None,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = None,\n    description:  | None = None,\n    deprecated:  |  |  | None = None,\n    examples: [] | None = None,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = None,\n    repr:  | None = None,\n    return_type:  = ,\n) ->  | [[], ] Usage Documentation The computed_field decorator Decorator to include property and cached_property when serializing models or dataclasses. This is useful for fields that are computed from other fields, or for fields that are expensive to compute and should be cached. from pydantic import BaseModel, computed_field\n\nclass Rectangle(BaseModel):\n    width: int\n    length: int\n\n    @computed_field\n    @property\n    def area(self) -> int:\n        return self.width * self.length\n\nprint(Rectangle(width=3, length=2).model_dump())\n#> {'width': 3, 'length': 2, 'area': 6} If applied to functions not yet decorated with @property or @cached_property , the function is\nautomatically wrapped with property . Although this is more concise, you will lose IntelliSense in your IDE,\nand confuse static type checkers, thus explicit use of @property is recommended. Mypy Warning Even with the @property or @cached_property applied to your function before @computed_field ,\nmypy may throw a Decorated property not supported error.\nSee mypy issue #1362 , for more information.\nTo avoid this error message, add # type: ignore[prop-decorator] to the @computed_field line. pyright supports @computed_field without error. import random\n\nfrom pydantic import BaseModel, computed_field\n\nclass Square(BaseModel):\n    width: float\n\n    @computed_field\n    def area(self) -> float:  # converted to a `property` by `computed_field`\n        return round(self.width**2, 2)\n\n    @area.setter\n    def area(self, new_area: float) -> None:\n        self.width = new_area**0.5\n\n    @computed_field(alias='the magic number', repr=False)\n    def random_number(self) -> int:\n        return random.randint(0, 1_000)\n\nsquare = Square(width=1.3)\n\n# `random_number` does not appear in representation\nprint(repr(square))\n#> Square(width=1.3, area=1.69)\n\nprint(square.random_number)\n#> 3\n\nsquare.area = 4\n\nprint(square.model_dump_json(by_alias=True))\n#> {\"width\":2.0,\"area\":4.0,\"the magic number\":3} Overriding with computed_field You can't override a field from a parent class with a computed_field in the child class. mypy complains about this behavior if allowed, and dataclasses doesn't allow this pattern either.\nSee the example below: from pydantic import BaseModel, computed_field\n\nclass Parent(BaseModel):\n    a: str\n\ntry:\n\n    class Child(Parent):\n        @computed_field\n        @property\n        def a(self) -> str:\n            return 'new a'\n\nexcept TypeError as e:\n    print(e)\n    '''\n    Field 'a' of class 'Child' overrides symbol of same name in a parent class. This override with a computed_field is incompatible.\n    ''' Private properties decorated with @computed_field have repr=False by default. from functools import cached_property\n\nfrom pydantic import BaseModel, computed_field\n\nclass Model(BaseModel):\n    foo: int\n\n    @computed_field\n    @cached_property\n    def _private_cached_property(self) -> int:\n        return -self.foo\n\n    @computed_field\n    @property\n    def _private_property(self) -> int:\n        return -self.foo\n\nm = Model(foo=1)\nprint(repr(m))\n#> Model(foo=1) Parameters: Name Type Description Default func | None the function to wrap. None alias | None alias to use when serializing this computed field, only used when by_alias=True None alias_priority | None priority of the alias. This affects whether an alias generator is used None title | None Title to use when including this computed field in JSON Schema None field_title_generator [[, ], ] | None A callable that takes a field name and returns title for it. None description | None Description to use when including this computed field in JSON Schema, defaults to the function's\ndocstring None deprecated |  |  | None A deprecation message (or an instance of warnings.deprecated or the typing_extensions.deprecated backport).\nto be emitted when accessing the field. Or a boolean. This will automatically be set if the property is decorated with the deprecated decorator. None examples [] | None Example values to use when including this computed field in JSON Schema None json_schema_extra | [[], None] | None A dict or callable to provide extra JSON schema properties. None repr | None whether to include this computed field in model repr.\nDefault is False for private properties and True for public properties. None return_type optional return for serialization logic to expect when serializing to JSON, if included\nthis must be correct, otherwise a TypeError is raised.\nIf you don't include a return type Any is used, which does runtime introspection to handle arbitrary\nobjects. Returns: Type Description | [[], ] A proxy wrapper for the property. ComputedFieldInfo dataclass ¶ ComputedFieldInfo(\n    wrapped_property: ,\n    return_type: ,\n    alias:  | None,\n    alias_priority:  | None,\n    title:  | None,\n    field_title_generator: (\n        [[, ], ] | None\n    ),\n    description:  | None,\n    deprecated:  |  |  | None,\n    examples: [] | None,\n    json_schema_extra: (\n         | [[], None] | None\n    ),\n    repr: ,\n) A container for data from @computed_field so that we can access it while building the pydantic-core schema. Attributes: Name Type Description A class variable representing the decorator string, '@computed_field'. The wrapped computed field property. The type of the computed field property's return value. | None The alias of the property to be used during serialization. | None The priority of the alias. This affects whether an alias generator is used. | None Title of the computed field to include in the serialization JSON schema. [[, ], ] | None A callable that takes a field name and returns title for it. | None Description of the computed field to include in the serialization JSON schema. |  |  | None A deprecation message, an instance of warnings.deprecated or the typing_extensions.deprecated backport,\nor a boolean. If True , a default deprecation message will be emitted when accessing the field. [] | None Example values of the computed field to include in the serialization JSON schema. | [[], None] | None A dict or callable to provide extra JSON schema properties. A boolean indicating whether to include the field in the repr output. deprecation_message property ¶ deprecation_message:  | None The deprecation message to be emitted, or None if not set.","pageID":"Fields","abs_url":"/latest/api/fields/#Fields","title":"Fields","objectID":"/latest/api/fields/#Fields","rank":100},{"content":"Field(\n    default: ,\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default:  | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    default: ,\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [True],\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    default: ,\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [False] = ...,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    *,\n    default_factory: (\n        [[], ] | [[[, ]], ]\n    ),\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [True],\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    *,\n    default_factory: (\n        [[], ] | [[[, ]], ]\n    ),\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default: [False] | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    *,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default:  | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Field(\n    default:  = ,\n    *,\n    default_factory: (\n        [[], ]\n        | [[[, ]], ]\n        | None\n    ) = ,\n    alias:  | None = ,\n    alias_priority:  | None = ,\n    validation_alias: (\n         |  |  | None\n    ) = ,\n    serialization_alias:  | None = ,\n    title:  | None = ,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = ,\n    description:  | None = ,\n    examples: [] | None = ,\n    exclude:  | None = ,\n    exclude_if: [[], ] | None = ,\n    discriminator:  |  | None = ,\n    deprecated:  |  |  | None = ,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = ,\n    frozen:  | None = ,\n    validate_default:  | None = ,\n    repr:  = ,\n    init:  | None = ,\n    init_var:  | None = ,\n    kw_only:  | None = ,\n    pattern:  | [] | None = ,\n    strict:  | None = ,\n    coerce_numbers_to_str:  | None = ,\n    gt:  | None = ,\n    ge:  | None = ,\n    lt:  | None = ,\n    le:  | None = ,\n    multiple_of:  | None = ,\n    allow_inf_nan:  | None = ,\n    max_digits:  | None = ,\n    decimal_places:  | None = ,\n    min_length:  | None = ,\n    max_length:  | None = ,\n    union_mode: [\"smart\", \"left_to_right\"] = ,\n    fail_fast:  | None = ,\n    **extra: []\n) -> Usage Documentation Fields Create a field for objects that can be configured. Used to provide extra information about a field, either for the model schema or complex validation. Some arguments\napply only to number fields ( int , float , Decimal ) and some apply only to str . Parameters: Name Type Description Default default Default value if the field is not set. default_factory [[], ] | [[[, ]], ] | None A callable to generate the default value. The callable can either take 0 arguments\n(in which case it is called as is) or a single argument containing the already validated data. alias | None The name to use for the attribute when validating or serializing by alias.\nThis is often used for things like converting between snake and camel case. alias_priority | None Priority of the alias. This affects whether an alias generator is used. validation_alias |  |  | None Like alias , but only affects validation, not serialization. serialization_alias | None Like alias , but only affects serialization, not validation. title | None Human-readable title. field_title_generator [[, ], ] | None A callable that takes a field name and returns title for it. description | None Human-readable description. examples [] | None Example values for this field. exclude | None Whether to exclude the field from the model serialization. exclude_if [[], ] | None A callable that determines whether to exclude a field during serialization based on its value. discriminator |  | None Field name or Discriminator for discriminating the type in a tagged union. deprecated |  |  | None A deprecation message, an instance of warnings.deprecated or the typing_extensions.deprecated backport,\nor a boolean. If True , a default deprecation message will be emitted when accessing the field. json_schema_extra | [[], None] | None A dict or callable to provide extra JSON schema properties. frozen | None Whether the field is frozen. If true, attempts to change the value on an instance will raise an error. validate_default | None If True , apply validation to the default value every time you create an instance.\nOtherwise, for performance reasons, the default value of the field is trusted and not validated. repr A boolean indicating whether to include the field in the __repr__ output. init | None Whether the field should be included in the constructor of the dataclass.\n(Only applies to dataclasses.) init_var | None Whether the field should only be included in the constructor of the dataclass.\n(Only applies to dataclasses.) kw_only | None Whether the field should be a keyword-only argument in the constructor of the dataclass.\n(Only applies to dataclasses.) coerce_numbers_to_str | None Whether to enable coercion of any Number type to str (not applicable in strict mode). strict | None If True , strict validation is applied to the field.\nSee Strict Mode for details. gt | None Greater than. If set, value must be greater than this. Only applicable to numbers. ge | None Greater than or equal. If set, value must be greater than or equal to this. Only applicable to numbers. lt | None Less than. If set, value must be less than this. Only applicable to numbers. le | None Less than or equal. If set, value must be less than or equal to this. Only applicable to numbers. multiple_of | None Value must be a multiple of this. Only applicable to numbers. min_length | None Minimum length for iterables. max_length | None Maximum length for iterables. pattern | [] | None Pattern for strings (a regular expression). allow_inf_nan | None Allow inf , -inf , nan . Only applicable to float and  numbers. max_digits | None Maximum number of allow digits for strings. decimal_places | None Maximum number of decimal places allowed for numbers. union_mode ['smart', 'left_to_right'] The strategy to apply when validating a union. Can be smart (the default), or left_to_right .\nSee Union Mode for details. fail_fast | None If True , validation will stop on the first error. If False , all validation errors will be collected.\nThis option can be applied only to iterable types (list, tuple, set, and frozenset). extra [] (Deprecated) Extra fields that will be included in the JSON schema. Warning The extra kwargs is deprecated. Use json_schema_extra instead. {} Returns: Type Description A new . The return annotation is Any so Field can be used on\ntype-annotated fields without causing a type error.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.Field","title":"Fields - Field","objectID":"/latest/api/fields/#pydantic.fields.Field","rank":95},{"content":"FieldInfo(**kwargs: []) Bases: This class holds information about a field. FieldInfo is used for any field definition regardless of whether the \nfunction is explicitly used. Warning You generally shouldn't be creating FieldInfo directly, you'll only need to use it when accessing .model_fields internals. Attributes: Name Type Description [] | None The type annotation of the field. The default value of the field. [[], ] | [[[, ]], ] | None A callable to generate the default value. The callable can either take 0 arguments\n(in which case it is called as is) or a single argument containing the already validated data. | None The alias name of the field. | None The priority of the field's alias. |  |  | None The validation alias of the field. | None The serialization alias of the field. | None The title of the field. [[, ], ] | None A callable that takes a field name and returns title for it. | None The description of the field. [] | None List of examples of the field. | None Whether to exclude the field from the model serialization. [[], ] | None A callable that determines whether to exclude a field during serialization based on its value. |  | None Field name or Discriminator for discriminating the type in a tagged union. |  |  | None A deprecation message, an instance of warnings.deprecated or the typing_extensions.deprecated backport,\nor a boolean. If True , a default deprecation message will be emitted when accessing the field. | [[], None] | None A dict or callable to provide extra JSON schema properties. | None Whether the field is frozen. | None Whether to validate the default value of the field. Whether to include the field in representation of the model. | None Whether the field should be included in the constructor of the dataclass. | None Whether the field should only be included in the constructor of the dataclass, and not stored. | None Whether the field should be a keyword-only argument in the constructor of the dataclass. [] List of metadata constraints. See the signature of pydantic.fields.Field for more details about the expected arguments. from_field staticmethod ¶ from_field(\n    default:  = ,\n    **kwargs: []\n) -> Create a new FieldInfo object with the Field function. Parameters: Name Type Description Default default The default value for the field. Defaults to Undefined. **kwargs [] Additional arguments dictionary. {} Raises: Type Description If 'annotation' is passed as a keyword argument. Returns: Type Description A new FieldInfo object with the given parameters. from_annotation staticmethod ¶ from_annotation(\n    annotation: [],\n    *,\n    _source:  = \n) -> Creates a FieldInfo instance from a bare annotation. This function is used internally to create a FieldInfo from a bare annotation like this: import pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: int  # <-- like this We also account for the case where the annotation can be an instance of Annotated and where\none of the (not first) arguments in Annotated is an instance of FieldInfo , e.g.: from typing import Annotated\n\nimport annotated_types\n\nimport pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: Annotated[int, annotated_types.Gt(42)]\n    bar: Annotated[int, pydantic.Field(gt=42)] Parameters: Name Type Description Default annotation [] An annotation object. required Returns: Type Description An instance of the field metadata. from_annotated_attribute staticmethod ¶ from_annotated_attribute(\n    annotation: [],\n    default: ,\n    *,\n    _source:  = \n) -> Create FieldInfo from an annotation with a default value. This is used in cases like the following: from typing import Annotated\n\nimport annotated_types\n\nimport pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: int = 4  # <-- like this\n    bar: Annotated[int, annotated_types.Gt(4)] = 4  # <-- or this\n    spam: Annotated[int, pydantic.Field(gt=4)] = 4  # <-- or this Parameters: Name Type Description Default annotation [] The type annotation of the field. required default The default value of the field. required Returns: Type Description A field object with the passed values. merge_field_infos staticmethod ¶ merge_field_infos(\n    *field_infos: , **overrides: \n) -> Merge FieldInfo instances keeping only explicitly set attributes. Later FieldInfo instances override earlier ones. Returns: Name Type Description FieldInfo A merged FieldInfo instance. deprecation_message property ¶ deprecation_message:  | None The deprecation message to be emitted, or None if not set. default_factory_takes_validated_data property ¶ default_factory_takes_validated_data:  | None Whether the provided default factory callable has a validated data parameter. Returns None if no default factory is set. get_default ¶ get_default(\n    *,\n    call_default_factory: [True],\n    validated_data: [, ] | None = None\n) -> get_default(\n    *, call_default_factory: [False] = ...\n) -> get_default(\n    *,\n    call_default_factory:  = False,\n    validated_data: [, ] | None = None\n) -> Get the default value. We expose an option for whether to call the default_factory (if present), as calling it may\nresult in side effects that we want to avoid. However, there are times when it really should\nbe called (namely, when instantiating a model via model_construct ). Parameters: Name Type Description Default call_default_factory Whether to call the default factory or not. False validated_data [, ] | None The already validated data to be passed to the default factory. None Returns: Type Description The default value, calling the default factory if requested or None if not set. is_required ¶ is_required() -> Check if the field is required (i.e., does not have a default value or factory). Returns: Type Description True if the field is required, False otherwise. rebuild_annotation ¶ rebuild_annotation() -> Attempts to rebuild the original annotation for use in function signatures. If metadata is present, it adds it to the original annotation using Annotated . Otherwise, it returns the original annotation as-is. Note that because the metadata has been flattened, the original annotation\nmay not be reconstructed exactly as originally provided, e.g. if the original\ntype had unrecognized annotations, or was annotated with a call to pydantic.Field . Returns: Type Description The rebuilt annotation. apply_typevars_map ¶ apply_typevars_map(\n    typevars_map: [, ] | None,\n    globalns:  | None = None,\n    localns:  | None = None,\n) -> None Apply a typevars_map to the annotation. This method is used when analyzing parametrized generic types to replace typevars with their concrete types. This method applies the typevars_map to the annotation in place. Parameters: Name Type Description Default typevars_map [, ] | None A dictionary mapping type variables to their concrete types. required globalns | None The globals namespace to use during type annotation evaluation. None localns | None The locals namespace to use during type annotation evaluation. None","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo","title":"Fields - FieldInfo","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo","rank":90},{"content":"from_field(\n    default:  = ,\n    **kwargs: []\n) -> Create a new FieldInfo object with the Field function. Parameters: Name Type Description Default default The default value for the field. Defaults to Undefined. **kwargs [] Additional arguments dictionary. {} Raises: Type Description If 'annotation' is passed as a keyword argument. Returns: Type Description A new FieldInfo object with the given parameters.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.from_field","title":"Fields - FieldInfo - from_field  staticmethod","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.from_field","rank":85},{"content":"from_annotation(\n    annotation: [],\n    *,\n    _source:  = \n) -> Creates a FieldInfo instance from a bare annotation. This function is used internally to create a FieldInfo from a bare annotation like this: import pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: int  # <-- like this We also account for the case where the annotation can be an instance of Annotated and where\none of the (not first) arguments in Annotated is an instance of FieldInfo , e.g.: from typing import Annotated\n\nimport annotated_types\n\nimport pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: Annotated[int, annotated_types.Gt(42)]\n    bar: Annotated[int, pydantic.Field(gt=42)] Parameters: Name Type Description Default annotation [] An annotation object. required Returns: Type Description An instance of the field metadata.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.from_annotation","title":"Fields - FieldInfo - from_annotation  staticmethod","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.from_annotation","rank":80},{"content":"from_annotated_attribute(\n    annotation: [],\n    default: ,\n    *,\n    _source:  = \n) -> Create FieldInfo from an annotation with a default value. This is used in cases like the following: from typing import Annotated\n\nimport annotated_types\n\nimport pydantic\n\nclass MyModel(pydantic.BaseModel):\n    foo: int = 4  # <-- like this\n    bar: Annotated[int, annotated_types.Gt(4)] = 4  # <-- or this\n    spam: Annotated[int, pydantic.Field(gt=4)] = 4  # <-- or this Parameters: Name Type Description Default annotation [] The type annotation of the field. required default The default value of the field. required Returns: Type Description A field object with the passed values.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.from_annotated_attribute","title":"Fields - FieldInfo - from_annotated_attribute  staticmethod","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.from_annotated_attribute","rank":75},{"content":"merge_field_infos(\n    *field_infos: , **overrides: \n) -> Merge FieldInfo instances keeping only explicitly set attributes. Later FieldInfo instances override earlier ones. Returns: Name Type Description FieldInfo A merged FieldInfo instance.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.merge_field_infos","title":"Fields - FieldInfo - merge_field_infos  staticmethod","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.merge_field_infos","rank":70},{"content":"deprecation_message:  | None The deprecation message to be emitted, or None if not set.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.deprecation_message","title":"Fields - FieldInfo - deprecation_message  property","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.deprecation_message","rank":65},{"content":"default_factory_takes_validated_data:  | None Whether the provided default factory callable has a validated data parameter. Returns None if no default factory is set.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.default_factory_takes_validated_data","title":"Fields - FieldInfo - default_factory_takes_validated_data  property","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.default_factory_takes_validated_data","rank":60},{"content":"get_default(\n    *,\n    call_default_factory: [True],\n    validated_data: [, ] | None = None\n) -> get_default(\n    *, call_default_factory: [False] = ...\n) -> get_default(\n    *,\n    call_default_factory:  = False,\n    validated_data: [, ] | None = None\n) -> Get the default value. We expose an option for whether to call the default_factory (if present), as calling it may\nresult in side effects that we want to avoid. However, there are times when it really should\nbe called (namely, when instantiating a model via model_construct ). Parameters: Name Type Description Default call_default_factory Whether to call the default factory or not. False validated_data [, ] | None The already validated data to be passed to the default factory. None Returns: Type Description The default value, calling the default factory if requested or None if not set.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.get_default","title":"Fields - FieldInfo - get_default","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.get_default","rank":55},{"content":"is_required() -> Check if the field is required (i.e., does not have a default value or factory). Returns: Type Description True if the field is required, False otherwise.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.is_required","title":"Fields - FieldInfo - is_required","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.is_required","rank":50},{"content":"rebuild_annotation() -> Attempts to rebuild the original annotation for use in function signatures. If metadata is present, it adds it to the original annotation using Annotated . Otherwise, it returns the original annotation as-is. Note that because the metadata has been flattened, the original annotation\nmay not be reconstructed exactly as originally provided, e.g. if the original\ntype had unrecognized annotations, or was annotated with a call to pydantic.Field . Returns: Type Description The rebuilt annotation.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.rebuild_annotation","title":"Fields - FieldInfo - rebuild_annotation","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.rebuild_annotation","rank":45},{"content":"apply_typevars_map(\n    typevars_map: [, ] | None,\n    globalns:  | None = None,\n    localns:  | None = None,\n) -> None Apply a typevars_map to the annotation. This method is used when analyzing parametrized generic types to replace typevars with their concrete types. This method applies the typevars_map to the annotation in place. Parameters: Name Type Description Default typevars_map [, ] | None A dictionary mapping type variables to their concrete types. required globalns | None The globals namespace to use during type annotation evaluation. None localns | None The locals namespace to use during type annotation evaluation. None","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.FieldInfo.apply_typevars_map","title":"Fields - FieldInfo - apply_typevars_map","objectID":"/latest/api/fields/#pydantic.fields.FieldInfo.apply_typevars_map","rank":40},{"content":"PrivateAttr(\n    default: , *, init: [False] = False\n) -> PrivateAttr(\n    *,\n    default_factory: [[], ],\n    init: [False] = False\n) -> PrivateAttr(*, init: [False] = False) -> PrivateAttr(\n    default:  = ,\n    *,\n    default_factory: [[], ] | None = None,\n    init: [False] = False\n) -> Usage Documentation Private Model Attributes Indicates that an attribute is intended for private use and not handled during normal validation/serialization. Private attributes are not validated by Pydantic, so it's up to you to ensure they are used in a type-safe manner. Private attributes are stored in __private_attributes__ on the model. Parameters: Name Type Description Default default The attribute's default value. Defaults to Undefined. default_factory [[], ] | None Callable that will be\ncalled when a default value is needed for this attribute.\nIf both default and default_factory are set, an error will be raised. None init [False] Whether the attribute should be included in the constructor of the dataclass. Always False . False Returns: Type Description An instance of  class. Raises: Type Description If both default and default_factory are set.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.PrivateAttr","title":"Fields - PrivateAttr","objectID":"/latest/api/fields/#pydantic.fields.PrivateAttr","rank":35},{"content":"ModelPrivateAttr(\n    default:  = ,\n    *,\n    default_factory: [[], ] | None = None\n) Bases: A descriptor for private attributes in class models. Warning You generally shouldn't be creating ModelPrivateAttr instances directly, instead use pydantic.fields.PrivateAttr . (This is similar to FieldInfo vs. Field .) Attributes: Name Type Description The default value of the attribute if not provided. A callable function that generates the default value of the\nattribute if not provided. get_default ¶ get_default() -> Retrieve the default value of the object. If self.default_factory is None , the method will return a deep copy of the self.default object. If self.default_factory is not None , it will call self.default_factory and return the value returned. Returns: Type Description The default value of the object.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.ModelPrivateAttr","title":"Fields - ModelPrivateAttr","objectID":"/latest/api/fields/#pydantic.fields.ModelPrivateAttr","rank":30},{"content":"get_default() -> Retrieve the default value of the object. If self.default_factory is None , the method will return a deep copy of the self.default object. If self.default_factory is not None , it will call self.default_factory and return the value returned. Returns: Type Description The default value of the object.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.ModelPrivateAttr.get_default","title":"Fields - ModelPrivateAttr - get_default","objectID":"/latest/api/fields/#pydantic.fields.ModelPrivateAttr.get_default","rank":25},{"content":"computed_field(func: ) -> computed_field(\n    *,\n    alias:  | None = None,\n    alias_priority:  | None = None,\n    title:  | None = None,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = None,\n    description:  | None = None,\n    deprecated:  |  |  | None = None,\n    examples: [] | None = None,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = None,\n    repr:  = True,\n    return_type:  = \n) -> [[], ] computed_field(\n    func:  | None = None,\n    /,\n    *,\n    alias:  | None = None,\n    alias_priority:  | None = None,\n    title:  | None = None,\n    field_title_generator: (\n        [[, ], ] | None\n    ) = None,\n    description:  | None = None,\n    deprecated:  |  |  | None = None,\n    examples: [] | None = None,\n    json_schema_extra: (\n         | [[], None] | None\n    ) = None,\n    repr:  | None = None,\n    return_type:  = ,\n) ->  | [[], ] Usage Documentation The computed_field decorator Decorator to include property and cached_property when serializing models or dataclasses. This is useful for fields that are computed from other fields, or for fields that are expensive to compute and should be cached. from pydantic import BaseModel, computed_field\n\nclass Rectangle(BaseModel):\n    width: int\n    length: int\n\n    @computed_field\n    @property\n    def area(self) -> int:\n        return self.width * self.length\n\nprint(Rectangle(width=3, length=2).model_dump())\n#> {'width': 3, 'length': 2, 'area': 6} If applied to functions not yet decorated with @property or @cached_property , the function is\nautomatically wrapped with property . Although this is more concise, you will lose IntelliSense in your IDE,\nand confuse static type checkers, thus explicit use of @property is recommended. Mypy Warning Even with the @property or @cached_property applied to your function before @computed_field ,\nmypy may throw a Decorated property not supported error.\nSee mypy issue #1362 , for more information.\nTo avoid this error message, add # type: ignore[prop-decorator] to the @computed_field line. pyright supports @computed_field without error. import random\n\nfrom pydantic import BaseModel, computed_field\n\nclass Square(BaseModel):\n    width: float\n\n    @computed_field\n    def area(self) -> float:  # converted to a `property` by `computed_field`\n        return round(self.width**2, 2)\n\n    @area.setter\n    def area(self, new_area: float) -> None:\n        self.width = new_area**0.5\n\n    @computed_field(alias='the magic number', repr=False)\n    def random_number(self) -> int:\n        return random.randint(0, 1_000)\n\nsquare = Square(width=1.3)\n\n# `random_number` does not appear in representation\nprint(repr(square))\n#> Square(width=1.3, area=1.69)\n\nprint(square.random_number)\n#> 3\n\nsquare.area = 4\n\nprint(square.model_dump_json(by_alias=True))\n#> {\"width\":2.0,\"area\":4.0,\"the magic number\":3} Overriding with computed_field You can't override a field from a parent class with a computed_field in the child class. mypy complains about this behavior if allowed, and dataclasses doesn't allow this pattern either.\nSee the example below: from pydantic import BaseModel, computed_field\n\nclass Parent(BaseModel):\n    a: str\n\ntry:\n\n    class Child(Parent):\n        @computed_field\n        @property\n        def a(self) -> str:\n            return 'new a'\n\nexcept TypeError as e:\n    print(e)\n    '''\n    Field 'a' of class 'Child' overrides symbol of same name in a parent class. This override with a computed_field is incompatible.\n    ''' Private properties decorated with @computed_field have repr=False by default. from functools import cached_property\n\nfrom pydantic import BaseModel, computed_field\n\nclass Model(BaseModel):\n    foo: int\n\n    @computed_field\n    @cached_property\n    def _private_cached_property(self) -> int:\n        return -self.foo\n\n    @computed_field\n    @property\n    def _private_property(self) -> int:\n        return -self.foo\n\nm = Model(foo=1)\nprint(repr(m))\n#> Model(foo=1) Parameters: Name Type Description Default func | None the function to wrap. None alias | None alias to use when serializing this computed field, only used when by_alias=True None alias_priority | None priority of the alias. This affects whether an alias generator is used None title | None Title to use when including this computed field in JSON Schema None field_title_generator [[, ], ] | None A callable that takes a field name and returns title for it. None description | None Description to use when including this computed field in JSON Schema, defaults to the function's\ndocstring None deprecated |  |  | None A deprecation message (or an instance of warnings.deprecated or the typing_extensions.deprecated backport).\nto be emitted when accessing the field. Or a boolean. This will automatically be set if the property is decorated with the deprecated decorator. None examples [] | None Example values to use when including this computed field in JSON Schema None json_schema_extra | [[], None] | None A dict or callable to provide extra JSON schema properties. None repr | None whether to include this computed field in model repr.\nDefault is False for private properties and True for public properties. None return_type optional return for serialization logic to expect when serializing to JSON, if included\nthis must be correct, otherwise a TypeError is raised.\nIf you don't include a return type Any is used, which does runtime introspection to handle arbitrary\nobjects. Returns: Type Description | [[], ] A proxy wrapper for the property.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.computed_field","title":"Fields - computed_field","objectID":"/latest/api/fields/#pydantic.fields.computed_field","rank":20},{"content":"ComputedFieldInfo(\n    wrapped_property: ,\n    return_type: ,\n    alias:  | None,\n    alias_priority:  | None,\n    title:  | None,\n    field_title_generator: (\n        [[, ], ] | None\n    ),\n    description:  | None,\n    deprecated:  |  |  | None,\n    examples: [] | None,\n    json_schema_extra: (\n         | [[], None] | None\n    ),\n    repr: ,\n) A container for data from @computed_field so that we can access it while building the pydantic-core schema. Attributes: Name Type Description A class variable representing the decorator string, '@computed_field'. The wrapped computed field property. The type of the computed field property's return value. | None The alias of the property to be used during serialization. | None The priority of the alias. This affects whether an alias generator is used. | None Title of the computed field to include in the serialization JSON schema. [[, ], ] | None A callable that takes a field name and returns title for it. | None Description of the computed field to include in the serialization JSON schema. |  |  | None A deprecation message, an instance of warnings.deprecated or the typing_extensions.deprecated backport,\nor a boolean. If True , a default deprecation message will be emitted when accessing the field. [] | None Example values of the computed field to include in the serialization JSON schema. | [[], None] | None A dict or callable to provide extra JSON schema properties. A boolean indicating whether to include the field in the repr output. deprecation_message property ¶ deprecation_message:  | None The deprecation message to be emitted, or None if not set.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.ComputedFieldInfo","title":"Fields - ComputedFieldInfo  dataclass","objectID":"/latest/api/fields/#pydantic.fields.ComputedFieldInfo","rank":15},{"content":"deprecation_message:  | None The deprecation message to be emitted, or None if not set.","pageID":"Fields","abs_url":"/latest/api/fields/#pydantic.fields.ComputedFieldInfo.deprecation_message","title":"Fields - ComputedFieldInfo  dataclass - deprecation_message  property","objectID":"/latest/api/fields/#pydantic.fields.ComputedFieldInfo.deprecation_message","rank":10},{"content":"This module contains related classes and functions for serialization. FieldPlainSerializer module-attribute ¶ FieldPlainSerializer:  = (\n    \"core_schema.SerializerFunction | _Partial\"\n) A field serializer method or function in plain mode. FieldWrapSerializer module-attribute ¶ FieldWrapSerializer:  = (\n    \"core_schema.WrapSerializerFunction | _Partial\"\n) A field serializer method or function in wrap mode. FieldSerializer module-attribute ¶ FieldSerializer:  = (\n    \"FieldPlainSerializer | FieldWrapSerializer\"\n) A field serializer method or function. ModelPlainSerializerWithInfo module-attribute ¶ ModelPlainSerializerWithInfo:  = [\n    [, []], \n] A model serializer method with the info argument, in plain mode. ModelPlainSerializerWithoutInfo module-attribute ¶ ModelPlainSerializerWithoutInfo:  = [\n    [], \n] A model serializer method without the info argument, in plain mode. ModelPlainSerializer module-attribute ¶ ModelPlainSerializer:  = (\n    \"ModelPlainSerializerWithInfo | ModelPlainSerializerWithoutInfo\"\n) A model serializer method in plain mode. ModelWrapSerializerWithInfo module-attribute ¶ ModelWrapSerializerWithInfo:  = [\n    [\n        ,\n        ,\n        [],\n    ],\n    ,\n] A model serializer method with the info argument, in wrap mode. ModelWrapSerializerWithoutInfo module-attribute ¶ ModelWrapSerializerWithoutInfo:  = [\n    [, ], \n] A model serializer method without the info argument, in wrap mode. ModelWrapSerializer module-attribute ¶ ModelWrapSerializer:  = (\n    \"ModelWrapSerializerWithInfo | ModelWrapSerializerWithoutInfo\"\n) A model serializer method in wrap mode. PlainSerializer dataclass ¶ PlainSerializer(\n    func: ,\n    return_type:  = ,\n    when_used:  = \"always\",\n) Plain serializers use a function to modify the output of serialization. This is particularly helpful when you want to customize the serialization for annotated types.\nConsider an input of list , which will be serialized into a space-delimited string. from typing import Annotated\n\nfrom pydantic import BaseModel, PlainSerializer\n\nCustomStr = Annotated[\n    list, PlainSerializer(lambda x: ' '.join(x), return_type=str)\n]\n\nclass StudentModel(BaseModel):\n    courses: CustomStr\n\nstudent = StudentModel(courses=['Math', 'Chemistry', 'English'])\nprint(student.model_dump())\n#> {'courses': 'Math Chemistry English'} Attributes: Name Type Description The serializer function. The return type for the function. If omitted it will be inferred from the type annotation. Determines when this serializer should be used. Accepts a string with values 'always' , 'unless-none' , 'json' , and 'json-unless-none' . Defaults to 'always'. WrapSerializer dataclass ¶ WrapSerializer(\n    func: ,\n    return_type:  = ,\n    when_used:  = \"always\",\n) Wrap serializers receive the raw inputs along with a handler function that applies the standard serialization\nlogic, and can modify the resulting value before returning it as the final output of serialization. For example, here's a scenario in which a wrap serializer transforms timezones to UTC and utilizes the existing datetime serialization logic. from datetime import datetime, timezone\nfrom typing import Annotated, Any\n\nfrom pydantic import BaseModel, WrapSerializer\n\nclass EventDatetime(BaseModel):\n    start: datetime\n    end: datetime\n\ndef convert_to_utc(value: Any, handler, info) -> dict[str, datetime]:\n    # Note that `handler` can actually help serialize the `value` for\n    # further custom serialization in case it's a subclass.\n    partial_result = handler(value, info)\n    if info.mode == 'json':\n        return {\n            k: datetime.fromisoformat(v).astimezone(timezone.utc)\n            for k, v in partial_result.items()\n        }\n    return {k: v.astimezone(timezone.utc) for k, v in partial_result.items()}\n\nUTCEventDatetime = Annotated[EventDatetime, WrapSerializer(convert_to_utc)]\n\nclass EventModel(BaseModel):\n    event_datetime: UTCEventDatetime\n\ndt = EventDatetime(\n    start='2024-01-01T07:00:00-08:00', end='2024-01-03T20:00:00+06:00'\n)\nevent = EventModel(event_datetime=dt)\nprint(event.model_dump())\n'''\n{\n    'event_datetime': {\n        'start': datetime.datetime(\n            2024, 1, 1, 15, 0, tzinfo=datetime.timezone.utc\n        ),\n        'end': datetime.datetime(\n            2024, 1, 3, 14, 0, tzinfo=datetime.timezone.utc\n        ),\n    }\n}\n'''\n\nprint(event.model_dump_json())\n'''\n{\"event_datetime\":{\"start\":\"2024-01-01T15:00:00Z\",\"end\":\"2024-01-03T14:00:00Z\"}}\n''' Attributes: Name Type Description The serializer function to be wrapped. The return type for the function. If omitted it will be inferred from the type annotation. Determines when this serializer should be used. Accepts a string with values 'always' , 'unless-none' , 'json' , and 'json-unless-none' . Defaults to 'always'. SerializeAsAny dataclass ¶ SerializeAsAny() Annotation used to mark a type as having duck-typing serialization behavior. See usage documentation for more details. field_serializer ¶ field_serializer(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"wrap\"],\n    return_type:  = ...,\n    when_used:  = ...,\n    check_fields:  | None = ...,\n) -> [\n    [], \n] field_serializer(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"plain\"] = ...,\n    return_type:  = ...,\n    when_used:  = ...,\n    check_fields:  | None = ...,\n) -> [\n    [], \n] field_serializer(\n    *fields: ,\n    mode: [\"plain\", \"wrap\"] = \"plain\",\n    return_type:  = ,\n    when_used:  = \"always\",\n    check_fields:  | None = None\n) -> (\n    [[], ]\n    | [\n        [], \n    ]\n) Decorator that enables custom field serialization. In the below example, a field of type set is used to mitigate duplication. A field_serializer is used to serialize the data as a sorted list. from pydantic import BaseModel, field_serializer\n\nclass StudentModel(BaseModel):\n    name: str = 'Jane'\n    courses: set[str]\n\n    @field_serializer('courses', when_used='json')\n    def serialize_courses_in_order(self, courses: set[str]):\n        return sorted(courses)\n\nstudent = StudentModel(courses={'Math', 'Chemistry', 'English'})\nprint(student.model_dump_json())\n#> {\"name\":\"Jane\",\"courses\":[\"Chemistry\",\"English\",\"Math\"]} See the usage documentation for more information. Four signatures are supported: (self, value: Any, info: FieldSerializationInfo) (self, value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo) (value: Any, info: SerializationInfo) (value: Any, nxt: SerializerFunctionWrapHandler, info: SerializationInfo) Parameters: Name Type Description Default fields Which field(s) the method should be called on. () mode ['plain', 'wrap'] The serialization mode. plain means the function will be called instead of the default serialization logic, wrap means the function will be called with an argument to optionally call the\n   default serialization logic. 'plain' return_type Optional return type for the function, if omitted it will be inferred from the type annotation. when_used Determines the serializer will be used for serialization. 'always' check_fields | None Whether to check that the fields actually exist on the model. None Returns: Type Description [[], ] | [[], ] The decorator function. model_serializer ¶ model_serializer(\n    f: ,\n) -> model_serializer(\n    *,\n    mode: [\"wrap\"],\n    when_used:  = \"always\",\n    return_type:  = ...\n) -> [\n    [], \n] model_serializer(\n    *,\n    mode: [\"plain\"] = ...,\n    when_used:  = \"always\",\n    return_type:  = ...\n) -> [\n    [], \n] model_serializer(\n    f: (\n        \n        | \n        | None\n    ) = None,\n    /,\n    *,\n    mode: [\"plain\", \"wrap\"] = \"plain\",\n    when_used:  = \"always\",\n    return_type:  = ,\n) -> (\n    \n    | [\n        [], \n    ]\n    | [\n        [], \n    ]\n) Decorator that enables custom model serialization. This is useful when a model need to be serialized in a customized manner, allowing for flexibility beyond just specific fields. An example would be to serialize temperature to the same temperature scale, such as degrees Celsius. from typing import Literal\n\nfrom pydantic import BaseModel, model_serializer\n\nclass TemperatureModel(BaseModel):\n    unit: Literal['C', 'F']\n    value: int\n\n    @model_serializer()\n    def serialize_model(self):\n        if self.unit == 'F':\n            return {'unit': 'C', 'value': int((self.value - 32) / 1.8)}\n        return {'unit': self.unit, 'value': self.value}\n\ntemperature = TemperatureModel(unit='F', value=212)\nprint(temperature.model_dump())\n#> {'unit': 'C', 'value': 100} Two signatures are supported for mode='plain' , which is the default: (self) (self, info: SerializationInfo) And two other signatures for mode='wrap' : (self, nxt: SerializerFunctionWrapHandler) (self, nxt: SerializerFunctionWrapHandler, info: SerializationInfo) See the usage documentation for more information. Parameters: Name Type Description Default f |  | None The function to be decorated. None mode ['plain', 'wrap'] The serialization mode. 'plain' means the function will be called instead of the default serialization logic 'wrap' means the function will be called with an argument to optionally call the default\n    serialization logic. 'plain' when_used Determines when this serializer should be used. 'always' return_type The return type for the function. If omitted it will be inferred from the type annotation. Returns: Type Description | [[], ] | [[], ] The decorator function.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#Functional Serializers","title":"Functional Serializers","objectID":"/latest/api/functional_serializers/#Functional Serializers","rank":100},{"content":"FieldPlainSerializer:  = (\n    \"core_schema.SerializerFunction | _Partial\"\n) A field serializer method or function in plain mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.FieldPlainSerializer","title":"Functional Serializers - FieldPlainSerializer  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.FieldPlainSerializer","rank":95},{"content":"FieldWrapSerializer:  = (\n    \"core_schema.WrapSerializerFunction | _Partial\"\n) A field serializer method or function in wrap mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.FieldWrapSerializer","title":"Functional Serializers - FieldWrapSerializer  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.FieldWrapSerializer","rank":90},{"content":"FieldSerializer:  = (\n    \"FieldPlainSerializer | FieldWrapSerializer\"\n) A field serializer method or function.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.FieldSerializer","title":"Functional Serializers - FieldSerializer  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.FieldSerializer","rank":85},{"content":"ModelPlainSerializerWithInfo:  = [\n    [, []], \n] A model serializer method with the info argument, in plain mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelPlainSerializerWithInfo","title":"Functional Serializers - ModelPlainSerializerWithInfo  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelPlainSerializerWithInfo","rank":80},{"content":"ModelPlainSerializerWithoutInfo:  = [\n    [], \n] A model serializer method without the info argument, in plain mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelPlainSerializerWithoutInfo","title":"Functional Serializers - ModelPlainSerializerWithoutInfo  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelPlainSerializerWithoutInfo","rank":75},{"content":"ModelPlainSerializer:  = (\n    \"ModelPlainSerializerWithInfo | ModelPlainSerializerWithoutInfo\"\n) A model serializer method in plain mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelPlainSerializer","title":"Functional Serializers - ModelPlainSerializer  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelPlainSerializer","rank":70},{"content":"ModelWrapSerializerWithInfo:  = [\n    [\n        ,\n        ,\n        [],\n    ],\n    ,\n] A model serializer method with the info argument, in wrap mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelWrapSerializerWithInfo","title":"Functional Serializers - ModelWrapSerializerWithInfo  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelWrapSerializerWithInfo","rank":65},{"content":"ModelWrapSerializerWithoutInfo:  = [\n    [, ], \n] A model serializer method without the info argument, in wrap mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelWrapSerializerWithoutInfo","title":"Functional Serializers - ModelWrapSerializerWithoutInfo  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelWrapSerializerWithoutInfo","rank":60},{"content":"ModelWrapSerializer:  = (\n    \"ModelWrapSerializerWithInfo | ModelWrapSerializerWithoutInfo\"\n) A model serializer method in wrap mode.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelWrapSerializer","title":"Functional Serializers - ModelWrapSerializer  module-attribute","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.ModelWrapSerializer","rank":55},{"content":"PlainSerializer(\n    func: ,\n    return_type:  = ,\n    when_used:  = \"always\",\n) Plain serializers use a function to modify the output of serialization. This is particularly helpful when you want to customize the serialization for annotated types.\nConsider an input of list , which will be serialized into a space-delimited string. from typing import Annotated\n\nfrom pydantic import BaseModel, PlainSerializer\n\nCustomStr = Annotated[\n    list, PlainSerializer(lambda x: ' '.join(x), return_type=str)\n]\n\nclass StudentModel(BaseModel):\n    courses: CustomStr\n\nstudent = StudentModel(courses=['Math', 'Chemistry', 'English'])\nprint(student.model_dump())\n#> {'courses': 'Math Chemistry English'} Attributes: Name Type Description The serializer function. The return type for the function. If omitted it will be inferred from the type annotation. Determines when this serializer should be used. Accepts a string with values 'always' , 'unless-none' , 'json' , and 'json-unless-none' . Defaults to 'always'.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.PlainSerializer","title":"Functional Serializers - PlainSerializer  dataclass","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.PlainSerializer","rank":50},{"content":"WrapSerializer(\n    func: ,\n    return_type:  = ,\n    when_used:  = \"always\",\n) Wrap serializers receive the raw inputs along with a handler function that applies the standard serialization\nlogic, and can modify the resulting value before returning it as the final output of serialization. For example, here's a scenario in which a wrap serializer transforms timezones to UTC and utilizes the existing datetime serialization logic. from datetime import datetime, timezone\nfrom typing import Annotated, Any\n\nfrom pydantic import BaseModel, WrapSerializer\n\nclass EventDatetime(BaseModel):\n    start: datetime\n    end: datetime\n\ndef convert_to_utc(value: Any, handler, info) -> dict[str, datetime]:\n    # Note that `handler` can actually help serialize the `value` for\n    # further custom serialization in case it's a subclass.\n    partial_result = handler(value, info)\n    if info.mode == 'json':\n        return {\n            k: datetime.fromisoformat(v).astimezone(timezone.utc)\n            for k, v in partial_result.items()\n        }\n    return {k: v.astimezone(timezone.utc) for k, v in partial_result.items()}\n\nUTCEventDatetime = Annotated[EventDatetime, WrapSerializer(convert_to_utc)]\n\nclass EventModel(BaseModel):\n    event_datetime: UTCEventDatetime\n\ndt = EventDatetime(\n    start='2024-01-01T07:00:00-08:00', end='2024-01-03T20:00:00+06:00'\n)\nevent = EventModel(event_datetime=dt)\nprint(event.model_dump())\n'''\n{\n    'event_datetime': {\n        'start': datetime.datetime(\n            2024, 1, 1, 15, 0, tzinfo=datetime.timezone.utc\n        ),\n        'end': datetime.datetime(\n            2024, 1, 3, 14, 0, tzinfo=datetime.timezone.utc\n        ),\n    }\n}\n'''\n\nprint(event.model_dump_json())\n'''\n{\"event_datetime\":{\"start\":\"2024-01-01T15:00:00Z\",\"end\":\"2024-01-03T14:00:00Z\"}}\n''' Attributes: Name Type Description The serializer function to be wrapped. The return type for the function. If omitted it will be inferred from the type annotation. Determines when this serializer should be used. Accepts a string with values 'always' , 'unless-none' , 'json' , and 'json-unless-none' . Defaults to 'always'.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.WrapSerializer","title":"Functional Serializers - WrapSerializer  dataclass","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.WrapSerializer","rank":45},{"content":"SerializeAsAny() Annotation used to mark a type as having duck-typing serialization behavior. See usage documentation for more details.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.SerializeAsAny","title":"Functional Serializers - SerializeAsAny  dataclass","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.SerializeAsAny","rank":40},{"content":"field_serializer(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"wrap\"],\n    return_type:  = ...,\n    when_used:  = ...,\n    check_fields:  | None = ...,\n) -> [\n    [], \n] field_serializer(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"plain\"] = ...,\n    return_type:  = ...,\n    when_used:  = ...,\n    check_fields:  | None = ...,\n) -> [\n    [], \n] field_serializer(\n    *fields: ,\n    mode: [\"plain\", \"wrap\"] = \"plain\",\n    return_type:  = ,\n    when_used:  = \"always\",\n    check_fields:  | None = None\n) -> (\n    [[], ]\n    | [\n        [], \n    ]\n) Decorator that enables custom field serialization. In the below example, a field of type set is used to mitigate duplication. A field_serializer is used to serialize the data as a sorted list. from pydantic import BaseModel, field_serializer\n\nclass StudentModel(BaseModel):\n    name: str = 'Jane'\n    courses: set[str]\n\n    @field_serializer('courses', when_used='json')\n    def serialize_courses_in_order(self, courses: set[str]):\n        return sorted(courses)\n\nstudent = StudentModel(courses={'Math', 'Chemistry', 'English'})\nprint(student.model_dump_json())\n#> {\"name\":\"Jane\",\"courses\":[\"Chemistry\",\"English\",\"Math\"]} See the usage documentation for more information. Four signatures are supported: (self, value: Any, info: FieldSerializationInfo) (self, value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo) (value: Any, info: SerializationInfo) (value: Any, nxt: SerializerFunctionWrapHandler, info: SerializationInfo) Parameters: Name Type Description Default fields Which field(s) the method should be called on. () mode ['plain', 'wrap'] The serialization mode. plain means the function will be called instead of the default serialization logic, wrap means the function will be called with an argument to optionally call the\n   default serialization logic. 'plain' return_type Optional return type for the function, if omitted it will be inferred from the type annotation. when_used Determines the serializer will be used for serialization. 'always' check_fields | None Whether to check that the fields actually exist on the model. None Returns: Type Description [[], ] | [[], ] The decorator function.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.field_serializer","title":"Functional Serializers - field_serializer","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.field_serializer","rank":35},{"content":"model_serializer(\n    f: ,\n) -> model_serializer(\n    *,\n    mode: [\"wrap\"],\n    when_used:  = \"always\",\n    return_type:  = ...\n) -> [\n    [], \n] model_serializer(\n    *,\n    mode: [\"plain\"] = ...,\n    when_used:  = \"always\",\n    return_type:  = ...\n) -> [\n    [], \n] model_serializer(\n    f: (\n        \n        | \n        | None\n    ) = None,\n    /,\n    *,\n    mode: [\"plain\", \"wrap\"] = \"plain\",\n    when_used:  = \"always\",\n    return_type:  = ,\n) -> (\n    \n    | [\n        [], \n    ]\n    | [\n        [], \n    ]\n) Decorator that enables custom model serialization. This is useful when a model need to be serialized in a customized manner, allowing for flexibility beyond just specific fields. An example would be to serialize temperature to the same temperature scale, such as degrees Celsius. from typing import Literal\n\nfrom pydantic import BaseModel, model_serializer\n\nclass TemperatureModel(BaseModel):\n    unit: Literal['C', 'F']\n    value: int\n\n    @model_serializer()\n    def serialize_model(self):\n        if self.unit == 'F':\n            return {'unit': 'C', 'value': int((self.value - 32) / 1.8)}\n        return {'unit': self.unit, 'value': self.value}\n\ntemperature = TemperatureModel(unit='F', value=212)\nprint(temperature.model_dump())\n#> {'unit': 'C', 'value': 100} Two signatures are supported for mode='plain' , which is the default: (self) (self, info: SerializationInfo) And two other signatures for mode='wrap' : (self, nxt: SerializerFunctionWrapHandler) (self, nxt: SerializerFunctionWrapHandler, info: SerializationInfo) See the usage documentation for more information. Parameters: Name Type Description Default f |  | None The function to be decorated. None mode ['plain', 'wrap'] The serialization mode. 'plain' means the function will be called instead of the default serialization logic 'wrap' means the function will be called with an argument to optionally call the default\n    serialization logic. 'plain' when_used Determines when this serializer should be used. 'always' return_type The return type for the function. If omitted it will be inferred from the type annotation. Returns: Type Description | [[], ] | [[], ] The decorator function.","pageID":"Functional Serializers","abs_url":"/latest/api/functional_serializers/#pydantic.functional_serializers.model_serializer","title":"Functional Serializers - model_serializer","objectID":"/latest/api/functional_serializers/#pydantic.functional_serializers.model_serializer","rank":30},{"content":"This module contains related classes and functions for validation. ModelAfterValidatorWithoutInfo module-attribute ¶ ModelAfterValidatorWithoutInfo = [\n    [], \n] A @model_validator decorated function signature. This is used when mode='after' and the function does not\nhave info argument. ModelAfterValidator module-attribute ¶ ModelAfterValidator = [\n    [, []], \n] A @model_validator decorated function signature. This is used when mode='after' . AfterValidator dataclass ¶ AfterValidator(\n    func: (\n         | \n    ),\n) Usage Documentation field after validators A metadata class that indicates that a validation should be applied after the inner validation logic. Attributes: Name Type Description | The validator function. BeforeValidator dataclass ¶ BeforeValidator(\n    func: (\n         | \n    ),\n    json_schema_input_type:  = ,\n) Usage Documentation field before validators A metadata class that indicates that a validation should be applied before the inner validation logic. Attributes: Name Type Description | The validator function. The input type used to generate the appropriate\nJSON Schema (in validation mode). The actual input type is Any . PlainValidator dataclass ¶ PlainValidator(\n    func: (\n         | \n    ),\n    json_schema_input_type:  = ,\n) Usage Documentation field plain validators A metadata class that indicates that a validation should be applied instead of the inner validation logic. Note Before v2.9, PlainValidator wasn't always compatible with JSON Schema generation for mode='validation' .\nYou can now use the json_schema_input_type argument to specify the input type of the function\nto be used in the JSON schema when mode='validation' (the default). See the example below for more details. Attributes: Name Type Description | The validator function. The input type used to generate the appropriate\nJSON Schema (in validation mode). The actual input type is Any . WrapValidator dataclass ¶ WrapValidator(\n    func: (\n        \n        | \n    ),\n    json_schema_input_type:  = ,\n) Usage Documentation field wrap validators A metadata class that indicates that a validation should be applied around the inner validation logic. Attributes: Name Type Description | The validator function. The input type used to generate the appropriate\nJSON Schema (in validation mode). The actual input type is Any . from datetime import datetime\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, ValidationError, WrapValidator\n\ndef validate_timestamp(v, handler):\n    if v == 'now':\n        # we don't want to bother with further validation, just return the new value\n        return datetime.now()\n    try:\n        return handler(v)\n    except ValidationError:\n        # validation failed, in this case we want to return a default value\n        return datetime(2000, 1, 1)\n\nMyTimestamp = Annotated[datetime, WrapValidator(validate_timestamp)]\n\nclass Model(BaseModel):\n    a: MyTimestamp\n\nprint(Model(a='now').a)\n#> 2032-01-02 03:04:05.000006\nprint(Model(a='invalid').a)\n#> 2000-01-01 00:00:00 ModelWrapValidatorHandler ¶ Bases: , [] @model_validator decorated function handler argument type. This is used when mode='wrap' . ModelWrapValidatorWithoutInfo ¶ Bases: [] A @model_validator decorated function signature.\nThis is used when mode='wrap' and the function does not have info argument. ModelWrapValidator ¶ Bases: [] A @model_validator decorated function signature. This is used when mode='wrap' . FreeModelBeforeValidatorWithoutInfo ¶ Bases: A @model_validator decorated function signature.\nThis is used when mode='before' and the function does not have info argument. ModelBeforeValidatorWithoutInfo ¶ Bases: A @model_validator decorated function signature.\nThis is used when mode='before' and the function does not have info argument. FreeModelBeforeValidator ¶ Bases: A @model_validator decorated function signature. This is used when mode='before' . ModelBeforeValidator ¶ Bases: A @model_validator decorated function signature. This is used when mode='before' . InstanceOf dataclass ¶ InstanceOf() Generic type for annotating a type that is an instance of a given class. SkipValidation dataclass ¶ SkipValidation() If this is applied as an annotation (e.g., via x: Annotated[int, SkipValidation] ), validation will be\n    skipped. You can also use SkipValidation[int] as a shorthand for Annotated[int, SkipValidation] . This can be useful if you want to use a type annotation for documentation/IDE/type-checking purposes,\nand know that it is safe to skip validation for one or more of the fields. Because this converts the validation schema to any_schema , subsequent annotation-applied transformations\nmay not have the expected effects. Therefore, when used, this annotation should generally be the final\nannotation applied to a type. ValidateAs ¶ ValidateAs(\n    from_type: [],\n    /,\n    instantiation_hook: [[], ],\n) A helper class to validate a custom type from a type that is natively supported by Pydantic. Parameters: Name Type Description Default from_type [] The type natively supported by Pydantic to use to perform validation. required instantiation_hook [[], ] A callable taking the validated type as an argument, and returning\nthe populated custom type. required field_validator ¶ field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"wrap\"],\n    check_fields:  | None = ...,\n    json_schema_input_type:  = ...,\n) -> [[], ] field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"before\", \"plain\"],\n    check_fields:  | None = ...,\n    json_schema_input_type:  = ...,\n) -> [\n    [],\n    ,\n] field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"after\"] = ...,\n    check_fields:  | None = ...,\n) -> [\n    [],\n    ,\n] field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode:  = \"after\",\n    check_fields:  | None = None,\n    json_schema_input_type:  = ,\n) -> [[], ] Usage Documentation field validators Decorate methods on the class indicating that they should be used to validate fields. Example usage: from typing import Any\n\nfrom pydantic import (\n    BaseModel,\n    ValidationError,\n    field_validator,\n)\n\nclass Model(BaseModel):\n    a: str\n\n    @field_validator('a')\n    @classmethod\n    def ensure_foobar(cls, v: Any):\n        if 'foobar' not in v:\n            raise ValueError('\"foobar\" not found in a')\n        return v\n\nprint(repr(Model(a='this is foobar good')))\n#> Model(a='this is foobar good')\n\ntry:\n    Model(a='snap')\nexcept ValidationError as exc_info:\n    print(exc_info)\n    '''\n    1 validation error for Model\n    a\n      Value error, \"foobar\" not found in a [type=value_error, input_value='snap', input_type=str]\n    ''' For more in depth examples, see Field Validators . Parameters: Name Type Description Default field The first field the field_validator should be called on; this is separate\nfrom fields to ensure an error is raised if you don't pass at least one. required *fields Additional field(s) the field_validator should be called on. () mode Specifies whether to validate the fields before or after validation. 'after' check_fields | None Whether to check that the fields actually exist on the model. None json_schema_input_type The input type of the function. This is only used to generate\nthe appropriate JSON Schema (in validation mode) and can only specified\nwhen mode is either 'before' , 'plain' or 'wrap' . Returns: Type Description [[], ] A decorator that can be used to decorate a function to be used as a field_validator. Raises: Type Description If @field_validator is used bare (with no fields). If the args passed to @field_validator as fields are not strings. If @field_validator applied to instance methods. model_validator ¶ model_validator(*, mode: [\"wrap\"]) -> [\n    [[]],\n    [],\n] model_validator(*, mode: [\"before\"]) -> [\n    [],\n    [],\n] model_validator(*, mode: [\"after\"]) -> [\n    [[]],\n    [],\n] model_validator(\n    *, mode: [\"wrap\", \"before\", \"after\"]\n) -> Usage Documentation Model Validators Decorate model methods for validation purposes. Example usage: from typing_extensions import Self\n\nfrom pydantic import BaseModel, ValidationError, model_validator\n\nclass Square(BaseModel):\n    width: float\n    height: float\n\n    @model_validator(mode='after')\n    def verify_square(self) -> Self:\n        if self.width != self.height:\n            raise ValueError('width and height do not match')\n        return self\n\ns = Square(width=1, height=1)\nprint(repr(s))\n#> Square(width=1.0, height=1.0)\n\ntry:\n    Square(width=1, height=2)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Square\n      Value error, width and height do not match [type=value_error, input_value={'width': 1, 'height': 2}, input_type=dict]\n    ''' For more in depth examples, see Model Validators . Parameters: Name Type Description Default mode ['wrap', 'before', 'after'] A required string literal that specifies the validation mode.\nIt can be one of the following: 'wrap', 'before', or 'after'. required Returns: Type Description A decorator that can be used to decorate a function to be used as a model validator.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#Functional Validators","title":"Functional Validators","objectID":"/latest/api/functional_validators/#Functional Validators","rank":100},{"content":"ModelAfterValidatorWithoutInfo = [\n    [], \n] A @model_validator decorated function signature. This is used when mode='after' and the function does not\nhave info argument.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ModelAfterValidatorWithoutInfo","title":"Functional Validators - ModelAfterValidatorWithoutInfo  module-attribute","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ModelAfterValidatorWithoutInfo","rank":95},{"content":"ModelAfterValidator = [\n    [, []], \n] A @model_validator decorated function signature. This is used when mode='after' .","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ModelAfterValidator","title":"Functional Validators - ModelAfterValidator  module-attribute","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ModelAfterValidator","rank":90},{"content":"AfterValidator(\n    func: (\n         | \n    ),\n) Usage Documentation field after validators A metadata class that indicates that a validation should be applied after the inner validation logic. Attributes: Name Type Description | The validator function.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.AfterValidator","title":"Functional Validators - AfterValidator  dataclass","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.AfterValidator","rank":85},{"content":"BeforeValidator(\n    func: (\n         | \n    ),\n    json_schema_input_type:  = ,\n) Usage Documentation field before validators A metadata class that indicates that a validation should be applied before the inner validation logic. Attributes: Name Type Description | The validator function. The input type used to generate the appropriate\nJSON Schema (in validation mode). The actual input type is Any .","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.BeforeValidator","title":"Functional Validators - BeforeValidator  dataclass","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.BeforeValidator","rank":80},{"content":"PlainValidator(\n    func: (\n         | \n    ),\n    json_schema_input_type:  = ,\n) Usage Documentation field plain validators A metadata class that indicates that a validation should be applied instead of the inner validation logic. Note Before v2.9, PlainValidator wasn't always compatible with JSON Schema generation for mode='validation' .\nYou can now use the json_schema_input_type argument to specify the input type of the function\nto be used in the JSON schema when mode='validation' (the default). See the example below for more details. Attributes: Name Type Description | The validator function. The input type used to generate the appropriate\nJSON Schema (in validation mode). The actual input type is Any .","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.PlainValidator","title":"Functional Validators - PlainValidator  dataclass","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.PlainValidator","rank":75},{"content":"WrapValidator(\n    func: (\n        \n        | \n    ),\n    json_schema_input_type:  = ,\n) Usage Documentation field wrap validators A metadata class that indicates that a validation should be applied around the inner validation logic. Attributes: Name Type Description | The validator function. The input type used to generate the appropriate\nJSON Schema (in validation mode). The actual input type is Any . from datetime import datetime\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, ValidationError, WrapValidator\n\ndef validate_timestamp(v, handler):\n    if v == 'now':\n        # we don't want to bother with further validation, just return the new value\n        return datetime.now()\n    try:\n        return handler(v)\n    except ValidationError:\n        # validation failed, in this case we want to return a default value\n        return datetime(2000, 1, 1)\n\nMyTimestamp = Annotated[datetime, WrapValidator(validate_timestamp)]\n\nclass Model(BaseModel):\n    a: MyTimestamp\n\nprint(Model(a='now').a)\n#> 2032-01-02 03:04:05.000006\nprint(Model(a='invalid').a)\n#> 2000-01-01 00:00:00","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.WrapValidator","title":"Functional Validators - WrapValidator  dataclass","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.WrapValidator","rank":70},{"content":"Bases: , [] @model_validator decorated function handler argument type. This is used when mode='wrap' .","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ModelWrapValidatorHandler","title":"Functional Validators - ModelWrapValidatorHandler","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ModelWrapValidatorHandler","rank":65},{"content":"Bases: [] A @model_validator decorated function signature.\nThis is used when mode='wrap' and the function does not have info argument.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ModelWrapValidatorWithoutInfo","title":"Functional Validators - ModelWrapValidatorWithoutInfo","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ModelWrapValidatorWithoutInfo","rank":60},{"content":"Bases: [] A @model_validator decorated function signature. This is used when mode='wrap' .","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ModelWrapValidator","title":"Functional Validators - ModelWrapValidator","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ModelWrapValidator","rank":55},{"content":"Bases: A @model_validator decorated function signature.\nThis is used when mode='before' and the function does not have info argument.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.FreeModelBeforeValidatorWithoutInfo","title":"Functional Validators - FreeModelBeforeValidatorWithoutInfo","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.FreeModelBeforeValidatorWithoutInfo","rank":50},{"content":"Bases: A @model_validator decorated function signature.\nThis is used when mode='before' and the function does not have info argument.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ModelBeforeValidatorWithoutInfo","title":"Functional Validators - ModelBeforeValidatorWithoutInfo","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ModelBeforeValidatorWithoutInfo","rank":45},{"content":"Bases: A @model_validator decorated function signature. This is used when mode='before' .","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.FreeModelBeforeValidator","title":"Functional Validators - FreeModelBeforeValidator","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.FreeModelBeforeValidator","rank":40},{"content":"Bases: A @model_validator decorated function signature. This is used when mode='before' .","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ModelBeforeValidator","title":"Functional Validators - ModelBeforeValidator","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ModelBeforeValidator","rank":35},{"content":"InstanceOf() Generic type for annotating a type that is an instance of a given class.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.InstanceOf","title":"Functional Validators - InstanceOf  dataclass","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.InstanceOf","rank":30},{"content":"SkipValidation() If this is applied as an annotation (e.g., via x: Annotated[int, SkipValidation] ), validation will be\n    skipped. You can also use SkipValidation[int] as a shorthand for Annotated[int, SkipValidation] . This can be useful if you want to use a type annotation for documentation/IDE/type-checking purposes,\nand know that it is safe to skip validation for one or more of the fields. Because this converts the validation schema to any_schema , subsequent annotation-applied transformations\nmay not have the expected effects. Therefore, when used, this annotation should generally be the final\nannotation applied to a type.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.SkipValidation","title":"Functional Validators - SkipValidation  dataclass","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.SkipValidation","rank":25},{"content":"ValidateAs(\n    from_type: [],\n    /,\n    instantiation_hook: [[], ],\n) A helper class to validate a custom type from a type that is natively supported by Pydantic. Parameters: Name Type Description Default from_type [] The type natively supported by Pydantic to use to perform validation. required instantiation_hook [[], ] A callable taking the validated type as an argument, and returning\nthe populated custom type. required","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.ValidateAs","title":"Functional Validators - ValidateAs","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.ValidateAs","rank":20},{"content":"field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"wrap\"],\n    check_fields:  | None = ...,\n    json_schema_input_type:  = ...,\n) -> [[], ] field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"before\", \"plain\"],\n    check_fields:  | None = ...,\n    json_schema_input_type:  = ...,\n) -> [\n    [],\n    ,\n] field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode: [\"after\"] = ...,\n    check_fields:  | None = ...,\n) -> [\n    [],\n    ,\n] field_validator(\n    field: ,\n    /,\n    *fields: ,\n    mode:  = \"after\",\n    check_fields:  | None = None,\n    json_schema_input_type:  = ,\n) -> [[], ] Usage Documentation field validators Decorate methods on the class indicating that they should be used to validate fields. Example usage: from typing import Any\n\nfrom pydantic import (\n    BaseModel,\n    ValidationError,\n    field_validator,\n)\n\nclass Model(BaseModel):\n    a: str\n\n    @field_validator('a')\n    @classmethod\n    def ensure_foobar(cls, v: Any):\n        if 'foobar' not in v:\n            raise ValueError('\"foobar\" not found in a')\n        return v\n\nprint(repr(Model(a='this is foobar good')))\n#> Model(a='this is foobar good')\n\ntry:\n    Model(a='snap')\nexcept ValidationError as exc_info:\n    print(exc_info)\n    '''\n    1 validation error for Model\n    a\n      Value error, \"foobar\" not found in a [type=value_error, input_value='snap', input_type=str]\n    ''' For more in depth examples, see Field Validators . Parameters: Name Type Description Default field The first field the field_validator should be called on; this is separate\nfrom fields to ensure an error is raised if you don't pass at least one. required *fields Additional field(s) the field_validator should be called on. () mode Specifies whether to validate the fields before or after validation. 'after' check_fields | None Whether to check that the fields actually exist on the model. None json_schema_input_type The input type of the function. This is only used to generate\nthe appropriate JSON Schema (in validation mode) and can only specified\nwhen mode is either 'before' , 'plain' or 'wrap' . Returns: Type Description [[], ] A decorator that can be used to decorate a function to be used as a field_validator. Raises: Type Description If @field_validator is used bare (with no fields). If the args passed to @field_validator as fields are not strings. If @field_validator applied to instance methods.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.field_validator","title":"Functional Validators - field_validator","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.field_validator","rank":15},{"content":"model_validator(*, mode: [\"wrap\"]) -> [\n    [[]],\n    [],\n] model_validator(*, mode: [\"before\"]) -> [\n    [],\n    [],\n] model_validator(*, mode: [\"after\"]) -> [\n    [[]],\n    [],\n] model_validator(\n    *, mode: [\"wrap\", \"before\", \"after\"]\n) -> Usage Documentation Model Validators Decorate model methods for validation purposes. Example usage: from typing_extensions import Self\n\nfrom pydantic import BaseModel, ValidationError, model_validator\n\nclass Square(BaseModel):\n    width: float\n    height: float\n\n    @model_validator(mode='after')\n    def verify_square(self) -> Self:\n        if self.width != self.height:\n            raise ValueError('width and height do not match')\n        return self\n\ns = Square(width=1, height=1)\nprint(repr(s))\n#> Square(width=1.0, height=1.0)\n\ntry:\n    Square(width=1, height=2)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Square\n      Value error, width and height do not match [type=value_error, input_value={'width': 1, 'height': 2}, input_type=dict]\n    ''' For more in depth examples, see Model Validators . Parameters: Name Type Description Default mode ['wrap', 'before', 'after'] A required string literal that specifies the validation mode.\nIt can be one of the following: 'wrap', 'before', or 'after'. required Returns: Type Description A decorator that can be used to decorate a function to be used as a model validator.","pageID":"Functional Validators","abs_url":"/latest/api/functional_validators/#pydantic.functional_validators.model_validator","title":"Functional Validators - model_validator","objectID":"/latest/api/functional_validators/#pydantic.functional_validators.model_validator","rank":10},{"content":"Usage Documentation JSON Schema The json_schema module contains classes and functions to allow the way JSON Schema is generated to be customized. In general you shouldn't need to use this module directly; instead, you can use\n and\n. CoreSchemaOrFieldType module-attribute ¶ CoreSchemaOrFieldType = [\n    , \n] A type alias for defined schema types that represents a union of core_schema.CoreSchemaType and core_schema.CoreSchemaFieldType . JsonSchemaValue module-attribute ¶ JsonSchemaValue = [, ] A type alias for a JSON schema value. This is a dictionary of string keys to arbitrary JSON values. JsonSchemaMode module-attribute ¶ JsonSchemaMode = ['validation', 'serialization'] A type alias that represents the mode of a JSON schema; either 'validation' or 'serialization'. For some types, the inputs to validation differ from the outputs of serialization. For example,\ncomputed fields will only be present when serializing, and should not be provided when\nvalidating. This flag provides a way to indicate whether you want the JSON schema required\nfor validation inputs, or that will be matched by serialization outputs. JsonSchemaWarningKind module-attribute ¶ JsonSchemaWarningKind = [\n    \"skipped-choice\",\n    \"non-serializable-default\",\n    \"skipped-discriminator\",\n] A type alias representing the kinds of warnings that can be emitted during JSON schema generation. See \nfor more details. NoDefault module-attribute ¶ NoDefault = () A sentinel value used to indicate that no default value should be used when generating a JSON Schema\nfor a core schema with a default value. DEFAULT_REF_TEMPLATE module-attribute ¶ DEFAULT_REF_TEMPLATE = '#/$defs/{model}' The default format string used to generate reference names. PydanticJsonSchemaWarning ¶ Bases: This class is used to emit warnings produced during JSON schema generation.\nSee the  and\n\nmethods for more details; these can be overridden to control warning behavior. GenerateJsonSchema ¶ GenerateJsonSchema(\n    by_alias:  = True,\n    ref_template:  = ,\n) Usage Documentation Customizing the JSON Schema Generation Process A class for generating JSON schemas. This class generates JSON schemas based on configured parameters. The default schema dialect\nis https://json-schema.org/draft/2020-12/schema .\nThe class uses by_alias to configure how fields with\nmultiple names are handled and ref_template to format reference names. Attributes: Name Type Description The JSON schema dialect used to generate the schema. See Declaring a Dialect in the JSON Schema documentation for more information about dialects. [] Warnings to ignore when generating the schema. self.render_warning_message will\ndo nothing if its argument kind is in ignored_warning_kinds ;\nthis value can be modified on subclasses to easily control which warnings are emitted. Whether to use field aliases when generating the schema. The format string used when generating reference names. [, ] A mapping of core refs to JSON refs. [, ] A mapping of core refs to definition refs. [, ] A mapping of definition refs to core refs. [, ] A mapping of JSON refs to definition refs. [, ] Definitions in the schema. Parameters: Name Type Description Default by_alias Whether to use field aliases in the generated schemas. True ref_template The format string to use when generating reference names. Raises: Type Description If the instance of the class is inadvertently reused after generating a schema. ValidationsMapping ¶ This class just contains mappings from core_schema attribute names to the corresponding\nJSON schema attribute names. While I suspect it is unlikely to be necessary, you can in\nprinciple override this class in a subclass of GenerateJsonSchema (by inheriting from\nGenerateJsonSchema.ValidationsMapping) to change these mappings. build_schema_type_to_method ¶ build_schema_type_to_method() -> [\n    ,\n    [[], ],\n] Builds a dictionary mapping fields to methods for generating JSON schemas. Returns: Type Description [, [[], ]] A dictionary containing the mapping of CoreSchemaOrFieldType to a handler method. Raises: Type Description If no method has been defined for generating a JSON schema for a given pydantic core schema type. generate_definitions ¶ generate_definitions(\n    inputs: [\n        [, , ]\n    ]\n) -> [\n    [\n        [, ],\n        ,\n    ],\n    [, ],\n] Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a\nmapping that links the input keys to the definition references. Parameters: Name Type Description Default inputs [[, , ]] A sequence of tuples, where: The first element is a JSON schema key type. The second element is the JSON mode: either 'validation' or 'serialization'. The third element is a core schema. required Returns: Type Description [[[, ], ], [, ]] A tuple where: The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n    JsonRef references to definitions that are defined in the second returned element.) The second element is a dictionary whose keys are definition references for the JSON schemas\n    from the first returned element, and whose values are the actual JSON schema definitions. Raises: Type Description Raised if the JSON schema generator has already been used to generate a JSON schema. generate ¶ generate(\n    schema: , mode:  = \"validation\"\n) -> Generates a JSON schema for a specified schema in a specified mode. Parameters: Name Type Description Default schema A Pydantic model. required mode The mode in which to generate the schema. Defaults to 'validation'. 'validation' Returns: Type Description A JSON schema representing the specified schema. Raises: Type Description If the JSON schema generator has already been used to generate a JSON schema. generate_inner ¶ generate_inner(\n    schema: ,\n) -> Generates a JSON schema for a given core schema. Parameters: Name Type Description Default schema The given core schema. required Returns: Type Description The generated JSON schema. TODO: the nested function definitions here seem like bad practice, I'd like to unpack these\nin a future PR. It'd be great if we could shorten the call stack a bit for JSON schema generation,\nand I think there's potential for that here. sort ¶ sort(\n    value: , parent_key:  | None = None\n) -> Override this method to customize the sorting of the JSON schema (e.g., don't sort at all, sort all keys unconditionally, etc.) By default, alphabetically sort the keys in the JSON schema, skipping the 'properties' and 'default' keys to preserve field definition order.\nThis sort is recursive, so it will sort all nested dictionaries as well. invalid_schema ¶ invalid_schema(schema: ) -> Placeholder - should never be called. any_schema ¶ any_schema(schema: ) -> Generates a JSON schema that matches any value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. none_schema ¶ none_schema(schema: ) -> Generates a JSON schema that matches None . Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. bool_schema ¶ bool_schema(schema: ) -> Generates a JSON schema that matches a bool value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. int_schema ¶ int_schema(schema: ) -> Generates a JSON schema that matches an int value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. float_schema ¶ float_schema(schema: ) -> Generates a JSON schema that matches a float value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. decimal_schema ¶ decimal_schema(schema: ) -> Generates a JSON schema that matches a decimal value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. str_schema ¶ str_schema(schema: ) -> Generates a JSON schema that matches a string value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. bytes_schema ¶ bytes_schema(schema: ) -> Generates a JSON schema that matches a bytes value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. date_schema ¶ date_schema(schema: ) -> Generates a JSON schema that matches a date value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. time_schema ¶ time_schema(schema: ) -> Generates a JSON schema that matches a time value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. datetime_schema ¶ datetime_schema(schema: ) -> Generates a JSON schema that matches a datetime value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. timedelta_schema ¶ timedelta_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a timedelta value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. literal_schema ¶ literal_schema(schema: ) -> Generates a JSON schema that matches a literal value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. missing_sentinel_schema ¶ missing_sentinel_schema(\n    schema: ,\n) -> Generates a JSON schema that matches the MISSING sentinel value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. enum_schema ¶ enum_schema(schema: ) -> Generates a JSON schema that matches an Enum value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. is_instance_schema ¶ is_instance_schema(\n    schema: ,\n) -> Handles JSON schema generation for a core schema that checks if a value is an instance of a class. Unless overridden in a subclass, this raises an error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. is_subclass_schema ¶ is_subclass_schema(\n    schema: ,\n) -> Handles JSON schema generation for a core schema that checks if a value is a subclass of a class. For backwards compatibility with v1, this does not raise an error, but can be overridden to change this. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. callable_schema ¶ callable_schema(schema: ) -> Generates a JSON schema that matches a callable value. Unless overridden in a subclass, this raises an error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. list_schema ¶ list_schema(schema: ) -> Returns a schema that matches a list schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. tuple_positional_schema ¶ tuple_positional_schema(\n    schema: ,\n) -> Replaced by tuple_schema . tuple_variable_schema ¶ tuple_variable_schema(\n    schema: ,\n) -> Replaced by tuple_schema . tuple_schema ¶ tuple_schema(schema: ) -> Generates a JSON schema that matches a tuple schema e.g. tuple[int,\nstr, bool] or tuple[int, ...] . Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. set_schema ¶ set_schema(schema: ) -> Generates a JSON schema that matches a set schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. frozenset_schema ¶ frozenset_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a frozenset schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. generator_schema ¶ generator_schema(\n    schema: ,\n) -> Returns a JSON schema that represents the provided GeneratorSchema. Parameters: Name Type Description Default schema The schema. required Returns: Type Description The generated JSON schema. dict_schema ¶ dict_schema(schema: ) -> Generates a JSON schema that matches a dict schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_before_schema ¶ function_before_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-before schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_after_schema ¶ function_after_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-after schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_plain_schema ¶ function_plain_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-plain schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_wrap_schema ¶ function_wrap_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-wrap schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. default_schema ¶ default_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema with a default value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. get_default_value ¶ get_default_value(schema: ) -> Get the default value to be used when generating a JSON Schema for a core schema with a default. The default implementation is to use the statically defined default value. This method can be overridden\nif you want to make use of the default factory. Parameters: Name Type Description Default schema The 'with-default' core schema. required Returns: Type Description The default value to use, or  if no default\nvalue is available. nullable_schema ¶ nullable_schema(schema: ) -> Generates a JSON schema that matches a schema that allows null values. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. union_schema ¶ union_schema(schema: ) -> Generates a JSON schema that matches a schema that allows values matching any of the given schemas. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. tagged_union_schema ¶ tagged_union_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where\nthe schemas are tagged with a discriminator field that indicates which schema should be used to validate\nthe value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. chain_schema ¶ chain_schema(schema: ) -> Generates a JSON schema that matches a core_schema.ChainSchema. When generating a schema for validation, we return the validation JSON schema for the first step in the chain.\nFor serialization, we return the serialization JSON schema for the last step in the chain. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. lax_or_strict_schema ¶ lax_or_strict_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching either the lax schema or the\nstrict schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. json_or_python_schema ¶ json_or_python_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the\nPython schema. The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override\nthis method. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. typed_dict_schema ¶ typed_dict_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a typed dict. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. typed_dict_field_schema ¶ typed_dict_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a typed dict field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. dataclass_field_schema ¶ dataclass_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. model_field_schema ¶ model_field_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a model field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. computed_field_schema ¶ computed_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a computed field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. model_schema ¶ model_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a model. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. resolve_ref_schema ¶ resolve_ref_schema(\n    json_schema: ,\n) -> Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema. Parameters: Name Type Description Default json_schema The schema to resolve. required Returns: Type Description The resolved schema. Raises: Type Description If the schema reference can't be found in definitions. model_fields_schema ¶ model_fields_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a model's fields. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. field_is_present ¶ field_is_present(field: ) -> Whether the field should be included in the generated JSON schema. Parameters: Name Type Description Default field The schema for the field itself. required Returns: Type Description True if the field should be included in the generated JSON schema, False otherwise. field_is_required ¶ field_is_required(\n    field:  |  | ,\n    total: ,\n) -> Whether the field should be marked as required in the generated JSON schema.\n(Note that this is irrelevant if the field is not present in the JSON schema.). Parameters: Name Type Description Default field |  | The schema for the field itself. required total Only applies to TypedDictField s.\nIndicates if the TypedDict this field belongs to is total, in which case any fields that don't\nexplicitly specify required=False are required. required Returns: Type Description True if the field should be marked as required in the generated JSON schema, False otherwise. dataclass_args_schema ¶ dataclass_args_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. dataclass_schema ¶ dataclass_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. arguments_schema ¶ arguments_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a function's arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. kw_arguments_schema ¶ kw_arguments_schema(\n    arguments: [],\n    var_kwargs_schema:  | None,\n) -> Generates a JSON schema that matches a schema that defines a function's keyword arguments. Parameters: Name Type Description Default arguments [] The core schema. required Returns: Type Description The generated JSON schema. p_arguments_schema ¶ p_arguments_schema(\n    arguments: [],\n    var_args_schema:  | None,\n) -> Generates a JSON schema that matches a schema that defines a function's positional arguments. Parameters: Name Type Description Default arguments [] The core schema. required Returns: Type Description The generated JSON schema. get_argument_name ¶ get_argument_name(\n    argument:  | ,\n) -> Retrieves the name of an argument. Parameters: Name Type Description Default argument | The core schema. required Returns: Type Description The name of the argument. arguments_v3_schema ¶ arguments_v3_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a function's arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. call_schema ¶ call_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a function call. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. custom_error_schema ¶ custom_error_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a custom error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. json_schema ¶ json_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a JSON object. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. url_schema ¶ url_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a URL. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. multi_host_url_schema ¶ multi_host_url_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. uuid_schema ¶ uuid_schema(schema: ) -> Generates a JSON schema that matches a UUID. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. definitions_schema ¶ definitions_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a JSON object with definitions. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. definition_ref_schema ¶ definition_ref_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that references a definition. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. ser_schema ¶ ser_schema(\n    schema: (\n         |  | \n    ),\n) ->  | None Generates a JSON schema that matches a schema that defines a serialized object. Parameters: Name Type Description Default schema |  | The core schema. required Returns: Type Description | None The generated JSON schema. complex_schema ¶ complex_schema(schema: ) -> Generates a JSON schema that matches a complex number. JSON has no standard way to represent complex numbers. Complex number is not a numeric\ntype. Here we represent complex number as strings following the rule defined by Python.\nFor instance, '1+2j' is an accepted complex string. Details can be found in\n. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. get_title_from_name ¶ get_title_from_name(name: ) -> Retrieves a title from a name. Parameters: Name Type Description Default name The name to retrieve a title from. required Returns: Type Description The title. field_title_should_be_set ¶ field_title_should_be_set(\n    schema: ,\n) -> Returns true if a field with the given schema should have a title set based on the field name. Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title\n(e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses). Parameters: Name Type Description Default schema The schema to check. required Returns: Type Description True if the field should have a title set, False otherwise. normalize_name ¶ normalize_name(name: ) -> Normalizes a name to be used as a key in a dictionary. Parameters: Name Type Description Default name The name to normalize. required Returns: Type Description The normalized name. get_defs_ref ¶ get_defs_ref(core_mode_ref: ) -> Override this method to change the way that definitions keys are generated from a core reference. Parameters: Name Type Description Default core_mode_ref The core reference. required Returns: Type Description The definitions key. get_cache_defs_ref_schema ¶ get_cache_defs_ref_schema(\n    core_ref: ,\n) -> [, ] This method wraps the get_defs_ref method with some cache-lookup/population logic,\nand returns both the produced defs_ref and the JSON schema that will refer to the right definition. Parameters: Name Type Description Default core_ref The core reference to get the definitions reference for. required Returns: Type Description [, ] A tuple of the definitions reference and the JSON schema that will refer to it. handle_ref_overrides ¶ handle_ref_overrides(\n    json_schema: ,\n) -> Remove any sibling keys that are redundant with the referenced schema. Parameters: Name Type Description Default json_schema The schema to remove redundant sibling keys from. required Returns: Type Description The schema with redundant sibling keys removed. encode_default ¶ encode_default(dft: ) -> Encode a default value to a JSON-serializable value. This is used to encode default values for fields in the generated JSON schema. Parameters: Name Type Description Default dft The default value to encode. required Returns: Type Description The encoded default value. update_with_validations ¶ update_with_validations(\n    json_schema: ,\n    core_schema: ,\n    mapping: [, ],\n) -> None Update the json_schema with the corresponding validations specified in the core_schema,\nusing the provided mapping to translate keys in core_schema to the appropriate keys for a JSON schema. Parameters: Name Type Description Default json_schema The JSON schema to update. required core_schema The core schema to get the validations from. required mapping [, ] A mapping from core_schema attribute names to the corresponding JSON schema attribute names. required get_json_ref_counts ¶ get_json_ref_counts(\n    json_schema: ,\n) -> [, ] Get all values corresponding to the key '$ref' anywhere in the json_schema. emit_warning ¶ emit_warning(\n    kind: , detail: \n) -> None This method simply emits PydanticJsonSchemaWarnings based on handling in the warning_message method. render_warning_message ¶ render_warning_message(\n    kind: , detail: \n) ->  | None This method is responsible for ignoring warnings as desired, and for formatting the warning messages. You can override the value of ignored_warning_kinds in a subclass of GenerateJsonSchema\nto modify what warnings are generated. If you want more control, you can override this method;\njust return None in situations where you don't want warnings to be emitted. Parameters: Name Type Description Default kind The kind of warning to render. It can be one of the following: 'skipped-choice': A choice field was skipped because it had no valid choices. 'non-serializable-default': A default value was skipped because it was not JSON-serializable. required detail A string with additional details about the warning. required Returns: Type Description | None The formatted warning message, or None if no warning should be emitted. WithJsonSchema dataclass ¶ WithJsonSchema(\n    json_schema:  | None,\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Usage Documentation WithJsonSchema Annotation Add this as an annotation on a field to override the (base) JSON schema that would be generated for that field.\nThis provides a way to set a JSON schema for types that would otherwise raise errors when producing a JSON schema,\nsuch as Callable, or types that have an is-instance core schema, without needing to go so far as creating a\ncustom subclass of pydantic.json_schema.GenerateJsonSchema.\nNote that any modifications to the schema that would normally be made (such as setting the title for model fields)\nwill still be performed. If mode is set this will only apply to that schema generation mode, allowing you\nto set different json schemas for validation and serialization. Examples ¶ Examples(\n    examples: [, ],\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Examples(\n    examples: [],\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Examples(\n    examples: [, ] | [],\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Add examples to a JSON schema. If the JSON Schema already contains examples, the provided examples\nwill be appended. If mode is set this will only apply to that schema generation mode,\nallowing you to add different examples for validation and serialization. SkipJsonSchema dataclass ¶ SkipJsonSchema() Usage Documentation SkipJsonSchema Annotation Add this as an annotation on a field to skip generating a JSON schema for that field. model_json_schema ¶ model_json_schema(\n    cls: [] | [],\n    by_alias:  = True,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n    mode:  = \"validation\",\n) -> [, ] Utility function to generate a JSON Schema for a model. Parameters: Name Type Description Default cls [] | [] The model class to generate a JSON Schema for. required by_alias If True (the default), fields will be serialized according to their alias.\nIf False , fields will be serialized according to their attribute name. True ref_template The template to use for generating JSON Schema references. schema_generator [] The class to use for generating the JSON Schema. mode The mode to use for generating the JSON Schema. It can be one of the following: 'validation': Generate a JSON Schema for validating data. 'serialization': Generate a JSON Schema for serializing data. 'validation' Returns: Type Description [, ] The generated JSON Schema. models_json_schema ¶ models_json_schema(\n    models: [\n        [\n            [] | [],\n            ,\n        ]\n    ],\n    *,\n    by_alias:  = True,\n    title:  | None = None,\n    description:  | None = None,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = \n) -> [\n    [\n        [\n            [] | [],\n            ,\n        ],\n        ,\n    ],\n    ,\n] Utility function to generate a JSON Schema for multiple models. Parameters: Name Type Description Default models [[[] | [], ]] A sequence of tuples of the form (model, mode). required by_alias Whether field aliases should be used as keys in the generated JSON Schema. True title | None The title of the generated JSON Schema. None description | None The description of the generated JSON Schema. None ref_template The reference template to use for generating JSON Schema references. schema_generator [] The schema generator to use for generating the JSON Schema. Returns: Type Description [[[[] | [], ], ], ] A tuple where:\n- The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n    JsonRef references to definitions that are defined in the second returned element.)\n- The second element is a JSON schema containing all definitions referenced in the first returned\n        element, along with the optional title and description keys.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#JSON Schema","title":"JSON Schema","objectID":"/latest/api/json_schema/#JSON Schema","rank":100},{"content":"CoreSchemaOrFieldType = [\n    , \n] A type alias for defined schema types that represents a union of core_schema.CoreSchemaType and core_schema.CoreSchemaFieldType .","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.CoreSchemaOrFieldType","title":"JSON Schema - CoreSchemaOrFieldType  module-attribute","objectID":"/latest/api/json_schema/#pydantic.json_schema.CoreSchemaOrFieldType","rank":95},{"content":"JsonSchemaValue = [, ] A type alias for a JSON schema value. This is a dictionary of string keys to arbitrary JSON values.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.JsonSchemaValue","title":"JSON Schema - JsonSchemaValue  module-attribute","objectID":"/latest/api/json_schema/#pydantic.json_schema.JsonSchemaValue","rank":90},{"content":"JsonSchemaMode = ['validation', 'serialization'] A type alias that represents the mode of a JSON schema; either 'validation' or 'serialization'. For some types, the inputs to validation differ from the outputs of serialization. For example,\ncomputed fields will only be present when serializing, and should not be provided when\nvalidating. This flag provides a way to indicate whether you want the JSON schema required\nfor validation inputs, or that will be matched by serialization outputs.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode","title":"JSON Schema - JsonSchemaMode  module-attribute","objectID":"/latest/api/json_schema/#pydantic.json_schema.JsonSchemaMode","rank":85},{"content":"JsonSchemaWarningKind = [\n    \"skipped-choice\",\n    \"non-serializable-default\",\n    \"skipped-discriminator\",\n] A type alias representing the kinds of warnings that can be emitted during JSON schema generation. See \nfor more details.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.JsonSchemaWarningKind","title":"JSON Schema - JsonSchemaWarningKind  module-attribute","objectID":"/latest/api/json_schema/#pydantic.json_schema.JsonSchemaWarningKind","rank":80},{"content":"NoDefault = () A sentinel value used to indicate that no default value should be used when generating a JSON Schema\nfor a core schema with a default value.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.NoDefault","title":"JSON Schema - NoDefault  module-attribute","objectID":"/latest/api/json_schema/#pydantic.json_schema.NoDefault","rank":75},{"content":"DEFAULT_REF_TEMPLATE = '#/$defs/{model}' The default format string used to generate reference names.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.DEFAULT_REF_TEMPLATE","title":"JSON Schema - DEFAULT_REF_TEMPLATE  module-attribute","objectID":"/latest/api/json_schema/#pydantic.json_schema.DEFAULT_REF_TEMPLATE","rank":70},{"content":"Bases: This class is used to emit warnings produced during JSON schema generation.\nSee the  and\n\nmethods for more details; these can be overridden to control warning behavior.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.PydanticJsonSchemaWarning","title":"JSON Schema - PydanticJsonSchemaWarning","objectID":"/latest/api/json_schema/#pydantic.json_schema.PydanticJsonSchemaWarning","rank":65},{"content":"GenerateJsonSchema(\n    by_alias:  = True,\n    ref_template:  = ,\n) Usage Documentation Customizing the JSON Schema Generation Process A class for generating JSON schemas. This class generates JSON schemas based on configured parameters. The default schema dialect\nis https://json-schema.org/draft/2020-12/schema .\nThe class uses by_alias to configure how fields with\nmultiple names are handled and ref_template to format reference names. Attributes: Name Type Description The JSON schema dialect used to generate the schema. See Declaring a Dialect in the JSON Schema documentation for more information about dialects. [] Warnings to ignore when generating the schema. self.render_warning_message will\ndo nothing if its argument kind is in ignored_warning_kinds ;\nthis value can be modified on subclasses to easily control which warnings are emitted. Whether to use field aliases when generating the schema. The format string used when generating reference names. [, ] A mapping of core refs to JSON refs. [, ] A mapping of core refs to definition refs. [, ] A mapping of definition refs to core refs. [, ] A mapping of JSON refs to definition refs. [, ] Definitions in the schema. Parameters: Name Type Description Default by_alias Whether to use field aliases in the generated schemas. True ref_template The format string to use when generating reference names. Raises: Type Description If the instance of the class is inadvertently reused after generating a schema. ValidationsMapping ¶ This class just contains mappings from core_schema attribute names to the corresponding\nJSON schema attribute names. While I suspect it is unlikely to be necessary, you can in\nprinciple override this class in a subclass of GenerateJsonSchema (by inheriting from\nGenerateJsonSchema.ValidationsMapping) to change these mappings. build_schema_type_to_method ¶ build_schema_type_to_method() -> [\n    ,\n    [[], ],\n] Builds a dictionary mapping fields to methods for generating JSON schemas. Returns: Type Description [, [[], ]] A dictionary containing the mapping of CoreSchemaOrFieldType to a handler method. Raises: Type Description If no method has been defined for generating a JSON schema for a given pydantic core schema type. generate_definitions ¶ generate_definitions(\n    inputs: [\n        [, , ]\n    ]\n) -> [\n    [\n        [, ],\n        ,\n    ],\n    [, ],\n] Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a\nmapping that links the input keys to the definition references. Parameters: Name Type Description Default inputs [[, , ]] A sequence of tuples, where: The first element is a JSON schema key type. The second element is the JSON mode: either 'validation' or 'serialization'. The third element is a core schema. required Returns: Type Description [[[, ], ], [, ]] A tuple where: The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n    JsonRef references to definitions that are defined in the second returned element.) The second element is a dictionary whose keys are definition references for the JSON schemas\n    from the first returned element, and whose values are the actual JSON schema definitions. Raises: Type Description Raised if the JSON schema generator has already been used to generate a JSON schema. generate ¶ generate(\n    schema: , mode:  = \"validation\"\n) -> Generates a JSON schema for a specified schema in a specified mode. Parameters: Name Type Description Default schema A Pydantic model. required mode The mode in which to generate the schema. Defaults to 'validation'. 'validation' Returns: Type Description A JSON schema representing the specified schema. Raises: Type Description If the JSON schema generator has already been used to generate a JSON schema. generate_inner ¶ generate_inner(\n    schema: ,\n) -> Generates a JSON schema for a given core schema. Parameters: Name Type Description Default schema The given core schema. required Returns: Type Description The generated JSON schema. TODO: the nested function definitions here seem like bad practice, I'd like to unpack these\nin a future PR. It'd be great if we could shorten the call stack a bit for JSON schema generation,\nand I think there's potential for that here. sort ¶ sort(\n    value: , parent_key:  | None = None\n) -> Override this method to customize the sorting of the JSON schema (e.g., don't sort at all, sort all keys unconditionally, etc.) By default, alphabetically sort the keys in the JSON schema, skipping the 'properties' and 'default' keys to preserve field definition order.\nThis sort is recursive, so it will sort all nested dictionaries as well. invalid_schema ¶ invalid_schema(schema: ) -> Placeholder - should never be called. any_schema ¶ any_schema(schema: ) -> Generates a JSON schema that matches any value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. none_schema ¶ none_schema(schema: ) -> Generates a JSON schema that matches None . Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. bool_schema ¶ bool_schema(schema: ) -> Generates a JSON schema that matches a bool value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. int_schema ¶ int_schema(schema: ) -> Generates a JSON schema that matches an int value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. float_schema ¶ float_schema(schema: ) -> Generates a JSON schema that matches a float value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. decimal_schema ¶ decimal_schema(schema: ) -> Generates a JSON schema that matches a decimal value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. str_schema ¶ str_schema(schema: ) -> Generates a JSON schema that matches a string value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. bytes_schema ¶ bytes_schema(schema: ) -> Generates a JSON schema that matches a bytes value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. date_schema ¶ date_schema(schema: ) -> Generates a JSON schema that matches a date value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. time_schema ¶ time_schema(schema: ) -> Generates a JSON schema that matches a time value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. datetime_schema ¶ datetime_schema(schema: ) -> Generates a JSON schema that matches a datetime value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. timedelta_schema ¶ timedelta_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a timedelta value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. literal_schema ¶ literal_schema(schema: ) -> Generates a JSON schema that matches a literal value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. missing_sentinel_schema ¶ missing_sentinel_schema(\n    schema: ,\n) -> Generates a JSON schema that matches the MISSING sentinel value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. enum_schema ¶ enum_schema(schema: ) -> Generates a JSON schema that matches an Enum value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. is_instance_schema ¶ is_instance_schema(\n    schema: ,\n) -> Handles JSON schema generation for a core schema that checks if a value is an instance of a class. Unless overridden in a subclass, this raises an error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. is_subclass_schema ¶ is_subclass_schema(\n    schema: ,\n) -> Handles JSON schema generation for a core schema that checks if a value is a subclass of a class. For backwards compatibility with v1, this does not raise an error, but can be overridden to change this. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. callable_schema ¶ callable_schema(schema: ) -> Generates a JSON schema that matches a callable value. Unless overridden in a subclass, this raises an error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. list_schema ¶ list_schema(schema: ) -> Returns a schema that matches a list schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. tuple_positional_schema ¶ tuple_positional_schema(\n    schema: ,\n) -> Replaced by tuple_schema . tuple_variable_schema ¶ tuple_variable_schema(\n    schema: ,\n) -> Replaced by tuple_schema . tuple_schema ¶ tuple_schema(schema: ) -> Generates a JSON schema that matches a tuple schema e.g. tuple[int,\nstr, bool] or tuple[int, ...] . Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. set_schema ¶ set_schema(schema: ) -> Generates a JSON schema that matches a set schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. frozenset_schema ¶ frozenset_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a frozenset schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. generator_schema ¶ generator_schema(\n    schema: ,\n) -> Returns a JSON schema that represents the provided GeneratorSchema. Parameters: Name Type Description Default schema The schema. required Returns: Type Description The generated JSON schema. dict_schema ¶ dict_schema(schema: ) -> Generates a JSON schema that matches a dict schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_before_schema ¶ function_before_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-before schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_after_schema ¶ function_after_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-after schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_plain_schema ¶ function_plain_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-plain schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. function_wrap_schema ¶ function_wrap_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-wrap schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. default_schema ¶ default_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema with a default value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. get_default_value ¶ get_default_value(schema: ) -> Get the default value to be used when generating a JSON Schema for a core schema with a default. The default implementation is to use the statically defined default value. This method can be overridden\nif you want to make use of the default factory. Parameters: Name Type Description Default schema The 'with-default' core schema. required Returns: Type Description The default value to use, or  if no default\nvalue is available. nullable_schema ¶ nullable_schema(schema: ) -> Generates a JSON schema that matches a schema that allows null values. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. union_schema ¶ union_schema(schema: ) -> Generates a JSON schema that matches a schema that allows values matching any of the given schemas. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. tagged_union_schema ¶ tagged_union_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where\nthe schemas are tagged with a discriminator field that indicates which schema should be used to validate\nthe value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. chain_schema ¶ chain_schema(schema: ) -> Generates a JSON schema that matches a core_schema.ChainSchema. When generating a schema for validation, we return the validation JSON schema for the first step in the chain.\nFor serialization, we return the serialization JSON schema for the last step in the chain. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. lax_or_strict_schema ¶ lax_or_strict_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching either the lax schema or the\nstrict schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. json_or_python_schema ¶ json_or_python_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the\nPython schema. The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override\nthis method. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. typed_dict_schema ¶ typed_dict_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a typed dict. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. typed_dict_field_schema ¶ typed_dict_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a typed dict field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. dataclass_field_schema ¶ dataclass_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. model_field_schema ¶ model_field_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a model field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. computed_field_schema ¶ computed_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a computed field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. model_schema ¶ model_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a model. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. resolve_ref_schema ¶ resolve_ref_schema(\n    json_schema: ,\n) -> Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema. Parameters: Name Type Description Default json_schema The schema to resolve. required Returns: Type Description The resolved schema. Raises: Type Description If the schema reference can't be found in definitions. model_fields_schema ¶ model_fields_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a model's fields. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. field_is_present ¶ field_is_present(field: ) -> Whether the field should be included in the generated JSON schema. Parameters: Name Type Description Default field The schema for the field itself. required Returns: Type Description True if the field should be included in the generated JSON schema, False otherwise. field_is_required ¶ field_is_required(\n    field:  |  | ,\n    total: ,\n) -> Whether the field should be marked as required in the generated JSON schema.\n(Note that this is irrelevant if the field is not present in the JSON schema.). Parameters: Name Type Description Default field |  | The schema for the field itself. required total Only applies to TypedDictField s.\nIndicates if the TypedDict this field belongs to is total, in which case any fields that don't\nexplicitly specify required=False are required. required Returns: Type Description True if the field should be marked as required in the generated JSON schema, False otherwise. dataclass_args_schema ¶ dataclass_args_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. dataclass_schema ¶ dataclass_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. arguments_schema ¶ arguments_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a function's arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. kw_arguments_schema ¶ kw_arguments_schema(\n    arguments: [],\n    var_kwargs_schema:  | None,\n) -> Generates a JSON schema that matches a schema that defines a function's keyword arguments. Parameters: Name Type Description Default arguments [] The core schema. required Returns: Type Description The generated JSON schema. p_arguments_schema ¶ p_arguments_schema(\n    arguments: [],\n    var_args_schema:  | None,\n) -> Generates a JSON schema that matches a schema that defines a function's positional arguments. Parameters: Name Type Description Default arguments [] The core schema. required Returns: Type Description The generated JSON schema. get_argument_name ¶ get_argument_name(\n    argument:  | ,\n) -> Retrieves the name of an argument. Parameters: Name Type Description Default argument | The core schema. required Returns: Type Description The name of the argument. arguments_v3_schema ¶ arguments_v3_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a function's arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. call_schema ¶ call_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a function call. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. custom_error_schema ¶ custom_error_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a custom error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. json_schema ¶ json_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a JSON object. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. url_schema ¶ url_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a URL. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. multi_host_url_schema ¶ multi_host_url_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. uuid_schema ¶ uuid_schema(schema: ) -> Generates a JSON schema that matches a UUID. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. definitions_schema ¶ definitions_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a JSON object with definitions. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. definition_ref_schema ¶ definition_ref_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that references a definition. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. ser_schema ¶ ser_schema(\n    schema: (\n         |  | \n    ),\n) ->  | None Generates a JSON schema that matches a schema that defines a serialized object. Parameters: Name Type Description Default schema |  | The core schema. required Returns: Type Description | None The generated JSON schema. complex_schema ¶ complex_schema(schema: ) -> Generates a JSON schema that matches a complex number. JSON has no standard way to represent complex numbers. Complex number is not a numeric\ntype. Here we represent complex number as strings following the rule defined by Python.\nFor instance, '1+2j' is an accepted complex string. Details can be found in\n. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema. get_title_from_name ¶ get_title_from_name(name: ) -> Retrieves a title from a name. Parameters: Name Type Description Default name The name to retrieve a title from. required Returns: Type Description The title. field_title_should_be_set ¶ field_title_should_be_set(\n    schema: ,\n) -> Returns true if a field with the given schema should have a title set based on the field name. Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title\n(e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses). Parameters: Name Type Description Default schema The schema to check. required Returns: Type Description True if the field should have a title set, False otherwise. normalize_name ¶ normalize_name(name: ) -> Normalizes a name to be used as a key in a dictionary. Parameters: Name Type Description Default name The name to normalize. required Returns: Type Description The normalized name. get_defs_ref ¶ get_defs_ref(core_mode_ref: ) -> Override this method to change the way that definitions keys are generated from a core reference. Parameters: Name Type Description Default core_mode_ref The core reference. required Returns: Type Description The definitions key. get_cache_defs_ref_schema ¶ get_cache_defs_ref_schema(\n    core_ref: ,\n) -> [, ] This method wraps the get_defs_ref method with some cache-lookup/population logic,\nand returns both the produced defs_ref and the JSON schema that will refer to the right definition. Parameters: Name Type Description Default core_ref The core reference to get the definitions reference for. required Returns: Type Description [, ] A tuple of the definitions reference and the JSON schema that will refer to it. handle_ref_overrides ¶ handle_ref_overrides(\n    json_schema: ,\n) -> Remove any sibling keys that are redundant with the referenced schema. Parameters: Name Type Description Default json_schema The schema to remove redundant sibling keys from. required Returns: Type Description The schema with redundant sibling keys removed. encode_default ¶ encode_default(dft: ) -> Encode a default value to a JSON-serializable value. This is used to encode default values for fields in the generated JSON schema. Parameters: Name Type Description Default dft The default value to encode. required Returns: Type Description The encoded default value. update_with_validations ¶ update_with_validations(\n    json_schema: ,\n    core_schema: ,\n    mapping: [, ],\n) -> None Update the json_schema with the corresponding validations specified in the core_schema,\nusing the provided mapping to translate keys in core_schema to the appropriate keys for a JSON schema. Parameters: Name Type Description Default json_schema The JSON schema to update. required core_schema The core schema to get the validations from. required mapping [, ] A mapping from core_schema attribute names to the corresponding JSON schema attribute names. required get_json_ref_counts ¶ get_json_ref_counts(\n    json_schema: ,\n) -> [, ] Get all values corresponding to the key '$ref' anywhere in the json_schema. emit_warning ¶ emit_warning(\n    kind: , detail: \n) -> None This method simply emits PydanticJsonSchemaWarnings based on handling in the warning_message method. render_warning_message ¶ render_warning_message(\n    kind: , detail: \n) ->  | None This method is responsible for ignoring warnings as desired, and for formatting the warning messages. You can override the value of ignored_warning_kinds in a subclass of GenerateJsonSchema\nto modify what warnings are generated. If you want more control, you can override this method;\njust return None in situations where you don't want warnings to be emitted. Parameters: Name Type Description Default kind The kind of warning to render. It can be one of the following: 'skipped-choice': A choice field was skipped because it had no valid choices. 'non-serializable-default': A default value was skipped because it was not JSON-serializable. required detail A string with additional details about the warning. required Returns: Type Description | None The formatted warning message, or None if no warning should be emitted.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema","title":"JSON Schema - GenerateJsonSchema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema","rank":60},{"content":"This class just contains mappings from core_schema attribute names to the corresponding\nJSON schema attribute names. While I suspect it is unlikely to be necessary, you can in\nprinciple override this class in a subclass of GenerateJsonSchema (by inheriting from\nGenerateJsonSchema.ValidationsMapping) to change these mappings.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.ValidationsMapping","title":"JSON Schema - GenerateJsonSchema - ValidationsMapping","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.ValidationsMapping","rank":55},{"content":"build_schema_type_to_method() -> [\n    ,\n    [[], ],\n] Builds a dictionary mapping fields to methods for generating JSON schemas. Returns: Type Description [, [[], ]] A dictionary containing the mapping of CoreSchemaOrFieldType to a handler method. Raises: Type Description If no method has been defined for generating a JSON schema for a given pydantic core schema type.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.build_schema_type_to_method","title":"JSON Schema - GenerateJsonSchema - build_schema_type_to_method","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.build_schema_type_to_method","rank":50},{"content":"generate_definitions(\n    inputs: [\n        [, , ]\n    ]\n) -> [\n    [\n        [, ],\n        ,\n    ],\n    [, ],\n] Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a\nmapping that links the input keys to the definition references. Parameters: Name Type Description Default inputs [[, , ]] A sequence of tuples, where: The first element is a JSON schema key type. The second element is the JSON mode: either 'validation' or 'serialization'. The third element is a core schema. required Returns: Type Description [[[, ], ], [, ]] A tuple where: The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n    JsonRef references to definitions that are defined in the second returned element.) The second element is a dictionary whose keys are definition references for the JSON schemas\n    from the first returned element, and whose values are the actual JSON schema definitions. Raises: Type Description Raised if the JSON schema generator has already been used to generate a JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generate_definitions","title":"JSON Schema - GenerateJsonSchema - generate_definitions","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generate_definitions","rank":45},{"content":"generate(\n    schema: , mode:  = \"validation\"\n) -> Generates a JSON schema for a specified schema in a specified mode. Parameters: Name Type Description Default schema A Pydantic model. required mode The mode in which to generate the schema. Defaults to 'validation'. 'validation' Returns: Type Description A JSON schema representing the specified schema. Raises: Type Description If the JSON schema generator has already been used to generate a JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generate","title":"JSON Schema - GenerateJsonSchema - generate","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generate","rank":40},{"content":"generate_inner(\n    schema: ,\n) -> Generates a JSON schema for a given core schema. Parameters: Name Type Description Default schema The given core schema. required Returns: Type Description The generated JSON schema. TODO: the nested function definitions here seem like bad practice, I'd like to unpack these\nin a future PR. It'd be great if we could shorten the call stack a bit for JSON schema generation,\nand I think there's potential for that here.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generate_inner","title":"JSON Schema - GenerateJsonSchema - generate_inner","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generate_inner","rank":35},{"content":"sort(\n    value: , parent_key:  | None = None\n) -> Override this method to customize the sorting of the JSON schema (e.g., don't sort at all, sort all keys unconditionally, etc.) By default, alphabetically sort the keys in the JSON schema, skipping the 'properties' and 'default' keys to preserve field definition order.\nThis sort is recursive, so it will sort all nested dictionaries as well.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.sort","title":"JSON Schema - GenerateJsonSchema - sort","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.sort","rank":30},{"content":"invalid_schema(schema: ) -> Placeholder - should never be called.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.invalid_schema","title":"JSON Schema - GenerateJsonSchema - invalid_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.invalid_schema","rank":25},{"content":"any_schema(schema: ) -> Generates a JSON schema that matches any value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.any_schema","title":"JSON Schema - GenerateJsonSchema - any_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.any_schema","rank":20},{"content":"none_schema(schema: ) -> Generates a JSON schema that matches None . Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.none_schema","title":"JSON Schema - GenerateJsonSchema - none_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.none_schema","rank":15},{"content":"bool_schema(schema: ) -> Generates a JSON schema that matches a bool value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.bool_schema","title":"JSON Schema - GenerateJsonSchema - bool_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.bool_schema","rank":10},{"content":"int_schema(schema: ) -> Generates a JSON schema that matches an int value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.int_schema","title":"JSON Schema - GenerateJsonSchema - int_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.int_schema","rank":5},{"content":"float_schema(schema: ) -> Generates a JSON schema that matches a float value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.float_schema","title":"JSON Schema - GenerateJsonSchema - float_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.float_schema","rank":0},{"content":"decimal_schema(schema: ) -> Generates a JSON schema that matches a decimal value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.decimal_schema","title":"JSON Schema - GenerateJsonSchema - decimal_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.decimal_schema","rank":-5},{"content":"str_schema(schema: ) -> Generates a JSON schema that matches a string value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.str_schema","title":"JSON Schema - GenerateJsonSchema - str_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.str_schema","rank":-10},{"content":"bytes_schema(schema: ) -> Generates a JSON schema that matches a bytes value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.bytes_schema","title":"JSON Schema - GenerateJsonSchema - bytes_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.bytes_schema","rank":-15},{"content":"date_schema(schema: ) -> Generates a JSON schema that matches a date value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.date_schema","title":"JSON Schema - GenerateJsonSchema - date_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.date_schema","rank":-20},{"content":"time_schema(schema: ) -> Generates a JSON schema that matches a time value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.time_schema","title":"JSON Schema - GenerateJsonSchema - time_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.time_schema","rank":-25},{"content":"datetime_schema(schema: ) -> Generates a JSON schema that matches a datetime value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.datetime_schema","title":"JSON Schema - GenerateJsonSchema - datetime_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.datetime_schema","rank":-30},{"content":"timedelta_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a timedelta value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.timedelta_schema","title":"JSON Schema - GenerateJsonSchema - timedelta_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.timedelta_schema","rank":-35},{"content":"literal_schema(schema: ) -> Generates a JSON schema that matches a literal value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.literal_schema","title":"JSON Schema - GenerateJsonSchema - literal_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.literal_schema","rank":-40},{"content":"missing_sentinel_schema(\n    schema: ,\n) -> Generates a JSON schema that matches the MISSING sentinel value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.missing_sentinel_schema","title":"JSON Schema - GenerateJsonSchema - missing_sentinel_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.missing_sentinel_schema","rank":-45},{"content":"enum_schema(schema: ) -> Generates a JSON schema that matches an Enum value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.enum_schema","title":"JSON Schema - GenerateJsonSchema - enum_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.enum_schema","rank":-50},{"content":"is_instance_schema(\n    schema: ,\n) -> Handles JSON schema generation for a core schema that checks if a value is an instance of a class. Unless overridden in a subclass, this raises an error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.is_instance_schema","title":"JSON Schema - GenerateJsonSchema - is_instance_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.is_instance_schema","rank":-55},{"content":"is_subclass_schema(\n    schema: ,\n) -> Handles JSON schema generation for a core schema that checks if a value is a subclass of a class. For backwards compatibility with v1, this does not raise an error, but can be overridden to change this. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.is_subclass_schema","title":"JSON Schema - GenerateJsonSchema - is_subclass_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.is_subclass_schema","rank":-60},{"content":"callable_schema(schema: ) -> Generates a JSON schema that matches a callable value. Unless overridden in a subclass, this raises an error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.callable_schema","title":"JSON Schema - GenerateJsonSchema - callable_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.callable_schema","rank":-65},{"content":"list_schema(schema: ) -> Returns a schema that matches a list schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.list_schema","title":"JSON Schema - GenerateJsonSchema - list_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.list_schema","rank":-70},{"content":"tuple_positional_schema(\n    schema: ,\n) -> Replaced by tuple_schema .","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tuple_positional_schema","title":"JSON Schema - GenerateJsonSchema - tuple_positional_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tuple_positional_schema","rank":-75},{"content":"tuple_variable_schema(\n    schema: ,\n) -> Replaced by tuple_schema .","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tuple_variable_schema","title":"JSON Schema - GenerateJsonSchema - tuple_variable_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tuple_variable_schema","rank":-80},{"content":"tuple_schema(schema: ) -> Generates a JSON schema that matches a tuple schema e.g. tuple[int,\nstr, bool] or tuple[int, ...] . Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tuple_schema","title":"JSON Schema - GenerateJsonSchema - tuple_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tuple_schema","rank":-85},{"content":"set_schema(schema: ) -> Generates a JSON schema that matches a set schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.set_schema","title":"JSON Schema - GenerateJsonSchema - set_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.set_schema","rank":-90},{"content":"frozenset_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a frozenset schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.frozenset_schema","title":"JSON Schema - GenerateJsonSchema - frozenset_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.frozenset_schema","rank":-95},{"content":"generator_schema(\n    schema: ,\n) -> Returns a JSON schema that represents the provided GeneratorSchema. Parameters: Name Type Description Default schema The schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generator_schema","title":"JSON Schema - GenerateJsonSchema - generator_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.generator_schema","rank":-100},{"content":"dict_schema(schema: ) -> Generates a JSON schema that matches a dict schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dict_schema","title":"JSON Schema - GenerateJsonSchema - dict_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dict_schema","rank":-105},{"content":"function_before_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-before schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_before_schema","title":"JSON Schema - GenerateJsonSchema - function_before_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_before_schema","rank":-110},{"content":"function_after_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-after schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_after_schema","title":"JSON Schema - GenerateJsonSchema - function_after_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_after_schema","rank":-115},{"content":"function_plain_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-plain schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_plain_schema","title":"JSON Schema - GenerateJsonSchema - function_plain_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_plain_schema","rank":-120},{"content":"function_wrap_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a function-wrap schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_wrap_schema","title":"JSON Schema - GenerateJsonSchema - function_wrap_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.function_wrap_schema","rank":-125},{"content":"default_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema with a default value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.default_schema","title":"JSON Schema - GenerateJsonSchema - default_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.default_schema","rank":-130},{"content":"get_default_value(schema: ) -> Get the default value to be used when generating a JSON Schema for a core schema with a default. The default implementation is to use the statically defined default value. This method can be overridden\nif you want to make use of the default factory. Parameters: Name Type Description Default schema The 'with-default' core schema. required Returns: Type Description The default value to use, or  if no default\nvalue is available.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_default_value","title":"JSON Schema - GenerateJsonSchema - get_default_value","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_default_value","rank":-135},{"content":"nullable_schema(schema: ) -> Generates a JSON schema that matches a schema that allows null values. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.nullable_schema","title":"JSON Schema - GenerateJsonSchema - nullable_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.nullable_schema","rank":-140},{"content":"union_schema(schema: ) -> Generates a JSON schema that matches a schema that allows values matching any of the given schemas. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.union_schema","title":"JSON Schema - GenerateJsonSchema - union_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.union_schema","rank":-145},{"content":"tagged_union_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where\nthe schemas are tagged with a discriminator field that indicates which schema should be used to validate\nthe value. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tagged_union_schema","title":"JSON Schema - GenerateJsonSchema - tagged_union_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.tagged_union_schema","rank":-150},{"content":"chain_schema(schema: ) -> Generates a JSON schema that matches a core_schema.ChainSchema. When generating a schema for validation, we return the validation JSON schema for the first step in the chain.\nFor serialization, we return the serialization JSON schema for the last step in the chain. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.chain_schema","title":"JSON Schema - GenerateJsonSchema - chain_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.chain_schema","rank":-155},{"content":"lax_or_strict_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching either the lax schema or the\nstrict schema. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.lax_or_strict_schema","title":"JSON Schema - GenerateJsonSchema - lax_or_strict_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.lax_or_strict_schema","rank":-160},{"content":"json_or_python_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the\nPython schema. The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override\nthis method. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.json_or_python_schema","title":"JSON Schema - GenerateJsonSchema - json_or_python_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.json_or_python_schema","rank":-165},{"content":"typed_dict_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a typed dict. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.typed_dict_schema","title":"JSON Schema - GenerateJsonSchema - typed_dict_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.typed_dict_schema","rank":-170},{"content":"typed_dict_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a typed dict field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.typed_dict_field_schema","title":"JSON Schema - GenerateJsonSchema - typed_dict_field_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.typed_dict_field_schema","rank":-175},{"content":"dataclass_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dataclass_field_schema","title":"JSON Schema - GenerateJsonSchema - dataclass_field_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dataclass_field_schema","rank":-180},{"content":"model_field_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a model field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.model_field_schema","title":"JSON Schema - GenerateJsonSchema - model_field_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.model_field_schema","rank":-185},{"content":"computed_field_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a computed field. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.computed_field_schema","title":"JSON Schema - GenerateJsonSchema - computed_field_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.computed_field_schema","rank":-190},{"content":"model_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a model. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.model_schema","title":"JSON Schema - GenerateJsonSchema - model_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.model_schema","rank":-195},{"content":"resolve_ref_schema(\n    json_schema: ,\n) -> Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema. Parameters: Name Type Description Default json_schema The schema to resolve. required Returns: Type Description The resolved schema. Raises: Type Description If the schema reference can't be found in definitions.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.resolve_ref_schema","title":"JSON Schema - GenerateJsonSchema - resolve_ref_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.resolve_ref_schema","rank":-200},{"content":"model_fields_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a model's fields. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.model_fields_schema","title":"JSON Schema - GenerateJsonSchema - model_fields_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.model_fields_schema","rank":-205},{"content":"field_is_present(field: ) -> Whether the field should be included in the generated JSON schema. Parameters: Name Type Description Default field The schema for the field itself. required Returns: Type Description True if the field should be included in the generated JSON schema, False otherwise.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.field_is_present","title":"JSON Schema - GenerateJsonSchema - field_is_present","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.field_is_present","rank":-210},{"content":"field_is_required(\n    field:  |  | ,\n    total: ,\n) -> Whether the field should be marked as required in the generated JSON schema.\n(Note that this is irrelevant if the field is not present in the JSON schema.). Parameters: Name Type Description Default field |  | The schema for the field itself. required total Only applies to TypedDictField s.\nIndicates if the TypedDict this field belongs to is total, in which case any fields that don't\nexplicitly specify required=False are required. required Returns: Type Description True if the field should be marked as required in the generated JSON schema, False otherwise.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.field_is_required","title":"JSON Schema - GenerateJsonSchema - field_is_required","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.field_is_required","rank":-215},{"content":"dataclass_args_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dataclass_args_schema","title":"JSON Schema - GenerateJsonSchema - dataclass_args_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dataclass_args_schema","rank":-220},{"content":"dataclass_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a dataclass. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dataclass_schema","title":"JSON Schema - GenerateJsonSchema - dataclass_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.dataclass_schema","rank":-225},{"content":"arguments_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a function's arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.arguments_schema","title":"JSON Schema - GenerateJsonSchema - arguments_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.arguments_schema","rank":-230},{"content":"kw_arguments_schema(\n    arguments: [],\n    var_kwargs_schema:  | None,\n) -> Generates a JSON schema that matches a schema that defines a function's keyword arguments. Parameters: Name Type Description Default arguments [] The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.kw_arguments_schema","title":"JSON Schema - GenerateJsonSchema - kw_arguments_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.kw_arguments_schema","rank":-235},{"content":"p_arguments_schema(\n    arguments: [],\n    var_args_schema:  | None,\n) -> Generates a JSON schema that matches a schema that defines a function's positional arguments. Parameters: Name Type Description Default arguments [] The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.p_arguments_schema","title":"JSON Schema - GenerateJsonSchema - p_arguments_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.p_arguments_schema","rank":-240},{"content":"get_argument_name(\n    argument:  | ,\n) -> Retrieves the name of an argument. Parameters: Name Type Description Default argument | The core schema. required Returns: Type Description The name of the argument.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_argument_name","title":"JSON Schema - GenerateJsonSchema - get_argument_name","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_argument_name","rank":-245},{"content":"arguments_v3_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a function's arguments. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.arguments_v3_schema","title":"JSON Schema - GenerateJsonSchema - arguments_v3_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.arguments_v3_schema","rank":-250},{"content":"call_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a function call. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.call_schema","title":"JSON Schema - GenerateJsonSchema - call_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.call_schema","rank":-255},{"content":"custom_error_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a custom error. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.custom_error_schema","title":"JSON Schema - GenerateJsonSchema - custom_error_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.custom_error_schema","rank":-260},{"content":"json_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a JSON object. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.json_schema","title":"JSON Schema - GenerateJsonSchema - json_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.json_schema","rank":-265},{"content":"url_schema(schema: ) -> Generates a JSON schema that matches a schema that defines a URL. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.url_schema","title":"JSON Schema - GenerateJsonSchema - url_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.url_schema","rank":-270},{"content":"multi_host_url_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.multi_host_url_schema","title":"JSON Schema - GenerateJsonSchema - multi_host_url_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.multi_host_url_schema","rank":-275},{"content":"uuid_schema(schema: ) -> Generates a JSON schema that matches a UUID. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.uuid_schema","title":"JSON Schema - GenerateJsonSchema - uuid_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.uuid_schema","rank":-280},{"content":"definitions_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that defines a JSON object with definitions. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.definitions_schema","title":"JSON Schema - GenerateJsonSchema - definitions_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.definitions_schema","rank":-285},{"content":"definition_ref_schema(\n    schema: ,\n) -> Generates a JSON schema that matches a schema that references a definition. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.definition_ref_schema","title":"JSON Schema - GenerateJsonSchema - definition_ref_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.definition_ref_schema","rank":-290},{"content":"ser_schema(\n    schema: (\n         |  | \n    ),\n) ->  | None Generates a JSON schema that matches a schema that defines a serialized object. Parameters: Name Type Description Default schema |  | The core schema. required Returns: Type Description | None The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.ser_schema","title":"JSON Schema - GenerateJsonSchema - ser_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.ser_schema","rank":-295},{"content":"complex_schema(schema: ) -> Generates a JSON schema that matches a complex number. JSON has no standard way to represent complex numbers. Complex number is not a numeric\ntype. Here we represent complex number as strings following the rule defined by Python.\nFor instance, '1+2j' is an accepted complex string. Details can be found in\n. Parameters: Name Type Description Default schema The core schema. required Returns: Type Description The generated JSON schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.complex_schema","title":"JSON Schema - GenerateJsonSchema - complex_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.complex_schema","rank":-300},{"content":"get_title_from_name(name: ) -> Retrieves a title from a name. Parameters: Name Type Description Default name The name to retrieve a title from. required Returns: Type Description The title.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_title_from_name","title":"JSON Schema - GenerateJsonSchema - get_title_from_name","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_title_from_name","rank":-305},{"content":"field_title_should_be_set(\n    schema: ,\n) -> Returns true if a field with the given schema should have a title set based on the field name. Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title\n(e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses). Parameters: Name Type Description Default schema The schema to check. required Returns: Type Description True if the field should have a title set, False otherwise.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.field_title_should_be_set","title":"JSON Schema - GenerateJsonSchema - field_title_should_be_set","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.field_title_should_be_set","rank":-310},{"content":"normalize_name(name: ) -> Normalizes a name to be used as a key in a dictionary. Parameters: Name Type Description Default name The name to normalize. required Returns: Type Description The normalized name.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.normalize_name","title":"JSON Schema - GenerateJsonSchema - normalize_name","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.normalize_name","rank":-315},{"content":"get_defs_ref(core_mode_ref: ) -> Override this method to change the way that definitions keys are generated from a core reference. Parameters: Name Type Description Default core_mode_ref The core reference. required Returns: Type Description The definitions key.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_defs_ref","title":"JSON Schema - GenerateJsonSchema - get_defs_ref","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_defs_ref","rank":-320},{"content":"get_cache_defs_ref_schema(\n    core_ref: ,\n) -> [, ] This method wraps the get_defs_ref method with some cache-lookup/population logic,\nand returns both the produced defs_ref and the JSON schema that will refer to the right definition. Parameters: Name Type Description Default core_ref The core reference to get the definitions reference for. required Returns: Type Description [, ] A tuple of the definitions reference and the JSON schema that will refer to it.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_cache_defs_ref_schema","title":"JSON Schema - GenerateJsonSchema - get_cache_defs_ref_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_cache_defs_ref_schema","rank":-325},{"content":"handle_ref_overrides(\n    json_schema: ,\n) -> Remove any sibling keys that are redundant with the referenced schema. Parameters: Name Type Description Default json_schema The schema to remove redundant sibling keys from. required Returns: Type Description The schema with redundant sibling keys removed.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.handle_ref_overrides","title":"JSON Schema - GenerateJsonSchema - handle_ref_overrides","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.handle_ref_overrides","rank":-330},{"content":"encode_default(dft: ) -> Encode a default value to a JSON-serializable value. This is used to encode default values for fields in the generated JSON schema. Parameters: Name Type Description Default dft The default value to encode. required Returns: Type Description The encoded default value.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.encode_default","title":"JSON Schema - GenerateJsonSchema - encode_default","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.encode_default","rank":-335},{"content":"update_with_validations(\n    json_schema: ,\n    core_schema: ,\n    mapping: [, ],\n) -> None Update the json_schema with the corresponding validations specified in the core_schema,\nusing the provided mapping to translate keys in core_schema to the appropriate keys for a JSON schema. Parameters: Name Type Description Default json_schema The JSON schema to update. required core_schema The core schema to get the validations from. required mapping [, ] A mapping from core_schema attribute names to the corresponding JSON schema attribute names. required","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.update_with_validations","title":"JSON Schema - GenerateJsonSchema - update_with_validations","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.update_with_validations","rank":-340},{"content":"get_json_ref_counts(\n    json_schema: ,\n) -> [, ] Get all values corresponding to the key '$ref' anywhere in the json_schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_json_ref_counts","title":"JSON Schema - GenerateJsonSchema - get_json_ref_counts","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.get_json_ref_counts","rank":-345},{"content":"emit_warning(\n    kind: , detail: \n) -> None This method simply emits PydanticJsonSchemaWarnings based on handling in the warning_message method.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.emit_warning","title":"JSON Schema - GenerateJsonSchema - emit_warning","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.emit_warning","rank":-350},{"content":"render_warning_message(\n    kind: , detail: \n) ->  | None This method is responsible for ignoring warnings as desired, and for formatting the warning messages. You can override the value of ignored_warning_kinds in a subclass of GenerateJsonSchema\nto modify what warnings are generated. If you want more control, you can override this method;\njust return None in situations where you don't want warnings to be emitted. Parameters: Name Type Description Default kind The kind of warning to render. It can be one of the following: 'skipped-choice': A choice field was skipped because it had no valid choices. 'non-serializable-default': A default value was skipped because it was not JSON-serializable. required detail A string with additional details about the warning. required Returns: Type Description | None The formatted warning message, or None if no warning should be emitted.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.render_warning_message","title":"JSON Schema - GenerateJsonSchema - render_warning_message","objectID":"/latest/api/json_schema/#pydantic.json_schema.GenerateJsonSchema.render_warning_message","rank":-355},{"content":"WithJsonSchema(\n    json_schema:  | None,\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Usage Documentation WithJsonSchema Annotation Add this as an annotation on a field to override the (base) JSON schema that would be generated for that field.\nThis provides a way to set a JSON schema for types that would otherwise raise errors when producing a JSON schema,\nsuch as Callable, or types that have an is-instance core schema, without needing to go so far as creating a\ncustom subclass of pydantic.json_schema.GenerateJsonSchema.\nNote that any modifications to the schema that would normally be made (such as setting the title for model fields)\nwill still be performed. If mode is set this will only apply to that schema generation mode, allowing you\nto set different json schemas for validation and serialization.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.WithJsonSchema","title":"JSON Schema - WithJsonSchema  dataclass","objectID":"/latest/api/json_schema/#pydantic.json_schema.WithJsonSchema","rank":-360},{"content":"Examples(\n    examples: [, ],\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Examples(\n    examples: [],\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Examples(\n    examples: [, ] | [],\n    mode: (\n        [\"validation\", \"serialization\"] | None\n    ) = None,\n) Add examples to a JSON schema. If the JSON Schema already contains examples, the provided examples\nwill be appended. If mode is set this will only apply to that schema generation mode,\nallowing you to add different examples for validation and serialization.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.Examples","title":"JSON Schema - Examples","objectID":"/latest/api/json_schema/#pydantic.json_schema.Examples","rank":-365},{"content":"SkipJsonSchema() Usage Documentation SkipJsonSchema Annotation Add this as an annotation on a field to skip generating a JSON schema for that field.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.SkipJsonSchema","title":"JSON Schema - SkipJsonSchema  dataclass","objectID":"/latest/api/json_schema/#pydantic.json_schema.SkipJsonSchema","rank":-370},{"content":"model_json_schema(\n    cls: [] | [],\n    by_alias:  = True,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n    mode:  = \"validation\",\n) -> [, ] Utility function to generate a JSON Schema for a model. Parameters: Name Type Description Default cls [] | [] The model class to generate a JSON Schema for. required by_alias If True (the default), fields will be serialized according to their alias.\nIf False , fields will be serialized according to their attribute name. True ref_template The template to use for generating JSON Schema references. schema_generator [] The class to use for generating the JSON Schema. mode The mode to use for generating the JSON Schema. It can be one of the following: 'validation': Generate a JSON Schema for validating data. 'serialization': Generate a JSON Schema for serializing data. 'validation' Returns: Type Description [, ] The generated JSON Schema.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.model_json_schema","title":"JSON Schema - model_json_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.model_json_schema","rank":-375},{"content":"models_json_schema(\n    models: [\n        [\n            [] | [],\n            ,\n        ]\n    ],\n    *,\n    by_alias:  = True,\n    title:  | None = None,\n    description:  | None = None,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = \n) -> [\n    [\n        [\n            [] | [],\n            ,\n        ],\n        ,\n    ],\n    ,\n] Utility function to generate a JSON Schema for multiple models. Parameters: Name Type Description Default models [[[] | [], ]] A sequence of tuples of the form (model, mode). required by_alias Whether field aliases should be used as keys in the generated JSON Schema. True title | None The title of the generated JSON Schema. None description | None The description of the generated JSON Schema. None ref_template The reference template to use for generating JSON Schema references. schema_generator [] The schema generator to use for generating the JSON Schema. Returns: Type Description [[[[] | [], ], ], ] A tuple where:\n- The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n    JsonRef references to definitions that are defined in the second returned element.)\n- The second element is a JSON schema containing all definitions referenced in the first returned\n        element, along with the optional title and description keys.","pageID":"JSON Schema","abs_url":"/latest/api/json_schema/#pydantic.json_schema.models_json_schema","title":"JSON Schema - models_json_schema","objectID":"/latest/api/json_schema/#pydantic.json_schema.models_json_schema","rank":-380},{"content":"The networks module contains types for common network-related fields. MAX_EMAIL_LENGTH module-attribute ¶ MAX_EMAIL_LENGTH = 2048 Maximum length for an email.\nA somewhat arbitrary but very generous number compared to what is allowed by most implementations. UrlConstraints dataclass ¶ UrlConstraints(\n    max_length:  | None = None,\n    allowed_schemes: [] | None = None,\n    host_required:  | None = None,\n    default_host:  | None = None,\n    default_port:  | None = None,\n    default_path:  | None = None,\n) Url constraints. Attributes: Name Type Description | None The maximum length of the url. Defaults to None . [] | None The allowed schemes. Defaults to None . | None Whether the host is required. Defaults to None . | None The default host. Defaults to None . | None The default port. Defaults to None . | None The default path. Defaults to None . defined_constraints property ¶ defined_constraints: [, ] Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates. AnyUrl ¶ AnyUrl(url:  |  | ) Bases: Base type for all URLs. Any scheme allowed Top-level domain (TLD) not required Host not required Assuming an input URL of http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit ,\nthe types export the following properties: scheme : the URL scheme ( http ), always set. host : the URL host ( example.com ). username : optional username if included ( samuel ). password : optional password if included ( pass ). port : optional port ( 8000 ). path : optional path ( /the/path/ ). query : optional URL query (for example, GET arguments or \"search string\", such as query=here ). fragment : optional fragment ( fragment=is;this=bit ). AnyHttpUrl ¶ AnyHttpUrl(url:  |  | ) Bases: A type that will accept any http or https URL. TLD not required Host not required HttpUrl ¶ HttpUrl(url:  |  | ) Bases: A type that will accept any http or https URL. TLD not required Host not required Max length 2083 from pydantic import BaseModel, HttpUrl, ValidationError\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm = MyModel(url='http://www.example.com')  # (1)!\nprint(m.url)\n#> http://www.example.com/\n\ntry:\n    MyModel(url='ftp://invalid.url')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyModel\n    url\n      URL scheme should be 'http' or 'https' [type=url_scheme, input_value='ftp://invalid.url', input_type=str]\n    '''\n\ntry:\n    MyModel(url='not a url')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyModel\n    url\n      Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='not a url', input_type=str]\n    ''' Note: mypy would prefer m = MyModel(url=HttpUrl('http://www.example.com')) , but Pydantic will convert the string to an HttpUrl instance anyway. \"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via punycode (see this article for a good description of why this is important): from pydantic import BaseModel, HttpUrl\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm1 = MyModel(url='http://puny£code.com')\nprint(m1.url)\n#> http://xn--punycode-eja.com/\nm2 = MyModel(url='https://www.аррӏе.com/')\nprint(m2.url)\n#> https://www.xn--80ak6aa92e.com/\nm3 = MyModel(url='https://www.example.珠宝/')\nprint(m3.url)\n#> https://www.example.xn--pbt977c/ Underscores in Hostnames In Pydantic, underscores are allowed in all parts of a domain except the TLD.\nTechnically this might be wrong - in theory the hostname cannot have underscores, but subdomains can. To explain this; consider the following two cases: exam_ple.co.uk : the hostname is exam_ple , which should not be allowed since it contains an underscore. foo_bar.example.com the hostname is example , which should be allowed since the underscore is in the subdomain. Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore\nunderscores are allowed, but you can always do further validation in a validator if desired. Also, Chrome, Firefox, and Safari all currently accept http://exam_ple.com as a URL, so we're in good\n(or at least big) company. AnyWebsocketUrl ¶ AnyWebsocketUrl(url:  |  | ) Bases: A type that will accept any ws or wss URL. TLD not required Host not required WebsocketUrl ¶ WebsocketUrl(url:  |  | ) Bases: A type that will accept any ws or wss URL. TLD not required Host not required Max length 2083 FileUrl ¶ FileUrl(url:  |  | ) Bases: A type that will accept any file URL. Host not required FtpUrl ¶ FtpUrl(url:  |  | ) Bases: A type that will accept ftp URL. TLD not required Host not required PostgresDsn ¶ PostgresDsn(url:  |  | ) Bases: A type that will accept any Postgres DSN. User info required TLD not required Host required Supports multiple hosts If further validation is required, these properties can be used by validators to enforce specific behaviour: from pydantic import (\n    BaseModel,\n    HttpUrl,\n    PostgresDsn,\n    ValidationError,\n    field_validator,\n)\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm = MyModel(url='http://www.example.com')\n\n# the repr() method for a url will display all properties of the url\nprint(repr(m.url))\n#> HttpUrl('http://www.example.com/')\nprint(m.url.scheme)\n#> http\nprint(m.url.host)\n#> www.example.com\nprint(m.url.port)\n#> 80\n\nclass MyDatabaseModel(BaseModel):\n    db: PostgresDsn\n\n    @field_validator('db')\n    def check_db_name(cls, v):\n        assert v.path and len(v.path) > 1, 'database must be provided'\n        return v\n\nm = MyDatabaseModel(db='postgres://user:pass@localhost:5432/foobar')\nprint(m.db)\n#> postgres://user:pass@localhost:5432/foobar\n\ntry:\n    MyDatabaseModel(db='postgres://user:pass@localhost:5432')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyDatabaseModel\n    db\n      Assertion failed, database must be provided\n    assert (None)\n     +  where None = PostgresDsn('postgres://user:pass@localhost:5432').path [type=assertion_error, input_value='postgres://user:pass@localhost:5432', input_type=str]\n    ''' host property ¶ host: The required URL host. CockroachDsn ¶ CockroachDsn(url:  |  | ) Bases: A type that will accept any Cockroach DSN. User info required TLD not required Host required host property ¶ host: The required URL host. AmqpDsn ¶ AmqpDsn(url:  |  | ) Bases: A type that will accept any AMQP DSN. User info required TLD not required Host not required RedisDsn ¶ RedisDsn(url:  |  | ) Bases: A type that will accept any Redis DSN. User info required TLD not required Host required (e.g., rediss://:pass@localhost ) host property ¶ host: The required URL host. MongoDsn ¶ MongoDsn(url:  |  | ) Bases: A type that will accept any MongoDB DSN. User info not required Database name not required Port not required User info may be passed without user part (e.g., mongodb://mongodb0.example.com:27017 ). KafkaDsn ¶ KafkaDsn(url:  |  | ) Bases: A type that will accept any Kafka DSN. User info required TLD not required Host not required NatsDsn ¶ NatsDsn(url:  |  | ) Bases: A type that will accept any NATS DSN. NATS is a connective technology built for the ever increasingly hyper-connected world.\nIt is a single technology that enables applications to securely communicate across\nany combination of cloud vendors, on-premise, edge, web and mobile, and devices.\nMore: https://nats.io MySQLDsn ¶ MySQLDsn(url:  |  | ) Bases: A type that will accept any MySQL DSN. User info required TLD not required Host not required MariaDBDsn ¶ MariaDBDsn(url:  |  | ) Bases: A type that will accept any MariaDB DSN. User info required TLD not required Host not required ClickHouseDsn ¶ ClickHouseDsn(url:  |  | ) Bases: A type that will accept any ClickHouse DSN. User info required TLD not required Host not required SnowflakeDsn ¶ SnowflakeDsn(url:  |  | ) Bases: A type that will accept any Snowflake DSN. User info required TLD not required Host required host property ¶ host: The required URL host. EmailStr ¶ Validate email addresses. from pydantic import BaseModel, EmailStr\n\nclass Model(BaseModel):\n    email: EmailStr\n\nprint(Model(email='contact@mail.com'))\n#> email='contact@mail.com' NameEmail ¶ NameEmail(name: , email: ) Bases: Validate a name and email address combination, as specified by RFC 5322 . The NameEmail has two properties: name and email .\nIn case the name is not provided, it's inferred from the email address. from pydantic import BaseModel, NameEmail\n\nclass User(BaseModel):\n    email: NameEmail\n\nuser = User(email='Fred Bloggs ')\nprint(user.email)\n#> Fred Bloggs print(user.email.name)\n#> Fred Bloggs\n\nuser = User(email='fred.bloggs@example.com')\nprint(user.email)\n#> fred.bloggs print(user.email.name)\n#> fred.bloggs IPvAnyAddress ¶ Validate an IPv4 or IPv6 address. from pydantic import BaseModel\nfrom pydantic.networks import IPvAnyAddress\n\nclass IpModel(BaseModel):\n    ip: IPvAnyAddress\n\nprint(IpModel(ip='127.0.0.1'))\n#> ip=IPv4Address('127.0.0.1')\n\ntry:\n    IpModel(ip='http://www.example.com')\nexcept ValueError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'ip_any_address',\n            'loc': ('ip',),\n            'msg': 'value is not a valid IPv4 or IPv6 address',\n            'input': 'http://www.example.com',\n        }\n    ]\n    ''' IPvAnyInterface ¶ Validate an IPv4 or IPv6 interface. IPvAnyNetwork ¶ Validate an IPv4 or IPv6 network. validate_email ¶ validate_email(value: ) -> [, ] Email address validation using email-validator . Returns: Type Description [, ] A tuple containing the local part of the email (or the name for \"pretty\" email addresses)\nand the normalized email. Raises: Type Description If the email is invalid.","pageID":"Network Types","abs_url":"/latest/api/networks/#Network Types","title":"Network Types","objectID":"/latest/api/networks/#Network Types","rank":100},{"content":"MAX_EMAIL_LENGTH = 2048 Maximum length for an email.\nA somewhat arbitrary but very generous number compared to what is allowed by most implementations.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.MAX_EMAIL_LENGTH","title":"Network Types - MAX_EMAIL_LENGTH  module-attribute","objectID":"/latest/api/networks/#pydantic.networks.MAX_EMAIL_LENGTH","rank":95},{"content":"UrlConstraints(\n    max_length:  | None = None,\n    allowed_schemes: [] | None = None,\n    host_required:  | None = None,\n    default_host:  | None = None,\n    default_port:  | None = None,\n    default_path:  | None = None,\n) Url constraints. Attributes: Name Type Description | None The maximum length of the url. Defaults to None . [] | None The allowed schemes. Defaults to None . | None Whether the host is required. Defaults to None . | None The default host. Defaults to None . | None The default port. Defaults to None . | None The default path. Defaults to None . defined_constraints property ¶ defined_constraints: [, ] Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.UrlConstraints","title":"Network Types - UrlConstraints  dataclass","objectID":"/latest/api/networks/#pydantic.networks.UrlConstraints","rank":90},{"content":"defined_constraints: [, ] Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.UrlConstraints.defined_constraints","title":"Network Types - UrlConstraints  dataclass - defined_constraints  property","objectID":"/latest/api/networks/#pydantic.networks.UrlConstraints.defined_constraints","rank":85},{"content":"AnyUrl(url:  |  | ) Bases: Base type for all URLs. Any scheme allowed Top-level domain (TLD) not required Host not required Assuming an input URL of http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit ,\nthe types export the following properties: scheme : the URL scheme ( http ), always set. host : the URL host ( example.com ). username : optional username if included ( samuel ). password : optional password if included ( pass ). port : optional port ( 8000 ). path : optional path ( /the/path/ ). query : optional URL query (for example, GET arguments or \"search string\", such as query=here ). fragment : optional fragment ( fragment=is;this=bit ).","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.AnyUrl","title":"Network Types - AnyUrl","objectID":"/latest/api/networks/#pydantic.networks.AnyUrl","rank":80},{"content":"AnyHttpUrl(url:  |  | ) Bases: A type that will accept any http or https URL. TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.AnyHttpUrl","title":"Network Types - AnyHttpUrl","objectID":"/latest/api/networks/#pydantic.networks.AnyHttpUrl","rank":75},{"content":"HttpUrl(url:  |  | ) Bases: A type that will accept any http or https URL. TLD not required Host not required Max length 2083 from pydantic import BaseModel, HttpUrl, ValidationError\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm = MyModel(url='http://www.example.com')  # (1)!\nprint(m.url)\n#> http://www.example.com/\n\ntry:\n    MyModel(url='ftp://invalid.url')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyModel\n    url\n      URL scheme should be 'http' or 'https' [type=url_scheme, input_value='ftp://invalid.url', input_type=str]\n    '''\n\ntry:\n    MyModel(url='not a url')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyModel\n    url\n      Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='not a url', input_type=str]\n    ''' Note: mypy would prefer m = MyModel(url=HttpUrl('http://www.example.com')) , but Pydantic will convert the string to an HttpUrl instance anyway. \"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via punycode (see this article for a good description of why this is important): from pydantic import BaseModel, HttpUrl\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm1 = MyModel(url='http://puny£code.com')\nprint(m1.url)\n#> http://xn--punycode-eja.com/\nm2 = MyModel(url='https://www.аррӏе.com/')\nprint(m2.url)\n#> https://www.xn--80ak6aa92e.com/\nm3 = MyModel(url='https://www.example.珠宝/')\nprint(m3.url)\n#> https://www.example.xn--pbt977c/ Underscores in Hostnames In Pydantic, underscores are allowed in all parts of a domain except the TLD.\nTechnically this might be wrong - in theory the hostname cannot have underscores, but subdomains can. To explain this; consider the following two cases: exam_ple.co.uk : the hostname is exam_ple , which should not be allowed since it contains an underscore. foo_bar.example.com the hostname is example , which should be allowed since the underscore is in the subdomain. Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore\nunderscores are allowed, but you can always do further validation in a validator if desired. Also, Chrome, Firefox, and Safari all currently accept http://exam_ple.com as a URL, so we're in good\n(or at least big) company.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.HttpUrl","title":"Network Types - HttpUrl","objectID":"/latest/api/networks/#pydantic.networks.HttpUrl","rank":70},{"content":"AnyWebsocketUrl(url:  |  | ) Bases: A type that will accept any ws or wss URL. TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.AnyWebsocketUrl","title":"Network Types - AnyWebsocketUrl","objectID":"/latest/api/networks/#pydantic.networks.AnyWebsocketUrl","rank":65},{"content":"WebsocketUrl(url:  |  | ) Bases: A type that will accept any ws or wss URL. TLD not required Host not required Max length 2083","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.WebsocketUrl","title":"Network Types - WebsocketUrl","objectID":"/latest/api/networks/#pydantic.networks.WebsocketUrl","rank":60},{"content":"FileUrl(url:  |  | ) Bases: A type that will accept any file URL. Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.FileUrl","title":"Network Types - FileUrl","objectID":"/latest/api/networks/#pydantic.networks.FileUrl","rank":55},{"content":"FtpUrl(url:  |  | ) Bases: A type that will accept ftp URL. TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.FtpUrl","title":"Network Types - FtpUrl","objectID":"/latest/api/networks/#pydantic.networks.FtpUrl","rank":50},{"content":"PostgresDsn(url:  |  | ) Bases: A type that will accept any Postgres DSN. User info required TLD not required Host required Supports multiple hosts If further validation is required, these properties can be used by validators to enforce specific behaviour: from pydantic import (\n    BaseModel,\n    HttpUrl,\n    PostgresDsn,\n    ValidationError,\n    field_validator,\n)\n\nclass MyModel(BaseModel):\n    url: HttpUrl\n\nm = MyModel(url='http://www.example.com')\n\n# the repr() method for a url will display all properties of the url\nprint(repr(m.url))\n#> HttpUrl('http://www.example.com/')\nprint(m.url.scheme)\n#> http\nprint(m.url.host)\n#> www.example.com\nprint(m.url.port)\n#> 80\n\nclass MyDatabaseModel(BaseModel):\n    db: PostgresDsn\n\n    @field_validator('db')\n    def check_db_name(cls, v):\n        assert v.path and len(v.path) > 1, 'database must be provided'\n        return v\n\nm = MyDatabaseModel(db='postgres://user:pass@localhost:5432/foobar')\nprint(m.db)\n#> postgres://user:pass@localhost:5432/foobar\n\ntry:\n    MyDatabaseModel(db='postgres://user:pass@localhost:5432')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for MyDatabaseModel\n    db\n      Assertion failed, database must be provided\n    assert (None)\n     +  where None = PostgresDsn('postgres://user:pass@localhost:5432').path [type=assertion_error, input_value='postgres://user:pass@localhost:5432', input_type=str]\n    ''' host property ¶ host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.PostgresDsn","title":"Network Types - PostgresDsn","objectID":"/latest/api/networks/#pydantic.networks.PostgresDsn","rank":45},{"content":"host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.PostgresDsn.host","title":"Network Types - PostgresDsn - host  property","objectID":"/latest/api/networks/#pydantic.networks.PostgresDsn.host","rank":40},{"content":"CockroachDsn(url:  |  | ) Bases: A type that will accept any Cockroach DSN. User info required TLD not required Host required host property ¶ host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.CockroachDsn","title":"Network Types - CockroachDsn","objectID":"/latest/api/networks/#pydantic.networks.CockroachDsn","rank":35},{"content":"host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.CockroachDsn.host","title":"Network Types - CockroachDsn - host  property","objectID":"/latest/api/networks/#pydantic.networks.CockroachDsn.host","rank":30},{"content":"AmqpDsn(url:  |  | ) Bases: A type that will accept any AMQP DSN. User info required TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.AmqpDsn","title":"Network Types - AmqpDsn","objectID":"/latest/api/networks/#pydantic.networks.AmqpDsn","rank":25},{"content":"RedisDsn(url:  |  | ) Bases: A type that will accept any Redis DSN. User info required TLD not required Host required (e.g., rediss://:pass@localhost ) host property ¶ host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.RedisDsn","title":"Network Types - RedisDsn","objectID":"/latest/api/networks/#pydantic.networks.RedisDsn","rank":20},{"content":"host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.RedisDsn.host","title":"Network Types - RedisDsn - host  property","objectID":"/latest/api/networks/#pydantic.networks.RedisDsn.host","rank":15},{"content":"MongoDsn(url:  |  | ) Bases: A type that will accept any MongoDB DSN. User info not required Database name not required Port not required User info may be passed without user part (e.g., mongodb://mongodb0.example.com:27017 ).","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.MongoDsn","title":"Network Types - MongoDsn","objectID":"/latest/api/networks/#pydantic.networks.MongoDsn","rank":10},{"content":"KafkaDsn(url:  |  | ) Bases: A type that will accept any Kafka DSN. User info required TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.KafkaDsn","title":"Network Types - KafkaDsn","objectID":"/latest/api/networks/#pydantic.networks.KafkaDsn","rank":5},{"content":"NatsDsn(url:  |  | ) Bases: A type that will accept any NATS DSN. NATS is a connective technology built for the ever increasingly hyper-connected world.\nIt is a single technology that enables applications to securely communicate across\nany combination of cloud vendors, on-premise, edge, web and mobile, and devices.\nMore: https://nats.io","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.NatsDsn","title":"Network Types - NatsDsn","objectID":"/latest/api/networks/#pydantic.networks.NatsDsn","rank":0},{"content":"MySQLDsn(url:  |  | ) Bases: A type that will accept any MySQL DSN. User info required TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.MySQLDsn","title":"Network Types - MySQLDsn","objectID":"/latest/api/networks/#pydantic.networks.MySQLDsn","rank":-5},{"content":"MariaDBDsn(url:  |  | ) Bases: A type that will accept any MariaDB DSN. User info required TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.MariaDBDsn","title":"Network Types - MariaDBDsn","objectID":"/latest/api/networks/#pydantic.networks.MariaDBDsn","rank":-10},{"content":"ClickHouseDsn(url:  |  | ) Bases: A type that will accept any ClickHouse DSN. User info required TLD not required Host not required","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.ClickHouseDsn","title":"Network Types - ClickHouseDsn","objectID":"/latest/api/networks/#pydantic.networks.ClickHouseDsn","rank":-15},{"content":"SnowflakeDsn(url:  |  | ) Bases: A type that will accept any Snowflake DSN. User info required TLD not required Host required host property ¶ host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.SnowflakeDsn","title":"Network Types - SnowflakeDsn","objectID":"/latest/api/networks/#pydantic.networks.SnowflakeDsn","rank":-20},{"content":"host: The required URL host.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.SnowflakeDsn.host","title":"Network Types - SnowflakeDsn - host  property","objectID":"/latest/api/networks/#pydantic.networks.SnowflakeDsn.host","rank":-25},{"content":"Validate email addresses. from pydantic import BaseModel, EmailStr\n\nclass Model(BaseModel):\n    email: EmailStr\n\nprint(Model(email='contact@mail.com'))\n#> email='contact@mail.com'","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.EmailStr","title":"Network Types - EmailStr","objectID":"/latest/api/networks/#pydantic.networks.EmailStr","rank":-30},{"content":"NameEmail(name: , email: ) Bases: Validate a name and email address combination, as specified by RFC 5322 . The NameEmail has two properties: name and email .\nIn case the name is not provided, it's inferred from the email address. from pydantic import BaseModel, NameEmail\n\nclass User(BaseModel):\n    email: NameEmail\n\nuser = User(email='Fred Bloggs ')\nprint(user.email)\n#> Fred Bloggs print(user.email.name)\n#> Fred Bloggs\n\nuser = User(email='fred.bloggs@example.com')\nprint(user.email)\n#> fred.bloggs print(user.email.name)\n#> fred.bloggs","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.NameEmail","title":"Network Types - NameEmail","objectID":"/latest/api/networks/#pydantic.networks.NameEmail","rank":-35},{"content":"Validate an IPv4 or IPv6 address. from pydantic import BaseModel\nfrom pydantic.networks import IPvAnyAddress\n\nclass IpModel(BaseModel):\n    ip: IPvAnyAddress\n\nprint(IpModel(ip='127.0.0.1'))\n#> ip=IPv4Address('127.0.0.1')\n\ntry:\n    IpModel(ip='http://www.example.com')\nexcept ValueError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'ip_any_address',\n            'loc': ('ip',),\n            'msg': 'value is not a valid IPv4 or IPv6 address',\n            'input': 'http://www.example.com',\n        }\n    ]\n    '''","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.IPvAnyAddress","title":"Network Types - IPvAnyAddress","objectID":"/latest/api/networks/#pydantic.networks.IPvAnyAddress","rank":-40},{"content":"Validate an IPv4 or IPv6 interface.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.IPvAnyInterface","title":"Network Types - IPvAnyInterface","objectID":"/latest/api/networks/#pydantic.networks.IPvAnyInterface","rank":-45},{"content":"Validate an IPv4 or IPv6 network.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.IPvAnyNetwork","title":"Network Types - IPvAnyNetwork","objectID":"/latest/api/networks/#pydantic.networks.IPvAnyNetwork","rank":-50},{"content":"validate_email(value: ) -> [, ] Email address validation using email-validator . Returns: Type Description [, ] A tuple containing the local part of the email (or the name for \"pretty\" email addresses)\nand the normalized email. Raises: Type Description If the email is invalid.","pageID":"Network Types","abs_url":"/latest/api/networks/#pydantic.networks.validate_email","title":"Network Types - validate_email","objectID":"/latest/api/networks/#pydantic.networks.validate_email","rank":-55},{"content":"__version__ module-attribute ¶ __version__: SchemaValidator ¶ SchemaValidator(\n    schema: , config:  | None = None\n) SchemaValidator is the Python wrapper for pydantic-core 's Rust validation logic, internally it owns one CombinedValidator which may in turn own more CombinedValidator s which make up the full schema validator. Parameters: Name Type Description Default schema The CoreSchema to use for validation. required config | None Optionally a  to configure validation. None title property ¶ title: The title of the schema, as used in the heading of . validate_python ¶ validate_python(\n    input: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a Python object against the schema and return the validated object. Parameters: Name Type Description Default input The Python object to validate. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None from_attributes | None Whether to validate objects as inputs to models by extracting attributes.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None self_instance | None An instance of a model set attributes on from validation, this is used when running\nvalidation from the __init__ method of a model. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True errors in the last element of sequences\nand mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description The validated object. isinstance_python ¶ isinstance_python(\n    input: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Similar to  but returns a boolean. Arguments match validate_python() . This method will not raise ValidationError s but will raise internal\nerrors. Returns: Type Description True if validation succeeds, False if validation fails. validate_json ¶ validate_json(\n    input:  |  | ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate JSON data directly against the schema and return the validated Python object. This method should be significantly faster than validate_python(json.loads(json_data)) as it avoids the\nneed to create intermediate Python objects It also handles constructing the correct Python type even in strict mode, where validate_python(json.loads(json_data)) would fail validation. Parameters: Name Type Description Default input |  | The JSON data to validate. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None self_instance | None An instance of a model set attributes on from validation. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True incomplete JSON will be parsed successfully\nand errors in the last element of sequences and mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails or if the JSON data is invalid. Other error types maybe raised if internal errors occur. Returns: Type Description The validated Python object. validate_strings ¶ validate_strings(\n    input: ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a string against the schema and return the validated Python object. This is similar to validate_json but applies to scenarios where the input will be a string but not\nJSON data, e.g. URL fragments, query parameters, etc. Parameters: Name Type Description Default input The input as a string, or bytes/bytearray if strict=False . required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True errors in the last element of sequences\nand mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails or if the JSON data is invalid. Other error types maybe raised if internal errors occur. Returns: Type Description The validated Python object. validate_assignment ¶ validate_assignment(\n    obj: ,\n    field_name: ,\n    field_value: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> (\n    [, ]\n    | [[, ], [, ] | None, []]\n) Validate an assignment to a field on a model. Parameters: Name Type Description Default obj The model instance being assigned to. required field_name The name of the field to validate assignment for. required field_value The value to assign to the field. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None from_attributes | None Whether to validate objects as inputs to models by extracting attributes.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description [, ] | [[, ], [, ] | None, []] Either the model dict or a tuple of (model_data, model_extra, fields_set) get_default_value ¶ get_default_value(\n    *, strict:  | None = None, context:  = None\n) ->  | None Get the default value for the schema, including running default value validation. Parameters: Name Type Description Default strict | None Whether to validate the default value in strict mode.\nIf None , the value of  is used. None context The context to use for validation, this is passed to functional validators as\n. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description | None None if the schema has no default value, otherwise a  containing the default. SchemaSerializer ¶ SchemaSerializer(\n    schema: , config:  | None = None\n) SchemaSerializer is the Python wrapper for pydantic-core 's Rust serialization logic, internally it owns one CombinedSerializer which may in turn own more CombinedSerializer s which make up the full schema serializer. Parameters: Name Type Description Default schema The CoreSchema to use for serialization. required config | None Optionally a  to to configure serialization. None to_python ¶ to_python(\n    value: ,\n    *,\n    mode:  | None = None,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize/marshal a Python object to a Python object including transforming and filtering data. Parameters: Name Type Description Default value The Python object to serialize. required mode | None The serialization mode to use, either 'python' or 'json' , defaults to 'python' . In JSON mode,\nall values are converted to JSON compatible types, e.g. None , int , float , str , list , dict . None include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias | None Whether to use the alias names of fields. None exclude_unset Whether to exclude fields that are not set,\ne.g. are not included in __pydantic_fields_set__ . False exclude_defaults Whether to exclude fields that are equal to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False warnings | ['none', 'warn', 'error'] How to handle invalid fields. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description The serialized Python object. to_json ¶ to_json(\n    value: ,\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize a Python object to JSON including transforming and filtering data. Parameters: Name Type Description Default value The Python object to serialize. required indent | None If None , the JSON will be compact, otherwise it will be pretty-printed with the indent provided. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias | None Whether to use the alias names of fields. None exclude_unset Whether to exclude fields that are not set,\ne.g. are not included in __pydantic_fields_set__ . False exclude_defaults Whether to exclude fields that are equal to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False warnings | ['none', 'warn', 'error'] How to handle invalid fields. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description JSON bytes. ValidationError ¶ Bases: ValidationError is the exception raised by pydantic-core when validation fails, it contains a list of errors\nwhich detail why validation failed. title property ¶ title: The title of the error, as used in the heading of str(validation_error) . from_exception_data classmethod ¶ from_exception_data(\n    title: ,\n    line_errors: [],\n    input_type: [\"python\", \"json\"] = \"python\",\n    hide_input:  = False,\n) -> Python constructor for a Validation Error. The API for constructing validation errors will probably change in the future,\nhence the static method rather than __init__ . Parameters: Name Type Description Default title The title of the error, as used in the heading of str(validation_error) required line_errors [] A list of  which contain information\nabout errors that occurred during validation. required input_type ['python', 'json'] Whether the error is for a Python object or JSON. 'python' hide_input Whether to hide the input value in the error message. False error_count ¶ error_count() -> Returns: Type Description The number of errors in the validation error. errors ¶ errors(\n    *,\n    include_url:  = True,\n    include_context:  = True,\n    include_input:  = True\n) -> [] Details about each error in the validation error. Parameters: Name Type Description Default include_url Whether to include a URL to documentation on the error each error. True include_context Whether to include the context of each error. True include_input Whether to include the input value of each error. True Returns: Type Description [] A list of  for each error in the validation error. json ¶ json(\n    *,\n    indent:  | None = None,\n    include_url:  = True,\n    include_context:  = True,\n    include_input:  = True\n) -> Same as  but returns a JSON string. Parameters: Name Type Description Default indent | None The number of spaces to indent the JSON by, or None for no indentation - compact JSON. None include_url Whether to include a URL to documentation on the error each error. True include_context Whether to include the context of each error. True include_input Whether to include the input value of each error. True Returns: Type Description a JSON string. ErrorDetails ¶ Bases: type instance-attribute ¶ type: The type of error that occurred, this is an identifier designed for\nprogrammatic use that will change rarely or never. type is unique for each error message, and can hence be used as an identifier to build custom error messages. loc instance-attribute ¶ loc: [ | , ...] Tuple of strings and ints identifying where in the schema the error occurred. msg instance-attribute ¶ msg: A human readable error message. input instance-attribute ¶ input: The input data at this loc that caused the error. ctx instance-attribute ¶ ctx: [[, ]] Values which are required to render the error message, and could hence be useful in rendering custom error messages.\nAlso useful for passing custom error data forward. url instance-attribute ¶ url: [] The documentation URL giving information about the error. No URL is available if\na  is used. InitErrorDetails ¶ Bases: type instance-attribute ¶ type:  | The type of error that occurred, this should be a \"slug\" identifier that changes rarely or never. loc instance-attribute ¶ loc: [[ | , ...]] Tuple of strings and ints identifying where in the schema the error occurred. input instance-attribute ¶ input: The input data at this loc that caused the error. ctx instance-attribute ¶ ctx: [[, ]] Values which are required to render the error message, and could hence be useful in rendering custom error messages.\nAlso useful for passing custom error data forward. SchemaError ¶ Bases: Information about errors that occur while building a \nor . error_count ¶ error_count() -> Returns: Type Description The number of errors in the schema. errors ¶ errors() -> [] Returns: Type Description [] A list of  for each error in the schema. PydanticCustomError ¶ PydanticCustomError(\n    error_type: ,\n    message_template: ,\n    context: [, ] | None = None,\n) Bases: A custom exception providing flexible error handling for Pydantic validators. You can raise this error in custom validators when you'd like flexibility in regards to the error type, message, and context. Parameters: Name Type Description Default error_type The error type. required message_template The message template. required context [, ] | None The data to inject into the message template. None context property ¶ context: [, ] | None Values which are required to render the error message, and could hence be useful in passing error data forward. type property ¶ type: The error type associated with the error. For consistency with Pydantic, this is typically a snake_case string. message_template property ¶ message_template: The message template associated with the error. This is a string that can be formatted with context variables in {curly_braces} . message ¶ message() -> The formatted message associated with the error. This presents as the message template with context variables appropriately injected. PydanticKnownError ¶ PydanticKnownError(\n    error_type: ,\n    context: [, ] | None = None,\n) Bases: A helper class for raising exceptions that mimic Pydantic's built-in exceptions, with more flexibility in regards to context. Unlike , the error_type argument must be a known ErrorType . Parameters: Name Type Description Default error_type The error type. required context [, ] | None The data to inject into the message template. None context property ¶ context: [, ] | None Values which are required to render the error message, and could hence be useful in passing error data forward. type property ¶ type: The type of the error. message_template property ¶ message_template: The message template associated with the provided error type. This is a string that can be formatted with context variables in {curly_braces} . message ¶ message() -> The formatted message associated with the error. This presents as the message template with context variables appropriately injected. PydanticOmit ¶ Bases: An exception to signal that a field should be omitted from a generated result. This could span from omitting a field from a JSON Schema to omitting a field from a serialized result.\nUpcoming: more robust support for using PydanticOmit in custom serializers is still in development.\nRight now, this is primarily used in the JSON Schema generation process. For a more in depth example / explanation, see the customizing JSON schema docs. PydanticUseDefault ¶ Bases: An exception to signal that standard validation either failed or should be skipped, and the default value should be used instead. This warning can be raised in custom valiation functions to redirect the flow of validation. For an additional example, see the validating partial json data section of the Pydantic documentation. PydanticSerializationError ¶ PydanticSerializationError(message: ) Bases: An error raised when an issue occurs during serialization. In custom serializers, this error can be used to indicate that serialization has failed. Parameters: Name Type Description Default message The message associated with the error. required PydanticSerializationUnexpectedValue ¶ PydanticSerializationUnexpectedValue(message: ) Bases: An error raised when an unexpected value is encountered during serialization. This error is often caught and coerced into a warning, as pydantic-core generally makes a best attempt\nat serializing values, in contrast with validation where errors are eagerly raised. This is often used internally in pydantic-core when unexpected types are encountered during serialization,\nbut it can also be used by users in custom serializers, as seen above. Parameters: Name Type Description Default message The message associated with the unexpected value. required Url ¶ Url(url: ) Bases: A URL type, internal logic uses the url rust crate originally developed\nby Mozilla. MultiHostUrl ¶ MultiHostUrl(url: ) Bases: A URL type with support for multiple hosts, as used by some databases for DSNs, e.g. https://foo.com,bar.com/path . Internal URL logic uses the url rust crate originally developed\nby Mozilla. MultiHostHost ¶ Bases: A host part of a multi-host URL. username instance-attribute ¶ username:  | None The username part of this host, or None . password instance-attribute ¶ password:  | None The password part of this host, or None . host instance-attribute ¶ host:  | None The host part of this host, or None . port instance-attribute ¶ port:  | None The port part of this host, or None . ArgsKwargs ¶ ArgsKwargs(\n    args: [, ...],\n    kwargs: [, ] | None = None,\n) A construct used to store arguments and keyword arguments for a function call. This data structure is generally used to store information for core schemas associated with functions (like in an arguments schema).\nThis data structure is also currently used for some validation against dataclasses. Parameters: Name Type Description Default args [, ...] The arguments (inherently ordered) for a function call. required kwargs [, ] | None The keyword arguments for a function call None args property ¶ args: [, ...] The arguments (inherently ordered) for a function call. kwargs property ¶ kwargs: [, ] | None The keyword arguments for a function call. Some ¶ Bases: [] Similar to Rust's Option::Some type, this\nidentifies a value as being present, and provides a way to access it. Generally used in a union with None to different between \"some value which could be None\" and no value. value property ¶ value: Returns the value wrapped by Some . TzInfo ¶ Bases: An pydantic-core implementation of the abstract  class. tzname ¶ tzname(dt:  | None) ->  | None Return the time zone name corresponding to the  object dt , as a string. For more info, see . utcoffset ¶ utcoffset(dt:  | None) ->  | None Return offset of local time from UTC, as a  object that is positive east of UTC. If local time is west of UTC, this should be negative. More info can be found at . dst ¶ dst(dt:  | None) ->  | None Return the daylight saving time (DST) adjustment, as a  object or None if DST information isn’t known. More info can be found at. fromutc ¶ fromutc(dt: ) -> Adjust the date and time data associated datetime object dt , returning an equivalent datetime in self’s local time. More info can be found at . ErrorTypeInfo ¶ Bases: Gives information about errors. type instance-attribute ¶ type: The type of error that occurred, this should a \"slug\" identifier that changes rarely or never. message_template_python instance-attribute ¶ message_template_python: String template to render a human readable error message from using context, when the input is Python. example_message_python instance-attribute ¶ example_message_python: Example of a human readable error message, when the input is Python. message_template_json instance-attribute ¶ message_template_json: [] String template to render a human readable error message from using context, when the input is JSON data. example_message_json instance-attribute ¶ example_message_json: [] Example of a human readable error message, when the input is JSON data. example_context instance-attribute ¶ example_context: [, ] | None Example of context values. to_json ¶ to_json(\n    value: ,\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  = True,\n    exclude_none:  = False,\n    round_trip:  = False,\n    timedelta_mode: [\"iso8601\", \"float\"] = \"iso8601\",\n    temporal_mode: [\n        \"iso8601\", \"seconds\", \"milliseconds\"\n    ] = \"iso8601\",\n    bytes_mode: [\"utf8\", \"base64\", \"hex\"] = \"utf8\",\n    inf_nan_mode: [\n        \"null\", \"constants\", \"strings\"\n    ] = \"constants\",\n    serialize_unknown:  = False,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize a Python object to JSON including transforming and filtering data. This is effectively a standalone version of . Parameters: Name Type Description Default value The Python object to serialize. required indent | None If None , the JSON will be compact, otherwise it will be pretty-printed with the indent provided. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias Whether to use the alias names of fields. True exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False timedelta_mode ['iso8601', 'float'] How to serialize timedelta objects, either 'iso8601' or 'float' . 'iso8601' temporal_mode ['iso8601', 'seconds', 'milliseconds'] How to serialize datetime-like objects ( datetime , date , time ), either 'iso8601' , 'seconds' , or 'milliseconds' . iso8601 returns an ISO 8601 string; seconds returns the Unix timestamp in seconds as a float; milliseconds returns the Unix timestamp in milliseconds as a float. 'iso8601' bytes_mode ['utf8', 'base64', 'hex'] How to serialize bytes objects, either 'utf8' , 'base64' , or 'hex' . 'utf8' inf_nan_mode ['null', 'constants', 'strings'] How to serialize Infinity , -Infinity and NaN values, either 'null' , 'constants' , or 'strings' . 'constants' serialize_unknown Attempt to serialize unknown types, str(value) will be used, if that fails \"<Unserializable {value_type} object>\" will be used. False fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description JSON bytes. from_json ¶ from_json(\n    data:  |  | ,\n    *,\n    allow_inf_nan:  = True,\n    cache_strings: (\n         | [\"all\", \"keys\", \"none\"]\n    ) = True,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False\n) -> Deserialize JSON data to a Python object. This is effectively a faster version of json.loads() , with some extra functionality. Parameters: Name Type Description Default data |  | The JSON data to deserialize. required allow_inf_nan Whether to allow Infinity , -Infinity and NaN values as json.loads() does by default. True cache_strings | ['all', 'keys', 'none'] Whether to cache strings to avoid constructing new Python objects,\nthis should have a significant impact on performance while increasing memory usage slightly, all/True means cache all strings, keys means cache only dict keys, none/False means no caching. True allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial deserialization, if True JSON data is returned if the end of the\ninput is reached before the full object is deserialized, e.g. [\"aa\", \"bb\", \"c would return ['aa', 'bb'] . 'trailing-strings' means any final unfinished JSON string is included in the result. False Raises: Type Description If deserialization fails. Returns: Type Description The deserialized Python object. to_jsonable_python ¶ to_jsonable_python(\n    value: ,\n    *,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  = True,\n    exclude_none:  = False,\n    round_trip:  = False,\n    timedelta_mode: [\"iso8601\", \"float\"] = \"iso8601\",\n    temporal_mode: [\n        \"iso8601\", \"seconds\", \"milliseconds\"\n    ] = \"iso8601\",\n    bytes_mode: [\"utf8\", \"base64\", \"hex\"] = \"utf8\",\n    inf_nan_mode: [\n        \"null\", \"constants\", \"strings\"\n    ] = \"constants\",\n    serialize_unknown:  = False,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize/marshal a Python object to a JSON-serializable Python object including transforming and filtering data. This is effectively a standalone version of\n. Parameters: Name Type Description Default value The Python object to serialize. required include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias Whether to use the alias names of fields. True exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False timedelta_mode ['iso8601', 'float'] How to serialize timedelta objects, either 'iso8601' or 'float' . 'iso8601' temporal_mode ['iso8601', 'seconds', 'milliseconds'] How to serialize datetime-like objects ( datetime , date , time ), either 'iso8601' , 'seconds' , or 'milliseconds' . iso8601 returns an ISO 8601 string; seconds returns the Unix timestamp in seconds as a float; milliseconds returns the Unix timestamp in milliseconds as a float. 'iso8601' bytes_mode ['utf8', 'base64', 'hex'] How to serialize bytes objects, either 'utf8' , 'base64' , or 'hex' . 'utf8' inf_nan_mode ['null', 'constants', 'strings'] How to serialize Infinity , -Infinity and NaN values, either 'null' , 'constants' , or 'strings' . 'constants' serialize_unknown Attempt to serialize unknown types, str(value) will be used, if that fails \"<Unserializable {value_type} object>\" will be used. False fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description The serialized Python object.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core","title":"pydantic_core","objectID":"/latest/api/pydantic_core/#pydantic_core","rank":100},{"content":"__version__:","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.__version__","title":"pydantic_core - __version__  module-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.__version__","rank":95},{"content":"SchemaValidator(\n    schema: , config:  | None = None\n) SchemaValidator is the Python wrapper for pydantic-core 's Rust validation logic, internally it owns one CombinedValidator which may in turn own more CombinedValidator s which make up the full schema validator. Parameters: Name Type Description Default schema The CoreSchema to use for validation. required config | None Optionally a  to configure validation. None title property ¶ title: The title of the schema, as used in the heading of . validate_python ¶ validate_python(\n    input: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a Python object against the schema and return the validated object. Parameters: Name Type Description Default input The Python object to validate. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None from_attributes | None Whether to validate objects as inputs to models by extracting attributes.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None self_instance | None An instance of a model set attributes on from validation, this is used when running\nvalidation from the __init__ method of a model. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True errors in the last element of sequences\nand mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description The validated object. isinstance_python ¶ isinstance_python(\n    input: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Similar to  but returns a boolean. Arguments match validate_python() . This method will not raise ValidationError s but will raise internal\nerrors. Returns: Type Description True if validation succeeds, False if validation fails. validate_json ¶ validate_json(\n    input:  |  | ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate JSON data directly against the schema and return the validated Python object. This method should be significantly faster than validate_python(json.loads(json_data)) as it avoids the\nneed to create intermediate Python objects It also handles constructing the correct Python type even in strict mode, where validate_python(json.loads(json_data)) would fail validation. Parameters: Name Type Description Default input |  | The JSON data to validate. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None self_instance | None An instance of a model set attributes on from validation. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True incomplete JSON will be parsed successfully\nand errors in the last element of sequences and mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails or if the JSON data is invalid. Other error types maybe raised if internal errors occur. Returns: Type Description The validated Python object. validate_strings ¶ validate_strings(\n    input: ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a string against the schema and return the validated Python object. This is similar to validate_json but applies to scenarios where the input will be a string but not\nJSON data, e.g. URL fragments, query parameters, etc. Parameters: Name Type Description Default input The input as a string, or bytes/bytearray if strict=False . required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True errors in the last element of sequences\nand mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails or if the JSON data is invalid. Other error types maybe raised if internal errors occur. Returns: Type Description The validated Python object. validate_assignment ¶ validate_assignment(\n    obj: ,\n    field_name: ,\n    field_value: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> (\n    [, ]\n    | [[, ], [, ] | None, []]\n) Validate an assignment to a field on a model. Parameters: Name Type Description Default obj The model instance being assigned to. required field_name The name of the field to validate assignment for. required field_value The value to assign to the field. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None from_attributes | None Whether to validate objects as inputs to models by extracting attributes.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description [, ] | [[, ], [, ] | None, []] Either the model dict or a tuple of (model_data, model_extra, fields_set) get_default_value ¶ get_default_value(\n    *, strict:  | None = None, context:  = None\n) ->  | None Get the default value for the schema, including running default value validation. Parameters: Name Type Description Default strict | None Whether to validate the default value in strict mode.\nIf None , the value of  is used. None context The context to use for validation, this is passed to functional validators as\n. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description | None None if the schema has no default value, otherwise a  containing the default.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator","title":"pydantic_core - SchemaValidator","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator","rank":90},{"content":"title: The title of the schema, as used in the heading of .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.title","title":"pydantic_core - SchemaValidator - title  property","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.title","rank":85},{"content":"validate_python(\n    input: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a Python object against the schema and return the validated object. Parameters: Name Type Description Default input The Python object to validate. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None from_attributes | None Whether to validate objects as inputs to models by extracting attributes.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None self_instance | None An instance of a model set attributes on from validation, this is used when running\nvalidation from the __init__ method of a model. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True errors in the last element of sequences\nand mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description The validated object.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_python","title":"pydantic_core - SchemaValidator - validate_python","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_python","rank":80},{"content":"isinstance_python(\n    input: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Similar to  but returns a boolean. Arguments match validate_python() . This method will not raise ValidationError s but will raise internal\nerrors. Returns: Type Description True if validation succeeds, False if validation fails.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.isinstance_python","title":"pydantic_core - SchemaValidator - isinstance_python","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.isinstance_python","rank":75},{"content":"validate_json(\n    input:  |  | ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    self_instance:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate JSON data directly against the schema and return the validated Python object. This method should be significantly faster than validate_python(json.loads(json_data)) as it avoids the\nneed to create intermediate Python objects It also handles constructing the correct Python type even in strict mode, where validate_python(json.loads(json_data)) would fail validation. Parameters: Name Type Description Default input |  | The JSON data to validate. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None self_instance | None An instance of a model set attributes on from validation. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True incomplete JSON will be parsed successfully\nand errors in the last element of sequences and mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails or if the JSON data is invalid. Other error types maybe raised if internal errors occur. Returns: Type Description The validated Python object.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_json","title":"pydantic_core - SchemaValidator - validate_json","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_json","rank":70},{"content":"validate_strings(\n    input: ,\n    *,\n    strict:  | None = None,\n    context:  | None = None,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> Validate a string against the schema and return the validated Python object. This is similar to validate_json but applies to scenarios where the input will be a string but not\nJSON data, e.g. URL fragments, query parameters, etc. Parameters: Name Type Description Default input The input as a string, or bytes/bytearray if strict=False . required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial validation; if True errors in the last element of sequences\nand mappings are ignored. 'trailing-strings' means any final unfinished JSON string is included in the result. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails or if the JSON data is invalid. Other error types maybe raised if internal errors occur. Returns: Type Description The validated Python object.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_strings","title":"pydantic_core - SchemaValidator - validate_strings","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_strings","rank":65},{"content":"validate_assignment(\n    obj: ,\n    field_name: ,\n    field_value: ,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context:  | None = None,\n    by_alias:  | None = None,\n    by_name:  | None = None\n) -> (\n    [, ]\n    | [[, ], [, ] | None, []]\n) Validate an assignment to a field on a model. Parameters: Name Type Description Default obj The model instance being assigned to. required field_name The name of the field to validate assignment for. required field_value The value to assign to the field. required strict | None Whether to validate the object in strict mode.\nIf None , the value of  is used. None from_attributes | None Whether to validate objects as inputs to models by extracting attributes.\nIf None , the value of  is used. None context | None The context to use for validation, this is passed to functional validators as\n. None by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description [, ] | [[, ], [, ] | None, []] Either the model dict or a tuple of (model_data, model_extra, fields_set)","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_assignment","title":"pydantic_core - SchemaValidator - validate_assignment","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.validate_assignment","rank":60},{"content":"get_default_value(\n    *, strict:  | None = None, context:  = None\n) ->  | None Get the default value for the schema, including running default value validation. Parameters: Name Type Description Default strict | None Whether to validate the default value in strict mode.\nIf None , the value of  is used. None context The context to use for validation, this is passed to functional validators as\n. None Raises: Type Description If validation fails. Other error types maybe raised if internal errors occur. Returns: Type Description | None None if the schema has no default value, otherwise a  containing the default.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.get_default_value","title":"pydantic_core - SchemaValidator - get_default_value","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaValidator.get_default_value","rank":55},{"content":"SchemaSerializer(\n    schema: , config:  | None = None\n) SchemaSerializer is the Python wrapper for pydantic-core 's Rust serialization logic, internally it owns one CombinedSerializer which may in turn own more CombinedSerializer s which make up the full schema serializer. Parameters: Name Type Description Default schema The CoreSchema to use for serialization. required config | None Optionally a  to to configure serialization. None to_python ¶ to_python(\n    value: ,\n    *,\n    mode:  | None = None,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize/marshal a Python object to a Python object including transforming and filtering data. Parameters: Name Type Description Default value The Python object to serialize. required mode | None The serialization mode to use, either 'python' or 'json' , defaults to 'python' . In JSON mode,\nall values are converted to JSON compatible types, e.g. None , int , float , str , list , dict . None include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias | None Whether to use the alias names of fields. None exclude_unset Whether to exclude fields that are not set,\ne.g. are not included in __pydantic_fields_set__ . False exclude_defaults Whether to exclude fields that are equal to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False warnings | ['none', 'warn', 'error'] How to handle invalid fields. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description The serialized Python object. to_json ¶ to_json(\n    value: ,\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize a Python object to JSON including transforming and filtering data. Parameters: Name Type Description Default value The Python object to serialize. required indent | None If None , the JSON will be compact, otherwise it will be pretty-printed with the indent provided. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias | None Whether to use the alias names of fields. None exclude_unset Whether to exclude fields that are not set,\ne.g. are not included in __pydantic_fields_set__ . False exclude_defaults Whether to exclude fields that are equal to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False warnings | ['none', 'warn', 'error'] How to handle invalid fields. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description JSON bytes.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaSerializer","title":"pydantic_core - SchemaSerializer","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaSerializer","rank":50},{"content":"to_python(\n    value: ,\n    *,\n    mode:  | None = None,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize/marshal a Python object to a Python object including transforming and filtering data. Parameters: Name Type Description Default value The Python object to serialize. required mode | None The serialization mode to use, either 'python' or 'json' , defaults to 'python' . In JSON mode,\nall values are converted to JSON compatible types, e.g. None , int , float , str , list , dict . None include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias | None Whether to use the alias names of fields. None exclude_unset Whether to exclude fields that are not set,\ne.g. are not included in __pydantic_fields_set__ . False exclude_defaults Whether to exclude fields that are equal to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False warnings | ['none', 'warn', 'error'] How to handle invalid fields. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description The serialized Python object.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaSerializer.to_python","title":"pydantic_core - SchemaSerializer - to_python","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaSerializer.to_python","rank":45},{"content":"to_json(\n    value: ,\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize a Python object to JSON including transforming and filtering data. Parameters: Name Type Description Default value The Python object to serialize. required indent | None If None , the JSON will be compact, otherwise it will be pretty-printed with the indent provided. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias | None Whether to use the alias names of fields. None exclude_unset Whether to exclude fields that are not set,\ne.g. are not included in __pydantic_fields_set__ . False exclude_defaults Whether to exclude fields that are equal to their default value. False exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False warnings | ['none', 'warn', 'error'] How to handle invalid fields. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description JSON bytes.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaSerializer.to_json","title":"pydantic_core - SchemaSerializer - to_json","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaSerializer.to_json","rank":40},{"content":"Bases: ValidationError is the exception raised by pydantic-core when validation fails, it contains a list of errors\nwhich detail why validation failed. title property ¶ title: The title of the error, as used in the heading of str(validation_error) . from_exception_data classmethod ¶ from_exception_data(\n    title: ,\n    line_errors: [],\n    input_type: [\"python\", \"json\"] = \"python\",\n    hide_input:  = False,\n) -> Python constructor for a Validation Error. The API for constructing validation errors will probably change in the future,\nhence the static method rather than __init__ . Parameters: Name Type Description Default title The title of the error, as used in the heading of str(validation_error) required line_errors [] A list of  which contain information\nabout errors that occurred during validation. required input_type ['python', 'json'] Whether the error is for a Python object or JSON. 'python' hide_input Whether to hide the input value in the error message. False error_count ¶ error_count() -> Returns: Type Description The number of errors in the validation error. errors ¶ errors(\n    *,\n    include_url:  = True,\n    include_context:  = True,\n    include_input:  = True\n) -> [] Details about each error in the validation error. Parameters: Name Type Description Default include_url Whether to include a URL to documentation on the error each error. True include_context Whether to include the context of each error. True include_input Whether to include the input value of each error. True Returns: Type Description [] A list of  for each error in the validation error. json ¶ json(\n    *,\n    indent:  | None = None,\n    include_url:  = True,\n    include_context:  = True,\n    include_input:  = True\n) -> Same as  but returns a JSON string. Parameters: Name Type Description Default indent | None The number of spaces to indent the JSON by, or None for no indentation - compact JSON. None include_url Whether to include a URL to documentation on the error each error. True include_context Whether to include the context of each error. True include_input Whether to include the input value of each error. True Returns: Type Description a JSON string.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ValidationError","title":"pydantic_core - ValidationError","objectID":"/latest/api/pydantic_core/#pydantic_core.ValidationError","rank":35},{"content":"title: The title of the error, as used in the heading of str(validation_error) .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ValidationError.title","title":"pydantic_core - ValidationError - title  property","objectID":"/latest/api/pydantic_core/#pydantic_core.ValidationError.title","rank":30},{"content":"from_exception_data(\n    title: ,\n    line_errors: [],\n    input_type: [\"python\", \"json\"] = \"python\",\n    hide_input:  = False,\n) -> Python constructor for a Validation Error. The API for constructing validation errors will probably change in the future,\nhence the static method rather than __init__ . Parameters: Name Type Description Default title The title of the error, as used in the heading of str(validation_error) required line_errors [] A list of  which contain information\nabout errors that occurred during validation. required input_type ['python', 'json'] Whether the error is for a Python object or JSON. 'python' hide_input Whether to hide the input value in the error message. False","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ValidationError.from_exception_data","title":"pydantic_core - ValidationError - from_exception_data  classmethod","objectID":"/latest/api/pydantic_core/#pydantic_core.ValidationError.from_exception_data","rank":25},{"content":"error_count() -> Returns: Type Description The number of errors in the validation error.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ValidationError.error_count","title":"pydantic_core - ValidationError - error_count","objectID":"/latest/api/pydantic_core/#pydantic_core.ValidationError.error_count","rank":20},{"content":"errors(\n    *,\n    include_url:  = True,\n    include_context:  = True,\n    include_input:  = True\n) -> [] Details about each error in the validation error. Parameters: Name Type Description Default include_url Whether to include a URL to documentation on the error each error. True include_context Whether to include the context of each error. True include_input Whether to include the input value of each error. True Returns: Type Description [] A list of  for each error in the validation error.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ValidationError.errors","title":"pydantic_core - ValidationError - errors","objectID":"/latest/api/pydantic_core/#pydantic_core.ValidationError.errors","rank":15},{"content":"json(\n    *,\n    indent:  | None = None,\n    include_url:  = True,\n    include_context:  = True,\n    include_input:  = True\n) -> Same as  but returns a JSON string. Parameters: Name Type Description Default indent | None The number of spaces to indent the JSON by, or None for no indentation - compact JSON. None include_url Whether to include a URL to documentation on the error each error. True include_context Whether to include the context of each error. True include_input Whether to include the input value of each error. True Returns: Type Description a JSON string.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ValidationError.json","title":"pydantic_core - ValidationError - json","objectID":"/latest/api/pydantic_core/#pydantic_core.ValidationError.json","rank":10},{"content":"Bases: type instance-attribute ¶ type: The type of error that occurred, this is an identifier designed for\nprogrammatic use that will change rarely or never. type is unique for each error message, and can hence be used as an identifier to build custom error messages. loc instance-attribute ¶ loc: [ | , ...] Tuple of strings and ints identifying where in the schema the error occurred. msg instance-attribute ¶ msg: A human readable error message. input instance-attribute ¶ input: The input data at this loc that caused the error. ctx instance-attribute ¶ ctx: [[, ]] Values which are required to render the error message, and could hence be useful in rendering custom error messages.\nAlso useful for passing custom error data forward. url instance-attribute ¶ url: [] The documentation URL giving information about the error. No URL is available if\na  is used.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails","title":"pydantic_core - ErrorDetails","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails","rank":5},{"content":"type: The type of error that occurred, this is an identifier designed for\nprogrammatic use that will change rarely or never. type is unique for each error message, and can hence be used as an identifier to build custom error messages.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.type","title":"pydantic_core - ErrorDetails - type  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.type","rank":0},{"content":"loc: [ | , ...] Tuple of strings and ints identifying where in the schema the error occurred.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.loc","title":"pydantic_core - ErrorDetails - loc  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.loc","rank":-5},{"content":"msg: A human readable error message.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.msg","title":"pydantic_core - ErrorDetails - msg  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.msg","rank":-10},{"content":"input: The input data at this loc that caused the error.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.input","title":"pydantic_core - ErrorDetails - input  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.input","rank":-15},{"content":"ctx: [[, ]] Values which are required to render the error message, and could hence be useful in rendering custom error messages.\nAlso useful for passing custom error data forward.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.ctx","title":"pydantic_core - ErrorDetails - ctx  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.ctx","rank":-20},{"content":"url: [] The documentation URL giving information about the error. No URL is available if\na  is used.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.url","title":"pydantic_core - ErrorDetails - url  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorDetails.url","rank":-25},{"content":"Bases: type instance-attribute ¶ type:  | The type of error that occurred, this should be a \"slug\" identifier that changes rarely or never. loc instance-attribute ¶ loc: [[ | , ...]] Tuple of strings and ints identifying where in the schema the error occurred. input instance-attribute ¶ input: The input data at this loc that caused the error. ctx instance-attribute ¶ ctx: [[, ]] Values which are required to render the error message, and could hence be useful in rendering custom error messages.\nAlso useful for passing custom error data forward.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails","title":"pydantic_core - InitErrorDetails","objectID":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails","rank":-30},{"content":"type:  | The type of error that occurred, this should be a \"slug\" identifier that changes rarely or never.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.type","title":"pydantic_core - InitErrorDetails - type  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.type","rank":-35},{"content":"loc: [[ | , ...]] Tuple of strings and ints identifying where in the schema the error occurred.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.loc","title":"pydantic_core - InitErrorDetails - loc  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.loc","rank":-40},{"content":"input: The input data at this loc that caused the error.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.input","title":"pydantic_core - InitErrorDetails - input  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.input","rank":-45},{"content":"ctx: [[, ]] Values which are required to render the error message, and could hence be useful in rendering custom error messages.\nAlso useful for passing custom error data forward.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.ctx","title":"pydantic_core - InitErrorDetails - ctx  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.InitErrorDetails.ctx","rank":-50},{"content":"Bases: Information about errors that occur while building a \nor . error_count ¶ error_count() -> Returns: Type Description The number of errors in the schema. errors ¶ errors() -> [] Returns: Type Description [] A list of  for each error in the schema.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaError","title":"pydantic_core - SchemaError","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaError","rank":-55},{"content":"error_count() -> Returns: Type Description The number of errors in the schema.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaError.error_count","title":"pydantic_core - SchemaError - error_count","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaError.error_count","rank":-60},{"content":"errors() -> [] Returns: Type Description [] A list of  for each error in the schema.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.SchemaError.errors","title":"pydantic_core - SchemaError - errors","objectID":"/latest/api/pydantic_core/#pydantic_core.SchemaError.errors","rank":-65},{"content":"PydanticCustomError(\n    error_type: ,\n    message_template: ,\n    context: [, ] | None = None,\n) Bases: A custom exception providing flexible error handling for Pydantic validators. You can raise this error in custom validators when you'd like flexibility in regards to the error type, message, and context. Parameters: Name Type Description Default error_type The error type. required message_template The message template. required context [, ] | None The data to inject into the message template. None context property ¶ context: [, ] | None Values which are required to render the error message, and could hence be useful in passing error data forward. type property ¶ type: The error type associated with the error. For consistency with Pydantic, this is typically a snake_case string. message_template property ¶ message_template: The message template associated with the error. This is a string that can be formatted with context variables in {curly_braces} . message ¶ message() -> The formatted message associated with the error. This presents as the message template with context variables appropriately injected.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError","title":"pydantic_core - PydanticCustomError","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError","rank":-70},{"content":"context: [, ] | None Values which are required to render the error message, and could hence be useful in passing error data forward.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.context","title":"pydantic_core - PydanticCustomError - context  property","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.context","rank":-75},{"content":"type: The error type associated with the error. For consistency with Pydantic, this is typically a snake_case string.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.type","title":"pydantic_core - PydanticCustomError - type  property","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.type","rank":-80},{"content":"message_template: The message template associated with the error. This is a string that can be formatted with context variables in {curly_braces} .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.message_template","title":"pydantic_core - PydanticCustomError - message_template  property","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.message_template","rank":-85},{"content":"message() -> The formatted message associated with the error. This presents as the message template with context variables appropriately injected.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.message","title":"pydantic_core - PydanticCustomError - message","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticCustomError.message","rank":-90},{"content":"PydanticKnownError(\n    error_type: ,\n    context: [, ] | None = None,\n) Bases: A helper class for raising exceptions that mimic Pydantic's built-in exceptions, with more flexibility in regards to context. Unlike , the error_type argument must be a known ErrorType . Parameters: Name Type Description Default error_type The error type. required context [, ] | None The data to inject into the message template. None context property ¶ context: [, ] | None Values which are required to render the error message, and could hence be useful in passing error data forward. type property ¶ type: The type of the error. message_template property ¶ message_template: The message template associated with the provided error type. This is a string that can be formatted with context variables in {curly_braces} . message ¶ message() -> The formatted message associated with the error. This presents as the message template with context variables appropriately injected.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError","title":"pydantic_core - PydanticKnownError","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError","rank":-95},{"content":"context: [, ] | None Values which are required to render the error message, and could hence be useful in passing error data forward.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.context","title":"pydantic_core - PydanticKnownError - context  property","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.context","rank":-100},{"content":"type: The type of the error.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.type","title":"pydantic_core - PydanticKnownError - type  property","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.type","rank":-105},{"content":"message_template: The message template associated with the provided error type. This is a string that can be formatted with context variables in {curly_braces} .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.message_template","title":"pydantic_core - PydanticKnownError - message_template  property","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.message_template","rank":-110},{"content":"message() -> The formatted message associated with the error. This presents as the message template with context variables appropriately injected.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.message","title":"pydantic_core - PydanticKnownError - message","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticKnownError.message","rank":-115},{"content":"Bases: An exception to signal that a field should be omitted from a generated result. This could span from omitting a field from a JSON Schema to omitting a field from a serialized result.\nUpcoming: more robust support for using PydanticOmit in custom serializers is still in development.\nRight now, this is primarily used in the JSON Schema generation process. For a more in depth example / explanation, see the customizing JSON schema docs.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticOmit","title":"pydantic_core - PydanticOmit","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticOmit","rank":-120},{"content":"Bases: An exception to signal that standard validation either failed or should be skipped, and the default value should be used instead. This warning can be raised in custom valiation functions to redirect the flow of validation. For an additional example, see the validating partial json data section of the Pydantic documentation.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticUseDefault","title":"pydantic_core - PydanticUseDefault","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticUseDefault","rank":-125},{"content":"PydanticSerializationError(message: ) Bases: An error raised when an issue occurs during serialization. In custom serializers, this error can be used to indicate that serialization has failed. Parameters: Name Type Description Default message The message associated with the error. required","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticSerializationError","title":"pydantic_core - PydanticSerializationError","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticSerializationError","rank":-130},{"content":"PydanticSerializationUnexpectedValue(message: ) Bases: An error raised when an unexpected value is encountered during serialization. This error is often caught and coerced into a warning, as pydantic-core generally makes a best attempt\nat serializing values, in contrast with validation where errors are eagerly raised. This is often used internally in pydantic-core when unexpected types are encountered during serialization,\nbut it can also be used by users in custom serializers, as seen above. Parameters: Name Type Description Default message The message associated with the unexpected value. required","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.PydanticSerializationUnexpectedValue","title":"pydantic_core - PydanticSerializationUnexpectedValue","objectID":"/latest/api/pydantic_core/#pydantic_core.PydanticSerializationUnexpectedValue","rank":-135},{"content":"Url(url: ) Bases: A URL type, internal logic uses the url rust crate originally developed\nby Mozilla.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.Url","title":"pydantic_core - Url","objectID":"/latest/api/pydantic_core/#pydantic_core.Url","rank":-140},{"content":"MultiHostUrl(url: ) Bases: A URL type with support for multiple hosts, as used by some databases for DSNs, e.g. https://foo.com,bar.com/path . Internal URL logic uses the url rust crate originally developed\nby Mozilla.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.MultiHostUrl","title":"pydantic_core - MultiHostUrl","objectID":"/latest/api/pydantic_core/#pydantic_core.MultiHostUrl","rank":-145},{"content":"Bases: A host part of a multi-host URL. username instance-attribute ¶ username:  | None The username part of this host, or None . password instance-attribute ¶ password:  | None The password part of this host, or None . host instance-attribute ¶ host:  | None The host part of this host, or None . port instance-attribute ¶ port:  | None The port part of this host, or None .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost","title":"pydantic_core - MultiHostHost","objectID":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost","rank":-150},{"content":"username:  | None The username part of this host, or None .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.username","title":"pydantic_core - MultiHostHost - username  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.username","rank":-155},{"content":"password:  | None The password part of this host, or None .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.password","title":"pydantic_core - MultiHostHost - password  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.password","rank":-160},{"content":"host:  | None The host part of this host, or None .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.host","title":"pydantic_core - MultiHostHost - host  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.host","rank":-165},{"content":"port:  | None The port part of this host, or None .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.port","title":"pydantic_core - MultiHostHost - port  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.MultiHostHost.port","rank":-170},{"content":"ArgsKwargs(\n    args: [, ...],\n    kwargs: [, ] | None = None,\n) A construct used to store arguments and keyword arguments for a function call. This data structure is generally used to store information for core schemas associated with functions (like in an arguments schema).\nThis data structure is also currently used for some validation against dataclasses. Parameters: Name Type Description Default args [, ...] The arguments (inherently ordered) for a function call. required kwargs [, ] | None The keyword arguments for a function call None args property ¶ args: [, ...] The arguments (inherently ordered) for a function call. kwargs property ¶ kwargs: [, ] | None The keyword arguments for a function call.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ArgsKwargs","title":"pydantic_core - ArgsKwargs","objectID":"/latest/api/pydantic_core/#pydantic_core.ArgsKwargs","rank":-175},{"content":"args: [, ...] The arguments (inherently ordered) for a function call.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ArgsKwargs.args","title":"pydantic_core - ArgsKwargs - args  property","objectID":"/latest/api/pydantic_core/#pydantic_core.ArgsKwargs.args","rank":-180},{"content":"kwargs: [, ] | None The keyword arguments for a function call.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ArgsKwargs.kwargs","title":"pydantic_core - ArgsKwargs - kwargs  property","objectID":"/latest/api/pydantic_core/#pydantic_core.ArgsKwargs.kwargs","rank":-185},{"content":"Bases: [] Similar to Rust's Option::Some type, this\nidentifies a value as being present, and provides a way to access it. Generally used in a union with None to different between \"some value which could be None\" and no value. value property ¶ value: Returns the value wrapped by Some .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.Some","title":"pydantic_core - Some","objectID":"/latest/api/pydantic_core/#pydantic_core.Some","rank":-190},{"content":"value: Returns the value wrapped by Some .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.Some.value","title":"pydantic_core - Some - value  property","objectID":"/latest/api/pydantic_core/#pydantic_core.Some.value","rank":-195},{"content":"Bases: An pydantic-core implementation of the abstract  class. tzname ¶ tzname(dt:  | None) ->  | None Return the time zone name corresponding to the  object dt , as a string. For more info, see . utcoffset ¶ utcoffset(dt:  | None) ->  | None Return offset of local time from UTC, as a  object that is positive east of UTC. If local time is west of UTC, this should be negative. More info can be found at . dst ¶ dst(dt:  | None) ->  | None Return the daylight saving time (DST) adjustment, as a  object or None if DST information isn’t known. More info can be found at. fromutc ¶ fromutc(dt: ) -> Adjust the date and time data associated datetime object dt , returning an equivalent datetime in self’s local time. More info can be found at .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.TzInfo","title":"pydantic_core - TzInfo","objectID":"/latest/api/pydantic_core/#pydantic_core.TzInfo","rank":-200},{"content":"tzname(dt:  | None) ->  | None Return the time zone name corresponding to the  object dt , as a string. For more info, see .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.TzInfo.tzname","title":"pydantic_core - TzInfo - tzname","objectID":"/latest/api/pydantic_core/#pydantic_core.TzInfo.tzname","rank":-205},{"content":"utcoffset(dt:  | None) ->  | None Return offset of local time from UTC, as a  object that is positive east of UTC. If local time is west of UTC, this should be negative. More info can be found at .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.TzInfo.utcoffset","title":"pydantic_core - TzInfo - utcoffset","objectID":"/latest/api/pydantic_core/#pydantic_core.TzInfo.utcoffset","rank":-210},{"content":"dst(dt:  | None) ->  | None Return the daylight saving time (DST) adjustment, as a  object or None if DST information isn’t known. More info can be found at.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.TzInfo.dst","title":"pydantic_core - TzInfo - dst","objectID":"/latest/api/pydantic_core/#pydantic_core.TzInfo.dst","rank":-215},{"content":"fromutc(dt: ) -> Adjust the date and time data associated datetime object dt , returning an equivalent datetime in self’s local time. More info can be found at .","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.TzInfo.fromutc","title":"pydantic_core - TzInfo - fromutc","objectID":"/latest/api/pydantic_core/#pydantic_core.TzInfo.fromutc","rank":-220},{"content":"Bases: Gives information about errors. type instance-attribute ¶ type: The type of error that occurred, this should a \"slug\" identifier that changes rarely or never. message_template_python instance-attribute ¶ message_template_python: String template to render a human readable error message from using context, when the input is Python. example_message_python instance-attribute ¶ example_message_python: Example of a human readable error message, when the input is Python. message_template_json instance-attribute ¶ message_template_json: [] String template to render a human readable error message from using context, when the input is JSON data. example_message_json instance-attribute ¶ example_message_json: [] Example of a human readable error message, when the input is JSON data. example_context instance-attribute ¶ example_context: [, ] | None Example of context values.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo","title":"pydantic_core - ErrorTypeInfo","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo","rank":-225},{"content":"type: The type of error that occurred, this should a \"slug\" identifier that changes rarely or never.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.type","title":"pydantic_core - ErrorTypeInfo - type  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.type","rank":-230},{"content":"message_template_python: String template to render a human readable error message from using context, when the input is Python.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.message_template_python","title":"pydantic_core - ErrorTypeInfo - message_template_python  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.message_template_python","rank":-235},{"content":"example_message_python: Example of a human readable error message, when the input is Python.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.example_message_python","title":"pydantic_core - ErrorTypeInfo - example_message_python  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.example_message_python","rank":-240},{"content":"message_template_json: [] String template to render a human readable error message from using context, when the input is JSON data.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.message_template_json","title":"pydantic_core - ErrorTypeInfo - message_template_json  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.message_template_json","rank":-245},{"content":"example_message_json: [] Example of a human readable error message, when the input is JSON data.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.example_message_json","title":"pydantic_core - ErrorTypeInfo - example_message_json  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.example_message_json","rank":-250},{"content":"example_context: [, ] | None Example of context values.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.example_context","title":"pydantic_core - ErrorTypeInfo - example_context  instance-attribute","objectID":"/latest/api/pydantic_core/#pydantic_core.ErrorTypeInfo.example_context","rank":-255},{"content":"to_json(\n    value: ,\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  = True,\n    exclude_none:  = False,\n    round_trip:  = False,\n    timedelta_mode: [\"iso8601\", \"float\"] = \"iso8601\",\n    temporal_mode: [\n        \"iso8601\", \"seconds\", \"milliseconds\"\n    ] = \"iso8601\",\n    bytes_mode: [\"utf8\", \"base64\", \"hex\"] = \"utf8\",\n    inf_nan_mode: [\n        \"null\", \"constants\", \"strings\"\n    ] = \"constants\",\n    serialize_unknown:  = False,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize a Python object to JSON including transforming and filtering data. This is effectively a standalone version of . Parameters: Name Type Description Default value The Python object to serialize. required indent | None If None , the JSON will be compact, otherwise it will be pretty-printed with the indent provided. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias Whether to use the alias names of fields. True exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False timedelta_mode ['iso8601', 'float'] How to serialize timedelta objects, either 'iso8601' or 'float' . 'iso8601' temporal_mode ['iso8601', 'seconds', 'milliseconds'] How to serialize datetime-like objects ( datetime , date , time ), either 'iso8601' , 'seconds' , or 'milliseconds' . iso8601 returns an ISO 8601 string; seconds returns the Unix timestamp in seconds as a float; milliseconds returns the Unix timestamp in milliseconds as a float. 'iso8601' bytes_mode ['utf8', 'base64', 'hex'] How to serialize bytes objects, either 'utf8' , 'base64' , or 'hex' . 'utf8' inf_nan_mode ['null', 'constants', 'strings'] How to serialize Infinity , -Infinity and NaN values, either 'null' , 'constants' , or 'strings' . 'constants' serialize_unknown Attempt to serialize unknown types, str(value) will be used, if that fails \"<Unserializable {value_type} object>\" will be used. False fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description JSON bytes.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.to_json","title":"pydantic_core - to_json","objectID":"/latest/api/pydantic_core/#pydantic_core.to_json","rank":-260},{"content":"from_json(\n    data:  |  | ,\n    *,\n    allow_inf_nan:  = True,\n    cache_strings: (\n         | [\"all\", \"keys\", \"none\"]\n    ) = True,\n    allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False\n) -> Deserialize JSON data to a Python object. This is effectively a faster version of json.loads() , with some extra functionality. Parameters: Name Type Description Default data |  | The JSON data to deserialize. required allow_inf_nan Whether to allow Infinity , -Infinity and NaN values as json.loads() does by default. True cache_strings | ['all', 'keys', 'none'] Whether to cache strings to avoid constructing new Python objects,\nthis should have a significant impact on performance while increasing memory usage slightly, all/True means cache all strings, keys means cache only dict keys, none/False means no caching. True allow_partial | ['off', 'on', 'trailing-strings'] Whether to allow partial deserialization, if True JSON data is returned if the end of the\ninput is reached before the full object is deserialized, e.g. [\"aa\", \"bb\", \"c would return ['aa', 'bb'] . 'trailing-strings' means any final unfinished JSON string is included in the result. False Raises: Type Description If deserialization fails. Returns: Type Description The deserialized Python object.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.from_json","title":"pydantic_core - from_json","objectID":"/latest/api/pydantic_core/#pydantic_core.from_json","rank":-265},{"content":"to_jsonable_python(\n    value: ,\n    *,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  = True,\n    exclude_none:  = False,\n    round_trip:  = False,\n    timedelta_mode: [\"iso8601\", \"float\"] = \"iso8601\",\n    temporal_mode: [\n        \"iso8601\", \"seconds\", \"milliseconds\"\n    ] = \"iso8601\",\n    bytes_mode: [\"utf8\", \"base64\", \"hex\"] = \"utf8\",\n    inf_nan_mode: [\n        \"null\", \"constants\", \"strings\"\n    ] = \"constants\",\n    serialize_unknown:  = False,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context:  | None = None\n) -> Serialize/marshal a Python object to a JSON-serializable Python object including transforming and filtering data. This is effectively a standalone version of\n. Parameters: Name Type Description Default value The Python object to serialize. required include | None A set of fields to include, if None all fields are included. None exclude | None A set of fields to exclude, if None no fields are excluded. None by_alias Whether to use the alias names of fields. True exclude_none Whether to exclude fields that have a value of None . False round_trip Whether to enable serialization and validation round-trip support. False timedelta_mode ['iso8601', 'float'] How to serialize timedelta objects, either 'iso8601' or 'float' . 'iso8601' temporal_mode ['iso8601', 'seconds', 'milliseconds'] How to serialize datetime-like objects ( datetime , date , time ), either 'iso8601' , 'seconds' , or 'milliseconds' . iso8601 returns an ISO 8601 string; seconds returns the Unix timestamp in seconds as a float; milliseconds returns the Unix timestamp in milliseconds as a float. 'iso8601' bytes_mode ['utf8', 'base64', 'hex'] How to serialize bytes objects, either 'utf8' , 'base64' , or 'hex' . 'utf8' inf_nan_mode ['null', 'constants', 'strings'] How to serialize Infinity , -Infinity and NaN values, either 'null' , 'constants' , or 'strings' . 'constants' serialize_unknown Attempt to serialize unknown types, str(value) will be used, if that fails \"<Unserializable {value_type} object>\" will be used. False fallback [[], ] | None A function to call when an unknown value is encountered,\nif None a  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context | None The context to use for serialization, this is passed to functional serializers as\n. None Raises: Type Description If serialization fails and no fallback function is provided. Returns: Type Description The serialized Python object.","pageID":"pydantic_core","abs_url":"/latest/api/pydantic_core/#pydantic_core.to_jsonable_python","title":"pydantic_core - to_jsonable_python","objectID":"/latest/api/pydantic_core/#pydantic_core.to_jsonable_python","rank":-270},{"content":"This module contains definitions to build schemas which pydantic_core can\nvalidate and serialize. WhenUsed module-attribute ¶ WhenUsed = [\n    \"always\", \"unless-none\", \"json\", \"json-unless-none\"\n] Values have the following meanings: 'always' means always use 'unless-none' means use unless the value is None 'json' means use when serializing to JSON 'json-unless-none' means use when serializing to JSON and the value is not None CoreConfig ¶ Bases: Base class for schema configuration options. Attributes: Name Type Description The name of the configuration. Whether the configuration should strictly adhere to specified rules. The behavior for handling extra fields. Whether the TypedDict should be considered total. Default is True . Whether to use attributes for models, dataclasses, and tagged union keys. Whether to use the used alias (or first alias for \"field required\" errors) instead of field_names to construct error loc s. Default is True . ['always', 'never', 'subclass-instances'] Whether instances of models and dataclasses should re-validate. Default is 'never'. Whether to validate default values during validation. Default is False . The maximum length for string fields. The minimum length for string fields. Whether to strip whitespace from string fields. Whether to convert string fields to lowercase. Whether to convert string fields to uppercase. Whether to allow infinity and NaN values for float fields. Default is True . ['iso8601', 'float'] The serialization option for timedelta values. Default is 'iso8601'.\nNote that if ser_json_temporal is set, then this param will be ignored. ['iso8601', 'seconds', 'milliseconds'] The serialization option for datetime like values. Default is 'iso8601'.\nThe types this covers are datetime, date, time and timedelta.\nIf this is set, it will take precedence over ser_json_timedelta ['utf8', 'base64', 'hex'] The serialization option for bytes values. Default is 'utf8'. ['null', 'constants', 'strings'] The serialization option for infinity and NaN values\nin float fields. Default is 'null'. ['utf8', 'base64', 'hex'] The validation option for bytes values, complementing ser_json_bytes. Default is 'utf8'. Whether to hide input data from ValidationError representation. Whether to add user-python excs to the cause of a ValidationError.\nRequires exceptiongroup backport pre Python 3.11. Whether to enable coercion of any Number type to str (not applicable in strict mode). ['rust-regex', 'python-re'] The regex engine to use for regex pattern validation. Default is 'rust-regex'. See StringSchema . [, ['all', 'keys', 'none']] Whether to cache strings. Default is True , True or 'all' is required to cache strings\nduring general validation since validators don't know if they're in a key or a value. Whether to use the field's alias when validating against the provided input data. Default is True . Whether to use the field's name when validating against the provided input data. Default is False . Replacement for populate_by_name . Whether to serialize by alias. Default is False , expected to change to True in V3. SerializationInfo ¶ Bases: [] Extra data used during serialization. include property ¶ include: The include argument set during serialization. exclude property ¶ exclude: The exclude argument set during serialization. context property ¶ context: The current serialization context. mode property ¶ mode: ['python', 'json'] The serialization mode set during serialization. by_alias property ¶ by_alias: The by_alias argument set during serialization. exclude_unset property ¶ exclude_unset: The exclude_unset argument set during serialization. exclude_defaults property ¶ exclude_defaults: The exclude_defaults argument set during serialization. exclude_none property ¶ exclude_none: The exclude_none argument set during serialization. serialize_as_any property ¶ serialize_as_any: The serialize_as_any argument set during serialization. round_trip property ¶ round_trip: The round_trip argument set during serialization. FieldSerializationInfo ¶ Bases: [] , Extra data used during field serialization. field_name property ¶ field_name: The name of the current field being serialized. ValidationInfo ¶ Bases: [] Extra data used during validation. context property ¶ context: The current validation context. config property ¶ config:  | None The CoreConfig that applies to this validation. mode property ¶ mode: ['python', 'json'] The type of input data we are currently validating. data property ¶ data: [, ] The data being validated for this model. field_name property ¶ field_name:  | None The name of the current field being validated if this validator is\nattached to a model field. simple_ser_schema ¶ simple_ser_schema(\n    type: ,\n) -> Returns a schema for serialization with a custom type. Parameters: Name Type Description Default type The type to use for serialization required plain_serializer_function_ser_schema ¶ plain_serializer_function_ser_schema(\n    function: ,\n    *,\n    is_field_serializer:  | None = None,\n    info_arg:  | None = None,\n    return_schema:  | None = None,\n    when_used:  = \"always\"\n) -> Returns a schema for serialization with a function, can be either a \"general\" or \"field\" function. Parameters: Name Type Description Default function The function to use for serialization required is_field_serializer | None Whether the serializer is for a field, e.g. takes model as the first argument,\nand info includes field_name None info_arg | None Whether the function takes an info argument None return_schema | None Schema to use for serializing return value None when_used When the function should be called 'always' wrap_serializer_function_ser_schema ¶ wrap_serializer_function_ser_schema(\n    function: ,\n    *,\n    is_field_serializer:  | None = None,\n    info_arg:  | None = None,\n    schema:  | None = None,\n    return_schema:  | None = None,\n    when_used:  = \"always\"\n) -> Returns a schema for serialization with a wrap function, can be either a \"general\" or \"field\" function. Parameters: Name Type Description Default function The function to use for serialization required is_field_serializer | None Whether the serializer is for a field, e.g. takes model as the first argument,\nand info includes field_name None info_arg | None Whether the function takes an info argument None schema | None The schema to use for the inner serialization None return_schema | None Schema to use for serializing return value None when_used When the function should be called 'always' format_ser_schema ¶ format_ser_schema(\n    formatting_string: ,\n    *,\n    when_used:  = \"json-unless-none\"\n) -> Returns a schema for serialization using python's format method. Parameters: Name Type Description Default formatting_string String defining the format to use required when_used Same meaning as for [general_function_plain_ser_schema], but with a different default 'json-unless-none' to_string_ser_schema ¶ to_string_ser_schema(\n    *, when_used:  = \"json-unless-none\"\n) -> Returns a schema for serialization using python's str() / __str__ method. Parameters: Name Type Description Default when_used Same meaning as for [general_function_plain_ser_schema], but with a different default 'json-unless-none' model_ser_schema ¶ model_ser_schema(\n    cls: [], schema: \n) -> Returns a schema for serialization using a model. Parameters: Name Type Description Default cls [] The expected class type, used to generate warnings if the wrong type is passed required schema Internal schema to use to serialize the model dict required invalid_schema ¶ invalid_schema(\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n) -> Returns an invalid schema, used to indicate that a schema is invalid. Returns a schema that matches any value, e.g.: Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None computed_field ¶ computed_field(\n    property_name: ,\n    return_schema: ,\n    *,\n    alias:  | None = None,\n    metadata: [, ] | None = None\n) -> ComputedFields are properties of a model or dataclass that are included in serialization. Parameters: Name Type Description Default property_name The name of the property on the model or dataclass required return_schema The schema used for the type returned by the computed field required alias | None The name to use in the serialized output None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None any_schema ¶ any_schema(\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches any value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . any_schema () v = SchemaValidator ( schema ) assert v . validate_python ( 1 ) == 1 Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None none_schema ¶ none_schema(\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a None value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . none_schema () v = SchemaValidator ( schema ) assert v . validate_python ( None ) is None Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None bool_schema ¶ bool_schema(\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n) -> Returns a schema that matches a bool value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . bool_schema () v = SchemaValidator ( schema ) assert v . validate_python ( 'True' ) is True Parameters: Name Type Description Default strict | None Whether the value should be a bool or a value that can be converted to a bool None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None int_schema ¶ int_schema(\n    *,\n    multiple_of:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a int value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . int_schema ( multiple_of = 2 , le = 6 , ge = 2 ) v = SchemaValidator ( schema ) assert v . validate_python ( '4' ) == 4 Parameters: Name Type Description Default multiple_of | None The value must be a multiple of this number None le | None The value must be less than or equal to this number None ge | None The value must be greater than or equal to this number None lt | None The value must be strictly less than this number None gt | None The value must be strictly greater than this number None strict | None Whether the value should be a int or a value that can be converted to a int None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None float_schema ¶ float_schema(\n    *,\n    allow_inf_nan:  | None = None,\n    multiple_of:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a float value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . float_schema ( le = 0.8 , ge = 0.2 ) v = SchemaValidator ( schema ) assert v . validate_python ( '0.5' ) == 0.5 Parameters: Name Type Description Default allow_inf_nan | None Whether to allow inf and nan values None multiple_of | None The value must be a multiple of this number None le | None The value must be less than or equal to this number None ge | None The value must be greater than or equal to this number None lt | None The value must be strictly less than this number None gt | None The value must be strictly greater than this number None strict | None Whether the value should be a float or a value that can be converted to a float None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None decimal_schema ¶ decimal_schema(\n    *,\n    allow_inf_nan:  | None = None,\n    multiple_of:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    max_digits:  | None = None,\n    decimal_places:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a decimal value, e.g.: from decimal import Decimal from pydantic_core import SchemaValidator , core_schema schema = core_schema . decimal_schema ( le = 0.8 , ge = 0.2 ) v = SchemaValidator ( schema ) assert v . validate_python ( '0.5' ) == Decimal ( '0.5' ) Parameters: Name Type Description Default allow_inf_nan | None Whether to allow inf and nan values None multiple_of | None The value must be a multiple of this number None le | None The value must be less than or equal to this number None ge | None The value must be greater than or equal to this number None lt | None The value must be strictly less than this number None gt | None The value must be strictly greater than this number None max_digits | None The maximum number of decimal digits allowed None decimal_places | None The maximum number of decimal places allowed None strict | None Whether the value should be a float or a value that can be converted to a float None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None complex_schema ¶ complex_schema(\n    *,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a complex value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . complex_schema () v = SchemaValidator ( schema ) assert v . validate_python ( '1+2j' ) == complex ( 1 , 2 ) assert v . validate_python ( complex ( 1 , 2 )) == complex ( 1 , 2 ) Parameters: Name Type Description Default strict | None Whether the value should be a complex object instance or a value that can be converted to a complex object None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None str_schema ¶ str_schema(\n    *,\n    pattern:  | [] | None = None,\n    max_length:  | None = None,\n    min_length:  | None = None,\n    strip_whitespace:  | None = None,\n    to_lower:  | None = None,\n    to_upper:  | None = None,\n    regex_engine: (\n        [\"rust-regex\", \"python-re\"] | None\n    ) = None,\n    strict:  | None = None,\n    coerce_numbers_to_str:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a string value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . str_schema ( max_length = 10 , min_length = 2 ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello' Parameters: Name Type Description Default pattern | [] | None A regex pattern that the value must match None max_length | None The value must be at most this length None min_length | None The value must be at least this length None strip_whitespace | None Whether to strip whitespace from the value None to_lower | None Whether to convert the value to lowercase None to_upper | None Whether to convert the value to uppercase None regex_engine ['rust-regex', 'python-re'] | None The regex engine to use for pattern validation. Default is 'rust-regex'.\n- rust-regex uses the regex Rust\n  crate, which is non-backtracking and therefore more DDoS\n  resistant, but does not support all regex features.\n- python-re use the re module,\n  which supports all regex features, but may be slower. None strict | None Whether the value should be a string or a value that can be converted to a string None coerce_numbers_to_str | None Whether to enable coercion of any Number type to str (not applicable in strict mode). None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None bytes_schema ¶ bytes_schema(\n    *,\n    max_length:  | None = None,\n    min_length:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a bytes value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . bytes_schema ( max_length = 10 , min_length = 2 ) v = SchemaValidator ( schema ) assert v . validate_python ( b 'hello' ) == b 'hello' Parameters: Name Type Description Default max_length | None The value must be at most this length None min_length | None The value must be at least this length None strict | None Whether the value should be a bytes or a value that can be converted to a bytes None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None date_schema ¶ date_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    now_op: [\"past\", \"future\"] | None = None,\n    now_utc_offset:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a date value, e.g.: from datetime import date from pydantic_core import SchemaValidator , core_schema schema = core_schema . date_schema ( le = date ( 2020 , 1 , 1 ), ge = date ( 2019 , 1 , 1 )) v = SchemaValidator ( schema ) assert v . validate_python ( date ( 2019 , 6 , 1 )) == date ( 2019 , 6 , 1 ) Parameters: Name Type Description Default strict | None Whether the value should be a date or a value that can be converted to a date None le | None The value must be less than or equal to this date None ge | None The value must be greater than or equal to this date None lt | None The value must be strictly less than this date None gt | None The value must be strictly greater than this date None now_op ['past', 'future'] | None The value must be in the past or future relative to the current date None now_utc_offset | None The value must be in the past or future relative to the current date with this utc offset None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None time_schema ¶ time_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    tz_constraint: (\n        [\"aware\", \"naive\"] |  | None\n    ) = None,\n    microseconds_precision: [\n        \"truncate\", \"error\"\n    ] = \"truncate\",\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a time value, e.g.: from datetime import time from pydantic_core import SchemaValidator , core_schema schema = core_schema . time_schema ( le = time ( 12 , 0 , 0 ), ge = time ( 6 , 0 , 0 )) v = SchemaValidator ( schema ) assert v . validate_python ( time ( 9 , 0 , 0 )) == time ( 9 , 0 , 0 ) Parameters: Name Type Description Default strict | None Whether the value should be a time or a value that can be converted to a time None le | None The value must be less than or equal to this time None ge | None The value must be greater than or equal to this time None lt | None The value must be strictly less than this time None gt | None The value must be strictly greater than this time None tz_constraint ['aware', 'naive'] |  | None The value must be timezone aware or naive, or an int to indicate required tz offset None microseconds_precision ['truncate', 'error'] The behavior when seconds have more than 6 digits or microseconds is too large 'truncate' ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None datetime_schema ¶ datetime_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    now_op: [\"past\", \"future\"] | None = None,\n    tz_constraint: (\n        [\"aware\", \"naive\"] |  | None\n    ) = None,\n    now_utc_offset:  | None = None,\n    microseconds_precision: [\n        \"truncate\", \"error\"\n    ] = \"truncate\",\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a datetime value, e.g.: from datetime import datetime from pydantic_core import SchemaValidator , core_schema schema = core_schema . datetime_schema () v = SchemaValidator ( schema ) now = datetime . now () assert v . validate_python ( str ( now )) == now Parameters: Name Type Description Default strict | None Whether the value should be a datetime or a value that can be converted to a datetime None le | None The value must be less than or equal to this datetime None ge | None The value must be greater than or equal to this datetime None lt | None The value must be strictly less than this datetime None gt | None The value must be strictly greater than this datetime None now_op ['past', 'future'] | None The value must be in the past or future relative to the current datetime None tz_constraint ['aware', 'naive'] |  | None The value must be timezone aware or naive, or an int to indicate required tz offset\nTODO: use of a tzinfo where offset changes based on the datetime is not yet supported None now_utc_offset | None The value must be in the past or future relative to the current datetime with this utc offset None microseconds_precision ['truncate', 'error'] The behavior when seconds have more than 6 digits or microseconds is too large 'truncate' ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None timedelta_schema ¶ timedelta_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    microseconds_precision: [\n        \"truncate\", \"error\"\n    ] = \"truncate\",\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a timedelta value, e.g.: from datetime import timedelta from pydantic_core import SchemaValidator , core_schema schema = core_schema . timedelta_schema ( le = timedelta ( days = 1 ), ge = timedelta ( days = 0 )) v = SchemaValidator ( schema ) assert v . validate_python ( timedelta ( hours = 12 )) == timedelta ( hours = 12 ) Parameters: Name Type Description Default strict | None Whether the value should be a timedelta or a value that can be converted to a timedelta None le | None The value must be less than or equal to this timedelta None ge | None The value must be greater than or equal to this timedelta None lt | None The value must be strictly less than this timedelta None gt | None The value must be strictly greater than this timedelta None microseconds_precision ['truncate', 'error'] The behavior when seconds have more than 6 digits or microseconds is too large 'truncate' ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None literal_schema ¶ literal_schema(\n    expected: [],\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a literal value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . literal_schema ([ 'hello' , 'world' ]) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello' Parameters: Name Type Description Default expected [] The value must be one of these values required ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None enum_schema ¶ enum_schema(\n    cls: ,\n    members: [],\n    *,\n    sub_type: [\"str\", \"int\", \"float\"] | None = None,\n    missing: [[], ] | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an enum value, e.g.: from enum import Enum from pydantic_core import SchemaValidator , core_schema class Color ( Enum ): RED = 1 GREEN = 2 BLUE = 3 schema = core_schema . enum_schema ( Color , list ( Color . __members__ . values ())) v = SchemaValidator ( schema ) assert v . validate_python ( 2 ) is Color . GREEN Parameters: Name Type Description Default cls The enum class required members [] The members of the enum, generally list(MyEnum.__members__.values()) required sub_type ['str', 'int', 'float'] | None The type of the enum, either 'str' or 'int' or None for plain enums None missing [[], ] | None A function to use when the value is not found in the enum, from _missing_ None strict | None Whether to use strict mode, defaults to False None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None missing_sentinel_schema ¶ missing_sentinel_schema(\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n) -> Returns a schema for the MISSING sentinel. is_instance_schema ¶ is_instance_schema(\n    cls: ,\n    *,\n    cls_repr:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that checks if a value is an instance of a class, equivalent to python's isinstance method, e.g.: from pydantic_core import SchemaValidator , core_schema class A : pass schema = core_schema . is_instance_schema ( cls = A ) v = SchemaValidator ( schema ) v . validate_python ( A ()) Parameters: Name Type Description Default cls The value must be an instance of this class required cls_repr | None If provided this string is used in the validator name instead of repr(cls) None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None is_subclass_schema ¶ is_subclass_schema(\n    cls: [],\n    *,\n    cls_repr:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that checks if a value is a subtype of a class, equivalent to python's issubclass method, e.g.: from pydantic_core import SchemaValidator , core_schema class A : pass class B ( A ): pass schema = core_schema . is_subclass_schema ( cls = A ) v = SchemaValidator ( schema ) v . validate_python ( B ) Parameters: Name Type Description Default cls [] The value must be a subclass of this class required cls_repr | None If provided this string is used in the validator name instead of repr(cls) None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None callable_schema ¶ callable_schema(\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that checks if a value is callable, equivalent to python's callable method, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . callable_schema () v = SchemaValidator ( schema ) v . validate_python ( min ) Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None list_schema ¶ list_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a list value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . list_schema ( core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python ([ '4' ]) == [ 4 ] Parameters: Name Type Description Default items_schema | None The value must be a list of items that match this schema None min_length | None The value must be a list with at least this many items None max_length | None The value must be a list with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a list with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None tuple_positional_schema ¶ tuple_positional_schema(\n    items_schema: [],\n    *,\n    extras_schema:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tuple of schemas, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . tuple_positional_schema ( [ core_schema . int_schema (), core_schema . str_schema ()] ) v = SchemaValidator ( schema ) assert v . validate_python (( 1 , 'hello' )) == ( 1 , 'hello' ) Parameters: Name Type Description Default items_schema [] The value must be a tuple with items that match these schemas required extras_schema | None The value must be a tuple with items that match this schema\nThis was inspired by JSON schema's prefixItems and items fields.\nIn python's typing.Tuple , you can't specify a type for \"extra\" items -- they must all be the same type\nif the length is variable. So this field won't be set from a typing.Tuple annotation on a pydantic model. None strict | None The value must be a tuple with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None tuple_variable_schema ¶ tuple_variable_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tuple of a given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . tuple_variable_schema ( items_schema = core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python (( '1' , 2 , 3 )) == ( 1 , 2 , 3 ) Parameters: Name Type Description Default items_schema | None The value must be a tuple with items that match this schema None min_length | None The value must be a tuple with at least this many items None max_length | None The value must be a tuple with at most this many items None strict | None The value must be a tuple with exactly this many items None ref | None Optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None tuple_schema ¶ tuple_schema(\n    items_schema: [],\n    *,\n    variadic_item_index:  | None = None,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tuple of schemas, with an optional variadic item, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . tuple_schema ( [ core_schema . int_schema (), core_schema . str_schema (), core_schema . float_schema ()], variadic_item_index = 1 , ) v = SchemaValidator ( schema ) assert v . validate_python (( 1 , 'hello' , 'world' , 1.5 )) == ( 1 , 'hello' , 'world' , 1.5 ) Parameters: Name Type Description Default items_schema [] The value must be a tuple with items that match these schemas required variadic_item_index | None The index of the schema in items_schema to be treated as variadic (following PEP 646) None min_length | None The value must be a tuple with at least this many items None max_length | None The value must be a tuple with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a tuple with exactly this many items None ref | None Optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None set_schema ¶ set_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a set of a given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . set_schema ( items_schema = core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python ({ 1 , '2' , 3 }) == { 1 , 2 , 3 } Parameters: Name Type Description Default items_schema | None The value must be a set with items that match this schema None min_length | None The value must be a set with at least this many items None max_length | None The value must be a set with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a set with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None frozenset_schema ¶ frozenset_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a frozenset of a given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . frozenset_schema ( items_schema = core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python ( frozenset ( range ( 3 ))) == frozenset ({ 0 , 1 , 2 }) Parameters: Name Type Description Default items_schema | None The value must be a frozenset with items that match this schema None min_length | None The value must be a frozenset with at least this many items None max_length | None The value must be a frozenset with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a frozenset with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None generator_schema ¶ generator_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a generator value, e.g.: from typing import Iterator from pydantic_core import SchemaValidator , core_schema def gen () -> Iterator [ int ]: yield 1 schema = core_schema . generator_schema ( items_schema = core_schema . int_schema ()) v = SchemaValidator ( schema ) v . validate_python ( gen ()) Unlike other types, validated generators do not raise ValidationErrors eagerly,\nbut instead will raise a ValidationError when a violating value is actually read from the generator.\nThis is to ensure that \"validated\" generators retain the benefit of lazy evaluation. Parameters: Name Type Description Default items_schema | None The value must be a generator with items that match this schema None min_length | None The value must be a generator that yields at least this many items None max_length | None The value must be a generator that yields at most this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None dict_schema ¶ dict_schema(\n    keys_schema:  | None = None,\n    values_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a dict value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . dict_schema ( keys_schema = core_schema . str_schema (), values_schema = core_schema . int_schema () ) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : '1' , 'b' : 2 }) == { 'a' : 1 , 'b' : 2 } Parameters: Name Type Description Default keys_schema | None The value must be a dict with keys that match this schema None values_schema | None The value must be a dict with values that match this schema None min_length | None The value must be a dict with at least this many items None max_length | None The value must be a dict with at most this many items None strict | None Whether the keys and values should be validated with strict mode None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None no_info_before_validator_function ¶ no_info_before_validator_function(\n    function: ,\n    schema: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function before validating, no info argument is provided, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : bytes ) -> str : return v . decode () + 'world' func_schema = core_schema . no_info_before_validator_function ( function = fn , schema = core_schema . str_schema () ) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call required schema The schema to validate the output of the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None with_info_before_validator_function ¶ with_info_before_validator_function(\n    function: ,\n    schema: ,\n    *,\n    field_name:  | None = None,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function before validation, the function is called with\nan info argument, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : bytes , info : core_schema . ValidationInfo ) -> str : assert info . data is not None assert info . field_name is not None return v . decode () + 'world' func_schema = core_schema . with_info_before_validator_function ( function = fn , schema = core_schema . str_schema () ) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call required field_name | None The name of the field this validator is applied to, if any (deprecated) None schema The schema to validate the output of the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None no_info_after_validator_function ¶ no_info_after_validator_function(\n    function: ,\n    schema: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function after validating, no info argument is provided, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str ) -> str : return v + 'world' func_schema = core_schema . no_info_after_validator_function ( fn , core_schema . str_schema ()) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call after the schema is validated required schema The schema to validate before the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None with_info_after_validator_function ¶ with_info_after_validator_function(\n    function: ,\n    schema: ,\n    *,\n    field_name:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function after validation, the function is called with\nan info argument, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert info . data is not None assert info . field_name is not None return v + 'world' func_schema = core_schema . with_info_after_validator_function ( function = fn , schema = core_schema . str_schema () ) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call after the schema is validated required schema The schema to validate before the validator function required field_name | None The name of the field this validator is applied to, if any (deprecated) None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None no_info_wrap_validator_function ¶ no_info_wrap_validator_function(\n    function: ,\n    schema: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema which calls a function with a validator callable argument which can\noptionally be used to call inner validation with the function logic, this is much like the\n\"onion\" implementation of middleware in many popular web frameworks, no info argument is passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , validator : core_schema . ValidatorFunctionWrapHandler , ) -> str : return validator ( input_value = v ) + 'world' schema = core_schema . no_info_wrap_validator_function ( function = fn , schema = core_schema . str_schema () ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required schema The schema to validate the output of the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None with_info_wrap_validator_function ¶ with_info_wrap_validator_function(\n    function: ,\n    schema: ,\n    *,\n    field_name:  | None = None,\n    json_schema_input_schema:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema which calls a function with a validator callable argument which can\noptionally be used to call inner validation with the function logic, this is much like the\n\"onion\" implementation of middleware in many popular web frameworks, an info argument is also passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , validator : core_schema . ValidatorFunctionWrapHandler , info : core_schema . ValidationInfo , ) -> str : return validator ( input_value = v ) + 'world' schema = core_schema . with_info_wrap_validator_function ( function = fn , schema = core_schema . str_schema () ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required schema The schema to validate the output of the validator function required field_name | None The name of the field this validator is applied to, if any (deprecated) None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None no_info_plain_validator_function ¶ no_info_plain_validator_function(\n    function: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the provided function for validation, no info argument is passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str ) -> str : assert 'hello' in v return v + 'world' schema = core_schema . no_info_plain_validator_function ( function = fn ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None with_info_plain_validator_function ¶ with_info_plain_validator_function(\n    function: ,\n    *,\n    field_name:  | None = None,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the provided function for validation, an info argument is passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert 'hello' in v return v + 'world' schema = core_schema . with_info_plain_validator_function ( function = fn ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required field_name | None The name of the field this validator is applied to, if any (deprecated) None ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None with_default_schema ¶ with_default_schema(\n    schema: ,\n    *,\n    default:  = ,\n    default_factory: [\n        [[], ],\n        [[[, ]], ],\n        None,\n    ] = None,\n    default_factory_takes_data:  | None = None,\n    on_error: (\n        [\"raise\", \"omit\", \"default\"] | None\n    ) = None,\n    validate_default:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that adds a default value to the given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . with_default_schema ( core_schema . str_schema (), default = 'hello' ) wrapper_schema = core_schema . typed_dict_schema ( { 'a' : core_schema . typed_dict_field ( schema )} ) v = SchemaValidator ( wrapper_schema ) assert v . validate_python ({}) == v . validate_python ({ 'a' : 'hello' }) Parameters: Name Type Description Default schema The schema to add a default value to required default The default value to use default_factory [[[], ], [[[, ]], ], None] A callable that returns the default value to use None default_factory_takes_data | None Whether the default factory takes a validated data argument None on_error ['raise', 'omit', 'default'] | None What to do if the schema validation fails. One of 'raise', 'omit', 'default' None validate_default | None Whether the default value should be validated None strict | None Whether the underlying schema should be validated with strict mode None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None nullable_schema ¶ nullable_schema(\n    schema: ,\n    *,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a nullable value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . nullable_schema ( core_schema . str_schema ()) v = SchemaValidator ( schema ) assert v . validate_python ( None ) is None Parameters: Name Type Description Default schema The schema to wrap required strict | None Whether the underlying schema should be validated with strict mode None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None union_schema ¶ union_schema(\n    choices: [ | [, ]],\n    *,\n    auto_collapse:  | None = None,\n    custom_error_type:  | None = None,\n    custom_error_message:  | None = None,\n    custom_error_context: (\n        [,  | ] | None\n    ) = None,\n    mode: [\"smart\", \"left_to_right\"] | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a union value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . union_schema ([ core_schema . str_schema (), core_schema . int_schema ()]) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello' assert v . validate_python ( 1 ) == 1 Parameters: Name Type Description Default choices [ | [, ]] The schemas to match. If a tuple, the second item is used as the label for the case. required auto_collapse | None whether to automatically collapse unions with one element to the inner validator, default true None custom_error_type | None The custom error type to use if the validation fails None custom_error_message | None The custom error message to use if the validation fails None custom_error_context [,  | ] | None The custom error context to use if the validation fails None mode ['smart', 'left_to_right'] | None How to select which choice to return\n* smart (default) will try to return the choice which is the closest match to the input value\n* left_to_right will return the first choice in choices which succeeds validation None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None tagged_union_schema ¶ tagged_union_schema(\n    choices: [, ],\n    discriminator: (\n        \n        | [ | ]\n        | [[ | ]]\n        | [[], ]\n    ),\n    *,\n    custom_error_type:  | None = None,\n    custom_error_message:  | None = None,\n    custom_error_context: (\n        [,  |  | ] | None\n    ) = None,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tagged union value, e.g.: from pydantic_core import SchemaValidator , core_schema apple_schema = core_schema . typed_dict_schema ( { 'foo' : core_schema . typed_dict_field ( core_schema . str_schema ()), 'bar' : core_schema . typed_dict_field ( core_schema . int_schema ()), } ) banana_schema = core_schema . typed_dict_schema ( { 'foo' : core_schema . typed_dict_field ( core_schema . str_schema ()), 'spam' : core_schema . typed_dict_field ( core_schema . list_schema ( items_schema = core_schema . int_schema ()) ), } ) schema = core_schema . tagged_union_schema ( choices = { 'apple' : apple_schema , 'banana' : banana_schema , }, discriminator = 'foo' , ) v = SchemaValidator ( schema ) assert v . validate_python ({ 'foo' : 'apple' , 'bar' : '123' }) == { 'foo' : 'apple' , 'bar' : 123 } assert v . validate_python ({ 'foo' : 'banana' , 'spam' : [ 1 , 2 , 3 ]}) == { 'foo' : 'banana' , 'spam' : [ 1 , 2 , 3 ], } Parameters: Name Type Description Default choices [, ] The schemas to match\nWhen retrieving a schema from choices using the discriminator value, if the value is a str,\nit should be fed back into the choices map until a schema is obtained\n(This approach is to prevent multiple ownership of a single schema in Rust) required discriminator | [ | ] | [[ | ]] | [[], ] The discriminator to use to determine the schema to use\n* If discriminator is a str, it is the name of the attribute to use as the discriminator\n* If discriminator is a list of int/str, it should be used as a \"path\" to access the discriminator\n* If discriminator is a list of lists, each inner list is a path, and the first path that exists is used\n* If discriminator is a callable, it should return the discriminator when called on the value to validate;\n  the callable can return None to indicate that there is no matching discriminator present on the input required custom_error_type | None The custom error type to use if the validation fails None custom_error_message | None The custom error message to use if the validation fails None custom_error_context [,  |  | ] | None The custom error context to use if the validation fails None strict | None Whether the underlying schemas should be validated with strict mode None from_attributes | None Whether to use the attributes of the object to retrieve the discriminator value None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None chain_schema ¶ chain_schema(\n    steps: [],\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that chains the provided validation schemas, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert 'hello' in v return v + ' world' fn_schema = core_schema . with_info_plain_validator_function ( function = fn ) schema = core_schema . chain_schema ( [ fn_schema , fn_schema , fn_schema , core_schema . str_schema ()] ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello world world world' Parameters: Name Type Description Default steps [] The schemas to chain required ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None lax_or_strict_schema ¶ lax_or_strict_schema(\n    lax_schema: ,\n    strict_schema: ,\n    *,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the lax or strict schema, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert 'hello' in v return v + ' world' lax_schema = core_schema . int_schema ( strict = False ) strict_schema = core_schema . int_schema ( strict = True ) schema = core_schema . lax_or_strict_schema ( lax_schema = lax_schema , strict_schema = strict_schema , strict = True ) v = SchemaValidator ( schema ) assert v . validate_python ( 123 ) == 123 schema = core_schema . lax_or_strict_schema ( lax_schema = lax_schema , strict_schema = strict_schema , strict = False ) v = SchemaValidator ( schema ) assert v . validate_python ( '123' ) == 123 Parameters: Name Type Description Default lax_schema The lax schema to use required strict_schema The strict schema to use required strict | None Whether the strict schema should be used None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None json_or_python_schema ¶ json_or_python_schema(\n    json_schema: ,\n    python_schema: ,\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the Json or Python schema depending on the input: from pydantic_core import SchemaValidator , ValidationError , core_schema v = SchemaValidator ( core_schema . json_or_python_schema ( json_schema = core_schema . int_schema (), python_schema = core_schema . int_schema ( strict = True ), ) ) assert v . validate_json ( '\"123\"' ) == 123 try : v . validate_python ( '123' ) except ValidationError : pass else : raise AssertionError ( 'Validation should have failed' ) Parameters: Name Type Description Default json_schema The schema to use for Json inputs required python_schema The schema to use for Python inputs required ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None typed_dict_field ¶ typed_dict_field(\n    schema: ,\n    *,\n    required:  | None = None,\n    validation_alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None,\n    serialization_alias:  | None = None,\n    serialization_exclude:  | None = None,\n    metadata: [, ] | None = None,\n    serialization_exclude_if: (\n        [[], ] | None\n    ) = None\n) -> Returns a schema that matches a typed dict field, e.g.: from pydantic_core import core_schema field = core_schema . typed_dict_field ( schema = core_schema . int_schema (), required = True ) Parameters: Name Type Description Default schema The schema to use for the field required required | None Whether the field is required, otherwise uses the value from total on the typed dict None validation_alias | [ | ] | [[ | ]] | None The alias(es) to use to find the field in the validation data None serialization_alias | None The alias to use as a key when serializing None serialization_exclude | None Whether to exclude the field when serializing None serialization_exclude_if [[], ] | None A callable that determines whether to exclude the field when serializing based on its value. None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None typed_dict_schema ¶ typed_dict_schema(\n    fields: [, ],\n    *,\n    cls: [] | None = None,\n    cls_name:  | None = None,\n    computed_fields: [] | None = None,\n    strict:  | None = None,\n    extras_schema:  | None = None,\n    extra_behavior:  | None = None,\n    total:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n    config:  | None = None\n) -> Returns a schema that matches a typed dict, e.g.: from typing_extensions import TypedDict from pydantic_core import SchemaValidator , core_schema class MyTypedDict ( TypedDict ): a : str wrapper_schema = core_schema . typed_dict_schema ( { 'a' : core_schema . typed_dict_field ( core_schema . str_schema ())}, cls = MyTypedDict ) v = SchemaValidator ( wrapper_schema ) assert v . validate_python ({ 'a' : 'hello' }) == { 'a' : 'hello' } Parameters: Name Type Description Default fields [, ] The fields to use for the typed dict required cls [] | None The class to use for the typed dict None cls_name | None The name to use in error locations. Falls back to cls.__name__ , or the validator name if no class\nis provided. None computed_fields [] | None Computed fields to use when serializing the model, only applies when directly inside a model None strict | None Whether the typed dict is strict None extras_schema | None The extra validator to use for the typed dict None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None extra_behavior | None The extra behavior to use for the typed dict None total | None Whether the typed dict is total, otherwise uses typed_dict_total from config None serialization | None Custom serialization schema None model_field ¶ model_field(\n    schema: ,\n    *,\n    validation_alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None,\n    serialization_alias:  | None = None,\n    serialization_exclude:  | None = None,\n    serialization_exclude_if: (\n        [[], ] | None\n    ) = None,\n    frozen:  | None = None,\n    metadata: [, ] | None = None\n) -> Returns a schema for a model field, e.g.: from pydantic_core import core_schema field = core_schema . model_field ( schema = core_schema . int_schema ()) Parameters: Name Type Description Default schema The schema to use for the field required validation_alias | [ | ] | [[ | ]] | None The alias(es) to use to find the field in the validation data None serialization_alias | None The alias to use as a key when serializing None serialization_exclude | None Whether to exclude the field when serializing None serialization_exclude_if [[], ] | None A Callable that determines whether to exclude a field during serialization based on its value. None frozen | None Whether the field is frozen None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None model_fields_schema ¶ model_fields_schema(\n    fields: [, ],\n    *,\n    model_name:  | None = None,\n    computed_fields: [] | None = None,\n    strict:  | None = None,\n    extras_schema:  | None = None,\n    extras_keys_schema:  | None = None,\n    extra_behavior:  | None = None,\n    from_attributes:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches the fields of a Pydantic model, e.g.: from pydantic_core import SchemaValidator , core_schema wrapper_schema = core_schema . model_fields_schema ( { 'a' : core_schema . model_field ( core_schema . str_schema ())} ) v = SchemaValidator ( wrapper_schema ) print ( v . validate_python ({ 'a' : 'hello' })) #> ({'a': 'hello'}, None, {'a'}) Parameters: Name Type Description Default fields [, ] The fields of the model required model_name | None The name of the model, used for error messages, defaults to \"Model\" None computed_fields [] | None Computed fields to use when serializing the model, only applies when directly inside a model None strict | None Whether the model is strict None extras_schema | None The schema to use when validating extra input data None extras_keys_schema | None The schema to use when validating the keys of extra input data None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None extra_behavior | None The extra behavior to use for the model fields None from_attributes | None Whether the model fields should be populated from attributes None serialization | None Custom serialization schema None model_schema ¶ model_schema(\n    cls: [],\n    schema: ,\n    *,\n    generic_origin: [] | None = None,\n    custom_init:  | None = None,\n    root_model:  | None = None,\n    post_init:  | None = None,\n    revalidate_instances: (\n        [\"always\", \"never\", \"subclass-instances\"]\n        | None\n    ) = None,\n    strict:  | None = None,\n    frozen:  | None = None,\n    extra_behavior:  | None = None,\n    config:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> A model schema generally contains a typed-dict schema.\nIt will run the typed dict validator, then create a new class\nand set the dict and fields set returned from the typed dict validator\nto __dict__ and __pydantic_fields_set__ respectively. Example: from pydantic_core import CoreConfig , SchemaValidator , core_schema class MyModel : __slots__ = ( '__dict__' , '__pydantic_fields_set__' , '__pydantic_extra__' , '__pydantic_private__' , ) schema = core_schema . model_schema ( cls = MyModel , config = CoreConfig ( str_max_length = 5 ), schema = core_schema . model_fields_schema ( fields = { 'a' : core_schema . model_field ( core_schema . str_schema ())}, ), ) v = SchemaValidator ( schema ) assert v . isinstance_python ({ 'a' : 'hello' }) is True assert v . isinstance_python ({ 'a' : 'too long' }) is False Parameters: Name Type Description Default cls [] The class to use for the model required schema The schema to use for the model required generic_origin [] | None The origin type used for this model, if it's a parametrized generic. Ex,\nif this model schema represents SomeModel[int] , generic_origin is SomeModel None custom_init | None Whether the model has a custom init method None root_model | None Whether the model is a RootModel None post_init | None The call after init to use for the model None revalidate_instances ['always', 'never', 'subclass-instances'] | None whether instances of models and dataclasses (including subclass instances)\nshould re-validate defaults to config.revalidate_instances, else 'never' None strict | None Whether the model is strict None frozen | None Whether the model is frozen None extra_behavior | None The extra behavior to use for the model, used in serialization None config | None The config to use for the model None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None dataclass_field ¶ dataclass_field(\n    name: ,\n    schema: ,\n    *,\n    kw_only:  | None = None,\n    init:  | None = None,\n    init_only:  | None = None,\n    validation_alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None,\n    serialization_alias:  | None = None,\n    serialization_exclude:  | None = None,\n    metadata: [, ] | None = None,\n    serialization_exclude_if: (\n        [[], ] | None\n    ) = None,\n    frozen:  | None = None\n) -> Returns a schema for a dataclass field, e.g.: from pydantic_core import SchemaValidator , core_schema field = core_schema . dataclass_field ( name = 'a' , schema = core_schema . str_schema (), kw_only = False ) schema = core_schema . dataclass_args_schema ( 'Foobar' , [ field ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hello' }) == ({ 'a' : 'hello' }, None ) Parameters: Name Type Description Default name The name to use for the argument parameter required schema The schema to use for the argument parameter required kw_only | None Whether the field can be set with a positional argument as well as a keyword argument None init | None Whether the field should be validated during initialization None init_only | None Whether the field should be omitted  from __dict__ and passed to __post_init__ None validation_alias | [ | ] | [[ | ]] | None The alias(es) to use to find the field in the validation data None serialization_alias | None The alias to use as a key when serializing None serialization_exclude | None Whether to exclude the field when serializing None serialization_exclude_if [[], ] | None A callable that determines whether to exclude the field when serializing based on its value. None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None frozen | None Whether the field is frozen None dataclass_args_schema ¶ dataclass_args_schema(\n    dataclass_name: ,\n    fields: [],\n    *,\n    computed_fields: [] | None = None,\n    collect_init_only:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n    extra_behavior:  | None = None\n) -> Returns a schema for validating dataclass arguments, e.g.: from pydantic_core import SchemaValidator , core_schema field_a = core_schema . dataclass_field ( name = 'a' , schema = core_schema . str_schema (), kw_only = False ) field_b = core_schema . dataclass_field ( name = 'b' , schema = core_schema . bool_schema (), kw_only = False ) schema = core_schema . dataclass_args_schema ( 'Foobar' , [ field_a , field_b ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hello' , 'b' : True }) == ({ 'a' : 'hello' , 'b' : True }, None ) Parameters: Name Type Description Default dataclass_name The name of the dataclass being validated required fields [] The fields to use for the dataclass required computed_fields [] | None Computed fields to use when serializing the dataclass None collect_init_only | None Whether to collect init only fields into a dict to pass to __post_init__ None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None extra_behavior | None How to handle extra fields None dataclass_schema ¶ dataclass_schema(\n    cls: [],\n    schema: ,\n    fields: [],\n    *,\n    generic_origin: [] | None = None,\n    cls_name:  | None = None,\n    post_init:  | None = None,\n    revalidate_instances: (\n        [\"always\", \"never\", \"subclass-instances\"]\n        | None\n    ) = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n    frozen:  | None = None,\n    slots:  | None = None,\n    config:  | None = None\n) -> Returns a schema for a dataclass. As with ModelSchema , this schema can only be used as a field within\nanother schema, not as the root type. Parameters: Name Type Description Default cls [] The dataclass type, used to perform subclass checks required schema The schema to use for the dataclass fields required fields [] Fields of the dataclass, this is used in serialization and in validation during re-validation\nand while validating assignment required generic_origin [] | None The origin type used for this dataclass, if it's a parametrized generic. Ex,\nif this model schema represents SomeDataclass[int] , generic_origin is SomeDataclass None cls_name | None The name to use in error locs, etc; this is useful for generics (default: cls.__name__ ) None post_init | None Whether to call __post_init__ after validation None revalidate_instances ['always', 'never', 'subclass-instances'] | None whether instances of models and dataclasses (including subclass instances)\nshould re-validate defaults to config.revalidate_instances, else 'never' None strict | None Whether to require an exact instance of cls None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None frozen | None Whether the dataclass is frozen None slots | None Whether slots=True on the dataclass, means each field is assigned independently, rather than\nsimply setting __dict__ , default false None arguments_parameter ¶ arguments_parameter(\n    name: ,\n    schema: ,\n    *,\n    mode: (\n        [\n            \"positional_only\",\n            \"positional_or_keyword\",\n            \"keyword_only\",\n        ]\n        | None\n    ) = None,\n    alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None\n) -> Returns a schema that matches an argument parameter, e.g.: from pydantic_core import SchemaValidator , core_schema param = core_schema . arguments_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) schema = core_schema . arguments_schema ([ param ]) v = SchemaValidator ( schema ) assert v . validate_python (( 'hello' ,)) == (( 'hello' ,), {}) Parameters: Name Type Description Default name The name to use for the argument parameter required schema The schema to use for the argument parameter required mode ['positional_only', 'positional_or_keyword', 'keyword_only'] | None The mode to use for the argument parameter None alias | [ | ] | [[ | ]] | None The alias to use for the argument parameter None arguments_schema ¶ arguments_schema(\n    arguments: [],\n    *,\n    validate_by_name:  | None = None,\n    validate_by_alias:  | None = None,\n    var_args_schema:  | None = None,\n    var_kwargs_mode:  | None = None,\n    var_kwargs_schema:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an arguments schema, e.g.: from pydantic_core import SchemaValidator , core_schema param_a = core_schema . arguments_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) param_b = core_schema . arguments_parameter ( name = 'b' , schema = core_schema . bool_schema (), mode = 'positional_only' ) schema = core_schema . arguments_schema ([ param_a , param_b ]) v = SchemaValidator ( schema ) assert v . validate_python (( 'hello' , True )) == (( 'hello' , True ), {}) Parameters: Name Type Description Default arguments [] The arguments to use for the arguments schema required validate_by_name | None Whether to populate by the parameter names, defaults to False . None validate_by_alias | None Whether to populate by the parameter aliases, defaults to True . None var_args_schema | None The variable args schema to use for the arguments schema None var_kwargs_mode | None The validation mode to use for variadic keyword arguments. If 'uniform' , every value of the\nkeyword arguments will be validated against the var_kwargs_schema schema. If 'unpacked-typed-dict' ,\nthe var_kwargs_schema argument must be a None var_kwargs_schema | None The variable kwargs schema to use for the arguments schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None arguments_v3_parameter ¶ arguments_v3_parameter(\n    name: ,\n    schema: ,\n    *,\n    mode: (\n        [\n            \"positional_only\",\n            \"positional_or_keyword\",\n            \"keyword_only\",\n            \"var_args\",\n            \"var_kwargs_uniform\",\n            \"var_kwargs_unpacked_typed_dict\",\n        ]\n        | None\n    ) = None,\n    alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None\n) -> Returns a schema that matches an argument parameter, e.g.: from pydantic_core import SchemaValidator , core_schema param = core_schema . arguments_v3_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) schema = core_schema . arguments_v3_schema ([ param ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hello' }) == (( 'hello' ,), {}) Parameters: Name Type Description Default name The name to use for the argument parameter required schema The schema to use for the argument parameter required mode ['positional_only', 'positional_or_keyword', 'keyword_only', 'var_args', 'var_kwargs_uniform', 'var_kwargs_unpacked_typed_dict'] | None The mode to use for the argument parameter None alias | [ | ] | [[ | ]] | None The alias to use for the argument parameter None arguments_v3_schema ¶ arguments_v3_schema(\n    arguments: [],\n    *,\n    validate_by_name:  | None = None,\n    validate_by_alias:  | None = None,\n    extra_behavior: (\n        [\"forbid\", \"ignore\"] | None\n    ) = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an arguments schema, e.g.: from pydantic_core import SchemaValidator , core_schema param_a = core_schema . arguments_v3_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) param_b = core_schema . arguments_v3_parameter ( name = 'kwargs' , schema = core_schema . bool_schema (), mode = 'var_kwargs_uniform' ) schema = core_schema . arguments_v3_schema ([ param_a , param_b ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hi' , 'kwargs' : { 'b' : True }}) == (( 'hi' ,), { 'b' : True }) This schema is currently not used by other Pydantic components. In V3, it will most likely\nbecome the default arguments schema for the 'call' schema. Parameters: Name Type Description Default arguments [] The arguments to use for the arguments schema. required validate_by_name | None Whether to populate by the parameter names, defaults to False . None validate_by_alias | None Whether to populate by the parameter aliases, defaults to True . None extra_behavior ['forbid', 'ignore'] | None The extra behavior to use. None ref | None optional unique identifier of the schema, used to reference the schema in other places. None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core. None serialization | None Custom serialization schema. None call_schema ¶ call_schema(\n    arguments: ,\n    function: [..., ],\n    *,\n    function_name:  | None = None,\n    return_schema:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an arguments schema, then calls a function, e.g.: from pydantic_core import SchemaValidator , core_schema param_a = core_schema . arguments_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) param_b = core_schema . arguments_parameter ( name = 'b' , schema = core_schema . bool_schema (), mode = 'positional_only' ) args_schema = core_schema . arguments_schema ([ param_a , param_b ]) schema = core_schema . call_schema ( arguments = args_schema , function = lambda a , b : a + str ( not b ), return_schema = core_schema . str_schema (), ) v = SchemaValidator ( schema ) assert v . validate_python ((( 'hello' , True ))) == 'helloFalse' Parameters: Name Type Description Default arguments The arguments to use for the arguments schema required function [..., ] The function to use for the call schema required function_name | None The function name to use for the call schema, if not provided function.__name__ is used None return_schema | None The return schema to use for the call schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None custom_error_schema ¶ custom_error_schema(\n    schema: ,\n    custom_error_type: ,\n    *,\n    custom_error_message:  | None = None,\n    custom_error_context: [, ] | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a custom error value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . custom_error_schema ( schema = core_schema . int_schema (), custom_error_type = 'MyError' , custom_error_message = 'Error msg' , ) v = SchemaValidator ( schema ) v . validate_python ( 1 ) Parameters: Name Type Description Default schema The schema to use for the custom error schema required custom_error_type The custom error type to use for the custom error schema required custom_error_message | None The custom error message to use for the custom error schema None custom_error_context [, ] | None The custom error context to use for the custom error schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None json_schema ¶ json_schema(\n    schema:  | None = None,\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a JSON value, e.g.: from pydantic_core import SchemaValidator , core_schema dict_schema = core_schema . model_fields_schema ( { 'field_a' : core_schema . model_field ( core_schema . str_schema ()), 'field_b' : core_schema . model_field ( core_schema . bool_schema ()), }, ) class MyModel : __slots__ = ( '__dict__' , '__pydantic_fields_set__' , '__pydantic_extra__' , '__pydantic_private__' , ) field_a : str field_b : bool json_schema = core_schema . json_schema ( schema = dict_schema ) schema = core_schema . model_schema ( cls = MyModel , schema = json_schema ) v = SchemaValidator ( schema ) m = v . validate_python ( '{\"field_a\": \"hello\", \"field_b\": true}' ) assert isinstance ( m , MyModel ) Parameters: Name Type Description Default schema | None The schema to use for the JSON schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None url_schema ¶ url_schema(\n    *,\n    max_length:  | None = None,\n    allowed_schemes: [] | None = None,\n    host_required:  | None = None,\n    default_host:  | None = None,\n    default_port:  | None = None,\n    default_path:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a URL value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . url_schema () v = SchemaValidator ( schema ) print ( v . validate_python ( 'https://example.com' )) #> https://example.com/ Parameters: Name Type Description Default max_length | None The maximum length of the URL None allowed_schemes [] | None The allowed URL schemes None host_required | None Whether the URL must have a host None default_host | None The default host to use if the URL does not have a host None default_port | None The default port to use if the URL does not have a port None default_path | None The default path to use if the URL does not have a path None strict | None Whether to use strict URL parsing None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None multi_host_url_schema ¶ multi_host_url_schema(\n    *,\n    max_length:  | None = None,\n    allowed_schemes: [] | None = None,\n    host_required:  | None = None,\n    default_host:  | None = None,\n    default_port:  | None = None,\n    default_path:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a URL value with possibly multiple hosts, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . multi_host_url_schema () v = SchemaValidator ( schema ) print ( v . validate_python ( 'redis://localhost,0.0.0.0,127.0.0.1' )) #> redis://localhost,0.0.0.0,127.0.0.1 Parameters: Name Type Description Default max_length | None The maximum length of the URL None allowed_schemes [] | None The allowed URL schemes None host_required | None Whether the URL must have a host None default_host | None The default host to use if the URL does not have a host None default_port | None The default port to use if the URL does not have a port None default_path | None The default path to use if the URL does not have a path None strict | None Whether to use strict URL parsing None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None definitions_schema ¶ definitions_schema(\n    schema: , definitions: []\n) -> Build a schema that contains both an inner schema and a list of definitions which can be used\nwithin the inner schema. from pydantic_core import SchemaValidator , core_schema schema = core_schema . definitions_schema ( core_schema . list_schema ( core_schema . definition_reference_schema ( 'foobar' )), [ core_schema . int_schema ( ref = 'foobar' )], ) v = SchemaValidator ( schema ) assert v . validate_python ([ 1 , 2 , '3' ]) == [ 1 , 2 , 3 ] Parameters: Name Type Description Default schema The inner schema required definitions [] List of definitions which can be referenced within inner schema required definition_reference_schema ¶ definition_reference_schema(\n    schema_ref: ,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n) -> Returns a schema that points to a schema stored in \"definitions\", this is useful for nested recursive\nmodels and also when you want to define validators separately from the main schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema_definition = core_schema . definition_reference_schema ( 'list-schema' ) schema = core_schema . definitions_schema ( schema = schema_definition , definitions = [ core_schema . list_schema ( items_schema = schema_definition , ref = 'list-schema' ), ], ) v = SchemaValidator ( schema ) assert v . validate_python ([()]) == [[]] Parameters: Name Type Description Default schema_ref The schema ref to use for the definition reference schema required metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema","title":"pydantic_core.core_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema","rank":100},{"content":"WhenUsed = [\n    \"always\", \"unless-none\", \"json\", \"json-unless-none\"\n] Values have the following meanings: 'always' means always use 'unless-none' means use unless the value is None 'json' means use when serializing to JSON 'json-unless-none' means use when serializing to JSON and the value is not None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.WhenUsed","title":"pydantic_core.core_schema - WhenUsed  module-attribute","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.WhenUsed","rank":95},{"content":"Bases: Base class for schema configuration options. Attributes: Name Type Description The name of the configuration. Whether the configuration should strictly adhere to specified rules. The behavior for handling extra fields. Whether the TypedDict should be considered total. Default is True . Whether to use attributes for models, dataclasses, and tagged union keys. Whether to use the used alias (or first alias for \"field required\" errors) instead of field_names to construct error loc s. Default is True . ['always', 'never', 'subclass-instances'] Whether instances of models and dataclasses should re-validate. Default is 'never'. Whether to validate default values during validation. Default is False . The maximum length for string fields. The minimum length for string fields. Whether to strip whitespace from string fields. Whether to convert string fields to lowercase. Whether to convert string fields to uppercase. Whether to allow infinity and NaN values for float fields. Default is True . ['iso8601', 'float'] The serialization option for timedelta values. Default is 'iso8601'.\nNote that if ser_json_temporal is set, then this param will be ignored. ['iso8601', 'seconds', 'milliseconds'] The serialization option for datetime like values. Default is 'iso8601'.\nThe types this covers are datetime, date, time and timedelta.\nIf this is set, it will take precedence over ser_json_timedelta ['utf8', 'base64', 'hex'] The serialization option for bytes values. Default is 'utf8'. ['null', 'constants', 'strings'] The serialization option for infinity and NaN values\nin float fields. Default is 'null'. ['utf8', 'base64', 'hex'] The validation option for bytes values, complementing ser_json_bytes. Default is 'utf8'. Whether to hide input data from ValidationError representation. Whether to add user-python excs to the cause of a ValidationError.\nRequires exceptiongroup backport pre Python 3.11. Whether to enable coercion of any Number type to str (not applicable in strict mode). ['rust-regex', 'python-re'] The regex engine to use for regex pattern validation. Default is 'rust-regex'. See StringSchema . [, ['all', 'keys', 'none']] Whether to cache strings. Default is True , True or 'all' is required to cache strings\nduring general validation since validators don't know if they're in a key or a value. Whether to use the field's alias when validating against the provided input data. Default is True . Whether to use the field's name when validating against the provided input data. Default is False . Replacement for populate_by_name . Whether to serialize by alias. Default is False , expected to change to True in V3.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.CoreConfig","title":"pydantic_core.core_schema - CoreConfig","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.CoreConfig","rank":90},{"content":"Bases: [] Extra data used during serialization. include property ¶ include: The include argument set during serialization. exclude property ¶ exclude: The exclude argument set during serialization. context property ¶ context: The current serialization context. mode property ¶ mode: ['python', 'json'] The serialization mode set during serialization. by_alias property ¶ by_alias: The by_alias argument set during serialization. exclude_unset property ¶ exclude_unset: The exclude_unset argument set during serialization. exclude_defaults property ¶ exclude_defaults: The exclude_defaults argument set during serialization. exclude_none property ¶ exclude_none: The exclude_none argument set during serialization. serialize_as_any property ¶ serialize_as_any: The serialize_as_any argument set during serialization. round_trip property ¶ round_trip: The round_trip argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo","title":"pydantic_core.core_schema - SerializationInfo","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo","rank":85},{"content":"include: The include argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.include","title":"pydantic_core.core_schema - SerializationInfo - include  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.include","rank":80},{"content":"exclude: The exclude argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude","title":"pydantic_core.core_schema - SerializationInfo - exclude  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude","rank":75},{"content":"context: The current serialization context.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.context","title":"pydantic_core.core_schema - SerializationInfo - context  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.context","rank":70},{"content":"mode: ['python', 'json'] The serialization mode set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.mode","title":"pydantic_core.core_schema - SerializationInfo - mode  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.mode","rank":65},{"content":"by_alias: The by_alias argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.by_alias","title":"pydantic_core.core_schema - SerializationInfo - by_alias  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.by_alias","rank":60},{"content":"exclude_unset: The exclude_unset argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude_unset","title":"pydantic_core.core_schema - SerializationInfo - exclude_unset  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude_unset","rank":55},{"content":"exclude_defaults: The exclude_defaults argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude_defaults","title":"pydantic_core.core_schema - SerializationInfo - exclude_defaults  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude_defaults","rank":50},{"content":"exclude_none: The exclude_none argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude_none","title":"pydantic_core.core_schema - SerializationInfo - exclude_none  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.exclude_none","rank":45},{"content":"serialize_as_any: The serialize_as_any argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.serialize_as_any","title":"pydantic_core.core_schema - SerializationInfo - serialize_as_any  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.serialize_as_any","rank":40},{"content":"round_trip: The round_trip argument set during serialization.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.round_trip","title":"pydantic_core.core_schema - SerializationInfo - round_trip  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.SerializationInfo.round_trip","rank":35},{"content":"Bases: [] , Extra data used during field serialization. field_name property ¶ field_name: The name of the current field being serialized.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.FieldSerializationInfo","title":"pydantic_core.core_schema - FieldSerializationInfo","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.FieldSerializationInfo","rank":30},{"content":"field_name: The name of the current field being serialized.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.FieldSerializationInfo.field_name","title":"pydantic_core.core_schema - FieldSerializationInfo - field_name  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.FieldSerializationInfo.field_name","rank":25},{"content":"Bases: [] Extra data used during validation. context property ¶ context: The current validation context. config property ¶ config:  | None The CoreConfig that applies to this validation. mode property ¶ mode: ['python', 'json'] The type of input data we are currently validating. data property ¶ data: [, ] The data being validated for this model. field_name property ¶ field_name:  | None The name of the current field being validated if this validator is\nattached to a model field.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo","title":"pydantic_core.core_schema - ValidationInfo","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo","rank":20},{"content":"context: The current validation context.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.context","title":"pydantic_core.core_schema - ValidationInfo - context  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.context","rank":15},{"content":"config:  | None The CoreConfig that applies to this validation.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.config","title":"pydantic_core.core_schema - ValidationInfo - config  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.config","rank":10},{"content":"mode: ['python', 'json'] The type of input data we are currently validating.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.mode","title":"pydantic_core.core_schema - ValidationInfo - mode  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.mode","rank":5},{"content":"data: [, ] The data being validated for this model.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.data","title":"pydantic_core.core_schema - ValidationInfo - data  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.data","rank":0},{"content":"field_name:  | None The name of the current field being validated if this validator is\nattached to a model field.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.field_name","title":"pydantic_core.core_schema - ValidationInfo - field_name  property","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.ValidationInfo.field_name","rank":-5},{"content":"simple_ser_schema(\n    type: ,\n) -> Returns a schema for serialization with a custom type. Parameters: Name Type Description Default type The type to use for serialization required","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.simple_ser_schema","title":"pydantic_core.core_schema - simple_ser_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.simple_ser_schema","rank":-10},{"content":"plain_serializer_function_ser_schema(\n    function: ,\n    *,\n    is_field_serializer:  | None = None,\n    info_arg:  | None = None,\n    return_schema:  | None = None,\n    when_used:  = \"always\"\n) -> Returns a schema for serialization with a function, can be either a \"general\" or \"field\" function. Parameters: Name Type Description Default function The function to use for serialization required is_field_serializer | None Whether the serializer is for a field, e.g. takes model as the first argument,\nand info includes field_name None info_arg | None Whether the function takes an info argument None return_schema | None Schema to use for serializing return value None when_used When the function should be called 'always'","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.plain_serializer_function_ser_schema","title":"pydantic_core.core_schema - plain_serializer_function_ser_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.plain_serializer_function_ser_schema","rank":-15},{"content":"wrap_serializer_function_ser_schema(\n    function: ,\n    *,\n    is_field_serializer:  | None = None,\n    info_arg:  | None = None,\n    schema:  | None = None,\n    return_schema:  | None = None,\n    when_used:  = \"always\"\n) -> Returns a schema for serialization with a wrap function, can be either a \"general\" or \"field\" function. Parameters: Name Type Description Default function The function to use for serialization required is_field_serializer | None Whether the serializer is for a field, e.g. takes model as the first argument,\nand info includes field_name None info_arg | None Whether the function takes an info argument None schema | None The schema to use for the inner serialization None return_schema | None Schema to use for serializing return value None when_used When the function should be called 'always'","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.wrap_serializer_function_ser_schema","title":"pydantic_core.core_schema - wrap_serializer_function_ser_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.wrap_serializer_function_ser_schema","rank":-20},{"content":"format_ser_schema(\n    formatting_string: ,\n    *,\n    when_used:  = \"json-unless-none\"\n) -> Returns a schema for serialization using python's format method. Parameters: Name Type Description Default formatting_string String defining the format to use required when_used Same meaning as for [general_function_plain_ser_schema], but with a different default 'json-unless-none'","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.format_ser_schema","title":"pydantic_core.core_schema - format_ser_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.format_ser_schema","rank":-25},{"content":"to_string_ser_schema(\n    *, when_used:  = \"json-unless-none\"\n) -> Returns a schema for serialization using python's str() / __str__ method. Parameters: Name Type Description Default when_used Same meaning as for [general_function_plain_ser_schema], but with a different default 'json-unless-none'","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.to_string_ser_schema","title":"pydantic_core.core_schema - to_string_ser_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.to_string_ser_schema","rank":-30},{"content":"model_ser_schema(\n    cls: [], schema: \n) -> Returns a schema for serialization using a model. Parameters: Name Type Description Default cls [] The expected class type, used to generate warnings if the wrong type is passed required schema Internal schema to use to serialize the model dict required","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_ser_schema","title":"pydantic_core.core_schema - model_ser_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_ser_schema","rank":-35},{"content":"invalid_schema(\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n) -> Returns an invalid schema, used to indicate that a schema is invalid. Returns a schema that matches any value, e.g.: Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.invalid_schema","title":"pydantic_core.core_schema - invalid_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.invalid_schema","rank":-40},{"content":"computed_field(\n    property_name: ,\n    return_schema: ,\n    *,\n    alias:  | None = None,\n    metadata: [, ] | None = None\n) -> ComputedFields are properties of a model or dataclass that are included in serialization. Parameters: Name Type Description Default property_name The name of the property on the model or dataclass required return_schema The schema used for the type returned by the computed field required alias | None The name to use in the serialized output None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.computed_field","title":"pydantic_core.core_schema - computed_field","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.computed_field","rank":-45},{"content":"any_schema(\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches any value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . any_schema () v = SchemaValidator ( schema ) assert v . validate_python ( 1 ) == 1 Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.any_schema","title":"pydantic_core.core_schema - any_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.any_schema","rank":-50},{"content":"none_schema(\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a None value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . none_schema () v = SchemaValidator ( schema ) assert v . validate_python ( None ) is None Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.none_schema","title":"pydantic_core.core_schema - none_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.none_schema","rank":-55},{"content":"bool_schema(\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n) -> Returns a schema that matches a bool value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . bool_schema () v = SchemaValidator ( schema ) assert v . validate_python ( 'True' ) is True Parameters: Name Type Description Default strict | None Whether the value should be a bool or a value that can be converted to a bool None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.bool_schema","title":"pydantic_core.core_schema - bool_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.bool_schema","rank":-60},{"content":"int_schema(\n    *,\n    multiple_of:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a int value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . int_schema ( multiple_of = 2 , le = 6 , ge = 2 ) v = SchemaValidator ( schema ) assert v . validate_python ( '4' ) == 4 Parameters: Name Type Description Default multiple_of | None The value must be a multiple of this number None le | None The value must be less than or equal to this number None ge | None The value must be greater than or equal to this number None lt | None The value must be strictly less than this number None gt | None The value must be strictly greater than this number None strict | None Whether the value should be a int or a value that can be converted to a int None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.int_schema","title":"pydantic_core.core_schema - int_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.int_schema","rank":-65},{"content":"float_schema(\n    *,\n    allow_inf_nan:  | None = None,\n    multiple_of:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a float value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . float_schema ( le = 0.8 , ge = 0.2 ) v = SchemaValidator ( schema ) assert v . validate_python ( '0.5' ) == 0.5 Parameters: Name Type Description Default allow_inf_nan | None Whether to allow inf and nan values None multiple_of | None The value must be a multiple of this number None le | None The value must be less than or equal to this number None ge | None The value must be greater than or equal to this number None lt | None The value must be strictly less than this number None gt | None The value must be strictly greater than this number None strict | None Whether the value should be a float or a value that can be converted to a float None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.float_schema","title":"pydantic_core.core_schema - float_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.float_schema","rank":-70},{"content":"decimal_schema(\n    *,\n    allow_inf_nan:  | None = None,\n    multiple_of:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    max_digits:  | None = None,\n    decimal_places:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a decimal value, e.g.: from decimal import Decimal from pydantic_core import SchemaValidator , core_schema schema = core_schema . decimal_schema ( le = 0.8 , ge = 0.2 ) v = SchemaValidator ( schema ) assert v . validate_python ( '0.5' ) == Decimal ( '0.5' ) Parameters: Name Type Description Default allow_inf_nan | None Whether to allow inf and nan values None multiple_of | None The value must be a multiple of this number None le | None The value must be less than or equal to this number None ge | None The value must be greater than or equal to this number None lt | None The value must be strictly less than this number None gt | None The value must be strictly greater than this number None max_digits | None The maximum number of decimal digits allowed None decimal_places | None The maximum number of decimal places allowed None strict | None Whether the value should be a float or a value that can be converted to a float None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.decimal_schema","title":"pydantic_core.core_schema - decimal_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.decimal_schema","rank":-75},{"content":"complex_schema(\n    *,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a complex value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . complex_schema () v = SchemaValidator ( schema ) assert v . validate_python ( '1+2j' ) == complex ( 1 , 2 ) assert v . validate_python ( complex ( 1 , 2 )) == complex ( 1 , 2 ) Parameters: Name Type Description Default strict | None Whether the value should be a complex object instance or a value that can be converted to a complex object None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.complex_schema","title":"pydantic_core.core_schema - complex_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.complex_schema","rank":-80},{"content":"str_schema(\n    *,\n    pattern:  | [] | None = None,\n    max_length:  | None = None,\n    min_length:  | None = None,\n    strip_whitespace:  | None = None,\n    to_lower:  | None = None,\n    to_upper:  | None = None,\n    regex_engine: (\n        [\"rust-regex\", \"python-re\"] | None\n    ) = None,\n    strict:  | None = None,\n    coerce_numbers_to_str:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a string value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . str_schema ( max_length = 10 , min_length = 2 ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello' Parameters: Name Type Description Default pattern | [] | None A regex pattern that the value must match None max_length | None The value must be at most this length None min_length | None The value must be at least this length None strip_whitespace | None Whether to strip whitespace from the value None to_lower | None Whether to convert the value to lowercase None to_upper | None Whether to convert the value to uppercase None regex_engine ['rust-regex', 'python-re'] | None The regex engine to use for pattern validation. Default is 'rust-regex'.\n- rust-regex uses the regex Rust\n  crate, which is non-backtracking and therefore more DDoS\n  resistant, but does not support all regex features.\n- python-re use the re module,\n  which supports all regex features, but may be slower. None strict | None Whether the value should be a string or a value that can be converted to a string None coerce_numbers_to_str | None Whether to enable coercion of any Number type to str (not applicable in strict mode). None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.str_schema","title":"pydantic_core.core_schema - str_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.str_schema","rank":-85},{"content":"bytes_schema(\n    *,\n    max_length:  | None = None,\n    min_length:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a bytes value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . bytes_schema ( max_length = 10 , min_length = 2 ) v = SchemaValidator ( schema ) assert v . validate_python ( b 'hello' ) == b 'hello' Parameters: Name Type Description Default max_length | None The value must be at most this length None min_length | None The value must be at least this length None strict | None Whether the value should be a bytes or a value that can be converted to a bytes None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.bytes_schema","title":"pydantic_core.core_schema - bytes_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.bytes_schema","rank":-90},{"content":"date_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    now_op: [\"past\", \"future\"] | None = None,\n    now_utc_offset:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a date value, e.g.: from datetime import date from pydantic_core import SchemaValidator , core_schema schema = core_schema . date_schema ( le = date ( 2020 , 1 , 1 ), ge = date ( 2019 , 1 , 1 )) v = SchemaValidator ( schema ) assert v . validate_python ( date ( 2019 , 6 , 1 )) == date ( 2019 , 6 , 1 ) Parameters: Name Type Description Default strict | None Whether the value should be a date or a value that can be converted to a date None le | None The value must be less than or equal to this date None ge | None The value must be greater than or equal to this date None lt | None The value must be strictly less than this date None gt | None The value must be strictly greater than this date None now_op ['past', 'future'] | None The value must be in the past or future relative to the current date None now_utc_offset | None The value must be in the past or future relative to the current date with this utc offset None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.date_schema","title":"pydantic_core.core_schema - date_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.date_schema","rank":-95},{"content":"time_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    tz_constraint: (\n        [\"aware\", \"naive\"] |  | None\n    ) = None,\n    microseconds_precision: [\n        \"truncate\", \"error\"\n    ] = \"truncate\",\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a time value, e.g.: from datetime import time from pydantic_core import SchemaValidator , core_schema schema = core_schema . time_schema ( le = time ( 12 , 0 , 0 ), ge = time ( 6 , 0 , 0 )) v = SchemaValidator ( schema ) assert v . validate_python ( time ( 9 , 0 , 0 )) == time ( 9 , 0 , 0 ) Parameters: Name Type Description Default strict | None Whether the value should be a time or a value that can be converted to a time None le | None The value must be less than or equal to this time None ge | None The value must be greater than or equal to this time None lt | None The value must be strictly less than this time None gt | None The value must be strictly greater than this time None tz_constraint ['aware', 'naive'] |  | None The value must be timezone aware or naive, or an int to indicate required tz offset None microseconds_precision ['truncate', 'error'] The behavior when seconds have more than 6 digits or microseconds is too large 'truncate' ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.time_schema","title":"pydantic_core.core_schema - time_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.time_schema","rank":-100},{"content":"datetime_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    now_op: [\"past\", \"future\"] | None = None,\n    tz_constraint: (\n        [\"aware\", \"naive\"] |  | None\n    ) = None,\n    now_utc_offset:  | None = None,\n    microseconds_precision: [\n        \"truncate\", \"error\"\n    ] = \"truncate\",\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a datetime value, e.g.: from datetime import datetime from pydantic_core import SchemaValidator , core_schema schema = core_schema . datetime_schema () v = SchemaValidator ( schema ) now = datetime . now () assert v . validate_python ( str ( now )) == now Parameters: Name Type Description Default strict | None Whether the value should be a datetime or a value that can be converted to a datetime None le | None The value must be less than or equal to this datetime None ge | None The value must be greater than or equal to this datetime None lt | None The value must be strictly less than this datetime None gt | None The value must be strictly greater than this datetime None now_op ['past', 'future'] | None The value must be in the past or future relative to the current datetime None tz_constraint ['aware', 'naive'] |  | None The value must be timezone aware or naive, or an int to indicate required tz offset\nTODO: use of a tzinfo where offset changes based on the datetime is not yet supported None now_utc_offset | None The value must be in the past or future relative to the current datetime with this utc offset None microseconds_precision ['truncate', 'error'] The behavior when seconds have more than 6 digits or microseconds is too large 'truncate' ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.datetime_schema","title":"pydantic_core.core_schema - datetime_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.datetime_schema","rank":-105},{"content":"timedelta_schema(\n    *,\n    strict:  | None = None,\n    le:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    gt:  | None = None,\n    microseconds_precision: [\n        \"truncate\", \"error\"\n    ] = \"truncate\",\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a timedelta value, e.g.: from datetime import timedelta from pydantic_core import SchemaValidator , core_schema schema = core_schema . timedelta_schema ( le = timedelta ( days = 1 ), ge = timedelta ( days = 0 )) v = SchemaValidator ( schema ) assert v . validate_python ( timedelta ( hours = 12 )) == timedelta ( hours = 12 ) Parameters: Name Type Description Default strict | None Whether the value should be a timedelta or a value that can be converted to a timedelta None le | None The value must be less than or equal to this timedelta None ge | None The value must be greater than or equal to this timedelta None lt | None The value must be strictly less than this timedelta None gt | None The value must be strictly greater than this timedelta None microseconds_precision ['truncate', 'error'] The behavior when seconds have more than 6 digits or microseconds is too large 'truncate' ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.timedelta_schema","title":"pydantic_core.core_schema - timedelta_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.timedelta_schema","rank":-110},{"content":"literal_schema(\n    expected: [],\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a literal value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . literal_schema ([ 'hello' , 'world' ]) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello' Parameters: Name Type Description Default expected [] The value must be one of these values required ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.literal_schema","title":"pydantic_core.core_schema - literal_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.literal_schema","rank":-115},{"content":"enum_schema(\n    cls: ,\n    members: [],\n    *,\n    sub_type: [\"str\", \"int\", \"float\"] | None = None,\n    missing: [[], ] | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an enum value, e.g.: from enum import Enum from pydantic_core import SchemaValidator , core_schema class Color ( Enum ): RED = 1 GREEN = 2 BLUE = 3 schema = core_schema . enum_schema ( Color , list ( Color . __members__ . values ())) v = SchemaValidator ( schema ) assert v . validate_python ( 2 ) is Color . GREEN Parameters: Name Type Description Default cls The enum class required members [] The members of the enum, generally list(MyEnum.__members__.values()) required sub_type ['str', 'int', 'float'] | None The type of the enum, either 'str' or 'int' or None for plain enums None missing [[], ] | None A function to use when the value is not found in the enum, from _missing_ None strict | None Whether to use strict mode, defaults to False None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.enum_schema","title":"pydantic_core.core_schema - enum_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.enum_schema","rank":-120},{"content":"missing_sentinel_schema(\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n) -> Returns a schema for the MISSING sentinel.","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.missing_sentinel_schema","title":"pydantic_core.core_schema - missing_sentinel_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.missing_sentinel_schema","rank":-125},{"content":"is_instance_schema(\n    cls: ,\n    *,\n    cls_repr:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that checks if a value is an instance of a class, equivalent to python's isinstance method, e.g.: from pydantic_core import SchemaValidator , core_schema class A : pass schema = core_schema . is_instance_schema ( cls = A ) v = SchemaValidator ( schema ) v . validate_python ( A ()) Parameters: Name Type Description Default cls The value must be an instance of this class required cls_repr | None If provided this string is used in the validator name instead of repr(cls) None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.is_instance_schema","title":"pydantic_core.core_schema - is_instance_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.is_instance_schema","rank":-130},{"content":"is_subclass_schema(\n    cls: [],\n    *,\n    cls_repr:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that checks if a value is a subtype of a class, equivalent to python's issubclass method, e.g.: from pydantic_core import SchemaValidator , core_schema class A : pass class B ( A ): pass schema = core_schema . is_subclass_schema ( cls = A ) v = SchemaValidator ( schema ) v . validate_python ( B ) Parameters: Name Type Description Default cls [] The value must be a subclass of this class required cls_repr | None If provided this string is used in the validator name instead of repr(cls) None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.is_subclass_schema","title":"pydantic_core.core_schema - is_subclass_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.is_subclass_schema","rank":-135},{"content":"callable_schema(\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that checks if a value is callable, equivalent to python's callable method, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . callable_schema () v = SchemaValidator ( schema ) v . validate_python ( min ) Parameters: Name Type Description Default ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.callable_schema","title":"pydantic_core.core_schema - callable_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.callable_schema","rank":-140},{"content":"list_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a list value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . list_schema ( core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python ([ '4' ]) == [ 4 ] Parameters: Name Type Description Default items_schema | None The value must be a list of items that match this schema None min_length | None The value must be a list with at least this many items None max_length | None The value must be a list with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a list with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.list_schema","title":"pydantic_core.core_schema - list_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.list_schema","rank":-145},{"content":"tuple_positional_schema(\n    items_schema: [],\n    *,\n    extras_schema:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tuple of schemas, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . tuple_positional_schema ( [ core_schema . int_schema (), core_schema . str_schema ()] ) v = SchemaValidator ( schema ) assert v . validate_python (( 1 , 'hello' )) == ( 1 , 'hello' ) Parameters: Name Type Description Default items_schema [] The value must be a tuple with items that match these schemas required extras_schema | None The value must be a tuple with items that match this schema\nThis was inspired by JSON schema's prefixItems and items fields.\nIn python's typing.Tuple , you can't specify a type for \"extra\" items -- they must all be the same type\nif the length is variable. So this field won't be set from a typing.Tuple annotation on a pydantic model. None strict | None The value must be a tuple with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tuple_positional_schema","title":"pydantic_core.core_schema - tuple_positional_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tuple_positional_schema","rank":-150},{"content":"tuple_variable_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tuple of a given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . tuple_variable_schema ( items_schema = core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python (( '1' , 2 , 3 )) == ( 1 , 2 , 3 ) Parameters: Name Type Description Default items_schema | None The value must be a tuple with items that match this schema None min_length | None The value must be a tuple with at least this many items None max_length | None The value must be a tuple with at most this many items None strict | None The value must be a tuple with exactly this many items None ref | None Optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tuple_variable_schema","title":"pydantic_core.core_schema - tuple_variable_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tuple_variable_schema","rank":-155},{"content":"tuple_schema(\n    items_schema: [],\n    *,\n    variadic_item_index:  | None = None,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tuple of schemas, with an optional variadic item, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . tuple_schema ( [ core_schema . int_schema (), core_schema . str_schema (), core_schema . float_schema ()], variadic_item_index = 1 , ) v = SchemaValidator ( schema ) assert v . validate_python (( 1 , 'hello' , 'world' , 1.5 )) == ( 1 , 'hello' , 'world' , 1.5 ) Parameters: Name Type Description Default items_schema [] The value must be a tuple with items that match these schemas required variadic_item_index | None The index of the schema in items_schema to be treated as variadic (following PEP 646) None min_length | None The value must be a tuple with at least this many items None max_length | None The value must be a tuple with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a tuple with exactly this many items None ref | None Optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tuple_schema","title":"pydantic_core.core_schema - tuple_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tuple_schema","rank":-160},{"content":"set_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a set of a given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . set_schema ( items_schema = core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python ({ 1 , '2' , 3 }) == { 1 , 2 , 3 } Parameters: Name Type Description Default items_schema | None The value must be a set with items that match this schema None min_length | None The value must be a set with at least this many items None max_length | None The value must be a set with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a set with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.set_schema","title":"pydantic_core.core_schema - set_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.set_schema","rank":-165},{"content":"frozenset_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    fail_fast:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a frozenset of a given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . frozenset_schema ( items_schema = core_schema . int_schema (), min_length = 0 , max_length = 10 ) v = SchemaValidator ( schema ) assert v . validate_python ( frozenset ( range ( 3 ))) == frozenset ({ 0 , 1 , 2 }) Parameters: Name Type Description Default items_schema | None The value must be a frozenset with items that match this schema None min_length | None The value must be a frozenset with at least this many items None max_length | None The value must be a frozenset with at most this many items None fail_fast | None Stop validation on the first error None strict | None The value must be a frozenset with exactly this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.frozenset_schema","title":"pydantic_core.core_schema - frozenset_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.frozenset_schema","rank":-170},{"content":"generator_schema(\n    items_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a generator value, e.g.: from typing import Iterator from pydantic_core import SchemaValidator , core_schema def gen () -> Iterator [ int ]: yield 1 schema = core_schema . generator_schema ( items_schema = core_schema . int_schema ()) v = SchemaValidator ( schema ) v . validate_python ( gen ()) Unlike other types, validated generators do not raise ValidationErrors eagerly,\nbut instead will raise a ValidationError when a violating value is actually read from the generator.\nThis is to ensure that \"validated\" generators retain the benefit of lazy evaluation. Parameters: Name Type Description Default items_schema | None The value must be a generator with items that match this schema None min_length | None The value must be a generator that yields at least this many items None max_length | None The value must be a generator that yields at most this many items None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.generator_schema","title":"pydantic_core.core_schema - generator_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.generator_schema","rank":-175},{"content":"dict_schema(\n    keys_schema:  | None = None,\n    values_schema:  | None = None,\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a dict value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . dict_schema ( keys_schema = core_schema . str_schema (), values_schema = core_schema . int_schema () ) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : '1' , 'b' : 2 }) == { 'a' : 1 , 'b' : 2 } Parameters: Name Type Description Default keys_schema | None The value must be a dict with keys that match this schema None values_schema | None The value must be a dict with values that match this schema None min_length | None The value must be a dict with at least this many items None max_length | None The value must be a dict with at most this many items None strict | None Whether the keys and values should be validated with strict mode None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dict_schema","title":"pydantic_core.core_schema - dict_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dict_schema","rank":-180},{"content":"no_info_before_validator_function(\n    function: ,\n    schema: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function before validating, no info argument is provided, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : bytes ) -> str : return v . decode () + 'world' func_schema = core_schema . no_info_before_validator_function ( function = fn , schema = core_schema . str_schema () ) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call required schema The schema to validate the output of the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_before_validator_function","title":"pydantic_core.core_schema - no_info_before_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_before_validator_function","rank":-185},{"content":"with_info_before_validator_function(\n    function: ,\n    schema: ,\n    *,\n    field_name:  | None = None,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function before validation, the function is called with\nan info argument, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : bytes , info : core_schema . ValidationInfo ) -> str : assert info . data is not None assert info . field_name is not None return v . decode () + 'world' func_schema = core_schema . with_info_before_validator_function ( function = fn , schema = core_schema . str_schema () ) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call required field_name | None The name of the field this validator is applied to, if any (deprecated) None schema The schema to validate the output of the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_before_validator_function","title":"pydantic_core.core_schema - with_info_before_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_before_validator_function","rank":-190},{"content":"no_info_after_validator_function(\n    function: ,\n    schema: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function after validating, no info argument is provided, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str ) -> str : return v + 'world' func_schema = core_schema . no_info_after_validator_function ( fn , core_schema . str_schema ()) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call after the schema is validated required schema The schema to validate before the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_after_validator_function","title":"pydantic_core.core_schema - no_info_after_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_after_validator_function","rank":-195},{"content":"with_info_after_validator_function(\n    function: ,\n    schema: ,\n    *,\n    field_name:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that calls a validator function after validation, the function is called with\nan info argument, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert info . data is not None assert info . field_name is not None return v + 'world' func_schema = core_schema . with_info_after_validator_function ( function = fn , schema = core_schema . str_schema () ) schema = core_schema . typed_dict_schema ({ 'a' : core_schema . typed_dict_field ( func_schema )}) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : b 'hello ' }) == { 'a' : 'hello world' } Parameters: Name Type Description Default function The validator function to call after the schema is validated required schema The schema to validate before the validator function required field_name | None The name of the field this validator is applied to, if any (deprecated) None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_after_validator_function","title":"pydantic_core.core_schema - with_info_after_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_after_validator_function","rank":-200},{"content":"no_info_wrap_validator_function(\n    function: ,\n    schema: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema which calls a function with a validator callable argument which can\noptionally be used to call inner validation with the function logic, this is much like the\n\"onion\" implementation of middleware in many popular web frameworks, no info argument is passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , validator : core_schema . ValidatorFunctionWrapHandler , ) -> str : return validator ( input_value = v ) + 'world' schema = core_schema . no_info_wrap_validator_function ( function = fn , schema = core_schema . str_schema () ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required schema The schema to validate the output of the validator function required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_wrap_validator_function","title":"pydantic_core.core_schema - no_info_wrap_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_wrap_validator_function","rank":-205},{"content":"with_info_wrap_validator_function(\n    function: ,\n    schema: ,\n    *,\n    field_name:  | None = None,\n    json_schema_input_schema:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema which calls a function with a validator callable argument which can\noptionally be used to call inner validation with the function logic, this is much like the\n\"onion\" implementation of middleware in many popular web frameworks, an info argument is also passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , validator : core_schema . ValidatorFunctionWrapHandler , info : core_schema . ValidationInfo , ) -> str : return validator ( input_value = v ) + 'world' schema = core_schema . with_info_wrap_validator_function ( function = fn , schema = core_schema . str_schema () ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required schema The schema to validate the output of the validator function required field_name | None The name of the field this validator is applied to, if any (deprecated) None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_wrap_validator_function","title":"pydantic_core.core_schema - with_info_wrap_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_wrap_validator_function","rank":-210},{"content":"no_info_plain_validator_function(\n    function: ,\n    *,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the provided function for validation, no info argument is passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str ) -> str : assert 'hello' in v return v + 'world' schema = core_schema . no_info_plain_validator_function ( function = fn ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_plain_validator_function","title":"pydantic_core.core_schema - no_info_plain_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_plain_validator_function","rank":-215},{"content":"with_info_plain_validator_function(\n    function: ,\n    *,\n    field_name:  | None = None,\n    ref:  | None = None,\n    json_schema_input_schema:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the provided function for validation, an info argument is passed, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert 'hello' in v return v + 'world' schema = core_schema . with_info_plain_validator_function ( function = fn ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello ' ) == 'hello world' Parameters: Name Type Description Default function The validator function to call required field_name | None The name of the field this validator is applied to, if any (deprecated) None ref | None optional unique identifier of the schema, used to reference the schema in other places None json_schema_input_schema | None The core schema to be used to generate the corresponding JSON Schema input type None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_plain_validator_function","title":"pydantic_core.core_schema - with_info_plain_validator_function","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_info_plain_validator_function","rank":-220},{"content":"with_default_schema(\n    schema: ,\n    *,\n    default:  = ,\n    default_factory: [\n        [[], ],\n        [[[, ]], ],\n        None,\n    ] = None,\n    default_factory_takes_data:  | None = None,\n    on_error: (\n        [\"raise\", \"omit\", \"default\"] | None\n    ) = None,\n    validate_default:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that adds a default value to the given schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . with_default_schema ( core_schema . str_schema (), default = 'hello' ) wrapper_schema = core_schema . typed_dict_schema ( { 'a' : core_schema . typed_dict_field ( schema )} ) v = SchemaValidator ( wrapper_schema ) assert v . validate_python ({}) == v . validate_python ({ 'a' : 'hello' }) Parameters: Name Type Description Default schema The schema to add a default value to required default The default value to use default_factory [[[], ], [[[, ]], ], None] A callable that returns the default value to use None default_factory_takes_data | None Whether the default factory takes a validated data argument None on_error ['raise', 'omit', 'default'] | None What to do if the schema validation fails. One of 'raise', 'omit', 'default' None validate_default | None Whether the default value should be validated None strict | None Whether the underlying schema should be validated with strict mode None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_default_schema","title":"pydantic_core.core_schema - with_default_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.with_default_schema","rank":-225},{"content":"nullable_schema(\n    schema: ,\n    *,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a nullable value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . nullable_schema ( core_schema . str_schema ()) v = SchemaValidator ( schema ) assert v . validate_python ( None ) is None Parameters: Name Type Description Default schema The schema to wrap required strict | None Whether the underlying schema should be validated with strict mode None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.nullable_schema","title":"pydantic_core.core_schema - nullable_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.nullable_schema","rank":-230},{"content":"union_schema(\n    choices: [ | [, ]],\n    *,\n    auto_collapse:  | None = None,\n    custom_error_type:  | None = None,\n    custom_error_message:  | None = None,\n    custom_error_context: (\n        [,  | ] | None\n    ) = None,\n    mode: [\"smart\", \"left_to_right\"] | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a union value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . union_schema ([ core_schema . str_schema (), core_schema . int_schema ()]) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello' assert v . validate_python ( 1 ) == 1 Parameters: Name Type Description Default choices [ | [, ]] The schemas to match. If a tuple, the second item is used as the label for the case. required auto_collapse | None whether to automatically collapse unions with one element to the inner validator, default true None custom_error_type | None The custom error type to use if the validation fails None custom_error_message | None The custom error message to use if the validation fails None custom_error_context [,  | ] | None The custom error context to use if the validation fails None mode ['smart', 'left_to_right'] | None How to select which choice to return\n* smart (default) will try to return the choice which is the closest match to the input value\n* left_to_right will return the first choice in choices which succeeds validation None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.union_schema","title":"pydantic_core.core_schema - union_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.union_schema","rank":-235},{"content":"tagged_union_schema(\n    choices: [, ],\n    discriminator: (\n        \n        | [ | ]\n        | [[ | ]]\n        | [[], ]\n    ),\n    *,\n    custom_error_type:  | None = None,\n    custom_error_message:  | None = None,\n    custom_error_context: (\n        [,  |  | ] | None\n    ) = None,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a tagged union value, e.g.: from pydantic_core import SchemaValidator , core_schema apple_schema = core_schema . typed_dict_schema ( { 'foo' : core_schema . typed_dict_field ( core_schema . str_schema ()), 'bar' : core_schema . typed_dict_field ( core_schema . int_schema ()), } ) banana_schema = core_schema . typed_dict_schema ( { 'foo' : core_schema . typed_dict_field ( core_schema . str_schema ()), 'spam' : core_schema . typed_dict_field ( core_schema . list_schema ( items_schema = core_schema . int_schema ()) ), } ) schema = core_schema . tagged_union_schema ( choices = { 'apple' : apple_schema , 'banana' : banana_schema , }, discriminator = 'foo' , ) v = SchemaValidator ( schema ) assert v . validate_python ({ 'foo' : 'apple' , 'bar' : '123' }) == { 'foo' : 'apple' , 'bar' : 123 } assert v . validate_python ({ 'foo' : 'banana' , 'spam' : [ 1 , 2 , 3 ]}) == { 'foo' : 'banana' , 'spam' : [ 1 , 2 , 3 ], } Parameters: Name Type Description Default choices [, ] The schemas to match\nWhen retrieving a schema from choices using the discriminator value, if the value is a str,\nit should be fed back into the choices map until a schema is obtained\n(This approach is to prevent multiple ownership of a single schema in Rust) required discriminator | [ | ] | [[ | ]] | [[], ] The discriminator to use to determine the schema to use\n* If discriminator is a str, it is the name of the attribute to use as the discriminator\n* If discriminator is a list of int/str, it should be used as a \"path\" to access the discriminator\n* If discriminator is a list of lists, each inner list is a path, and the first path that exists is used\n* If discriminator is a callable, it should return the discriminator when called on the value to validate;\n  the callable can return None to indicate that there is no matching discriminator present on the input required custom_error_type | None The custom error type to use if the validation fails None custom_error_message | None The custom error message to use if the validation fails None custom_error_context [,  |  | ] | None The custom error context to use if the validation fails None strict | None Whether the underlying schemas should be validated with strict mode None from_attributes | None Whether to use the attributes of the object to retrieve the discriminator value None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tagged_union_schema","title":"pydantic_core.core_schema - tagged_union_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.tagged_union_schema","rank":-240},{"content":"chain_schema(\n    steps: [],\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that chains the provided validation schemas, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert 'hello' in v return v + ' world' fn_schema = core_schema . with_info_plain_validator_function ( function = fn ) schema = core_schema . chain_schema ( [ fn_schema , fn_schema , fn_schema , core_schema . str_schema ()] ) v = SchemaValidator ( schema ) assert v . validate_python ( 'hello' ) == 'hello world world world' Parameters: Name Type Description Default steps [] The schemas to chain required ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.chain_schema","title":"pydantic_core.core_schema - chain_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.chain_schema","rank":-245},{"content":"lax_or_strict_schema(\n    lax_schema: ,\n    strict_schema: ,\n    *,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the lax or strict schema, e.g.: from pydantic_core import SchemaValidator , core_schema def fn ( v : str , info : core_schema . ValidationInfo ) -> str : assert 'hello' in v return v + ' world' lax_schema = core_schema . int_schema ( strict = False ) strict_schema = core_schema . int_schema ( strict = True ) schema = core_schema . lax_or_strict_schema ( lax_schema = lax_schema , strict_schema = strict_schema , strict = True ) v = SchemaValidator ( schema ) assert v . validate_python ( 123 ) == 123 schema = core_schema . lax_or_strict_schema ( lax_schema = lax_schema , strict_schema = strict_schema , strict = False ) v = SchemaValidator ( schema ) assert v . validate_python ( '123' ) == 123 Parameters: Name Type Description Default lax_schema The lax schema to use required strict_schema The strict schema to use required strict | None Whether the strict schema should be used None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.lax_or_strict_schema","title":"pydantic_core.core_schema - lax_or_strict_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.lax_or_strict_schema","rank":-250},{"content":"json_or_python_schema(\n    json_schema: ,\n    python_schema: ,\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that uses the Json or Python schema depending on the input: from pydantic_core import SchemaValidator , ValidationError , core_schema v = SchemaValidator ( core_schema . json_or_python_schema ( json_schema = core_schema . int_schema (), python_schema = core_schema . int_schema ( strict = True ), ) ) assert v . validate_json ( '\"123\"' ) == 123 try : v . validate_python ( '123' ) except ValidationError : pass else : raise AssertionError ( 'Validation should have failed' ) Parameters: Name Type Description Default json_schema The schema to use for Json inputs required python_schema The schema to use for Python inputs required ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.json_or_python_schema","title":"pydantic_core.core_schema - json_or_python_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.json_or_python_schema","rank":-255},{"content":"typed_dict_field(\n    schema: ,\n    *,\n    required:  | None = None,\n    validation_alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None,\n    serialization_alias:  | None = None,\n    serialization_exclude:  | None = None,\n    metadata: [, ] | None = None,\n    serialization_exclude_if: (\n        [[], ] | None\n    ) = None\n) -> Returns a schema that matches a typed dict field, e.g.: from pydantic_core import core_schema field = core_schema . typed_dict_field ( schema = core_schema . int_schema (), required = True ) Parameters: Name Type Description Default schema The schema to use for the field required required | None Whether the field is required, otherwise uses the value from total on the typed dict None validation_alias | [ | ] | [[ | ]] | None The alias(es) to use to find the field in the validation data None serialization_alias | None The alias to use as a key when serializing None serialization_exclude | None Whether to exclude the field when serializing None serialization_exclude_if [[], ] | None A callable that determines whether to exclude the field when serializing based on its value. None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.typed_dict_field","title":"pydantic_core.core_schema - typed_dict_field","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.typed_dict_field","rank":-260},{"content":"typed_dict_schema(\n    fields: [, ],\n    *,\n    cls: [] | None = None,\n    cls_name:  | None = None,\n    computed_fields: [] | None = None,\n    strict:  | None = None,\n    extras_schema:  | None = None,\n    extra_behavior:  | None = None,\n    total:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n    config:  | None = None\n) -> Returns a schema that matches a typed dict, e.g.: from typing_extensions import TypedDict from pydantic_core import SchemaValidator , core_schema class MyTypedDict ( TypedDict ): a : str wrapper_schema = core_schema . typed_dict_schema ( { 'a' : core_schema . typed_dict_field ( core_schema . str_schema ())}, cls = MyTypedDict ) v = SchemaValidator ( wrapper_schema ) assert v . validate_python ({ 'a' : 'hello' }) == { 'a' : 'hello' } Parameters: Name Type Description Default fields [, ] The fields to use for the typed dict required cls [] | None The class to use for the typed dict None cls_name | None The name to use in error locations. Falls back to cls.__name__ , or the validator name if no class\nis provided. None computed_fields [] | None Computed fields to use when serializing the model, only applies when directly inside a model None strict | None Whether the typed dict is strict None extras_schema | None The extra validator to use for the typed dict None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None extra_behavior | None The extra behavior to use for the typed dict None total | None Whether the typed dict is total, otherwise uses typed_dict_total from config None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.typed_dict_schema","title":"pydantic_core.core_schema - typed_dict_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.typed_dict_schema","rank":-265},{"content":"model_field(\n    schema: ,\n    *,\n    validation_alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None,\n    serialization_alias:  | None = None,\n    serialization_exclude:  | None = None,\n    serialization_exclude_if: (\n        [[], ] | None\n    ) = None,\n    frozen:  | None = None,\n    metadata: [, ] | None = None\n) -> Returns a schema for a model field, e.g.: from pydantic_core import core_schema field = core_schema . model_field ( schema = core_schema . int_schema ()) Parameters: Name Type Description Default schema The schema to use for the field required validation_alias | [ | ] | [[ | ]] | None The alias(es) to use to find the field in the validation data None serialization_alias | None The alias to use as a key when serializing None serialization_exclude | None Whether to exclude the field when serializing None serialization_exclude_if [[], ] | None A Callable that determines whether to exclude a field during serialization based on its value. None frozen | None Whether the field is frozen None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_field","title":"pydantic_core.core_schema - model_field","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_field","rank":-270},{"content":"model_fields_schema(\n    fields: [, ],\n    *,\n    model_name:  | None = None,\n    computed_fields: [] | None = None,\n    strict:  | None = None,\n    extras_schema:  | None = None,\n    extras_keys_schema:  | None = None,\n    extra_behavior:  | None = None,\n    from_attributes:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches the fields of a Pydantic model, e.g.: from pydantic_core import SchemaValidator , core_schema wrapper_schema = core_schema . model_fields_schema ( { 'a' : core_schema . model_field ( core_schema . str_schema ())} ) v = SchemaValidator ( wrapper_schema ) print ( v . validate_python ({ 'a' : 'hello' })) #> ({'a': 'hello'}, None, {'a'}) Parameters: Name Type Description Default fields [, ] The fields of the model required model_name | None The name of the model, used for error messages, defaults to \"Model\" None computed_fields [] | None Computed fields to use when serializing the model, only applies when directly inside a model None strict | None Whether the model is strict None extras_schema | None The schema to use when validating extra input data None extras_keys_schema | None The schema to use when validating the keys of extra input data None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None extra_behavior | None The extra behavior to use for the model fields None from_attributes | None Whether the model fields should be populated from attributes None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_fields_schema","title":"pydantic_core.core_schema - model_fields_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_fields_schema","rank":-275},{"content":"model_schema(\n    cls: [],\n    schema: ,\n    *,\n    generic_origin: [] | None = None,\n    custom_init:  | None = None,\n    root_model:  | None = None,\n    post_init:  | None = None,\n    revalidate_instances: (\n        [\"always\", \"never\", \"subclass-instances\"]\n        | None\n    ) = None,\n    strict:  | None = None,\n    frozen:  | None = None,\n    extra_behavior:  | None = None,\n    config:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> A model schema generally contains a typed-dict schema.\nIt will run the typed dict validator, then create a new class\nand set the dict and fields set returned from the typed dict validator\nto __dict__ and __pydantic_fields_set__ respectively. Example: from pydantic_core import CoreConfig , SchemaValidator , core_schema class MyModel : __slots__ = ( '__dict__' , '__pydantic_fields_set__' , '__pydantic_extra__' , '__pydantic_private__' , ) schema = core_schema . model_schema ( cls = MyModel , config = CoreConfig ( str_max_length = 5 ), schema = core_schema . model_fields_schema ( fields = { 'a' : core_schema . model_field ( core_schema . str_schema ())}, ), ) v = SchemaValidator ( schema ) assert v . isinstance_python ({ 'a' : 'hello' }) is True assert v . isinstance_python ({ 'a' : 'too long' }) is False Parameters: Name Type Description Default cls [] The class to use for the model required schema The schema to use for the model required generic_origin [] | None The origin type used for this model, if it's a parametrized generic. Ex,\nif this model schema represents SomeModel[int] , generic_origin is SomeModel None custom_init | None Whether the model has a custom init method None root_model | None Whether the model is a RootModel None post_init | None The call after init to use for the model None revalidate_instances ['always', 'never', 'subclass-instances'] | None whether instances of models and dataclasses (including subclass instances)\nshould re-validate defaults to config.revalidate_instances, else 'never' None strict | None Whether the model is strict None frozen | None Whether the model is frozen None extra_behavior | None The extra behavior to use for the model, used in serialization None config | None The config to use for the model None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_schema","title":"pydantic_core.core_schema - model_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.model_schema","rank":-280},{"content":"dataclass_field(\n    name: ,\n    schema: ,\n    *,\n    kw_only:  | None = None,\n    init:  | None = None,\n    init_only:  | None = None,\n    validation_alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None,\n    serialization_alias:  | None = None,\n    serialization_exclude:  | None = None,\n    metadata: [, ] | None = None,\n    serialization_exclude_if: (\n        [[], ] | None\n    ) = None,\n    frozen:  | None = None\n) -> Returns a schema for a dataclass field, e.g.: from pydantic_core import SchemaValidator , core_schema field = core_schema . dataclass_field ( name = 'a' , schema = core_schema . str_schema (), kw_only = False ) schema = core_schema . dataclass_args_schema ( 'Foobar' , [ field ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hello' }) == ({ 'a' : 'hello' }, None ) Parameters: Name Type Description Default name The name to use for the argument parameter required schema The schema to use for the argument parameter required kw_only | None Whether the field can be set with a positional argument as well as a keyword argument None init | None Whether the field should be validated during initialization None init_only | None Whether the field should be omitted  from __dict__ and passed to __post_init__ None validation_alias | [ | ] | [[ | ]] | None The alias(es) to use to find the field in the validation data None serialization_alias | None The alias to use as a key when serializing None serialization_exclude | None Whether to exclude the field when serializing None serialization_exclude_if [[], ] | None A callable that determines whether to exclude the field when serializing based on its value. None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None frozen | None Whether the field is frozen None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dataclass_field","title":"pydantic_core.core_schema - dataclass_field","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dataclass_field","rank":-285},{"content":"dataclass_args_schema(\n    dataclass_name: ,\n    fields: [],\n    *,\n    computed_fields: [] | None = None,\n    collect_init_only:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n    extra_behavior:  | None = None\n) -> Returns a schema for validating dataclass arguments, e.g.: from pydantic_core import SchemaValidator , core_schema field_a = core_schema . dataclass_field ( name = 'a' , schema = core_schema . str_schema (), kw_only = False ) field_b = core_schema . dataclass_field ( name = 'b' , schema = core_schema . bool_schema (), kw_only = False ) schema = core_schema . dataclass_args_schema ( 'Foobar' , [ field_a , field_b ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hello' , 'b' : True }) == ({ 'a' : 'hello' , 'b' : True }, None ) Parameters: Name Type Description Default dataclass_name The name of the dataclass being validated required fields [] The fields to use for the dataclass required computed_fields [] | None Computed fields to use when serializing the dataclass None collect_init_only | None Whether to collect init only fields into a dict to pass to __post_init__ None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None extra_behavior | None How to handle extra fields None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dataclass_args_schema","title":"pydantic_core.core_schema - dataclass_args_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dataclass_args_schema","rank":-290},{"content":"dataclass_schema(\n    cls: [],\n    schema: ,\n    fields: [],\n    *,\n    generic_origin: [] | None = None,\n    cls_name:  | None = None,\n    post_init:  | None = None,\n    revalidate_instances: (\n        [\"always\", \"never\", \"subclass-instances\"]\n        | None\n    ) = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n    frozen:  | None = None,\n    slots:  | None = None,\n    config:  | None = None\n) -> Returns a schema for a dataclass. As with ModelSchema , this schema can only be used as a field within\nanother schema, not as the root type. Parameters: Name Type Description Default cls [] The dataclass type, used to perform subclass checks required schema The schema to use for the dataclass fields required fields [] Fields of the dataclass, this is used in serialization and in validation during re-validation\nand while validating assignment required generic_origin [] | None The origin type used for this dataclass, if it's a parametrized generic. Ex,\nif this model schema represents SomeDataclass[int] , generic_origin is SomeDataclass None cls_name | None The name to use in error locs, etc; this is useful for generics (default: cls.__name__ ) None post_init | None Whether to call __post_init__ after validation None revalidate_instances ['always', 'never', 'subclass-instances'] | None whether instances of models and dataclasses (including subclass instances)\nshould re-validate defaults to config.revalidate_instances, else 'never' None strict | None Whether to require an exact instance of cls None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None frozen | None Whether the dataclass is frozen None slots | None Whether slots=True on the dataclass, means each field is assigned independently, rather than\nsimply setting __dict__ , default false None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dataclass_schema","title":"pydantic_core.core_schema - dataclass_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.dataclass_schema","rank":-295},{"content":"arguments_parameter(\n    name: ,\n    schema: ,\n    *,\n    mode: (\n        [\n            \"positional_only\",\n            \"positional_or_keyword\",\n            \"keyword_only\",\n        ]\n        | None\n    ) = None,\n    alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None\n) -> Returns a schema that matches an argument parameter, e.g.: from pydantic_core import SchemaValidator , core_schema param = core_schema . arguments_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) schema = core_schema . arguments_schema ([ param ]) v = SchemaValidator ( schema ) assert v . validate_python (( 'hello' ,)) == (( 'hello' ,), {}) Parameters: Name Type Description Default name The name to use for the argument parameter required schema The schema to use for the argument parameter required mode ['positional_only', 'positional_or_keyword', 'keyword_only'] | None The mode to use for the argument parameter None alias | [ | ] | [[ | ]] | None The alias to use for the argument parameter None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_parameter","title":"pydantic_core.core_schema - arguments_parameter","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_parameter","rank":-300},{"content":"arguments_schema(\n    arguments: [],\n    *,\n    validate_by_name:  | None = None,\n    validate_by_alias:  | None = None,\n    var_args_schema:  | None = None,\n    var_kwargs_mode:  | None = None,\n    var_kwargs_schema:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an arguments schema, e.g.: from pydantic_core import SchemaValidator , core_schema param_a = core_schema . arguments_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) param_b = core_schema . arguments_parameter ( name = 'b' , schema = core_schema . bool_schema (), mode = 'positional_only' ) schema = core_schema . arguments_schema ([ param_a , param_b ]) v = SchemaValidator ( schema ) assert v . validate_python (( 'hello' , True )) == (( 'hello' , True ), {}) Parameters: Name Type Description Default arguments [] The arguments to use for the arguments schema required validate_by_name | None Whether to populate by the parameter names, defaults to False . None validate_by_alias | None Whether to populate by the parameter aliases, defaults to True . None var_args_schema | None The variable args schema to use for the arguments schema None var_kwargs_mode | None The validation mode to use for variadic keyword arguments. If 'uniform' , every value of the\nkeyword arguments will be validated against the var_kwargs_schema schema. If 'unpacked-typed-dict' ,\nthe var_kwargs_schema argument must be a None var_kwargs_schema | None The variable kwargs schema to use for the arguments schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_schema","title":"pydantic_core.core_schema - arguments_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_schema","rank":-305},{"content":"arguments_v3_parameter(\n    name: ,\n    schema: ,\n    *,\n    mode: (\n        [\n            \"positional_only\",\n            \"positional_or_keyword\",\n            \"keyword_only\",\n            \"var_args\",\n            \"var_kwargs_uniform\",\n            \"var_kwargs_unpacked_typed_dict\",\n        ]\n        | None\n    ) = None,\n    alias: (\n         | [ | ] | [[ | ]] | None\n    ) = None\n) -> Returns a schema that matches an argument parameter, e.g.: from pydantic_core import SchemaValidator , core_schema param = core_schema . arguments_v3_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) schema = core_schema . arguments_v3_schema ([ param ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hello' }) == (( 'hello' ,), {}) Parameters: Name Type Description Default name The name to use for the argument parameter required schema The schema to use for the argument parameter required mode ['positional_only', 'positional_or_keyword', 'keyword_only', 'var_args', 'var_kwargs_uniform', 'var_kwargs_unpacked_typed_dict'] | None The mode to use for the argument parameter None alias | [ | ] | [[ | ]] | None The alias to use for the argument parameter None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_v3_parameter","title":"pydantic_core.core_schema - arguments_v3_parameter","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_v3_parameter","rank":-310},{"content":"arguments_v3_schema(\n    arguments: [],\n    *,\n    validate_by_name:  | None = None,\n    validate_by_alias:  | None = None,\n    extra_behavior: (\n        [\"forbid\", \"ignore\"] | None\n    ) = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an arguments schema, e.g.: from pydantic_core import SchemaValidator , core_schema param_a = core_schema . arguments_v3_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) param_b = core_schema . arguments_v3_parameter ( name = 'kwargs' , schema = core_schema . bool_schema (), mode = 'var_kwargs_uniform' ) schema = core_schema . arguments_v3_schema ([ param_a , param_b ]) v = SchemaValidator ( schema ) assert v . validate_python ({ 'a' : 'hi' , 'kwargs' : { 'b' : True }}) == (( 'hi' ,), { 'b' : True }) This schema is currently not used by other Pydantic components. In V3, it will most likely\nbecome the default arguments schema for the 'call' schema. Parameters: Name Type Description Default arguments [] The arguments to use for the arguments schema. required validate_by_name | None Whether to populate by the parameter names, defaults to False . None validate_by_alias | None Whether to populate by the parameter aliases, defaults to True . None extra_behavior ['forbid', 'ignore'] | None The extra behavior to use. None ref | None optional unique identifier of the schema, used to reference the schema in other places. None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core. None serialization | None Custom serialization schema. None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_v3_schema","title":"pydantic_core.core_schema - arguments_v3_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.arguments_v3_schema","rank":-315},{"content":"call_schema(\n    arguments: ,\n    function: [..., ],\n    *,\n    function_name:  | None = None,\n    return_schema:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches an arguments schema, then calls a function, e.g.: from pydantic_core import SchemaValidator , core_schema param_a = core_schema . arguments_parameter ( name = 'a' , schema = core_schema . str_schema (), mode = 'positional_only' ) param_b = core_schema . arguments_parameter ( name = 'b' , schema = core_schema . bool_schema (), mode = 'positional_only' ) args_schema = core_schema . arguments_schema ([ param_a , param_b ]) schema = core_schema . call_schema ( arguments = args_schema , function = lambda a , b : a + str ( not b ), return_schema = core_schema . str_schema (), ) v = SchemaValidator ( schema ) assert v . validate_python ((( 'hello' , True ))) == 'helloFalse' Parameters: Name Type Description Default arguments The arguments to use for the arguments schema required function [..., ] The function to use for the call schema required function_name | None The function name to use for the call schema, if not provided function.__name__ is used None return_schema | None The return schema to use for the call schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.call_schema","title":"pydantic_core.core_schema - call_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.call_schema","rank":-320},{"content":"custom_error_schema(\n    schema: ,\n    custom_error_type: ,\n    *,\n    custom_error_message:  | None = None,\n    custom_error_context: [, ] | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a custom error value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . custom_error_schema ( schema = core_schema . int_schema (), custom_error_type = 'MyError' , custom_error_message = 'Error msg' , ) v = SchemaValidator ( schema ) v . validate_python ( 1 ) Parameters: Name Type Description Default schema The schema to use for the custom error schema required custom_error_type The custom error type to use for the custom error schema required custom_error_message | None The custom error message to use for the custom error schema None custom_error_context [, ] | None The custom error context to use for the custom error schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.custom_error_schema","title":"pydantic_core.core_schema - custom_error_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.custom_error_schema","rank":-325},{"content":"json_schema(\n    schema:  | None = None,\n    *,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a JSON value, e.g.: from pydantic_core import SchemaValidator , core_schema dict_schema = core_schema . model_fields_schema ( { 'field_a' : core_schema . model_field ( core_schema . str_schema ()), 'field_b' : core_schema . model_field ( core_schema . bool_schema ()), }, ) class MyModel : __slots__ = ( '__dict__' , '__pydantic_fields_set__' , '__pydantic_extra__' , '__pydantic_private__' , ) field_a : str field_b : bool json_schema = core_schema . json_schema ( schema = dict_schema ) schema = core_schema . model_schema ( cls = MyModel , schema = json_schema ) v = SchemaValidator ( schema ) m = v . validate_python ( '{\"field_a\": \"hello\", \"field_b\": true}' ) assert isinstance ( m , MyModel ) Parameters: Name Type Description Default schema | None The schema to use for the JSON schema None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.json_schema","title":"pydantic_core.core_schema - json_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.json_schema","rank":-330},{"content":"url_schema(\n    *,\n    max_length:  | None = None,\n    allowed_schemes: [] | None = None,\n    host_required:  | None = None,\n    default_host:  | None = None,\n    default_port:  | None = None,\n    default_path:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a URL value, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . url_schema () v = SchemaValidator ( schema ) print ( v . validate_python ( 'https://example.com' )) #> https://example.com/ Parameters: Name Type Description Default max_length | None The maximum length of the URL None allowed_schemes [] | None The allowed URL schemes None host_required | None Whether the URL must have a host None default_host | None The default host to use if the URL does not have a host None default_port | None The default port to use if the URL does not have a port None default_path | None The default path to use if the URL does not have a path None strict | None Whether to use strict URL parsing None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.url_schema","title":"pydantic_core.core_schema - url_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.url_schema","rank":-335},{"content":"multi_host_url_schema(\n    *,\n    max_length:  | None = None,\n    allowed_schemes: [] | None = None,\n    host_required:  | None = None,\n    default_host:  | None = None,\n    default_port:  | None = None,\n    default_path:  | None = None,\n    strict:  | None = None,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None\n) -> Returns a schema that matches a URL value with possibly multiple hosts, e.g.: from pydantic_core import SchemaValidator , core_schema schema = core_schema . multi_host_url_schema () v = SchemaValidator ( schema ) print ( v . validate_python ( 'redis://localhost,0.0.0.0,127.0.0.1' )) #> redis://localhost,0.0.0.0,127.0.0.1 Parameters: Name Type Description Default max_length | None The maximum length of the URL None allowed_schemes [] | None The allowed URL schemes None host_required | None Whether the URL must have a host None default_host | None The default host to use if the URL does not have a host None default_port | None The default port to use if the URL does not have a port None default_path | None The default path to use if the URL does not have a path None strict | None Whether to use strict URL parsing None ref | None optional unique identifier of the schema, used to reference the schema in other places None metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.multi_host_url_schema","title":"pydantic_core.core_schema - multi_host_url_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.multi_host_url_schema","rank":-340},{"content":"definitions_schema(\n    schema: , definitions: []\n) -> Build a schema that contains both an inner schema and a list of definitions which can be used\nwithin the inner schema. from pydantic_core import SchemaValidator , core_schema schema = core_schema . definitions_schema ( core_schema . list_schema ( core_schema . definition_reference_schema ( 'foobar' )), [ core_schema . int_schema ( ref = 'foobar' )], ) v = SchemaValidator ( schema ) assert v . validate_python ([ 1 , 2 , '3' ]) == [ 1 , 2 , 3 ] Parameters: Name Type Description Default schema The inner schema required definitions [] List of definitions which can be referenced within inner schema required","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.definitions_schema","title":"pydantic_core.core_schema - definitions_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.definitions_schema","rank":-345},{"content":"definition_reference_schema(\n    schema_ref: ,\n    ref:  | None = None,\n    metadata: [, ] | None = None,\n    serialization:  | None = None,\n) -> Returns a schema that points to a schema stored in \"definitions\", this is useful for nested recursive\nmodels and also when you want to define validators separately from the main schema, e.g.: from pydantic_core import SchemaValidator , core_schema schema_definition = core_schema . definition_reference_schema ( 'list-schema' ) schema = core_schema . definitions_schema ( schema = schema_definition , definitions = [ core_schema . list_schema ( items_schema = schema_definition , ref = 'list-schema' ), ], ) v = SchemaValidator ( schema ) assert v . validate_python ([()]) == [[]] Parameters: Name Type Description Default schema_ref The schema ref to use for the definition reference schema required metadata [, ] | None Any other information you want to include with the schema, not used by pydantic-core None serialization | None Custom serialization schema None","pageID":"pydantic_core.core_schema","abs_url":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.definition_reference_schema","title":"pydantic_core.core_schema - definition_reference_schema","objectID":"/latest/api/pydantic_core_schema/#pydantic_core.core_schema.definition_reference_schema","rank":-350},{"content":"Color definitions are used as per the CSS3 CSS Color Module Level 3 specification. A few colors have multiple names referring to the sames colors, eg. grey and gray or aqua and cyan . In these cases the last color when sorted alphabetically takes preferences,\neg. Color((0, 255, 255)).as_named() == 'cyan' because \"cyan\" comes after \"aqua\". RGBA ¶ RGBA(r: , g: , b: , alpha:  | None) Internal use only as a representation of a color. Color ¶ Color(value: ) Bases: Represents a color. original ¶ original() -> Original value passed to Color . as_named ¶ as_named(*, fallback:  = False) -> Returns the name of the color if it can be found in COLORS_BY_VALUE dictionary,\notherwise returns the hexadecimal representation of the color or raises ValueError . Parameters: Name Type Description Default fallback If True, falls back to returning the hexadecimal representation of\nthe color instead of raising a ValueError when no named color is found. False Returns: Type Description The name of the color, or the hexadecimal representation of the color. Raises: Type Description When no named color is found and fallback is False . as_hex ¶ as_hex(format: ['short', 'long'] = 'short') -> Returns the hexadecimal representation of the color. Hex string representing the color can be 3, 4, 6, or 8 characters depending on whether the string\na \"short\" representation of the color is possible and whether there's an alpha channel. Returns: Type Description The hexadecimal representation of the color. as_rgb ¶ as_rgb() -> Color as an rgb(<r>, <g>, <b>) or rgba(<r>, <g>, <b>, <a>) string. as_rgb_tuple ¶ as_rgb_tuple(*, alpha:  | None = None) -> Returns the color as an RGB or RGBA tuple. Parameters: Name Type Description Default alpha | None Whether to include the alpha channel. There are three options for this input: None (default): Include alpha only if it's set. (e.g. not None ) True : Always include alpha. False : Always omit alpha. None Returns: Type Description A tuple that contains the values of the red, green, and blue channels in the range 0 to 255.\nIf alpha is included, it is in the range 0 to 1. as_hsl ¶ as_hsl() -> Color as an hsl(<h>, <s>, <l>) or hsl(<h>, <s>, <l>, <a>) string. as_hsl_tuple ¶ as_hsl_tuple(*, alpha:  | None = None) -> Returns the color as an HSL or HSLA tuple. Parameters: Name Type Description Default alpha | None Whether to include the alpha channel. None (default): Include the alpha channel only if it's set (e.g. not None ). True : Always include alpha. False : Always omit alpha. None Returns: Type Description The color as a tuple of hue, saturation, lightness, and alpha (if included).\nAll elements are in the range 0 to 1. parse_tuple ¶ parse_tuple(value: [, ...]) -> Parse a tuple or list to get RGBA values. Parameters: Name Type Description Default value [, ...] A tuple or list. required Returns: Type Description An RGBA tuple parsed from the input tuple. Raises: Type Description If tuple is not valid. parse_str ¶ parse_str(value: ) -> Parse a string representing a color to an RGBA tuple. Possible formats for the input string include: named color, see COLORS_BY_NAME hex short eg. <prefix>fff (prefix can be # , 0x or nothing) hex long eg. <prefix>ffffff (prefix can be # , 0x or nothing) rgb(<r>, <g>, <b>) rgba(<r>, <g>, <b>, <a>) transparent Parameters: Name Type Description Default value A string representing a color. required Returns: Type Description An RGBA tuple parsed from the input string. Raises: Type Description If the input string cannot be parsed to an RGBA tuple. ints_to_rgba ¶ ints_to_rgba(\n    r:  | ,\n    g:  | ,\n    b:  | ,\n    alpha:  | None = None,\n) -> Converts integer or string values for RGB color and an optional alpha value to an RGBA object. Parameters: Name Type Description Default r | An integer or string representing the red color value. required g | An integer or string representing the green color value. required b | An integer or string representing the blue color value. required alpha | None A float representing the alpha value. Defaults to None. None Returns: Type Description An instance of the RGBA class with the corresponding color and alpha values. parse_color_value ¶ parse_color_value(\n    value:  | , max_val:  = 255\n) -> Parse the color value provided and return a number between 0 and 1. Parameters: Name Type Description Default value | An integer or string color value. required max_val Maximum range value. Defaults to 255. 255 Raises: Type Description If the value is not a valid color. Returns: Type Description A number between 0 and 1. parse_float_alpha ¶ parse_float_alpha(\n    value: None |  |  | ,\n) ->  | None Parse an alpha value checking it's a valid float in the range 0 to 1. Parameters: Name Type Description Default value None |  |  | The input value to parse. required Returns: Type Description | None The parsed value as a float, or None if the value was None or equal 1. Raises: Type Description If the input value cannot be successfully parsed as a float in the expected range. parse_hsl ¶ parse_hsl(\n    h: ,\n    h_units: ,\n    sat: ,\n    light: ,\n    alpha:  | None = None,\n) -> Parse raw hue, saturation, lightness, and alpha values and convert to RGBA. Parameters: Name Type Description Default h The hue value. required h_units The unit for hue value. required sat The saturation value. required light The lightness value. required alpha | None Alpha value. None Returns: Type Description An instance of RGBA . float_to_255 ¶ float_to_255(c: ) -> Converts a float value between 0 and 1 (inclusive) to an integer between 0 and 255 (inclusive). Parameters: Name Type Description Default c The float value to be converted. Must be between 0 and 1 (inclusive). required Returns: Type Description The integer equivalent of the given float value rounded to the nearest whole number.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#Color","title":"Color","objectID":"/latest/api/pydantic_extra_types_color/#Color","rank":100},{"content":"RGBA(r: , g: , b: , alpha:  | None) Internal use only as a representation of a color.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.RGBA","title":"Color - RGBA","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.RGBA","rank":95},{"content":"Color(value: ) Bases: Represents a color. original ¶ original() -> Original value passed to Color . as_named ¶ as_named(*, fallback:  = False) -> Returns the name of the color if it can be found in COLORS_BY_VALUE dictionary,\notherwise returns the hexadecimal representation of the color or raises ValueError . Parameters: Name Type Description Default fallback If True, falls back to returning the hexadecimal representation of\nthe color instead of raising a ValueError when no named color is found. False Returns: Type Description The name of the color, or the hexadecimal representation of the color. Raises: Type Description When no named color is found and fallback is False . as_hex ¶ as_hex(format: ['short', 'long'] = 'short') -> Returns the hexadecimal representation of the color. Hex string representing the color can be 3, 4, 6, or 8 characters depending on whether the string\na \"short\" representation of the color is possible and whether there's an alpha channel. Returns: Type Description The hexadecimal representation of the color. as_rgb ¶ as_rgb() -> Color as an rgb(<r>, <g>, <b>) or rgba(<r>, <g>, <b>, <a>) string. as_rgb_tuple ¶ as_rgb_tuple(*, alpha:  | None = None) -> Returns the color as an RGB or RGBA tuple. Parameters: Name Type Description Default alpha | None Whether to include the alpha channel. There are three options for this input: None (default): Include alpha only if it's set. (e.g. not None ) True : Always include alpha. False : Always omit alpha. None Returns: Type Description A tuple that contains the values of the red, green, and blue channels in the range 0 to 255.\nIf alpha is included, it is in the range 0 to 1. as_hsl ¶ as_hsl() -> Color as an hsl(<h>, <s>, <l>) or hsl(<h>, <s>, <l>, <a>) string. as_hsl_tuple ¶ as_hsl_tuple(*, alpha:  | None = None) -> Returns the color as an HSL or HSLA tuple. Parameters: Name Type Description Default alpha | None Whether to include the alpha channel. None (default): Include the alpha channel only if it's set (e.g. not None ). True : Always include alpha. False : Always omit alpha. None Returns: Type Description The color as a tuple of hue, saturation, lightness, and alpha (if included).\nAll elements are in the range 0 to 1.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color","title":"Color - Color","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color","rank":90},{"content":"original() -> Original value passed to Color .","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.original","title":"Color - Color - original","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.original","rank":85},{"content":"as_named(*, fallback:  = False) -> Returns the name of the color if it can be found in COLORS_BY_VALUE dictionary,\notherwise returns the hexadecimal representation of the color or raises ValueError . Parameters: Name Type Description Default fallback If True, falls back to returning the hexadecimal representation of\nthe color instead of raising a ValueError when no named color is found. False Returns: Type Description The name of the color, or the hexadecimal representation of the color. Raises: Type Description When no named color is found and fallback is False .","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_named","title":"Color - Color - as_named","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_named","rank":80},{"content":"as_hex(format: ['short', 'long'] = 'short') -> Returns the hexadecimal representation of the color. Hex string representing the color can be 3, 4, 6, or 8 characters depending on whether the string\na \"short\" representation of the color is possible and whether there's an alpha channel. Returns: Type Description The hexadecimal representation of the color.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_hex","title":"Color - Color - as_hex","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_hex","rank":75},{"content":"as_rgb() -> Color as an rgb(<r>, <g>, <b>) or rgba(<r>, <g>, <b>, <a>) string.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_rgb","title":"Color - Color - as_rgb","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_rgb","rank":70},{"content":"as_rgb_tuple(*, alpha:  | None = None) -> Returns the color as an RGB or RGBA tuple. Parameters: Name Type Description Default alpha | None Whether to include the alpha channel. There are three options for this input: None (default): Include alpha only if it's set. (e.g. not None ) True : Always include alpha. False : Always omit alpha. None Returns: Type Description A tuple that contains the values of the red, green, and blue channels in the range 0 to 255.\nIf alpha is included, it is in the range 0 to 1.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_rgb_tuple","title":"Color - Color - as_rgb_tuple","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_rgb_tuple","rank":65},{"content":"as_hsl() -> Color as an hsl(<h>, <s>, <l>) or hsl(<h>, <s>, <l>, <a>) string.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_hsl","title":"Color - Color - as_hsl","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_hsl","rank":60},{"content":"as_hsl_tuple(*, alpha:  | None = None) -> Returns the color as an HSL or HSLA tuple. Parameters: Name Type Description Default alpha | None Whether to include the alpha channel. None (default): Include the alpha channel only if it's set (e.g. not None ). True : Always include alpha. False : Always omit alpha. None Returns: Type Description The color as a tuple of hue, saturation, lightness, and alpha (if included).\nAll elements are in the range 0 to 1.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_hsl_tuple","title":"Color - Color - as_hsl_tuple","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.Color.as_hsl_tuple","rank":55},{"content":"parse_tuple(value: [, ...]) -> Parse a tuple or list to get RGBA values. Parameters: Name Type Description Default value [, ...] A tuple or list. required Returns: Type Description An RGBA tuple parsed from the input tuple. Raises: Type Description If tuple is not valid.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_tuple","title":"Color - parse_tuple","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_tuple","rank":50},{"content":"parse_str(value: ) -> Parse a string representing a color to an RGBA tuple. Possible formats for the input string include: named color, see COLORS_BY_NAME hex short eg. <prefix>fff (prefix can be # , 0x or nothing) hex long eg. <prefix>ffffff (prefix can be # , 0x or nothing) rgb(<r>, <g>, <b>) rgba(<r>, <g>, <b>, <a>) transparent Parameters: Name Type Description Default value A string representing a color. required Returns: Type Description An RGBA tuple parsed from the input string. Raises: Type Description If the input string cannot be parsed to an RGBA tuple.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_str","title":"Color - parse_str","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_str","rank":45},{"content":"ints_to_rgba(\n    r:  | ,\n    g:  | ,\n    b:  | ,\n    alpha:  | None = None,\n) -> Converts integer or string values for RGB color and an optional alpha value to an RGBA object. Parameters: Name Type Description Default r | An integer or string representing the red color value. required g | An integer or string representing the green color value. required b | An integer or string representing the blue color value. required alpha | None A float representing the alpha value. Defaults to None. None Returns: Type Description An instance of the RGBA class with the corresponding color and alpha values.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.ints_to_rgba","title":"Color - ints_to_rgba","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.ints_to_rgba","rank":40},{"content":"parse_color_value(\n    value:  | , max_val:  = 255\n) -> Parse the color value provided and return a number between 0 and 1. Parameters: Name Type Description Default value | An integer or string color value. required max_val Maximum range value. Defaults to 255. 255 Raises: Type Description If the value is not a valid color. Returns: Type Description A number between 0 and 1.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_color_value","title":"Color - parse_color_value","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_color_value","rank":35},{"content":"parse_float_alpha(\n    value: None |  |  | ,\n) ->  | None Parse an alpha value checking it's a valid float in the range 0 to 1. Parameters: Name Type Description Default value None |  |  | The input value to parse. required Returns: Type Description | None The parsed value as a float, or None if the value was None or equal 1. Raises: Type Description If the input value cannot be successfully parsed as a float in the expected range.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_float_alpha","title":"Color - parse_float_alpha","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_float_alpha","rank":30},{"content":"parse_hsl(\n    h: ,\n    h_units: ,\n    sat: ,\n    light: ,\n    alpha:  | None = None,\n) -> Parse raw hue, saturation, lightness, and alpha values and convert to RGBA. Parameters: Name Type Description Default h The hue value. required h_units The unit for hue value. required sat The saturation value. required light The lightness value. required alpha | None Alpha value. None Returns: Type Description An instance of RGBA .","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_hsl","title":"Color - parse_hsl","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.parse_hsl","rank":25},{"content":"float_to_255(c: ) -> Converts a float value between 0 and 1 (inclusive) to an integer between 0 and 255 (inclusive). Parameters: Name Type Description Default c The float value to be converted. Must be between 0 and 1 (inclusive). required Returns: Type Description The integer equivalent of the given float value rounded to the nearest whole number.","pageID":"Color","abs_url":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.float_to_255","title":"Color - float_to_255","objectID":"/latest/api/pydantic_extra_types_color/#pydantic_extra_types.color.float_to_255","rank":20},{"content":"The pydantic_extra_types.coordinate module provides the ,\n, and\n data types. Latitude ¶ Bases: Latitude value should be between -90 and 90, inclusive. from pydantic import BaseModel from pydantic_extra_types.coordinate import Latitude class Location ( BaseModel ): latitude : Latitude location = Location ( latitude = 41.40338 ) print ( location ) #> latitude=41.40338 Longitude ¶ Bases: Longitude value should be between -180 and 180, inclusive. from pydantic import BaseModel from pydantic_extra_types.coordinate import Longitude class Location ( BaseModel ): longitude : Longitude location = Location ( longitude = 2.17403 ) print ( location ) #> longitude=2.17403 Coordinate dataclass ¶ Coordinate(latitude: , longitude: ) Bases: Coordinate parses Latitude and Longitude. You can use the Coordinate data type for storing coordinates. Coordinates can be\ndefined using one of the following formats: Tuple: (Latitude, Longitude) . For example: (41.40338, 2.17403) . Coordinate instance: Coordinate(latitude=Latitude, longitude=Longitude) . from pydantic import BaseModel from pydantic_extra_types.coordinate import Coordinate class Location ( BaseModel ): coordinate : Coordinate location = Location ( coordinate = ( 41.40338 , 2.17403 )) #> coordinate=Coordinate(latitude=41.40338, longitude=2.17403)","pageID":"Coordinate","abs_url":"/latest/api/pydantic_extra_types_coordinate/#Coordinate","title":"Coordinate","objectID":"/latest/api/pydantic_extra_types_coordinate/#Coordinate","rank":100},{"content":"Bases: Latitude value should be between -90 and 90, inclusive. from pydantic import BaseModel from pydantic_extra_types.coordinate import Latitude class Location ( BaseModel ): latitude : Latitude location = Location ( latitude = 41.40338 ) print ( location ) #> latitude=41.40338","pageID":"Coordinate","abs_url":"/latest/api/pydantic_extra_types_coordinate/#pydantic_extra_types.coordinate.Latitude","title":"Coordinate - Latitude","objectID":"/latest/api/pydantic_extra_types_coordinate/#pydantic_extra_types.coordinate.Latitude","rank":95},{"content":"Bases: Longitude value should be between -180 and 180, inclusive. from pydantic import BaseModel from pydantic_extra_types.coordinate import Longitude class Location ( BaseModel ): longitude : Longitude location = Location ( longitude = 2.17403 ) print ( location ) #> longitude=2.17403","pageID":"Coordinate","abs_url":"/latest/api/pydantic_extra_types_coordinate/#pydantic_extra_types.coordinate.Longitude","title":"Coordinate - Longitude","objectID":"/latest/api/pydantic_extra_types_coordinate/#pydantic_extra_types.coordinate.Longitude","rank":90},{"content":"Coordinate(latitude: , longitude: ) Bases: Coordinate parses Latitude and Longitude. You can use the Coordinate data type for storing coordinates. Coordinates can be\ndefined using one of the following formats: Tuple: (Latitude, Longitude) . For example: (41.40338, 2.17403) . Coordinate instance: Coordinate(latitude=Latitude, longitude=Longitude) . from pydantic import BaseModel from pydantic_extra_types.coordinate import Coordinate class Location ( BaseModel ): coordinate : Coordinate location = Location ( coordinate = ( 41.40338 , 2.17403 )) #> coordinate=Coordinate(latitude=41.40338, longitude=2.17403)","pageID":"Coordinate","abs_url":"/latest/api/pydantic_extra_types_coordinate/#pydantic_extra_types.coordinate.Coordinate","title":"Coordinate - Coordinate  dataclass","objectID":"/latest/api/pydantic_extra_types_coordinate/#pydantic_extra_types.coordinate.Coordinate","rank":85},{"content":"Country definitions that are based on the ISO 3166 . CountryAlpha2 ¶ Bases: CountryAlpha2 parses country codes in the ISO 3166-1 alpha-2 format. from pydantic import BaseModel from pydantic_extra_types.country import CountryAlpha2 class Product ( BaseModel ): made_in : CountryAlpha2 product = Product ( made_in = 'ES' ) print ( product ) #> made_in='ES' alpha3 property ¶ alpha3: The country code in the ISO 3166-1 alpha-3 format. numeric_code property ¶ numeric_code: The country code in the ISO 3166-1 numeric format. short_name property ¶ short_name: The country short name. CountryAlpha3 ¶ Bases: CountryAlpha3 parses country codes in the ISO 3166-1 alpha-3 format. from pydantic import BaseModel from pydantic_extra_types.country import CountryAlpha3 class Product ( BaseModel ): made_in : CountryAlpha3 product = Product ( made_in = \"USA\" ) print ( product ) #> made_in='USA' alpha2 property ¶ alpha2: The country code in the ISO 3166-1 alpha-2 format. numeric_code property ¶ numeric_code: The country code in the ISO 3166-1 numeric format. short_name property ¶ short_name: The country short name. CountryNumericCode ¶ Bases: CountryNumericCode parses country codes in the ISO 3166-1 numeric format. from pydantic import BaseModel from pydantic_extra_types.country import CountryNumericCode class Product ( BaseModel ): made_in : CountryNumericCode product = Product ( made_in = \"840\" ) print ( product ) #> made_in='840' alpha2 property ¶ alpha2: The country code in the ISO 3166-1 alpha-2 format. alpha3 property ¶ alpha3: The country code in the ISO 3166-1 alpha-3 format. short_name property ¶ short_name: The country short name. CountryShortName ¶ Bases: CountryShortName parses country codes in the short name format. from pydantic import BaseModel from pydantic_extra_types.country import CountryShortName class Product ( BaseModel ): made_in : CountryShortName product = Product ( made_in = \"United States\" ) print ( product ) #> made_in='United States' alpha2 property ¶ alpha2: The country code in the ISO 3166-1 alpha-2 format. alpha3 property ¶ alpha3: The country code in the ISO 3166-1 alpha-3 format. numeric_code property ¶ numeric_code: The country code in the ISO 3166-1 numeric format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#Country","title":"Country","objectID":"/latest/api/pydantic_extra_types_country/#Country","rank":100},{"content":"Bases: CountryAlpha2 parses country codes in the ISO 3166-1 alpha-2 format. from pydantic import BaseModel from pydantic_extra_types.country import CountryAlpha2 class Product ( BaseModel ): made_in : CountryAlpha2 product = Product ( made_in = 'ES' ) print ( product ) #> made_in='ES' alpha3 property ¶ alpha3: The country code in the ISO 3166-1 alpha-3 format. numeric_code property ¶ numeric_code: The country code in the ISO 3166-1 numeric format. short_name property ¶ short_name: The country short name.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2","title":"Country - CountryAlpha2","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2","rank":95},{"content":"alpha3: The country code in the ISO 3166-1 alpha-3 format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2.alpha3","title":"Country - CountryAlpha2 - alpha3  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2.alpha3","rank":90},{"content":"numeric_code: The country code in the ISO 3166-1 numeric format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2.numeric_code","title":"Country - CountryAlpha2 - numeric_code  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2.numeric_code","rank":85},{"content":"short_name: The country short name.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2.short_name","title":"Country - CountryAlpha2 - short_name  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha2.short_name","rank":80},{"content":"Bases: CountryAlpha3 parses country codes in the ISO 3166-1 alpha-3 format. from pydantic import BaseModel from pydantic_extra_types.country import CountryAlpha3 class Product ( BaseModel ): made_in : CountryAlpha3 product = Product ( made_in = \"USA\" ) print ( product ) #> made_in='USA' alpha2 property ¶ alpha2: The country code in the ISO 3166-1 alpha-2 format. numeric_code property ¶ numeric_code: The country code in the ISO 3166-1 numeric format. short_name property ¶ short_name: The country short name.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3","title":"Country - CountryAlpha3","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3","rank":75},{"content":"alpha2: The country code in the ISO 3166-1 alpha-2 format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3.alpha2","title":"Country - CountryAlpha3 - alpha2  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3.alpha2","rank":70},{"content":"numeric_code: The country code in the ISO 3166-1 numeric format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3.numeric_code","title":"Country - CountryAlpha3 - numeric_code  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3.numeric_code","rank":65},{"content":"short_name: The country short name.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3.short_name","title":"Country - CountryAlpha3 - short_name  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryAlpha3.short_name","rank":60},{"content":"Bases: CountryNumericCode parses country codes in the ISO 3166-1 numeric format. from pydantic import BaseModel from pydantic_extra_types.country import CountryNumericCode class Product ( BaseModel ): made_in : CountryNumericCode product = Product ( made_in = \"840\" ) print ( product ) #> made_in='840' alpha2 property ¶ alpha2: The country code in the ISO 3166-1 alpha-2 format. alpha3 property ¶ alpha3: The country code in the ISO 3166-1 alpha-3 format. short_name property ¶ short_name: The country short name.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode","title":"Country - CountryNumericCode","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode","rank":55},{"content":"alpha2: The country code in the ISO 3166-1 alpha-2 format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode.alpha2","title":"Country - CountryNumericCode - alpha2  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode.alpha2","rank":50},{"content":"alpha3: The country code in the ISO 3166-1 alpha-3 format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode.alpha3","title":"Country - CountryNumericCode - alpha3  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode.alpha3","rank":45},{"content":"short_name: The country short name.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode.short_name","title":"Country - CountryNumericCode - short_name  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryNumericCode.short_name","rank":40},{"content":"Bases: CountryShortName parses country codes in the short name format. from pydantic import BaseModel from pydantic_extra_types.country import CountryShortName class Product ( BaseModel ): made_in : CountryShortName product = Product ( made_in = \"United States\" ) print ( product ) #> made_in='United States' alpha2 property ¶ alpha2: The country code in the ISO 3166-1 alpha-2 format. alpha3 property ¶ alpha3: The country code in the ISO 3166-1 alpha-3 format. numeric_code property ¶ numeric_code: The country code in the ISO 3166-1 numeric format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName","title":"Country - CountryShortName","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName","rank":35},{"content":"alpha2: The country code in the ISO 3166-1 alpha-2 format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName.alpha2","title":"Country - CountryShortName - alpha2  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName.alpha2","rank":30},{"content":"alpha3: The country code in the ISO 3166-1 alpha-3 format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName.alpha3","title":"Country - CountryShortName - alpha3  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName.alpha3","rank":25},{"content":"numeric_code: The country code in the ISO 3166-1 numeric format.","pageID":"Country","abs_url":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName.numeric_code","title":"Country - CountryShortName - numeric_code  property","objectID":"/latest/api/pydantic_extra_types_country/#pydantic_extra_types.country.CountryShortName.numeric_code","rank":20},{"content":"Currency definitions that are based on the ISO4217 . ISO4217 ¶ Bases: ISO4217 parses Currency in the ISO 4217 format. from pydantic import BaseModel from pydantic_extra_types.currency_code import ISO4217 class Currency ( BaseModel ): alpha_3 : ISO4217 currency = Currency ( alpha_3 = 'AED' ) print ( currency ) # > alpha_3='AED' Currency ¶ Bases: Currency parses currency subset of the ISO 4217 format.\nIt excludes bonds testing codes and precious metals. from pydantic import BaseModel from pydantic_extra_types.currency_code import Currency class currency ( BaseModel ): alpha_3 : Currency cur = currency ( alpha_3 = 'AED' ) print ( cur ) # > alpha_3='AED'","pageID":"Currency","abs_url":"/latest/api/pydantic_extra_types_currency_code/#Currency","title":"Currency","objectID":"/latest/api/pydantic_extra_types_currency_code/#Currency","rank":100},{"content":"Bases: ISO4217 parses Currency in the ISO 4217 format. from pydantic import BaseModel from pydantic_extra_types.currency_code import ISO4217 class Currency ( BaseModel ): alpha_3 : ISO4217 currency = Currency ( alpha_3 = 'AED' ) print ( currency ) # > alpha_3='AED'","pageID":"Currency","abs_url":"/latest/api/pydantic_extra_types_currency_code/#pydantic_extra_types.currency_code.ISO4217","title":"Currency - ISO4217","objectID":"/latest/api/pydantic_extra_types_currency_code/#pydantic_extra_types.currency_code.ISO4217","rank":95},{"content":"Bases: Currency parses currency subset of the ISO 4217 format.\nIt excludes bonds testing codes and precious metals. from pydantic import BaseModel from pydantic_extra_types.currency_code import Currency class currency ( BaseModel ): alpha_3 : Currency cur = currency ( alpha_3 = 'AED' ) print ( cur ) # > alpha_3='AED'","pageID":"Currency","abs_url":"/latest/api/pydantic_extra_types_currency_code/#pydantic_extra_types.currency_code.Currency","title":"Currency - Currency","objectID":"/latest/api/pydantic_extra_types_currency_code/#pydantic_extra_types.currency_code.Currency","rank":90},{"content":"The pydantic_extra_types.isbn module provides functionality to recieve and validate ISBN. ISBN (International Standard Book Number) is a numeric commercial book identifier which is intended to be unique. This module provides a ISBN type for Pydantic models. ISBN ¶ Bases: Represents a ISBN and provides methods for conversion, validation, and serialization. from pydantic import BaseModel from pydantic_extra_types.isbn import ISBN class Book ( BaseModel ): isbn : ISBN book = Book ( isbn = \"8537809667\" ) print ( book ) #> isbn='9788537809662' validate_isbn_format staticmethod ¶ validate_isbn_format(value: ) -> None Validate a ISBN format from the provided str value. Parameters: Name Type Description Default value The str value representing the ISBN in 10 or 13 digits. required Raises: Type Description If the ISBN is not valid. convert_isbn10_to_isbn13 staticmethod ¶ convert_isbn10_to_isbn13(value: ) -> Convert an ISBN-10 to ISBN-13. Parameters: Name Type Description Default value The ISBN-10 value to be converted. required Returns: Type Description The converted ISBN or the original value if no conversion is necessary. isbn10_digit_calc ¶ isbn10_digit_calc(isbn: ) -> Calc a ISBN-10 last digit from the provided str value. More information of validation algorithm on Wikipedia Parameters: Name Type Description Default isbn The str value representing the ISBN in 10 digits. required Returns: Type Description The calculated last digit of the ISBN-10 value. isbn13_digit_calc ¶ isbn13_digit_calc(isbn: ) -> Calc a ISBN-13 last digit from the provided str value. More information of validation algorithm on Wikipedia Parameters: Name Type Description Default isbn The str value representing the ISBN in 13 digits. required Returns: Type Description The calculated last digit of the ISBN-13 value.","pageID":"ISBN","abs_url":"/latest/api/pydantic_extra_types_isbn/#ISBN","title":"ISBN","objectID":"/latest/api/pydantic_extra_types_isbn/#ISBN","rank":100},{"content":"Bases: Represents a ISBN and provides methods for conversion, validation, and serialization. from pydantic import BaseModel from pydantic_extra_types.isbn import ISBN class Book ( BaseModel ): isbn : ISBN book = Book ( isbn = \"8537809667\" ) print ( book ) #> isbn='9788537809662' validate_isbn_format staticmethod ¶ validate_isbn_format(value: ) -> None Validate a ISBN format from the provided str value. Parameters: Name Type Description Default value The str value representing the ISBN in 10 or 13 digits. required Raises: Type Description If the ISBN is not valid. convert_isbn10_to_isbn13 staticmethod ¶ convert_isbn10_to_isbn13(value: ) -> Convert an ISBN-10 to ISBN-13. Parameters: Name Type Description Default value The ISBN-10 value to be converted. required Returns: Type Description The converted ISBN or the original value if no conversion is necessary.","pageID":"ISBN","abs_url":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.ISBN","title":"ISBN - ISBN","objectID":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.ISBN","rank":95},{"content":"validate_isbn_format(value: ) -> None Validate a ISBN format from the provided str value. Parameters: Name Type Description Default value The str value representing the ISBN in 10 or 13 digits. required Raises: Type Description If the ISBN is not valid.","pageID":"ISBN","abs_url":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.ISBN.validate_isbn_format","title":"ISBN - ISBN - validate_isbn_format  staticmethod","objectID":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.ISBN.validate_isbn_format","rank":90},{"content":"convert_isbn10_to_isbn13(value: ) -> Convert an ISBN-10 to ISBN-13. Parameters: Name Type Description Default value The ISBN-10 value to be converted. required Returns: Type Description The converted ISBN or the original value if no conversion is necessary.","pageID":"ISBN","abs_url":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.ISBN.convert_isbn10_to_isbn13","title":"ISBN - ISBN - convert_isbn10_to_isbn13  staticmethod","objectID":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.ISBN.convert_isbn10_to_isbn13","rank":85},{"content":"isbn10_digit_calc(isbn: ) -> Calc a ISBN-10 last digit from the provided str value. More information of validation algorithm on Wikipedia Parameters: Name Type Description Default isbn The str value representing the ISBN in 10 digits. required Returns: Type Description The calculated last digit of the ISBN-10 value.","pageID":"ISBN","abs_url":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.isbn10_digit_calc","title":"ISBN - isbn10_digit_calc","objectID":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.isbn10_digit_calc","rank":80},{"content":"isbn13_digit_calc(isbn: ) -> Calc a ISBN-13 last digit from the provided str value. More information of validation algorithm on Wikipedia Parameters: Name Type Description Default isbn The str value representing the ISBN in 13 digits. required Returns: Type Description The calculated last digit of the ISBN-13 value.","pageID":"ISBN","abs_url":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.isbn13_digit_calc","title":"ISBN - isbn13_digit_calc","objectID":"/latest/api/pydantic_extra_types_isbn/#pydantic_extra_types.isbn.isbn13_digit_calc","rank":75},{"content":"Language definitions that are based on the ISO 639-3 & ISO 639-5 . LanguageInfo dataclass ¶ LanguageInfo(\n    alpha2: [, None], alpha3: , name: \n) LanguageInfo is a dataclass that contains the language information. Parameters: Name Type Description Default alpha2 [, None] The language code in the ISO 639-1 alpha-2 format. required alpha3 The language code in the ISO 639-3 alpha-3 format. required name The language name. required LanguageAlpha2 ¶ Bases: LanguageAlpha2 parses languages codes in the ISO 639-1 alpha-2 format. from pydantic import BaseModel from pydantic_extra_types.language_code import LanguageAlpha2 class Movie ( BaseModel ): audio_lang : LanguageAlpha2 subtitles_lang : LanguageAlpha2 movie = Movie ( audio_lang = 'de' , subtitles_lang = 'fr' ) print ( movie ) #> audio_lang='de' subtitles_lang='fr' alpha3 property ¶ alpha3: The language code in the ISO 639-3 alpha-3 format. name property ¶ name: The language name. LanguageName ¶ Bases: LanguageName parses languages names listed in the ISO 639-3 standard format. from pydantic import BaseModel from pydantic_extra_types.language_code import LanguageName class Movie ( BaseModel ): audio_lang : LanguageName subtitles_lang : LanguageName movie = Movie ( audio_lang = 'Dutch' , subtitles_lang = 'Mandarin Chinese' ) print ( movie ) #> audio_lang='Dutch' subtitles_lang='Mandarin Chinese' alpha2 property ¶ alpha2: [, None] The language code in the ISO 639-1 alpha-2 format. Does not exist for all languages. alpha3 property ¶ alpha3: The language code in the ISO 639-3 alpha-3 format. ISO639_3 ¶ Bases: ISO639_3 parses Language in the ISO 639-3 alpha-3 format. from pydantic import BaseModel from pydantic_extra_types.language_code import ISO639_3 class Language ( BaseModel ): alpha_3 : ISO639_3 lang = Language ( alpha_3 = 'ssr' ) print ( lang ) # > alpha_3='ssr' ISO639_5 ¶ Bases: ISO639_5 parses Language in the ISO 639-5 alpha-3 format. from pydantic import BaseModel from pydantic_extra_types.language_code import ISO639_5 class Language ( BaseModel ): alpha_3 : ISO639_5 lang = Language ( alpha_3 = 'gem' ) print ( lang ) # > alpha_3='gem'","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#Language","title":"Language","objectID":"/latest/api/pydantic_extra_types_language_code/#Language","rank":100},{"content":"LanguageInfo(\n    alpha2: [, None], alpha3: , name: \n) LanguageInfo is a dataclass that contains the language information. Parameters: Name Type Description Default alpha2 [, None] The language code in the ISO 639-1 alpha-2 format. required alpha3 The language code in the ISO 639-3 alpha-3 format. required name The language name. required","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageInfo","title":"Language - LanguageInfo  dataclass","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageInfo","rank":95},{"content":"Bases: LanguageAlpha2 parses languages codes in the ISO 639-1 alpha-2 format. from pydantic import BaseModel from pydantic_extra_types.language_code import LanguageAlpha2 class Movie ( BaseModel ): audio_lang : LanguageAlpha2 subtitles_lang : LanguageAlpha2 movie = Movie ( audio_lang = 'de' , subtitles_lang = 'fr' ) print ( movie ) #> audio_lang='de' subtitles_lang='fr' alpha3 property ¶ alpha3: The language code in the ISO 639-3 alpha-3 format. name property ¶ name: The language name.","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageAlpha2","title":"Language - LanguageAlpha2","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageAlpha2","rank":90},{"content":"alpha3: The language code in the ISO 639-3 alpha-3 format.","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageAlpha2.alpha3","title":"Language - LanguageAlpha2 - alpha3  property","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageAlpha2.alpha3","rank":85},{"content":"name: The language name.","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageAlpha2.name","title":"Language - LanguageAlpha2 - name  property","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageAlpha2.name","rank":80},{"content":"Bases: LanguageName parses languages names listed in the ISO 639-3 standard format. from pydantic import BaseModel from pydantic_extra_types.language_code import LanguageName class Movie ( BaseModel ): audio_lang : LanguageName subtitles_lang : LanguageName movie = Movie ( audio_lang = 'Dutch' , subtitles_lang = 'Mandarin Chinese' ) print ( movie ) #> audio_lang='Dutch' subtitles_lang='Mandarin Chinese' alpha2 property ¶ alpha2: [, None] The language code in the ISO 639-1 alpha-2 format. Does not exist for all languages. alpha3 property ¶ alpha3: The language code in the ISO 639-3 alpha-3 format.","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageName","title":"Language - LanguageName","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageName","rank":75},{"content":"alpha2: [, None] The language code in the ISO 639-1 alpha-2 format. Does not exist for all languages.","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageName.alpha2","title":"Language - LanguageName - alpha2  property","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageName.alpha2","rank":70},{"content":"alpha3: The language code in the ISO 639-3 alpha-3 format.","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageName.alpha3","title":"Language - LanguageName - alpha3  property","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.LanguageName.alpha3","rank":65},{"content":"Bases: ISO639_3 parses Language in the ISO 639-3 alpha-3 format. from pydantic import BaseModel from pydantic_extra_types.language_code import ISO639_3 class Language ( BaseModel ): alpha_3 : ISO639_3 lang = Language ( alpha_3 = 'ssr' ) print ( lang ) # > alpha_3='ssr'","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.ISO639_3","title":"Language - ISO639_3","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.ISO639_3","rank":60},{"content":"Bases: ISO639_5 parses Language in the ISO 639-5 alpha-3 format. from pydantic import BaseModel from pydantic_extra_types.language_code import ISO639_5 class Language ( BaseModel ): alpha_3 : ISO639_5 lang = Language ( alpha_3 = 'gem' ) print ( lang ) # > alpha_3='gem'","pageID":"Language","abs_url":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.ISO639_5","title":"Language - ISO639_5","objectID":"/latest/api/pydantic_extra_types_language_code/#pydantic_extra_types.language_code.ISO639_5","rank":55},{"content":"The MAC address module provides functionality to parse and validate MAC addresses in different\nformats, such as IEEE 802 MAC-48, EUI-48, EUI-64, or a 20-octet format. MacAddress ¶ Bases: Represents a MAC address and provides methods for conversion, validation, and serialization. from pydantic import BaseModel from pydantic_extra_types.mac_address import MacAddress class Network ( BaseModel ): mac_address : MacAddress network = Network ( mac_address = \"00:00:5e:00:53:01\" ) print ( network ) #> mac_address='00:00:5e:00:53:01' validate_mac_address staticmethod ¶ validate_mac_address(value: ) -> Validate a MAC Address from the provided byte value.","pageID":"Mac Address","abs_url":"/latest/api/pydantic_extra_types_mac_address/#Mac Address","title":"Mac Address","objectID":"/latest/api/pydantic_extra_types_mac_address/#Mac Address","rank":100},{"content":"Bases: Represents a MAC address and provides methods for conversion, validation, and serialization. from pydantic import BaseModel from pydantic_extra_types.mac_address import MacAddress class Network ( BaseModel ): mac_address : MacAddress network = Network ( mac_address = \"00:00:5e:00:53:01\" ) print ( network ) #> mac_address='00:00:5e:00:53:01' validate_mac_address staticmethod ¶ validate_mac_address(value: ) -> Validate a MAC Address from the provided byte value.","pageID":"Mac Address","abs_url":"/latest/api/pydantic_extra_types_mac_address/#pydantic_extra_types.mac_address.MacAddress","title":"Mac Address - MacAddress","objectID":"/latest/api/pydantic_extra_types_mac_address/#pydantic_extra_types.mac_address.MacAddress","rank":95},{"content":"validate_mac_address(value: ) -> Validate a MAC Address from the provided byte value.","pageID":"Mac Address","abs_url":"/latest/api/pydantic_extra_types_mac_address/#pydantic_extra_types.mac_address.MacAddress.validate_mac_address","title":"Mac Address - MacAddress - validate_mac_address  staticmethod","objectID":"/latest/api/pydantic_extra_types_mac_address/#pydantic_extra_types.mac_address.MacAddress.validate_mac_address","rank":90},{"content":"The pydantic_extra_types.payment module provides the\n data type. PaymentCardBrand ¶ Bases: , Payment card brands supported by the . PaymentCardNumber ¶ PaymentCardNumber(card_number: ) Bases: A payment card number . strip_whitespace class-attribute ¶ strip_whitespace:  = True Whether to strip whitespace from the input value. min_length class-attribute ¶ min_length:  = 12 The minimum length of the card number. max_length class-attribute ¶ max_length:  = 19 The maximum length of the card number. bin instance-attribute ¶ bin:  = [:6] The first 6 digits of the card number. last4 instance-attribute ¶ last4:  = [-4:] The last 4 digits of the card number. brand instance-attribute ¶ brand:  = () The brand of the card. masked property ¶ masked: The masked card number. validate classmethod ¶ validate(\n    __input_value: , _: \n) -> Validate the PaymentCardNumber instance. Parameters: Name Type Description Default __input_value The input value to validate. required _ The validation info. required Returns: Type Description The validated PaymentCardNumber instance. validate_digits classmethod ¶ validate_digits(card_number: ) -> None Validate that the card number is all digits. Parameters: Name Type Description Default card_number The card number to validate. required Raises: Type Description If the card number is not all digits. validate_luhn_check_digit classmethod ¶ validate_luhn_check_digit(card_number: ) -> Validate the payment card number.\nBased on the Luhn algorithm . Parameters: Name Type Description Default card_number The card number to validate. required Returns: Type Description The validated card number. Raises: Type Description If the card number is not valid. validate_brand staticmethod ¶ validate_brand(card_number: ) -> Validate length based on BIN for major brands. Parameters: Name Type Description Default card_number The card number to validate. required Returns: Type Description The validated card brand. Raises: Type Description If the card number is not valid.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#Payment","title":"Payment","objectID":"/latest/api/pydantic_extra_types_payment/#Payment","rank":100},{"content":"Bases: , Payment card brands supported by the .","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardBrand","title":"Payment - PaymentCardBrand","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardBrand","rank":95},{"content":"PaymentCardNumber(card_number: ) Bases: A payment card number . strip_whitespace class-attribute ¶ strip_whitespace:  = True Whether to strip whitespace from the input value. min_length class-attribute ¶ min_length:  = 12 The minimum length of the card number. max_length class-attribute ¶ max_length:  = 19 The maximum length of the card number. bin instance-attribute ¶ bin:  = [:6] The first 6 digits of the card number. last4 instance-attribute ¶ last4:  = [-4:] The last 4 digits of the card number. brand instance-attribute ¶ brand:  = () The brand of the card. masked property ¶ masked: The masked card number. validate classmethod ¶ validate(\n    __input_value: , _: \n) -> Validate the PaymentCardNumber instance. Parameters: Name Type Description Default __input_value The input value to validate. required _ The validation info. required Returns: Type Description The validated PaymentCardNumber instance. validate_digits classmethod ¶ validate_digits(card_number: ) -> None Validate that the card number is all digits. Parameters: Name Type Description Default card_number The card number to validate. required Raises: Type Description If the card number is not all digits. validate_luhn_check_digit classmethod ¶ validate_luhn_check_digit(card_number: ) -> Validate the payment card number.\nBased on the Luhn algorithm . Parameters: Name Type Description Default card_number The card number to validate. required Returns: Type Description The validated card number. Raises: Type Description If the card number is not valid. validate_brand staticmethod ¶ validate_brand(card_number: ) -> Validate length based on BIN for major brands. Parameters: Name Type Description Default card_number The card number to validate. required Returns: Type Description The validated card brand. Raises: Type Description If the card number is not valid.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber","title":"Payment - PaymentCardNumber","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber","rank":90},{"content":"strip_whitespace:  = True Whether to strip whitespace from the input value.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.strip_whitespace","title":"Payment - PaymentCardNumber - strip_whitespace  class-attribute","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.strip_whitespace","rank":85},{"content":"min_length:  = 12 The minimum length of the card number.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.min_length","title":"Payment - PaymentCardNumber - min_length  class-attribute","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.min_length","rank":80},{"content":"max_length:  = 19 The maximum length of the card number.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.max_length","title":"Payment - PaymentCardNumber - max_length  class-attribute","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.max_length","rank":75},{"content":"bin:  = [:6] The first 6 digits of the card number.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.bin","title":"Payment - PaymentCardNumber - bin  instance-attribute","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.bin","rank":70},{"content":"last4:  = [-4:] The last 4 digits of the card number.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.last4","title":"Payment - PaymentCardNumber - last4  instance-attribute","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.last4","rank":65},{"content":"brand:  = () The brand of the card.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.brand","title":"Payment - PaymentCardNumber - brand  instance-attribute","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.brand","rank":60},{"content":"masked: The masked card number.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.masked","title":"Payment - PaymentCardNumber - masked  property","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.masked","rank":55},{"content":"validate(\n    __input_value: , _: \n) -> Validate the PaymentCardNumber instance. Parameters: Name Type Description Default __input_value The input value to validate. required _ The validation info. required Returns: Type Description The validated PaymentCardNumber instance.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate","title":"Payment - PaymentCardNumber - validate  classmethod","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate","rank":50},{"content":"validate_digits(card_number: ) -> None Validate that the card number is all digits. Parameters: Name Type Description Default card_number The card number to validate. required Raises: Type Description If the card number is not all digits.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate_digits","title":"Payment - PaymentCardNumber - validate_digits  classmethod","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate_digits","rank":45},{"content":"validate_luhn_check_digit(card_number: ) -> Validate the payment card number.\nBased on the Luhn algorithm . Parameters: Name Type Description Default card_number The card number to validate. required Returns: Type Description The validated card number. Raises: Type Description If the card number is not valid.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate_luhn_check_digit","title":"Payment - PaymentCardNumber - validate_luhn_check_digit  classmethod","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate_luhn_check_digit","rank":40},{"content":"validate_brand(card_number: ) -> Validate length based on BIN for major brands. Parameters: Name Type Description Default card_number The card number to validate. required Returns: Type Description The validated card brand. Raises: Type Description If the card number is not valid.","pageID":"Payment","abs_url":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate_brand","title":"Payment - PaymentCardNumber - validate_brand  staticmethod","objectID":"/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.validate_brand","rank":35},{"content":"Native Pendulum DateTime object implementation. This is a copy of the Pendulum DateTime object, but with a Pydantic\nCoreSchema implementation. This allows Pydantic to validate the DateTime object. DateTime ¶ Bases: A pendulum.DateTime object. At runtime, this type decomposes into pendulum.DateTime automatically.\nThis type exists because Pydantic throws a fit on unknown types. from pydantic import BaseModel\nfrom pydantic_extra_types.pendulum_dt import DateTime\n\nclass test_model(BaseModel):\n    dt: DateTime\n\nprint(test_model(dt='2021-01-01T00:00:00+00:00'))\n\n#> test_model(dt=DateTime(2021, 1, 1, 0, 0, 0, tzinfo=FixedTimezone(0, name=\"+00:00\"))) Date ¶ Bases: A pendulum.Date object. At runtime, this type decomposes into pendulum.Date automatically.\nThis type exists because Pydantic throws a fit on unknown types. from pydantic import BaseModel\nfrom pydantic_extra_types.pendulum_dt import Date\n\nclass test_model(BaseModel):\n    dt: Date\n\nprint(test_model(dt='2021-01-01'))\n\n#> test_model(dt=Date(2021, 1, 1)) Duration ¶ Bases: A pendulum.Duration object. At runtime, this type decomposes into pendulum.Duration automatically.\nThis type exists because Pydantic throws a fit on unknown types. from pydantic import BaseModel\nfrom pydantic_extra_types.pendulum_dt import Duration\n\nclass test_model(BaseModel):\n    delta_t: Duration\n\nprint(test_model(delta_t='P1DT25H'))\n\n#> test_model(delta_t=Duration(days=2, hours=1))","pageID":"Pendulum","abs_url":"/latest/api/pydantic_extra_types_pendulum_dt/#Pendulum","title":"Pendulum","objectID":"/latest/api/pydantic_extra_types_pendulum_dt/#Pendulum","rank":100},{"content":"Bases: A pendulum.DateTime object. At runtime, this type decomposes into pendulum.DateTime automatically.\nThis type exists because Pydantic throws a fit on unknown types. from pydantic import BaseModel\nfrom pydantic_extra_types.pendulum_dt import DateTime\n\nclass test_model(BaseModel):\n    dt: DateTime\n\nprint(test_model(dt='2021-01-01T00:00:00+00:00'))\n\n#> test_model(dt=DateTime(2021, 1, 1, 0, 0, 0, tzinfo=FixedTimezone(0, name=\"+00:00\")))","pageID":"Pendulum","abs_url":"/latest/api/pydantic_extra_types_pendulum_dt/#pydantic_extra_types.pendulum_dt.DateTime","title":"Pendulum - DateTime","objectID":"/latest/api/pydantic_extra_types_pendulum_dt/#pydantic_extra_types.pendulum_dt.DateTime","rank":95},{"content":"Bases: A pendulum.Date object. At runtime, this type decomposes into pendulum.Date automatically.\nThis type exists because Pydantic throws a fit on unknown types. from pydantic import BaseModel\nfrom pydantic_extra_types.pendulum_dt import Date\n\nclass test_model(BaseModel):\n    dt: Date\n\nprint(test_model(dt='2021-01-01'))\n\n#> test_model(dt=Date(2021, 1, 1))","pageID":"Pendulum","abs_url":"/latest/api/pydantic_extra_types_pendulum_dt/#pydantic_extra_types.pendulum_dt.Date","title":"Pendulum - Date","objectID":"/latest/api/pydantic_extra_types_pendulum_dt/#pydantic_extra_types.pendulum_dt.Date","rank":90},{"content":"Bases: A pendulum.Duration object. At runtime, this type decomposes into pendulum.Duration automatically.\nThis type exists because Pydantic throws a fit on unknown types. from pydantic import BaseModel\nfrom pydantic_extra_types.pendulum_dt import Duration\n\nclass test_model(BaseModel):\n    delta_t: Duration\n\nprint(test_model(delta_t='P1DT25H'))\n\n#> test_model(delta_t=Duration(days=2, hours=1))","pageID":"Pendulum","abs_url":"/latest/api/pydantic_extra_types_pendulum_dt/#pydantic_extra_types.pendulum_dt.Duration","title":"Pendulum - Duration","objectID":"/latest/api/pydantic_extra_types_pendulum_dt/#pydantic_extra_types.pendulum_dt.Duration","rank":85},{"content":"The pydantic_extra_types.phone_numbers module provides the\n data type. This class depends on the [phonenumbers] package, which is a Python port of Google's [libphonenumber]. PhoneNumber ¶ Bases: A wrapper around phonenumbers package, which\nis a Python port of Google's libphonenumber . supported_regions class-attribute instance-attribute ¶ supported_regions: [] = [] The supported regions. If empty, all regions are supported. default_region_code class-attribute ¶ default_region_code:  | None = None The default region code to use when parsing phone numbers without an international prefix. phone_format class-attribute instance-attribute ¶ phone_format:  = 'RFC3966' The format of the phone number. PhoneNumberValidator dataclass ¶ PhoneNumberValidator(\n    default_region: [] = None,\n    number_format:  = \"RFC3966\",\n    supported_regions: [[]] = None,\n) A pydantic before validator for phone numbers using the phonenumbers package,\na Python port of Google's libphonenumber . Intended to be used to create custom pydantic data types using the typing.Annotated type construct. Parameters: Name Type Description Default default_region | None The default region code to use when parsing phone numbers without an international prefix.\nIf None (default), the region must be supplied in the phone number as an international prefix. None number_format The format of the phone number to return. See phonenumbers.PhoneNumberFormat for valid values. 'RFC3966' supported_regions [] The supported regions. If empty, all regions are supported (default). None Returns:\n    str: The formatted phone number.","pageID":"Phone Numbers","abs_url":"/latest/api/pydantic_extra_types_phone_numbers/#Phone Numbers","title":"Phone Numbers","objectID":"/latest/api/pydantic_extra_types_phone_numbers/#Phone Numbers","rank":100},{"content":"Bases: A wrapper around phonenumbers package, which\nis a Python port of Google's libphonenumber . supported_regions class-attribute instance-attribute ¶ supported_regions: [] = [] The supported regions. If empty, all regions are supported. default_region_code class-attribute ¶ default_region_code:  | None = None The default region code to use when parsing phone numbers without an international prefix. phone_format class-attribute instance-attribute ¶ phone_format:  = 'RFC3966' The format of the phone number.","pageID":"Phone Numbers","abs_url":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber","title":"Phone Numbers - PhoneNumber","objectID":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber","rank":95},{"content":"supported_regions: [] = [] The supported regions. If empty, all regions are supported.","pageID":"Phone Numbers","abs_url":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber.supported_regions","title":"Phone Numbers - PhoneNumber - supported_regions  class-attribute instance-attribute","objectID":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber.supported_regions","rank":90},{"content":"default_region_code:  | None = None The default region code to use when parsing phone numbers without an international prefix.","pageID":"Phone Numbers","abs_url":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber.default_region_code","title":"Phone Numbers - PhoneNumber - default_region_code  class-attribute","objectID":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber.default_region_code","rank":85},{"content":"phone_format:  = 'RFC3966' The format of the phone number.","pageID":"Phone Numbers","abs_url":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber.phone_format","title":"Phone Numbers - PhoneNumber - phone_format  class-attribute instance-attribute","objectID":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumber.phone_format","rank":80},{"content":"PhoneNumberValidator(\n    default_region: [] = None,\n    number_format:  = \"RFC3966\",\n    supported_regions: [[]] = None,\n) A pydantic before validator for phone numbers using the phonenumbers package,\na Python port of Google's libphonenumber . Intended to be used to create custom pydantic data types using the typing.Annotated type construct. Parameters: Name Type Description Default default_region | None The default region code to use when parsing phone numbers without an international prefix.\nIf None (default), the region must be supplied in the phone number as an international prefix. None number_format The format of the phone number to return. See phonenumbers.PhoneNumberFormat for valid values. 'RFC3966' supported_regions [] The supported regions. If empty, all regions are supported (default). None Returns:\n    str: The formatted phone number.","pageID":"Phone Numbers","abs_url":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumberValidator","title":"Phone Numbers - PhoneNumberValidator  dataclass","objectID":"/latest/api/pydantic_extra_types_phone_numbers/#pydantic_extra_types.phone_numbers.PhoneNumberValidator","rank":75},{"content":"The pydantic_extra_types.routing_number module provides the\n data type. ABARoutingNumber ¶ ABARoutingNumber(routing_number: ) Bases: The ABARoutingNumber data type is a string of 9 digits representing an ABA routing transit number. The algorithm used to validate the routing number is described in the ABA routing transit number Wikipedia article. from pydantic import BaseModel from pydantic_extra_types.routing_number import ABARoutingNumber class BankAccount ( BaseModel ): routing_number : ABARoutingNumber account = BankAccount ( routing_number = '122105155' ) print ( account ) #> routing_number='122105155'","pageID":"Routing Numbers","abs_url":"/latest/api/pydantic_extra_types_routing_numbers/#Routing Numbers","title":"Routing Numbers","objectID":"/latest/api/pydantic_extra_types_routing_numbers/#Routing Numbers","rank":100},{"content":"ABARoutingNumber(routing_number: ) Bases: The ABARoutingNumber data type is a string of 9 digits representing an ABA routing transit number. The algorithm used to validate the routing number is described in the ABA routing transit number Wikipedia article. from pydantic import BaseModel from pydantic_extra_types.routing_number import ABARoutingNumber class BankAccount ( BaseModel ): routing_number : ABARoutingNumber account = BankAccount ( routing_number = '122105155' ) print ( account ) #> routing_number='122105155'","pageID":"Routing Numbers","abs_url":"/latest/api/pydantic_extra_types_routing_numbers/#pydantic_extra_types.routing_number.ABARoutingNumber","title":"Routing Numbers - ABARoutingNumber","objectID":"/latest/api/pydantic_extra_types_routing_numbers/#pydantic_extra_types.routing_number.ABARoutingNumber","rank":95},{"content":"script definitions that are based on the ISO 15924 ISO_15924 ¶ Bases: ISO_15924 parses script in the ISO 15924 format. from pydantic import BaseModel from pydantic_extra_types.language_code import ISO_15924 class Script ( BaseModel ): alpha_4 : ISO_15924 script = Script ( alpha_4 = 'Java' ) print ( lang ) # > script='Java'","pageID":"Script Code","abs_url":"/latest/api/pydantic_extra_types_script_code/#Script Code","title":"Script Code","objectID":"/latest/api/pydantic_extra_types_script_code/#Script Code","rank":100},{"content":"Bases: ISO_15924 parses script in the ISO 15924 format. from pydantic import BaseModel from pydantic_extra_types.language_code import ISO_15924 class Script ( BaseModel ): alpha_4 : ISO_15924 script = Script ( alpha_4 = 'Java' ) print ( lang ) # > script='Java'","pageID":"Script Code","abs_url":"/latest/api/pydantic_extra_types_script_code/#pydantic_extra_types.script_code.ISO_15924","title":"Script Code - ISO_15924","objectID":"/latest/api/pydantic_extra_types_script_code/#pydantic_extra_types.script_code.ISO_15924","rank":95},{"content":"SemanticVersion definition that is based on the Semantiv Versioning Specification semver . SemanticVersion ¶ Semantic version based on the official semver thread .","pageID":"Semantic Version","abs_url":"/latest/api/pydantic_extra_types_semantic_version/#Semantic Version","title":"Semantic Version","objectID":"/latest/api/pydantic_extra_types_semantic_version/#Semantic Version","rank":100},{"content":"Semantic version based on the official semver thread .","pageID":"Semantic Version","abs_url":"/latest/api/pydantic_extra_types_semantic_version/#pydantic_extra_types.semantic_version.SemanticVersion","title":"Semantic Version - SemanticVersion","objectID":"/latest/api/pydantic_extra_types_semantic_version/#pydantic_extra_types.semantic_version.SemanticVersion","rank":95},{"content":"Time zone name validation and serialization module. TimeZoneName ¶ Bases: TimeZoneName is a custom string subclass for validating and serializing timezone names. The TimeZoneName class uses the IANA Time Zone Database for validation.\nIt supports both strict and non-strict modes for timezone name validation. Examples: ¶ Some examples of using the TimeZoneName class: Normal usage: ¶ from pydantic_extra_types.timezone_name import TimeZoneName\nfrom pydantic import BaseModel\nclass Location(BaseModel):\n    city: str\n    timezone: TimeZoneName\n\nloc = Location(city=\"New York\", timezone=\"America/New_York\")\nprint(loc.timezone)\n\n>> America/New_York Non-strict mode: ¶ from pydantic_extra_types.timezone_name import TimeZoneName, timezone_name_settings\n\n@timezone_name_settings(strict=False)\nclass TZNonStrict(TimeZoneName):\n    pass\n\ntz = TZNonStrict(\"america/new_york\")\n\nprint(tz)\n\n>> america/new_york get_timezones ¶ get_timezones() -> [] Determine the timezone provider and return available timezones.","pageID":"Timezone Name","abs_url":"/latest/api/pydantic_extra_types_timezone_name/#Timezone Name","title":"Timezone Name","objectID":"/latest/api/pydantic_extra_types_timezone_name/#Timezone Name","rank":100},{"content":"Bases: TimeZoneName is a custom string subclass for validating and serializing timezone names. The TimeZoneName class uses the IANA Time Zone Database for validation.\nIt supports both strict and non-strict modes for timezone name validation. Examples: ¶ Some examples of using the TimeZoneName class: Normal usage: ¶ from pydantic_extra_types.timezone_name import TimeZoneName\nfrom pydantic import BaseModel\nclass Location(BaseModel):\n    city: str\n    timezone: TimeZoneName\n\nloc = Location(city=\"New York\", timezone=\"America/New_York\")\nprint(loc.timezone)\n\n>> America/New_York Non-strict mode: ¶ from pydantic_extra_types.timezone_name import TimeZoneName, timezone_name_settings\n\n@timezone_name_settings(strict=False)\nclass TZNonStrict(TimeZoneName):\n    pass\n\ntz = TZNonStrict(\"america/new_york\")\n\nprint(tz)\n\n>> america/new_york","pageID":"Timezone Name","abs_url":"/latest/api/pydantic_extra_types_timezone_name/#pydantic_extra_types.timezone_name.TimeZoneName","title":"Timezone Name - TimeZoneName","objectID":"/latest/api/pydantic_extra_types_timezone_name/#pydantic_extra_types.timezone_name.TimeZoneName","rank":95},{"content":"get_timezones() -> [] Determine the timezone provider and return available timezones.","pageID":"Timezone Name","abs_url":"/latest/api/pydantic_extra_types_timezone_name/#pydantic_extra_types.timezone_name.get_timezones","title":"Timezone Name - get_timezones","objectID":"/latest/api/pydantic_extra_types_timezone_name/#pydantic_extra_types.timezone_name.get_timezones","rank":90},{"content":"The pydantic_extra_types.ULID module provides the [ ULID ] data type. This class depends on the [python-ulid] package, which is a validate by the ULID-spec . ULID dataclass ¶ ULID(ulid: ) Bases: A wrapper around python-ulid package, which\nis a validate by the ULID-spec .","pageID":"ULID","abs_url":"/latest/api/pydantic_extra_types_ulid/#ULID","title":"ULID","objectID":"/latest/api/pydantic_extra_types_ulid/#ULID","rank":100},{"content":"ULID(ulid: ) Bases: A wrapper around python-ulid package, which\nis a validate by the ULID-spec .","pageID":"ULID","abs_url":"/latest/api/pydantic_extra_types_ulid/#pydantic_extra_types.ulid.ULID","title":"ULID - ULID  dataclass","objectID":"/latest/api/pydantic_extra_types_ulid/#pydantic_extra_types.ulid.ULID","rank":95},{"content":"BaseSettings ¶ BaseSettings(\n    __pydantic_self__,\n    _case_sensitive:  | None = None,\n    _nested_model_default_partial_update: (\n         | None\n    ) = None,\n    _env_prefix:  | None = None,\n    _env_file:  | None = ,\n    _env_file_encoding:  | None = None,\n    _env_ignore_empty:  | None = None,\n    _env_nested_delimiter:  | None = None,\n    _env_parse_none_str:  | None = None,\n    _env_parse_enums:  | None = None,\n    _cli_prog_name:  | None = None,\n    _cli_parse_args: (\n         | [] | [, ...] | None\n    ) = None,\n    _cli_settings_source: (\n        [] | None\n    ) = None,\n    _cli_parse_none_str:  | None = None,\n    _cli_hide_none_type:  | None = None,\n    _cli_avoid_json:  | None = None,\n    _cli_enforce_required:  | None = None,\n    _cli_use_class_docs_for_groups:  | None = None,\n    _cli_exit_on_error:  | None = None,\n    _cli_prefix:  | None = None,\n    _cli_flag_prefix_char:  | None = None,\n    _cli_implicit_flags:  | None = None,\n    _cli_ignore_unknown_args:  | None = None,\n    _cli_kebab_case:  | None = None,\n    _secrets_dir:  | None = None,\n    **values: \n) Bases: Base class for settings, allowing values to be overridden by environment variables. This is useful in production for secrets you do not wish to save in code, it plays nicely with docker(-compose),\nHeroku and any 12 factor app design. All the below attributes can be set via model_config . Parameters: Name Type Description Default _case_sensitive | None Whether environment and CLI variable names should be read with case-sensitivity.\nDefaults to None . None _nested_model_default_partial_update | None Whether to allow partial updates on nested model default object fields.\nDefaults to False . None _env_prefix | None Prefix for all environment variables. Defaults to None . None _env_file | None The env file(s) to load settings values from. Defaults to Path('') , which\nmeans that the value from model_config['env_file'] should be used. You can also pass None to indicate that environment variables should not be loaded from an env file. _env_file_encoding | None The env file encoding, e.g. 'latin-1' . Defaults to None . None _env_ignore_empty | None Ignore environment variables where the value is an empty string. Default to False . None _env_nested_delimiter | None The nested env values delimiter. Defaults to None . None _env_parse_none_str | None The env string value that should be parsed (e.g. \"null\", \"void\", \"None\", etc.)\ninto None type(None). Defaults to None type(None), which means no parsing should occur. None _env_parse_enums | None Parse enum field names to values. Defaults to None. , which means no parsing should occur. None _cli_prog_name | None The CLI program name to display in help text. Defaults to None if _cli_parse_args is None .\nOtherwse, defaults to sys.argv[0]. None _cli_parse_args | [] | [, ...] | None The list of CLI arguments to parse. Defaults to None.\nIf set to True , defaults to sys.argv[1:]. None _cli_settings_source [] | None Override the default CLI settings source with a user defined instance. Defaults to None. None _cli_parse_none_str | None The CLI string value that should be parsed (e.g. \"null\", \"void\", \"None\", etc.) into None type(None). Defaults to _env_parse_none_str value if set. Otherwise, defaults to \"null\" if\n_cli_avoid_json is False , and \"None\" if _cli_avoid_json is True . None _cli_hide_none_type | None Hide None values in CLI help text. Defaults to False . None _cli_avoid_json | None Avoid complex JSON objects in CLI help text. Defaults to False . None _cli_enforce_required | None Enforce required fields at the CLI. Defaults to False . None _cli_use_class_docs_for_groups | None Use class docstrings in CLI group help text instead of field descriptions.\nDefaults to False . None _cli_exit_on_error | None Determines whether or not the internal parser exits with error info when an error occurs.\nDefaults to True . None _cli_prefix | None The root parser command line arguments prefix. Defaults to \"\". None _cli_flag_prefix_char | None The flag prefix character to use for CLI optional arguments. Defaults to '-'. None _cli_implicit_flags | None Whether bool fields should be implicitly converted into CLI boolean flags.\n(e.g. --flag, --no-flag). Defaults to False . None _cli_ignore_unknown_args | None Whether to ignore unknown CLI args and parse only known ones. Defaults to False . None _cli_kebab_case | None CLI args use kebab case. Defaults to False . None _secrets_dir | None The secret files directory or a sequence of directories. Defaults to None . None settings_customise_sources classmethod ¶ settings_customise_sources(\n    settings_cls: [],\n    init_settings: ,\n    env_settings: ,\n    dotenv_settings: ,\n    file_secret_settings: ,\n) -> [, ...] Define the sources and their order for loading the settings values. Parameters: Name Type Description Default settings_cls [] The Settings class. required init_settings The InitSettingsSource instance. required env_settings The EnvSettingsSource instance. required dotenv_settings The DotEnvSettingsSource instance. required file_secret_settings The SecretsSettingsSource instance. required Returns: Type Description [, ...] A tuple containing the sources and their order for loading the settings values. CliApp ¶ A utility class for running Pydantic BaseSettings , BaseModel , or pydantic.dataclasses.dataclass as\nCLI applications. run staticmethod ¶ run(\n    model_cls: [],\n    cli_args: (\n        []\n        | \n        | \n        | [, ]\n        | None\n    ) = None,\n    cli_settings_source: (\n        [] | None\n    ) = None,\n    cli_exit_on_error:  | None = None,\n    cli_cmd_method_name:  = \"cli_cmd\",\n    **model_init_data: \n) -> Runs a Pydantic BaseSettings , BaseModel , or pydantic.dataclasses.dataclass as a CLI application.\nRunning a model as a CLI application requires the cli_cmd method to be defined in the model class. Parameters: Name Type Description Default model_cls [] The model class to run as a CLI application. required cli_args [] |  |  | [, ] | None The list of CLI arguments to parse. If cli_settings_source is specified, this may\nalso be a namespace or dictionary of pre-parsed CLI arguments. Defaults to sys.argv[1:] . None cli_settings_source [] | None Override the default CLI settings source with a user defined instance.\nDefaults to None . None cli_exit_on_error | None Determines whether this function exits on error. If model is subclass of BaseSettings , defaults to BaseSettings cli_exit_on_error value. Otherwise, defaults to True . None cli_cmd_method_name The CLI command method name to run. Defaults to \"cli_cmd\". 'cli_cmd' model_init_data The model init data. {} Returns: Type Description The ran instance of model. Raises: Type Description If model_cls is not subclass of BaseModel or pydantic.dataclasses.dataclass . If model_cls does not have a cli_cmd entrypoint defined. run_subcommand staticmethod ¶ run_subcommand(\n    model: ,\n    cli_exit_on_error:  | None = None,\n    cli_cmd_method_name:  = \"cli_cmd\",\n) -> Runs the model subcommand. Running a model subcommand requires the cli_cmd method to be defined in\nthe nested model subcommand class. Parameters: Name Type Description Default model The model to run the subcommand from. required cli_exit_on_error | None Determines whether this function exits with error if no subcommand is found.\nDefaults to model_config cli_exit_on_error value if set. Otherwise, defaults to True . None cli_cmd_method_name The CLI command method name to run. Defaults to \"cli_cmd\". 'cli_cmd' Returns: Type Description The ran subcommand model. Raises: Type Description When no subcommand is found and cli_exit_on_error= True (the default). When no subcommand is found and cli_exit_on_error= False . SettingsConfigDict ¶ Bases: pyproject_toml_depth instance-attribute ¶ pyproject_toml_depth: Number of levels up from the current working directory to attempt to find a pyproject.toml\nfile. This is only used when a pyproject.toml file is not found in the current working directory. pyproject_toml_table_header instance-attribute ¶ pyproject_toml_table_header: [, ...] Header of the TOML table within a pyproject.toml file to use when filling variables.\nThis is supplied as a tuple[str, ...] instead of a str to accommodate for headers\ncontaining a . . For example, toml_table_header = (\"tool\", \"my.tool\", \"foo\") can be used to fill variable\nvalues from a table with header [tool.\"my.tool\".foo] . To use the root table, exclude this config setting or provide an empty tuple. CliSettingsSource ¶ CliSettingsSource(\n    settings_cls: [],\n    cli_prog_name:  | None = None,\n    cli_parse_args: (\n         | [] | [, ...] | None\n    ) = None,\n    cli_parse_none_str:  | None = None,\n    cli_hide_none_type:  | None = None,\n    cli_avoid_json:  | None = None,\n    cli_enforce_required:  | None = None,\n    cli_use_class_docs_for_groups:  | None = None,\n    cli_exit_on_error:  | None = None,\n    cli_prefix:  | None = None,\n    cli_flag_prefix_char:  | None = None,\n    cli_implicit_flags:  | None = None,\n    cli_ignore_unknown_args:  | None = None,\n    cli_kebab_case:  | None = None,\n    case_sensitive:  | None = True,\n    root_parser:  = None,\n    parse_args_method: [..., ] | None = None,\n    add_argument_method: (\n        [..., ] | None\n    ) = ,\n    add_argument_group_method: (\n        [..., ] | None\n    ) = ,\n    add_parser_method: (\n        [..., ] | None\n    ) = ,\n    add_subparsers_method: (\n        [..., ] | None\n    ) = ,\n    formatter_class:  = ,\n) Bases: , [] Source class for loading settings values from CLI. Parameters: Name Type Description Default cli_prog_name | None The CLI program name to display in help text. Defaults to None if cli_parse_args is None .\nOtherwse, defaults to sys.argv[0]. None cli_parse_args | [] | [, ...] | None The list of CLI arguments to parse. Defaults to None.\nIf set to True , defaults to sys.argv[1:]. None cli_parse_none_str | None The CLI string value that should be parsed (e.g. \"null\", \"void\", \"None\", etc.) into None type(None). Defaults to \"null\" if cli_avoid_json is False , and \"None\" if cli_avoid_json is True . None cli_hide_none_type | None Hide None values in CLI help text. Defaults to False . None cli_avoid_json | None Avoid complex JSON objects in CLI help text. Defaults to False . None cli_enforce_required | None Enforce required fields at the CLI. Defaults to False . None cli_use_class_docs_for_groups | None Use class docstrings in CLI group help text instead of field descriptions.\nDefaults to False . None cli_exit_on_error | None Determines whether or not the internal parser exits with error info when an error occurs.\nDefaults to True . None cli_prefix | None Prefix for command line arguments added under the root parser. Defaults to \"\". None cli_flag_prefix_char | None The flag prefix character to use for CLI optional arguments. Defaults to '-'. None cli_implicit_flags | None Whether bool fields should be implicitly converted into CLI boolean flags.\n(e.g. --flag, --no-flag). Defaults to False . None cli_ignore_unknown_args | None Whether to ignore unknown CLI args and parse only known ones. Defaults to False . None cli_kebab_case | None CLI args use kebab case. Defaults to False . None case_sensitive | None Whether CLI \"--arg\" names should be read with case-sensitivity. Defaults to True .\nNote: Case-insensitive matching is only supported on the internal root parser and does not apply to CLI\nsubcommands. True root_parser The root parser object. None parse_args_method [..., ] | None The root parser parse args method. Defaults to argparse.ArgumentParser.parse_args . None add_argument_method [..., ] | None The root parser add argument method. Defaults to argparse.ArgumentParser.add_argument . add_argument_group_method [..., ] | None The root parser add argument group method.\nDefaults to argparse.ArgumentParser.add_argument_group . add_parser_method [..., ] | None The root parser add new parser (sub-command) method.\nDefaults to argparse._SubParsersAction.add_parser . add_subparsers_method [..., ] | None The root parser add subparsers (sub-commands) method.\nDefaults to argparse.ArgumentParser.add_subparsers . formatter_class A class for customizing the root parser help text. Defaults to argparse.RawDescriptionHelpFormatter . root_parser property ¶ root_parser: The connected root parser instance. DotEnvSettingsSource ¶ DotEnvSettingsSource(\n    settings_cls: [],\n    env_file:  | None = ,\n    env_file_encoding:  | None = None,\n    case_sensitive:  | None = None,\n    env_prefix:  | None = None,\n    env_nested_delimiter:  | None = None,\n    env_ignore_empty:  | None = None,\n    env_parse_none_str:  | None = None,\n    env_parse_enums:  | None = None,\n) Bases: Source class for loading settings values from env files. EnvSettingsSource ¶ EnvSettingsSource(\n    settings_cls: [],\n    case_sensitive:  | None = None,\n    env_prefix:  | None = None,\n    env_nested_delimiter:  | None = None,\n    env_ignore_empty:  | None = None,\n    env_parse_none_str:  | None = None,\n    env_parse_enums:  | None = None,\n) Bases: Source class for loading settings values from environment variables. get_field_value ¶ get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value for field from environment variables and a flag to determine whether value is complex. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value ( None if not found), key, and\na flag to determine whether value is complex. prepare_field_value ¶ prepare_field_value(\n    field_name: ,\n    field: ,\n    value: ,\n    value_is_complex: ,\n) -> Prepare value for the field. Extract value for nested field. Deserialize value to python object for complex field. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description A tuple contains prepared value for the field. Raises: Type Description When There is an error in deserializing value for complex field. next_field ¶ next_field(\n    field:  |  | None,\n    key: ,\n    case_sensitive:  | None = None,\n) ->  | None Find the field in a sub model by key(env name) By having the following models: ```py\nclass SubSubModel(BaseSettings):\n    dvals: Dict\n\nclass SubModel(BaseSettings):\n    vals: list[str]\n    sub_sub_model: SubSubModel\n\nclass Cfg(BaseSettings):\n    sub_model: SubModel\n``` Parameters: Name Type Description Default field |  | None The field. required key The key (env name). required case_sensitive | None Whether to search for key case sensitively. None Returns: Type Description | None Field if it finds the next field otherwise None . explode_env_vars ¶ explode_env_vars(\n    field_name: ,\n    field: ,\n    env_vars: [,  | None],\n) -> [, ] Process env_vars and extract the values of keys containing env_nested_delimiter into nested dictionaries. This is applied to a single field, hence filtering by env_var prefix. Parameters: Name Type Description Default field_name The field name. required field The field. required env_vars [,  | None] Environment variables. required Returns: Type Description [, ] A dictionary contains extracted values from nested env values. ForceDecode ¶ Annotation to force decoding of a field value. InitSettingsSource ¶ InitSettingsSource(\n    settings_cls: [],\n    init_kwargs: [, ],\n    nested_model_default_partial_update:  | None = None,\n) Bases: Source class for loading values provided during settings class initialization. JsonConfigSettingsSource ¶ JsonConfigSettingsSource(\n    settings_cls: [],\n    json_file:  | None = ,\n    json_file_encoding:  | None = None,\n) Bases: , A source class that loads variables from a JSON file NoDecode ¶ Annotation to prevent decoding of a field value. PydanticBaseSettingsSource ¶ PydanticBaseSettingsSource(\n    settings_cls: [],\n) Bases: Abstract base class for settings sources, every settings source classes should inherit from it. current_state property ¶ current_state: [, ] The current state of the settings, populated by the previous settings sources. settings_sources_data property ¶ settings_sources_data: [, [, ]] The state of all previous settings sources. get_field_value abstractmethod ¶ get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value, the key for model creation, and a flag to determine whether value is complex. This is an abstract method that should be overridden in every settings source classes. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value, key and a flag to determine whether value is complex. field_is_complex ¶ field_is_complex(field: ) -> Checks whether a field is complex, in which case it will attempt to be parsed as JSON. Parameters: Name Type Description Default field The field. required Returns: Type Description Whether the field is complex. prepare_field_value ¶ prepare_field_value(\n    field_name: ,\n    field: ,\n    value: ,\n    value_is_complex: ,\n) -> Prepares the value of a field. Parameters: Name Type Description Default field_name The field name. required field The field. required value The value of the field that has to be prepared. required value_is_complex A flag to determine whether value is complex. required Returns: Type Description The prepared value. decode_complex_value ¶ decode_complex_value(\n    field_name: , field: , value: \n) -> Decode the value for a complex field Parameters: Name Type Description Default field_name The field name. required field The field. required value The value of the field that has to be prepared. required Returns: Type Description The decoded value for further preparation PyprojectTomlConfigSettingsSource ¶ PyprojectTomlConfigSettingsSource(\n    settings_cls: [],\n    toml_file:  | None = None,\n) Bases: A source class that loads variables from a pyproject.toml file. SecretsSettingsSource ¶ SecretsSettingsSource(\n    settings_cls: [],\n    secrets_dir:  | None = None,\n    case_sensitive:  | None = None,\n    env_prefix:  | None = None,\n    env_ignore_empty:  | None = None,\n    env_parse_none_str:  | None = None,\n    env_parse_enums:  | None = None,\n) Bases: Source class for loading settings values from secret files. find_case_path classmethod ¶ find_case_path(\n    dir_path: , file_name: , case_sensitive: \n) ->  | None Find a file within path's directory matching filename, optionally ignoring case. Parameters: Name Type Description Default dir_path Directory path. required file_name File name. required case_sensitive Whether to search for file name case sensitively. required Returns: Type Description | None Whether file path or None if file does not exist in directory. get_field_value ¶ get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value for field from secret file and a flag to determine whether value is complex. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value ( None if the file does not exist), key, and\na flag to determine whether value is complex. TomlConfigSettingsSource ¶ TomlConfigSettingsSource(\n    settings_cls: [],\n    toml_file:  | None = ,\n) Bases: , A source class that loads variables from a TOML file YamlConfigSettingsSource ¶ YamlConfigSettingsSource(\n    settings_cls: [],\n    yaml_file:  | None = ,\n    yaml_file_encoding:  | None = None,\n) Bases: , A source class that loads variables from a yaml file get_subcommand ¶ get_subcommand(\n    model: ,\n    is_required:  = True,\n    cli_exit_on_error:  | None = None,\n) -> [] Get the subcommand from a model. Parameters: Name Type Description Default model The model to get the subcommand from. required is_required Determines whether a model must have subcommand set and raises error if not\nfound. Defaults to True . True cli_exit_on_error | None Determines whether this function exits with error if no subcommand is found.\nDefaults to model_config cli_exit_on_error value if set. Otherwise, defaults to True . None Returns: Type Description [] The subcommand model if found, otherwise None . Raises: Type Description When no subcommand is found and is_required= True and cli_exit_on_error= True (the default). When no subcommand is found and is_required= True and\ncli_exit_on_error= False .","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#Pydantic Settings","title":"Pydantic Settings","objectID":"/latest/api/pydantic_settings/#Pydantic Settings","rank":100},{"content":"BaseSettings(\n    __pydantic_self__,\n    _case_sensitive:  | None = None,\n    _nested_model_default_partial_update: (\n         | None\n    ) = None,\n    _env_prefix:  | None = None,\n    _env_file:  | None = ,\n    _env_file_encoding:  | None = None,\n    _env_ignore_empty:  | None = None,\n    _env_nested_delimiter:  | None = None,\n    _env_parse_none_str:  | None = None,\n    _env_parse_enums:  | None = None,\n    _cli_prog_name:  | None = None,\n    _cli_parse_args: (\n         | [] | [, ...] | None\n    ) = None,\n    _cli_settings_source: (\n        [] | None\n    ) = None,\n    _cli_parse_none_str:  | None = None,\n    _cli_hide_none_type:  | None = None,\n    _cli_avoid_json:  | None = None,\n    _cli_enforce_required:  | None = None,\n    _cli_use_class_docs_for_groups:  | None = None,\n    _cli_exit_on_error:  | None = None,\n    _cli_prefix:  | None = None,\n    _cli_flag_prefix_char:  | None = None,\n    _cli_implicit_flags:  | None = None,\n    _cli_ignore_unknown_args:  | None = None,\n    _cli_kebab_case:  | None = None,\n    _secrets_dir:  | None = None,\n    **values: \n) Bases: Base class for settings, allowing values to be overridden by environment variables. This is useful in production for secrets you do not wish to save in code, it plays nicely with docker(-compose),\nHeroku and any 12 factor app design. All the below attributes can be set via model_config . Parameters: Name Type Description Default _case_sensitive | None Whether environment and CLI variable names should be read with case-sensitivity.\nDefaults to None . None _nested_model_default_partial_update | None Whether to allow partial updates on nested model default object fields.\nDefaults to False . None _env_prefix | None Prefix for all environment variables. Defaults to None . None _env_file | None The env file(s) to load settings values from. Defaults to Path('') , which\nmeans that the value from model_config['env_file'] should be used. You can also pass None to indicate that environment variables should not be loaded from an env file. _env_file_encoding | None The env file encoding, e.g. 'latin-1' . Defaults to None . None _env_ignore_empty | None Ignore environment variables where the value is an empty string. Default to False . None _env_nested_delimiter | None The nested env values delimiter. Defaults to None . None _env_parse_none_str | None The env string value that should be parsed (e.g. \"null\", \"void\", \"None\", etc.)\ninto None type(None). Defaults to None type(None), which means no parsing should occur. None _env_parse_enums | None Parse enum field names to values. Defaults to None. , which means no parsing should occur. None _cli_prog_name | None The CLI program name to display in help text. Defaults to None if _cli_parse_args is None .\nOtherwse, defaults to sys.argv[0]. None _cli_parse_args | [] | [, ...] | None The list of CLI arguments to parse. Defaults to None.\nIf set to True , defaults to sys.argv[1:]. None _cli_settings_source [] | None Override the default CLI settings source with a user defined instance. Defaults to None. None _cli_parse_none_str | None The CLI string value that should be parsed (e.g. \"null\", \"void\", \"None\", etc.) into None type(None). Defaults to _env_parse_none_str value if set. Otherwise, defaults to \"null\" if\n_cli_avoid_json is False , and \"None\" if _cli_avoid_json is True . None _cli_hide_none_type | None Hide None values in CLI help text. Defaults to False . None _cli_avoid_json | None Avoid complex JSON objects in CLI help text. Defaults to False . None _cli_enforce_required | None Enforce required fields at the CLI. Defaults to False . None _cli_use_class_docs_for_groups | None Use class docstrings in CLI group help text instead of field descriptions.\nDefaults to False . None _cli_exit_on_error | None Determines whether or not the internal parser exits with error info when an error occurs.\nDefaults to True . None _cli_prefix | None The root parser command line arguments prefix. Defaults to \"\". None _cli_flag_prefix_char | None The flag prefix character to use for CLI optional arguments. Defaults to '-'. None _cli_implicit_flags | None Whether bool fields should be implicitly converted into CLI boolean flags.\n(e.g. --flag, --no-flag). Defaults to False . None _cli_ignore_unknown_args | None Whether to ignore unknown CLI args and parse only known ones. Defaults to False . None _cli_kebab_case | None CLI args use kebab case. Defaults to False . None _secrets_dir | None The secret files directory or a sequence of directories. Defaults to None . None settings_customise_sources classmethod ¶ settings_customise_sources(\n    settings_cls: [],\n    init_settings: ,\n    env_settings: ,\n    dotenv_settings: ,\n    file_secret_settings: ,\n) -> [, ...] Define the sources and their order for loading the settings values. Parameters: Name Type Description Default settings_cls [] The Settings class. required init_settings The InitSettingsSource instance. required env_settings The EnvSettingsSource instance. required dotenv_settings The DotEnvSettingsSource instance. required file_secret_settings The SecretsSettingsSource instance. required Returns: Type Description [, ...] A tuple containing the sources and their order for loading the settings values.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.BaseSettings","title":"Pydantic Settings - BaseSettings","objectID":"/latest/api/pydantic_settings/#pydantic_settings.BaseSettings","rank":95},{"content":"settings_customise_sources(\n    settings_cls: [],\n    init_settings: ,\n    env_settings: ,\n    dotenv_settings: ,\n    file_secret_settings: ,\n) -> [, ...] Define the sources and their order for loading the settings values. Parameters: Name Type Description Default settings_cls [] The Settings class. required init_settings The InitSettingsSource instance. required env_settings The EnvSettingsSource instance. required dotenv_settings The DotEnvSettingsSource instance. required file_secret_settings The SecretsSettingsSource instance. required Returns: Type Description [, ...] A tuple containing the sources and their order for loading the settings values.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.BaseSettings.settings_customise_sources","title":"Pydantic Settings - BaseSettings - settings_customise_sources  classmethod","objectID":"/latest/api/pydantic_settings/#pydantic_settings.BaseSettings.settings_customise_sources","rank":90},{"content":"A utility class for running Pydantic BaseSettings , BaseModel , or pydantic.dataclasses.dataclass as\nCLI applications. run staticmethod ¶ run(\n    model_cls: [],\n    cli_args: (\n        []\n        | \n        | \n        | [, ]\n        | None\n    ) = None,\n    cli_settings_source: (\n        [] | None\n    ) = None,\n    cli_exit_on_error:  | None = None,\n    cli_cmd_method_name:  = \"cli_cmd\",\n    **model_init_data: \n) -> Runs a Pydantic BaseSettings , BaseModel , or pydantic.dataclasses.dataclass as a CLI application.\nRunning a model as a CLI application requires the cli_cmd method to be defined in the model class. Parameters: Name Type Description Default model_cls [] The model class to run as a CLI application. required cli_args [] |  |  | [, ] | None The list of CLI arguments to parse. If cli_settings_source is specified, this may\nalso be a namespace or dictionary of pre-parsed CLI arguments. Defaults to sys.argv[1:] . None cli_settings_source [] | None Override the default CLI settings source with a user defined instance.\nDefaults to None . None cli_exit_on_error | None Determines whether this function exits on error. If model is subclass of BaseSettings , defaults to BaseSettings cli_exit_on_error value. Otherwise, defaults to True . None cli_cmd_method_name The CLI command method name to run. Defaults to \"cli_cmd\". 'cli_cmd' model_init_data The model init data. {} Returns: Type Description The ran instance of model. Raises: Type Description If model_cls is not subclass of BaseModel or pydantic.dataclasses.dataclass . If model_cls does not have a cli_cmd entrypoint defined. run_subcommand staticmethod ¶ run_subcommand(\n    model: ,\n    cli_exit_on_error:  | None = None,\n    cli_cmd_method_name:  = \"cli_cmd\",\n) -> Runs the model subcommand. Running a model subcommand requires the cli_cmd method to be defined in\nthe nested model subcommand class. Parameters: Name Type Description Default model The model to run the subcommand from. required cli_exit_on_error | None Determines whether this function exits with error if no subcommand is found.\nDefaults to model_config cli_exit_on_error value if set. Otherwise, defaults to True . None cli_cmd_method_name The CLI command method name to run. Defaults to \"cli_cmd\". 'cli_cmd' Returns: Type Description The ran subcommand model. Raises: Type Description When no subcommand is found and cli_exit_on_error= True (the default). When no subcommand is found and cli_exit_on_error= False .","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.CliApp","title":"Pydantic Settings - CliApp","objectID":"/latest/api/pydantic_settings/#pydantic_settings.CliApp","rank":85},{"content":"run(\n    model_cls: [],\n    cli_args: (\n        []\n        | \n        | \n        | [, ]\n        | None\n    ) = None,\n    cli_settings_source: (\n        [] | None\n    ) = None,\n    cli_exit_on_error:  | None = None,\n    cli_cmd_method_name:  = \"cli_cmd\",\n    **model_init_data: \n) -> Runs a Pydantic BaseSettings , BaseModel , or pydantic.dataclasses.dataclass as a CLI application.\nRunning a model as a CLI application requires the cli_cmd method to be defined in the model class. Parameters: Name Type Description Default model_cls [] The model class to run as a CLI application. required cli_args [] |  |  | [, ] | None The list of CLI arguments to parse. If cli_settings_source is specified, this may\nalso be a namespace or dictionary of pre-parsed CLI arguments. Defaults to sys.argv[1:] . None cli_settings_source [] | None Override the default CLI settings source with a user defined instance.\nDefaults to None . None cli_exit_on_error | None Determines whether this function exits on error. If model is subclass of BaseSettings , defaults to BaseSettings cli_exit_on_error value. Otherwise, defaults to True . None cli_cmd_method_name The CLI command method name to run. Defaults to \"cli_cmd\". 'cli_cmd' model_init_data The model init data. {} Returns: Type Description The ran instance of model. Raises: Type Description If model_cls is not subclass of BaseModel or pydantic.dataclasses.dataclass . If model_cls does not have a cli_cmd entrypoint defined.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.CliApp.run","title":"Pydantic Settings - CliApp - run  staticmethod","objectID":"/latest/api/pydantic_settings/#pydantic_settings.CliApp.run","rank":80},{"content":"run_subcommand(\n    model: ,\n    cli_exit_on_error:  | None = None,\n    cli_cmd_method_name:  = \"cli_cmd\",\n) -> Runs the model subcommand. Running a model subcommand requires the cli_cmd method to be defined in\nthe nested model subcommand class. Parameters: Name Type Description Default model The model to run the subcommand from. required cli_exit_on_error | None Determines whether this function exits with error if no subcommand is found.\nDefaults to model_config cli_exit_on_error value if set. Otherwise, defaults to True . None cli_cmd_method_name The CLI command method name to run. Defaults to \"cli_cmd\". 'cli_cmd' Returns: Type Description The ran subcommand model. Raises: Type Description When no subcommand is found and cli_exit_on_error= True (the default). When no subcommand is found and cli_exit_on_error= False .","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.CliApp.run_subcommand","title":"Pydantic Settings - CliApp - run_subcommand  staticmethod","objectID":"/latest/api/pydantic_settings/#pydantic_settings.CliApp.run_subcommand","rank":75},{"content":"Bases: pyproject_toml_depth instance-attribute ¶ pyproject_toml_depth: Number of levels up from the current working directory to attempt to find a pyproject.toml\nfile. This is only used when a pyproject.toml file is not found in the current working directory. pyproject_toml_table_header instance-attribute ¶ pyproject_toml_table_header: [, ...] Header of the TOML table within a pyproject.toml file to use when filling variables.\nThis is supplied as a tuple[str, ...] instead of a str to accommodate for headers\ncontaining a . . For example, toml_table_header = (\"tool\", \"my.tool\", \"foo\") can be used to fill variable\nvalues from a table with header [tool.\"my.tool\".foo] . To use the root table, exclude this config setting or provide an empty tuple.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.SettingsConfigDict","title":"Pydantic Settings - SettingsConfigDict","objectID":"/latest/api/pydantic_settings/#pydantic_settings.SettingsConfigDict","rank":70},{"content":"pyproject_toml_depth: Number of levels up from the current working directory to attempt to find a pyproject.toml\nfile. This is only used when a pyproject.toml file is not found in the current working directory.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.SettingsConfigDict.pyproject_toml_depth","title":"Pydantic Settings - SettingsConfigDict - pyproject_toml_depth  instance-attribute","objectID":"/latest/api/pydantic_settings/#pydantic_settings.SettingsConfigDict.pyproject_toml_depth","rank":65},{"content":"pyproject_toml_table_header: [, ...] Header of the TOML table within a pyproject.toml file to use when filling variables.\nThis is supplied as a tuple[str, ...] instead of a str to accommodate for headers\ncontaining a . . For example, toml_table_header = (\"tool\", \"my.tool\", \"foo\") can be used to fill variable\nvalues from a table with header [tool.\"my.tool\".foo] . To use the root table, exclude this config setting or provide an empty tuple.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.SettingsConfigDict.pyproject_toml_table_header","title":"Pydantic Settings - SettingsConfigDict - pyproject_toml_table_header  instance-attribute","objectID":"/latest/api/pydantic_settings/#pydantic_settings.SettingsConfigDict.pyproject_toml_table_header","rank":60},{"content":"CliSettingsSource(\n    settings_cls: [],\n    cli_prog_name:  | None = None,\n    cli_parse_args: (\n         | [] | [, ...] | None\n    ) = None,\n    cli_parse_none_str:  | None = None,\n    cli_hide_none_type:  | None = None,\n    cli_avoid_json:  | None = None,\n    cli_enforce_required:  | None = None,\n    cli_use_class_docs_for_groups:  | None = None,\n    cli_exit_on_error:  | None = None,\n    cli_prefix:  | None = None,\n    cli_flag_prefix_char:  | None = None,\n    cli_implicit_flags:  | None = None,\n    cli_ignore_unknown_args:  | None = None,\n    cli_kebab_case:  | None = None,\n    case_sensitive:  | None = True,\n    root_parser:  = None,\n    parse_args_method: [..., ] | None = None,\n    add_argument_method: (\n        [..., ] | None\n    ) = ,\n    add_argument_group_method: (\n        [..., ] | None\n    ) = ,\n    add_parser_method: (\n        [..., ] | None\n    ) = ,\n    add_subparsers_method: (\n        [..., ] | None\n    ) = ,\n    formatter_class:  = ,\n) Bases: , [] Source class for loading settings values from CLI. Parameters: Name Type Description Default cli_prog_name | None The CLI program name to display in help text. Defaults to None if cli_parse_args is None .\nOtherwse, defaults to sys.argv[0]. None cli_parse_args | [] | [, ...] | None The list of CLI arguments to parse. Defaults to None.\nIf set to True , defaults to sys.argv[1:]. None cli_parse_none_str | None The CLI string value that should be parsed (e.g. \"null\", \"void\", \"None\", etc.) into None type(None). Defaults to \"null\" if cli_avoid_json is False , and \"None\" if cli_avoid_json is True . None cli_hide_none_type | None Hide None values in CLI help text. Defaults to False . None cli_avoid_json | None Avoid complex JSON objects in CLI help text. Defaults to False . None cli_enforce_required | None Enforce required fields at the CLI. Defaults to False . None cli_use_class_docs_for_groups | None Use class docstrings in CLI group help text instead of field descriptions.\nDefaults to False . None cli_exit_on_error | None Determines whether or not the internal parser exits with error info when an error occurs.\nDefaults to True . None cli_prefix | None Prefix for command line arguments added under the root parser. Defaults to \"\". None cli_flag_prefix_char | None The flag prefix character to use for CLI optional arguments. Defaults to '-'. None cli_implicit_flags | None Whether bool fields should be implicitly converted into CLI boolean flags.\n(e.g. --flag, --no-flag). Defaults to False . None cli_ignore_unknown_args | None Whether to ignore unknown CLI args and parse only known ones. Defaults to False . None cli_kebab_case | None CLI args use kebab case. Defaults to False . None case_sensitive | None Whether CLI \"--arg\" names should be read with case-sensitivity. Defaults to True .\nNote: Case-insensitive matching is only supported on the internal root parser and does not apply to CLI\nsubcommands. True root_parser The root parser object. None parse_args_method [..., ] | None The root parser parse args method. Defaults to argparse.ArgumentParser.parse_args . None add_argument_method [..., ] | None The root parser add argument method. Defaults to argparse.ArgumentParser.add_argument . add_argument_group_method [..., ] | None The root parser add argument group method.\nDefaults to argparse.ArgumentParser.add_argument_group . add_parser_method [..., ] | None The root parser add new parser (sub-command) method.\nDefaults to argparse._SubParsersAction.add_parser . add_subparsers_method [..., ] | None The root parser add subparsers (sub-commands) method.\nDefaults to argparse.ArgumentParser.add_subparsers . formatter_class A class for customizing the root parser help text. Defaults to argparse.RawDescriptionHelpFormatter . root_parser property ¶ root_parser: The connected root parser instance.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.CliSettingsSource","title":"Pydantic Settings - CliSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.CliSettingsSource","rank":55},{"content":"root_parser: The connected root parser instance.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.CliSettingsSource.root_parser","title":"Pydantic Settings - CliSettingsSource - root_parser  property","objectID":"/latest/api/pydantic_settings/#pydantic_settings.CliSettingsSource.root_parser","rank":50},{"content":"DotEnvSettingsSource(\n    settings_cls: [],\n    env_file:  | None = ,\n    env_file_encoding:  | None = None,\n    case_sensitive:  | None = None,\n    env_prefix:  | None = None,\n    env_nested_delimiter:  | None = None,\n    env_ignore_empty:  | None = None,\n    env_parse_none_str:  | None = None,\n    env_parse_enums:  | None = None,\n) Bases: Source class for loading settings values from env files.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.DotEnvSettingsSource","title":"Pydantic Settings - DotEnvSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.DotEnvSettingsSource","rank":45},{"content":"EnvSettingsSource(\n    settings_cls: [],\n    case_sensitive:  | None = None,\n    env_prefix:  | None = None,\n    env_nested_delimiter:  | None = None,\n    env_ignore_empty:  | None = None,\n    env_parse_none_str:  | None = None,\n    env_parse_enums:  | None = None,\n) Bases: Source class for loading settings values from environment variables. get_field_value ¶ get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value for field from environment variables and a flag to determine whether value is complex. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value ( None if not found), key, and\na flag to determine whether value is complex. prepare_field_value ¶ prepare_field_value(\n    field_name: ,\n    field: ,\n    value: ,\n    value_is_complex: ,\n) -> Prepare value for the field. Extract value for nested field. Deserialize value to python object for complex field. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description A tuple contains prepared value for the field. Raises: Type Description When There is an error in deserializing value for complex field. next_field ¶ next_field(\n    field:  |  | None,\n    key: ,\n    case_sensitive:  | None = None,\n) ->  | None Find the field in a sub model by key(env name) By having the following models: ```py\nclass SubSubModel(BaseSettings):\n    dvals: Dict\n\nclass SubModel(BaseSettings):\n    vals: list[str]\n    sub_sub_model: SubSubModel\n\nclass Cfg(BaseSettings):\n    sub_model: SubModel\n``` Parameters: Name Type Description Default field |  | None The field. required key The key (env name). required case_sensitive | None Whether to search for key case sensitively. None Returns: Type Description | None Field if it finds the next field otherwise None . explode_env_vars ¶ explode_env_vars(\n    field_name: ,\n    field: ,\n    env_vars: [,  | None],\n) -> [, ] Process env_vars and extract the values of keys containing env_nested_delimiter into nested dictionaries. This is applied to a single field, hence filtering by env_var prefix. Parameters: Name Type Description Default field_name The field name. required field The field. required env_vars [,  | None] Environment variables. required Returns: Type Description [, ] A dictionary contains extracted values from nested env values.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource","title":"Pydantic Settings - EnvSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource","rank":40},{"content":"get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value for field from environment variables and a flag to determine whether value is complex. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value ( None if not found), key, and\na flag to determine whether value is complex.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.get_field_value","title":"Pydantic Settings - EnvSettingsSource - get_field_value","objectID":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.get_field_value","rank":35},{"content":"prepare_field_value(\n    field_name: ,\n    field: ,\n    value: ,\n    value_is_complex: ,\n) -> Prepare value for the field. Extract value for nested field. Deserialize value to python object for complex field. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description A tuple contains prepared value for the field. Raises: Type Description When There is an error in deserializing value for complex field.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.prepare_field_value","title":"Pydantic Settings - EnvSettingsSource - prepare_field_value","objectID":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.prepare_field_value","rank":30},{"content":"next_field(\n    field:  |  | None,\n    key: ,\n    case_sensitive:  | None = None,\n) ->  | None Find the field in a sub model by key(env name) By having the following models: ```py\nclass SubSubModel(BaseSettings):\n    dvals: Dict\n\nclass SubModel(BaseSettings):\n    vals: list[str]\n    sub_sub_model: SubSubModel\n\nclass Cfg(BaseSettings):\n    sub_model: SubModel\n``` Parameters: Name Type Description Default field |  | None The field. required key The key (env name). required case_sensitive | None Whether to search for key case sensitively. None Returns: Type Description | None Field if it finds the next field otherwise None .","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.next_field","title":"Pydantic Settings - EnvSettingsSource - next_field","objectID":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.next_field","rank":25},{"content":"explode_env_vars(\n    field_name: ,\n    field: ,\n    env_vars: [,  | None],\n) -> [, ] Process env_vars and extract the values of keys containing env_nested_delimiter into nested dictionaries. This is applied to a single field, hence filtering by env_var prefix. Parameters: Name Type Description Default field_name The field name. required field The field. required env_vars [,  | None] Environment variables. required Returns: Type Description [, ] A dictionary contains extracted values from nested env values.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.explode_env_vars","title":"Pydantic Settings - EnvSettingsSource - explode_env_vars","objectID":"/latest/api/pydantic_settings/#pydantic_settings.EnvSettingsSource.explode_env_vars","rank":20},{"content":"Annotation to force decoding of a field value.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.ForceDecode","title":"Pydantic Settings - ForceDecode","objectID":"/latest/api/pydantic_settings/#pydantic_settings.ForceDecode","rank":15},{"content":"InitSettingsSource(\n    settings_cls: [],\n    init_kwargs: [, ],\n    nested_model_default_partial_update:  | None = None,\n) Bases: Source class for loading values provided during settings class initialization.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.InitSettingsSource","title":"Pydantic Settings - InitSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.InitSettingsSource","rank":10},{"content":"JsonConfigSettingsSource(\n    settings_cls: [],\n    json_file:  | None = ,\n    json_file_encoding:  | None = None,\n) Bases: , A source class that loads variables from a JSON file","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.JsonConfigSettingsSource","title":"Pydantic Settings - JsonConfigSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.JsonConfigSettingsSource","rank":5},{"content":"Annotation to prevent decoding of a field value.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.NoDecode","title":"Pydantic Settings - NoDecode","objectID":"/latest/api/pydantic_settings/#pydantic_settings.NoDecode","rank":0},{"content":"PydanticBaseSettingsSource(\n    settings_cls: [],\n) Bases: Abstract base class for settings sources, every settings source classes should inherit from it. current_state property ¶ current_state: [, ] The current state of the settings, populated by the previous settings sources. settings_sources_data property ¶ settings_sources_data: [, [, ]] The state of all previous settings sources. get_field_value abstractmethod ¶ get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value, the key for model creation, and a flag to determine whether value is complex. This is an abstract method that should be overridden in every settings source classes. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value, key and a flag to determine whether value is complex. field_is_complex ¶ field_is_complex(field: ) -> Checks whether a field is complex, in which case it will attempt to be parsed as JSON. Parameters: Name Type Description Default field The field. required Returns: Type Description Whether the field is complex. prepare_field_value ¶ prepare_field_value(\n    field_name: ,\n    field: ,\n    value: ,\n    value_is_complex: ,\n) -> Prepares the value of a field. Parameters: Name Type Description Default field_name The field name. required field The field. required value The value of the field that has to be prepared. required value_is_complex A flag to determine whether value is complex. required Returns: Type Description The prepared value. decode_complex_value ¶ decode_complex_value(\n    field_name: , field: , value: \n) -> Decode the value for a complex field Parameters: Name Type Description Default field_name The field name. required field The field. required value The value of the field that has to be prepared. required Returns: Type Description The decoded value for further preparation","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource","title":"Pydantic Settings - PydanticBaseSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource","rank":-5},{"content":"current_state: [, ] The current state of the settings, populated by the previous settings sources.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.current_state","title":"Pydantic Settings - PydanticBaseSettingsSource - current_state  property","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.current_state","rank":-10},{"content":"settings_sources_data: [, [, ]] The state of all previous settings sources.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.settings_sources_data","title":"Pydantic Settings - PydanticBaseSettingsSource - settings_sources_data  property","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.settings_sources_data","rank":-15},{"content":"get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value, the key for model creation, and a flag to determine whether value is complex. This is an abstract method that should be overridden in every settings source classes. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value, key and a flag to determine whether value is complex.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.get_field_value","title":"Pydantic Settings - PydanticBaseSettingsSource - get_field_value  abstractmethod","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.get_field_value","rank":-20},{"content":"field_is_complex(field: ) -> Checks whether a field is complex, in which case it will attempt to be parsed as JSON. Parameters: Name Type Description Default field The field. required Returns: Type Description Whether the field is complex.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.field_is_complex","title":"Pydantic Settings - PydanticBaseSettingsSource - field_is_complex","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.field_is_complex","rank":-25},{"content":"prepare_field_value(\n    field_name: ,\n    field: ,\n    value: ,\n    value_is_complex: ,\n) -> Prepares the value of a field. Parameters: Name Type Description Default field_name The field name. required field The field. required value The value of the field that has to be prepared. required value_is_complex A flag to determine whether value is complex. required Returns: Type Description The prepared value.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.prepare_field_value","title":"Pydantic Settings - PydanticBaseSettingsSource - prepare_field_value","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.prepare_field_value","rank":-30},{"content":"decode_complex_value(\n    field_name: , field: , value: \n) -> Decode the value for a complex field Parameters: Name Type Description Default field_name The field name. required field The field. required value The value of the field that has to be prepared. required Returns: Type Description The decoded value for further preparation","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.decode_complex_value","title":"Pydantic Settings - PydanticBaseSettingsSource - decode_complex_value","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PydanticBaseSettingsSource.decode_complex_value","rank":-35},{"content":"PyprojectTomlConfigSettingsSource(\n    settings_cls: [],\n    toml_file:  | None = None,\n) Bases: A source class that loads variables from a pyproject.toml file.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.PyprojectTomlConfigSettingsSource","title":"Pydantic Settings - PyprojectTomlConfigSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.PyprojectTomlConfigSettingsSource","rank":-40},{"content":"SecretsSettingsSource(\n    settings_cls: [],\n    secrets_dir:  | None = None,\n    case_sensitive:  | None = None,\n    env_prefix:  | None = None,\n    env_ignore_empty:  | None = None,\n    env_parse_none_str:  | None = None,\n    env_parse_enums:  | None = None,\n) Bases: Source class for loading settings values from secret files. find_case_path classmethod ¶ find_case_path(\n    dir_path: , file_name: , case_sensitive: \n) ->  | None Find a file within path's directory matching filename, optionally ignoring case. Parameters: Name Type Description Default dir_path Directory path. required file_name File name. required case_sensitive Whether to search for file name case sensitively. required Returns: Type Description | None Whether file path or None if file does not exist in directory. get_field_value ¶ get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value for field from secret file and a flag to determine whether value is complex. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value ( None if the file does not exist), key, and\na flag to determine whether value is complex.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.SecretsSettingsSource","title":"Pydantic Settings - SecretsSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.SecretsSettingsSource","rank":-45},{"content":"find_case_path(\n    dir_path: , file_name: , case_sensitive: \n) ->  | None Find a file within path's directory matching filename, optionally ignoring case. Parameters: Name Type Description Default dir_path Directory path. required file_name File name. required case_sensitive Whether to search for file name case sensitively. required Returns: Type Description | None Whether file path or None if file does not exist in directory.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.SecretsSettingsSource.find_case_path","title":"Pydantic Settings - SecretsSettingsSource - find_case_path  classmethod","objectID":"/latest/api/pydantic_settings/#pydantic_settings.SecretsSettingsSource.find_case_path","rank":-50},{"content":"get_field_value(\n    field: , field_name: \n) -> [, , ] Gets the value for field from secret file and a flag to determine whether value is complex. Parameters: Name Type Description Default field The field. required field_name The field name. required Returns: Type Description [, , ] A tuple that contains the value ( None if the file does not exist), key, and\na flag to determine whether value is complex.","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.SecretsSettingsSource.get_field_value","title":"Pydantic Settings - SecretsSettingsSource - get_field_value","objectID":"/latest/api/pydantic_settings/#pydantic_settings.SecretsSettingsSource.get_field_value","rank":-55},{"content":"TomlConfigSettingsSource(\n    settings_cls: [],\n    toml_file:  | None = ,\n) Bases: , A source class that loads variables from a TOML file","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.TomlConfigSettingsSource","title":"Pydantic Settings - TomlConfigSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.TomlConfigSettingsSource","rank":-60},{"content":"YamlConfigSettingsSource(\n    settings_cls: [],\n    yaml_file:  | None = ,\n    yaml_file_encoding:  | None = None,\n) Bases: , A source class that loads variables from a yaml file","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.YamlConfigSettingsSource","title":"Pydantic Settings - YamlConfigSettingsSource","objectID":"/latest/api/pydantic_settings/#pydantic_settings.YamlConfigSettingsSource","rank":-65},{"content":"get_subcommand(\n    model: ,\n    is_required:  = True,\n    cli_exit_on_error:  | None = None,\n) -> [] Get the subcommand from a model. Parameters: Name Type Description Default model The model to get the subcommand from. required is_required Determines whether a model must have subcommand set and raises error if not\nfound. Defaults to True . True cli_exit_on_error | None Determines whether this function exits with error if no subcommand is found.\nDefaults to model_config cli_exit_on_error value if set. Otherwise, defaults to True . None Returns: Type Description [] The subcommand model if found, otherwise None . Raises: Type Description When no subcommand is found and is_required= True and cli_exit_on_error= True (the default). When no subcommand is found and is_required= True and\ncli_exit_on_error= False .","pageID":"Pydantic Settings","abs_url":"/latest/api/pydantic_settings/#pydantic_settings.get_subcommand","title":"Pydantic Settings - get_subcommand","objectID":"/latest/api/pydantic_settings/#pydantic_settings.get_subcommand","rank":-70},{"content":"RootModel class and type definitions. RootModel ¶ RootModel(\n    root:  = , **data\n) Bases: , [] Usage Documentation RootModel and Custom Root Types A Pydantic BaseModel for the root object of the model. Attributes: Name Type Description The root object of the model. Whether the model is a RootModel. Private fields in the model. Extra fields in the model. model_construct classmethod ¶ model_construct(\n    root: ,\n    _fields_set: [] | None = None,\n) -> Create a new model using the provided root object and update fields set. Parameters: Name Type Description Default root The root object of the model. required _fields_set [] | None The set of fields to be updated. None Returns: Type Description The new model. Raises: Type Description If the model is not a subclass of RootModel . model_dump ¶ model_dump(\n    *,\n    mode: [\"json\", \"python\"] |  = \"python\",\n    include:  = None,\n    exclude:  = None,\n    context: [, ] | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    serialize_as_any:  = False\n) -> This method is included just to get a more accurate return type for type checkers.\nIt is included in this if TYPE_CHECKING: block since no override is actually necessary. See the documentation of BaseModel.model_dump for more details about the arguments. Generally, this method will have a return type of RootModelRootType , assuming that RootModelRootType is\nnot a BaseModel subclass. If RootModelRootType is a BaseModel subclass, then the return\ntype will likely be dict[str, Any] , as model_dump calls are recursive. The return type could\neven be something different, in the case of a custom serializer.\nThus, Any is used here to catch all of these cases.","pageID":"RootModel","abs_url":"/latest/api/root_model/#RootModel","title":"RootModel","objectID":"/latest/api/root_model/#RootModel","rank":100},{"content":"RootModel(\n    root:  = , **data\n) Bases: , [] Usage Documentation RootModel and Custom Root Types A Pydantic BaseModel for the root object of the model. Attributes: Name Type Description The root object of the model. Whether the model is a RootModel. Private fields in the model. Extra fields in the model. model_construct classmethod ¶ model_construct(\n    root: ,\n    _fields_set: [] | None = None,\n) -> Create a new model using the provided root object and update fields set. Parameters: Name Type Description Default root The root object of the model. required _fields_set [] | None The set of fields to be updated. None Returns: Type Description The new model. Raises: Type Description If the model is not a subclass of RootModel . model_dump ¶ model_dump(\n    *,\n    mode: [\"json\", \"python\"] |  = \"python\",\n    include:  = None,\n    exclude:  = None,\n    context: [, ] | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    serialize_as_any:  = False\n) -> This method is included just to get a more accurate return type for type checkers.\nIt is included in this if TYPE_CHECKING: block since no override is actually necessary. See the documentation of BaseModel.model_dump for more details about the arguments. Generally, this method will have a return type of RootModelRootType , assuming that RootModelRootType is\nnot a BaseModel subclass. If RootModelRootType is a BaseModel subclass, then the return\ntype will likely be dict[str, Any] , as model_dump calls are recursive. The return type could\neven be something different, in the case of a custom serializer.\nThus, Any is used here to catch all of these cases.","pageID":"RootModel","abs_url":"/latest/api/root_model/#pydantic.root_model.RootModel","title":"RootModel - RootModel","objectID":"/latest/api/root_model/#pydantic.root_model.RootModel","rank":95},{"content":"model_construct(\n    root: ,\n    _fields_set: [] | None = None,\n) -> Create a new model using the provided root object and update fields set. Parameters: Name Type Description Default root The root object of the model. required _fields_set [] | None The set of fields to be updated. None Returns: Type Description The new model. Raises: Type Description If the model is not a subclass of RootModel .","pageID":"RootModel","abs_url":"/latest/api/root_model/#pydantic.root_model.RootModel.model_construct","title":"RootModel - RootModel - model_construct  classmethod","objectID":"/latest/api/root_model/#pydantic.root_model.RootModel.model_construct","rank":90},{"content":"model_dump(\n    *,\n    mode: [\"json\", \"python\"] |  = \"python\",\n    include:  = None,\n    exclude:  = None,\n    context: [, ] | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    serialize_as_any:  = False\n) -> This method is included just to get a more accurate return type for type checkers.\nIt is included in this if TYPE_CHECKING: block since no override is actually necessary. See the documentation of BaseModel.model_dump for more details about the arguments. Generally, this method will have a return type of RootModelRootType , assuming that RootModelRootType is\nnot a BaseModel subclass. If RootModelRootType is a BaseModel subclass, then the return\ntype will likely be dict[str, Any] , as model_dump calls are recursive. The return type could\neven be something different, in the case of a custom serializer.\nThus, Any is used here to catch all of these cases.","pageID":"RootModel","abs_url":"/latest/api/root_model/#pydantic.root_model.RootModel.model_dump","title":"RootModel - RootModel - model_dump","objectID":"/latest/api/root_model/#pydantic.root_model.RootModel.model_dump","rank":85},{"content":"Pydantic supports many common types from the Python standard library. If you need stricter processing see Strict Types , including if you need to constrain the values allowed (e.g. to require a positive int ). Note Pydantic still supports older (3.8-) typing constructs like typing.List and typing.Dict , but\nit's best practice to use the newer types like list and dict .","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#Standard Library Types","title":"Standard Library Types","objectID":"/latest/api/standard_library_types/#Standard Library Types","rank":100},{"content":"A standard bool field will raise a ValidationError if the value is not one of the following: A valid boolean (i.e. True or False ), The integers 0 or 1 , a str which when converted to lower case is one of '0', 'off', 'f', 'false', 'n', 'no', '1', 'on', 't', 'true', 'y', 'yes' a bytes which is valid per the previous rule when decoded to str Note If you want stricter boolean logic (e.g. a field which only permits True and False ) you can\nuse StrictBool . Here is a script demonstrating some of these behaviors: from pydantic import BaseModel, ValidationError\n\n\nclass BooleanModel(BaseModel):\n    bool_value: bool\n\n\nprint(BooleanModel(bool_value=False))\n#> bool_value=False\nprint(BooleanModel(bool_value='False'))\n#> bool_value=False\nprint(BooleanModel(bool_value=1))\n#> bool_value=True\ntry:\n    BooleanModel(bool_value=[])\nexcept ValidationError as e:\n    print(str(e))\n    \"\"\"\n    1 validation error for BooleanModel\n    bool_value\n      Input should be a valid boolean [type=bool_type, input_value=[], input_type=list]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#booleans","title":"Standard Library Types - Booleans","objectID":"/latest/api/standard_library_types/#booleans","rank":95},{"content":"Pydantic supports the following datetime types:","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#datetime-types","title":"Standard Library Types - Datetime Types","objectID":"/latest/api/standard_library_types/#datetime-types","rank":90},{"content":"datetime fields will accept values of type: datetime ; an existing datetime object int or float ; assumed as Unix time, i.e. seconds (if >= -2e10 and <= 2e10 ) or milliseconds\n  (if < -2e10 or > 2e10 ) since 1 January 1970 str ; the following formats are accepted: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z or [±]HH[:]MM] YYYY-MM-DD is accepted in lax mode, but not in strict mode int or float as a string (assumed as Unix time) instances are accepted in lax mode, but not in strict mode from datetime import datetime\n\nfrom pydantic import BaseModel\n\n\nclass Event(BaseModel):\n    dt: datetime = None\n\n\nevent = Event(dt='2032-04-23T10:20:30.400+02:30')\n\nprint(event.model_dump())\n\"\"\"\n{'dt': datetime.datetime(2032, 4, 23, 10, 20, 30, 400000, tzinfo=TzInfo(9000))}\n\"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#datetimedatetime","title":"Standard Library Types - Datetime Types - ","objectID":"/latest/api/standard_library_types/#datetimedatetime","rank":85},{"content":"date fields will accept values of type: date ; an existing date object int or float ; handled the same as described for datetime above str ; the following formats are accepted: YYYY-MM-DD int or float as a string (assumed as Unix time) from datetime import date\n\nfrom pydantic import BaseModel\n\n\nclass Birthday(BaseModel):\n    d: date = None\n\n\nmy_birthday = Birthday(d=1679616000.0)\n\nprint(my_birthday.model_dump())\n#> {'d': datetime.date(2023, 3, 24)}","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#datetimedate","title":"Standard Library Types - Datetime Types - ","objectID":"/latest/api/standard_library_types/#datetimedate","rank":80},{"content":"time fields will accept values of type: time ; an existing time object str ; the following formats are accepted: HH:MM[:SS[.ffffff]][Z or [±]HH[:]MM] from datetime import time\n\nfrom pydantic import BaseModel\n\n\nclass Meeting(BaseModel):\n    t: time = None\n\n\nm = Meeting(t=time(4, 8, 16))\n\nprint(m.model_dump())\n#> {'t': datetime.time(4, 8, 16)}","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#datetimetime","title":"Standard Library Types - Datetime Types - ","objectID":"/latest/api/standard_library_types/#datetimetime","rank":75},{"content":"timedelta fields will accept values of type: timedelta ; an existing timedelta object int or float ; assumed to be seconds str ; the following formats are accepted: [-][[DD]D,]HH:MM:SS[.ffffff] Ex: '1d,01:02:03.000004' or '1D01:02:03.000004' or '01:02:03' [±]P[DD]DT[HH]H[MM]M[SS]S ( ISO 8601 format for timedelta) from datetime import timedelta\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    td: timedelta = None\n\n\nm = Model(td='P3DT12H30M5S')\n\nprint(m.model_dump())\n#> {'td': datetime.timedelta(days=3, seconds=45005)}","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#datetimetimedelta","title":"Standard Library Types - Datetime Types - ","objectID":"/latest/api/standard_library_types/#datetimetimedelta","rank":70},{"content":"Pydantic supports the following numeric types from the Python standard library:","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#number-types","title":"Standard Library Types - Number Types","objectID":"/latest/api/standard_library_types/#number-types","rank":65},{"content":"Pydantic uses int(v) to coerce types to an int ;\n  see Data conversion for details on loss of information during data conversion.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#int","title":"Standard Library Types - Number Types - ","objectID":"/latest/api/standard_library_types/#int","rank":60},{"content":"Pydantic uses float(v) to coerce values to floats.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#float","title":"Standard Library Types - Number Types - ","objectID":"/latest/api/standard_library_types/#float","rank":55},{"content":"Validation: Pydantic checks that the value is a valid IntEnum instance. Validation for subclass of enum.IntEnum : checks that the value is a valid member of the integer enum;\n  see Enums and Choices for more details.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#enumintenum","title":"Standard Library Types - Number Types - ","objectID":"/latest/api/standard_library_types/#enumintenum","rank":50},{"content":"Validation: Pydantic attempts to convert the value to a string, then passes the string to Decimal(v) . Serialization: Pydantic serializes  types as strings.\nYou can use a custom serializer to override this behavior if desired. For example: from decimal import Decimal\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, PlainSerializer\n\n\nclass Model(BaseModel):\n    x: Decimal\n    y: Annotated[\n        Decimal,\n        PlainSerializer(\n            lambda x: float(x), return_type=float, when_used='json'\n        ),\n    ]\n\n\nmy_model = Model(x=Decimal('1.1'), y=Decimal('2.1'))\n\nprint(my_model.model_dump())  # (1)!\n#> {'x': Decimal('1.1'), 'y': Decimal('2.1')}\nprint(my_model.model_dump(mode='json'))  # (2)!\n#> {'x': '1.1', 'y': 2.1}\nprint(my_model.model_dump_json())  # (3)!\n#> {\"x\":\"1.1\",\"y\":2.1} Using , both x and y remain instances of the Decimal type Using  with mode='json' , x is serialized as a string , and y is serialized as a float because of the custom serializer applied. Using , x is serialized as a string , and y is serialized as a float because of the custom serializer applied.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#decimaldecimal","title":"Standard Library Types - Number Types - ","objectID":"/latest/api/standard_library_types/#decimaldecimal","rank":45},{"content":"Validation: Pydantic supports complex types or str values that can be converted to a complex type. Serialization: Pydantic serializes  types as strings.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#complex","title":"Standard Library Types - Number Types - ","objectID":"/latest/api/standard_library_types/#complex","rank":40},{"content":"Validation: Pydantic attempts to convert the value to a Fraction using Fraction(v) . Serialization: Pydantic serializes  types as strings.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#fractionsfraction","title":"Standard Library Types - Number Types - ","objectID":"/latest/api/standard_library_types/#fractionsfraction","rank":35},{"content":"Pydantic uses Python's standard  classes to define choices. enum.Enum checks that the value is a valid Enum instance.\nSubclass of enum.Enum checks that the value is a valid member of the enum. from enum import Enum, IntEnum\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass FruitEnum(str, Enum):\n    pear = 'pear'\n    banana = 'banana'\n\n\nclass ToolEnum(IntEnum):\n    spanner = 1\n    wrench = 2\n\n\nclass CookingModel(BaseModel):\n    fruit: FruitEnum = FruitEnum.pear\n    tool: ToolEnum = ToolEnum.spanner\n\n\nprint(CookingModel())\n#> fruit= tool= print(CookingModel(tool=2, fruit='banana'))\n#> fruit= tool= try:\n    CookingModel(fruit='other')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for CookingModel\n    fruit\n      Input should be 'pear' or 'banana' [type=enum, input_value='other', input_type=str]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#enum","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#enum","rank":30},{"content":"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#lists-and-tuples","title":"Standard Library Types - Lists and Tuples","objectID":"/latest/api/standard_library_types/#lists-and-tuples","rank":25},{"content":"Allows , , , , , or generators and casts to a .\nWhen a generic parameter is provided, the appropriate validation is applied to all items of the list. Python 3.9 and above Python 3.10 and above from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_list: Optional[list] = None\n    list_of_ints: Optional[list[int]] = None\n\n\nprint(Model(simple_list=['1', '2', '3']).simple_list)\n#> ['1', '2', '3']\nprint(Model(list_of_ints=['1', '2', '3']).list_of_ints)\n#> [1, 2, 3] from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_list: list | None = None\n    list_of_ints: list[int] | None = None\n\n\nprint(Model(simple_list=['1', '2', '3']).simple_list)\n#> ['1', '2', '3']\nprint(Model(list_of_ints=['1', '2', '3']).list_of_ints)\n#> [1, 2, 3]","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#list","title":"Standard Library Types - Lists and Tuples - ","objectID":"/latest/api/standard_library_types/#list","rank":20},{"content":"Allows , , , , , or generators and casts to a .\nWhen generic parameters are provided, the appropriate validation is applied to the respective items of the tuple","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#tuple","title":"Standard Library Types - Lists and Tuples - ","objectID":"/latest/api/standard_library_types/#tuple","rank":15},{"content":"Handled the same as tuple above. Python 3.9 and above Python 3.10 and above from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_tuple: Optional[tuple] = None\n    tuple_of_different_types: Optional[tuple[int, float, bool]] = None\n\n\nprint(Model(simple_tuple=[1, 2, 3, 4]).simple_tuple)\n#> (1, 2, 3, 4)\nprint(Model(tuple_of_different_types=[3, 2, 1]).tuple_of_different_types)\n#> (3, 2.0, True) from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_tuple: tuple | None = None\n    tuple_of_different_types: tuple[int, float, bool] | None = None\n\n\nprint(Model(simple_tuple=[1, 2, 3, 4]).simple_tuple)\n#> (1, 2, 3, 4)\nprint(Model(tuple_of_different_types=[3, 2, 1]).tuple_of_different_types)\n#> (3, 2.0, True)","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingtuple","title":"Standard Library Types - Lists and Tuples - ","objectID":"/latest/api/standard_library_types/#typingtuple","rank":10},{"content":"Subclasses of  are similar to tuple , but create instances of the given namedtuple class. Subclasses of  are similar to subclass of , but since field types are not specified,\nall fields are treated as having type . from typing import NamedTuple\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Point(NamedTuple):\n    x: int\n    y: int\n\n\nclass Model(BaseModel):\n    p: Point\n\n\ntry:\n    Model(p=('1.3', '2'))\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    p.0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='1.3', input_type=str]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingnamedtuple","title":"Standard Library Types - Lists and Tuples - ","objectID":"/latest/api/standard_library_types/#typingnamedtuple","rank":5},{"content":"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#deque","title":"Standard Library Types - Deque","objectID":"/latest/api/standard_library_types/#deque","rank":0},{"content":"Allows , , , , , or generators and casts to a .\nWhen generic parameters are provided, the appropriate validation is applied to the respective items of the deque .","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#deque_1","title":"Standard Library Types - Deque - ","objectID":"/latest/api/standard_library_types/#deque_1","rank":-5},{"content":"Handled the same as deque above. Python 3.9 and above Python 3.10 and above from typing import Deque, Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    deque: Optional[Deque[int]] = None\n\n\nprint(Model(deque=[1, 2, 3]).deque)\n#> deque([1, 2, 3]) from typing import Deque\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    deque: Deque[int] | None = None\n\n\nprint(Model(deque=[1, 2, 3]).deque)\n#> deque([1, 2, 3])","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingdeque","title":"Standard Library Types - Deque - ","objectID":"/latest/api/standard_library_types/#typingdeque","rank":-10},{"content":"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#sets","title":"Standard Library Types - Sets","objectID":"/latest/api/standard_library_types/#sets","rank":-15},{"content":"Allows , , , , , or generators and casts to a .\nWhen a generic parameter is provided, the appropriate validation is applied to all items of the set.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#set","title":"Standard Library Types - Sets - ","objectID":"/latest/api/standard_library_types/#set","rank":-20},{"content":"Handled the same as set above. Python 3.9 and above Python 3.10 and above from typing import Optional, Set\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_set: Optional[set] = None\n    set_of_ints: Optional[Set[int]] = None\n\n\nprint(Model(simple_set={'1', '2', '3'}).simple_set)\n#> {'1', '2', '3'}\nprint(Model(simple_set=['1', '2', '3']).simple_set)\n#> {'1', '2', '3'}\nprint(Model(set_of_ints=['1', '2', '3']).set_of_ints)\n#> {1, 2, 3} from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_set: set | None = None\n    set_of_ints: set[int] | None = None\n\n\nprint(Model(simple_set={'1', '2', '3'}).simple_set)\n#> {'1', '2', '3'}\nprint(Model(simple_set=['1', '2', '3']).simple_set)\n#> {'1', '2', '3'}\nprint(Model(set_of_ints=['1', '2', '3']).set_of_ints)\n#> {1, 2, 3}","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingset","title":"Standard Library Types - Sets - ","objectID":"/latest/api/standard_library_types/#typingset","rank":-25},{"content":"Allows , , , , , or generators and casts to a .\nWhen a generic parameter is provided, the appropriate validation is applied to all items of the frozen set.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#frozenset","title":"Standard Library Types - Sets - ","objectID":"/latest/api/standard_library_types/#frozenset","rank":-30},{"content":"Handled the same as frozenset above. Python 3.9 and above Python 3.10 and above from typing import FrozenSet, Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_frozenset: Optional[frozenset] = None\n    frozenset_of_ints: Optional[FrozenSet[int]] = None\n\n\nm1 = Model(simple_frozenset=['1', '2', '3'])\nprint(type(m1.simple_frozenset))\n#> print(sorted(m1.simple_frozenset))\n#> ['1', '2', '3']\n\nm2 = Model(frozenset_of_ints=['1', '2', '3'])\nprint(type(m2.frozenset_of_ints))\n#> print(sorted(m2.frozenset_of_ints))\n#> [1, 2, 3] from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    simple_frozenset: frozenset | None = None\n    frozenset_of_ints: frozenset[int] | None = None\n\n\nm1 = Model(simple_frozenset=['1', '2', '3'])\nprint(type(m1.simple_frozenset))\n#> print(sorted(m1.simple_frozenset))\n#> ['1', '2', '3']\n\nm2 = Model(frozenset_of_ints=['1', '2', '3'])\nprint(type(m2.frozenset_of_ints))\n#> print(sorted(m2.frozenset_of_ints))\n#> [1, 2, 3]","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingfrozenset","title":"Standard Library Types - Sets - ","objectID":"/latest/api/standard_library_types/#typingfrozenset","rank":-35},{"content":"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#other-iterables","title":"Standard Library Types - Other Iterables","objectID":"/latest/api/standard_library_types/#other-iterables","rank":-40},{"content":"This is intended for use when the provided value should meet the requirements of the Sequence ABC, and it is\ndesirable to do eager validation of the values in the container. Note that when validation must be performed on the\nvalues of the container, the type of the container may not be preserved since validation may end up replacing values.\nWe guarantee that the validated value will be a valid , but it may have a different type than was\nprovided (generally, it will become a list ).","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingsequence","title":"Standard Library Types - Other Iterables - ","objectID":"/latest/api/standard_library_types/#typingsequence","rank":-45},{"content":"This is intended for use when the provided value may be an iterable that shouldn't be consumed.\nSee Infinite Generators below for more detail on parsing and validation.\nSimilar to , we guarantee that the validated result will be a valid ,\nbut it may have a different type than was provided. In particular, even if a non-generator type such as a list is provided, the post-validation value of a field of type  will be a generator. Here is a simple example using : from collections.abc import Sequence\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    sequence_of_ints: Sequence[int]\n\n\nprint(Model(sequence_of_ints=[1, 2, 3, 4]).sequence_of_ints)\n#> [1, 2, 3, 4]\nprint(Model(sequence_of_ints=(1, 2, 3, 4)).sequence_of_ints)\n#> (1, 2, 3, 4)","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingiterable","title":"Standard Library Types - Other Iterables - ","objectID":"/latest/api/standard_library_types/#typingiterable","rank":-50},{"content":"If you have a generator you want to validate, you can still use Sequence as described above.\nIn that case, the generator will be consumed and stored on the model as a list and its values will be\nvalidated against the type parameter of the Sequence (e.g. int in Sequence[int] ). However, if you have a generator that you don't want to be eagerly consumed (e.g. an infinite\ngenerator or a remote data loader), you can use a field of type : from collections.abc import Iterable\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    infinite: Iterable[int]\n\n\ndef infinite_ints():\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n\nm = Model(infinite=infinite_ints())\nprint(m)\n\"\"\"\ninfinite=ValidatorIterator(index=0, schema=Some(Int(IntValidator { strict: false })))\n\"\"\"\n\nfor i in m.infinite:\n    print(i)\n    #> 0\n    #> 1\n    #> 2\n    #> 3\n    #> 4\n    #> 5\n    #> 6\n    #> 7\n    #> 8\n    #> 9\n    #> 10\n    if i == 10:\n        break Warning During initial validation, Iterable fields only perform a simple check that the provided argument is iterable.\nTo prevent it from being consumed, no validation of the yielded values is performed eagerly. Though the yielded values are not validated eagerly, they are still validated when yielded, and will raise a ValidationError at yield time when appropriate: from collections.abc import Iterable\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    int_iterator: Iterable[int]\n\n\ndef my_iterator():\n    yield 13\n    yield '27'\n    yield 'a'\n\n\nm = Model(int_iterator=my_iterator())\nprint(next(m.int_iterator))\n#> 13\nprint(next(m.int_iterator))\n#> 27\ntry:\n    next(m.int_iterator)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for ValidatorIterator\n    2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#infinite-generators","title":"Standard Library Types - Other Iterables - Infinite Generators","objectID":"/latest/api/standard_library_types/#infinite-generators","rank":-55},{"content":"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#mapping-types","title":"Standard Library Types - Mapping Types","objectID":"/latest/api/standard_library_types/#mapping-types","rank":-60},{"content":"dict(v) is used to attempt to convert a dictionary. from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: dict[str, int]\n\n\nm = Model(x={'foo': 1})\nprint(m.model_dump())\n#> {'x': {'foo': 1}}\n\ntry:\n    Model(x={'foo': '1'})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    x\n      Input should be a valid dictionary [type=dict_type, input_value='test', input_type=str]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#dict","title":"Standard Library Types - Mapping Types - ","objectID":"/latest/api/standard_library_types/#dict","rank":-65},{"content":"Note This is a new feature of the Python standard library as of Python 3.8.\nBecause of limitations in  before 3.12, the typing-extensions package is required for Python <3.12. You'll need to import TypedDict from typing_extensions instead of typing and will\nget a build time error if you don't. declares a dictionary type that expects all of\nits instances to have a certain set of keys, where each key is associated with a value of a consistent type. It is same as  but Pydantic will validate the dictionary since keys are annotated. Python 3.9 and above Python 3.13 and above from typing_extensions import TypedDict\n\nfrom pydantic import TypeAdapter, ValidationError\n\n\nclass User(TypedDict):\n    name: str\n    id: int\n\n\nta = TypeAdapter(User)\n\nprint(ta.validate_python({'name': 'foo', 'id': 1}))\n#> {'name': 'foo', 'id': 1}\n\ntry:\n    ta.validate_python({'name': 'foo'})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    id\n      Field required [type=missing, input_value={'name': 'foo'}, input_type=dict]\n    \"\"\" from typing import TypedDict\n\nfrom pydantic import TypeAdapter, ValidationError\n\n\nclass User(TypedDict):\n    name: str\n    id: int\n\n\nta = TypeAdapter(User)\n\nprint(ta.validate_python({'name': 'foo', 'id': 1}))\n#> {'name': 'foo', 'id': 1}\n\ntry:\n    ta.validate_python({'name': 'foo'})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    id\n      Field required [type=missing, input_value={'name': 'foo'}, input_type=dict]\n    \"\"\" You can define __pydantic_config__ to change the model inherited from .\nSee the  for more details. Python 3.9 and above Python 3.10 and above Python 3.13 and above from typing import Optional\n\nfrom typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, ValidationError\n\n\n# `total=False` means keys are non-required\nclass UserIdentity(TypedDict, total=False):\n    name: Optional[str]\n    surname: str\n\n\nclass User(TypedDict):\n    __pydantic_config__ = ConfigDict(extra='forbid')\n\n    identity: UserIdentity\n    age: int\n\n\nta = TypeAdapter(User)\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}\n    )\n)\n#> {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': None, 'surname': 'John'}, 'age': 37}\n    )\n)\n#> {'identity': {'name': None, 'surname': 'John'}, 'age': 37}\n\nprint(ta.validate_python({'identity': {}, 'age': 37}))\n#> {'identity': {}, 'age': 37}\n\n\ntry:\n    ta.validate_python(\n        {'identity': {'name': ['Smith'], 'surname': 'John'}, 'age': 24}\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    identity.name\n      Input should be a valid string [type=string_type, input_value=['Smith'], input_type=list]\n    \"\"\"\n\ntry:\n    ta.validate_python(\n        {\n            'identity': {'name': 'Smith', 'surname': 'John'},\n            'age': '37',\n            'email': 'john.smith@me.com',\n        }\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    email\n      Extra inputs are not permitted [type=extra_forbidden, input_value='john.smith@me.com', input_type=str]\n    \"\"\" from typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, ValidationError\n\n\n# `total=False` means keys are non-required\nclass UserIdentity(TypedDict, total=False):\n    name: str | None\n    surname: str\n\n\nclass User(TypedDict):\n    __pydantic_config__ = ConfigDict(extra='forbid')\n\n    identity: UserIdentity\n    age: int\n\n\nta = TypeAdapter(User)\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}\n    )\n)\n#> {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': None, 'surname': 'John'}, 'age': 37}\n    )\n)\n#> {'identity': {'name': None, 'surname': 'John'}, 'age': 37}\n\nprint(ta.validate_python({'identity': {}, 'age': 37}))\n#> {'identity': {}, 'age': 37}\n\n\ntry:\n    ta.validate_python(\n        {'identity': {'name': ['Smith'], 'surname': 'John'}, 'age': 24}\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    identity.name\n      Input should be a valid string [type=string_type, input_value=['Smith'], input_type=list]\n    \"\"\"\n\ntry:\n    ta.validate_python(\n        {\n            'identity': {'name': 'Smith', 'surname': 'John'},\n            'age': '37',\n            'email': 'john.smith@me.com',\n        }\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    email\n      Extra inputs are not permitted [type=extra_forbidden, input_value='john.smith@me.com', input_type=str]\n    \"\"\" from typing import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, ValidationError\n\n\n# `total=False` means keys are non-required\nclass UserIdentity(TypedDict, total=False):\n    name: str | None\n    surname: str\n\n\nclass User(TypedDict):\n    __pydantic_config__ = ConfigDict(extra='forbid')\n\n    identity: UserIdentity\n    age: int\n\n\nta = TypeAdapter(User)\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}\n    )\n)\n#> {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}\n\nprint(\n    ta.validate_python(\n        {'identity': {'name': None, 'surname': 'John'}, 'age': 37}\n    )\n)\n#> {'identity': {'name': None, 'surname': 'John'}, 'age': 37}\n\nprint(ta.validate_python({'identity': {}, 'age': 37}))\n#> {'identity': {}, 'age': 37}\n\n\ntry:\n    ta.validate_python(\n        {'identity': {'name': ['Smith'], 'surname': 'John'}, 'age': 24}\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    identity.name\n      Input should be a valid string [type=string_type, input_value=['Smith'], input_type=list]\n    \"\"\"\n\ntry:\n    ta.validate_python(\n        {\n            'identity': {'name': 'Smith', 'surname': 'John'},\n            'age': '37',\n            'email': 'john.smith@me.com',\n        }\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    email\n      Extra inputs are not permitted [type=extra_forbidden, input_value='john.smith@me.com', input_type=str]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typeddict","title":"Standard Library Types - Mapping Types - TypedDict","objectID":"/latest/api/standard_library_types/#typeddict","rank":-70},{"content":"See below for more detail on parsing and validation Fields can also be of type : Python 3.9 and above Python 3.10 and above from typing import Callable\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    callback: Callable[[int], int]\n\n\nm = Foo(callback=lambda x: x)\nprint(m)\n#> callback= at 0x0123456789ab> from collections.abc import Callable\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    callback: Callable[[int], int]\n\n\nm = Foo(callback=lambda x: x)\nprint(m)\n#> callback= at 0x0123456789ab> Warning Callable fields only perform a simple check that the argument is\ncallable; no validation of arguments, their types, or the return\ntype is performed.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#callable","title":"Standard Library Types - Callable","objectID":"/latest/api/standard_library_types/#callable","rank":-75},{"content":": Uses the type itself for validation by passing the value to IPv4Address(v) . : Uses the type itself for validation by passing the value to IPv4Address(v) . : Uses the type itself for validation by passing the value to IPv4Network(v) . : Uses the type itself for validation by passing the value to IPv6Address(v) . : Uses the type itself for validation by passing the value to IPv6Interface(v) . : Uses the type itself for validation by passing the value to IPv6Network(v) . See Network Types for other custom IP address types.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#ip-address-types","title":"Standard Library Types - IP Address Types","objectID":"/latest/api/standard_library_types/#ip-address-types","rank":-80},{"content":"For UUID, Pydantic tries to use the type itself for validation by passing the value to UUID(v) .\nThere's a fallback to UUID(bytes=v) for bytes and bytearray . In case you want to constrain the UUID version, you can check the following types: : requires UUID version 1. : requires UUID version 3. : requires UUID version 4. : requires UUID version 5.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#uuid","title":"Standard Library Types - UUID","objectID":"/latest/api/standard_library_types/#uuid","rank":-85},{"content":"Pydantic has extensive support for union validation, both  and Python 3.10's pipe syntax ( A | B ) are supported.\nRead more in the Unions section of the concepts docs.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#union","title":"Standard Library Types - Union","objectID":"/latest/api/standard_library_types/#union","rank":-90},{"content":"Pydantic supports the use of type[T] to specify that a field may only accept classes (not instances)\nthat are subclasses of T . from pydantic import BaseModel, ValidationError\n\n\nclass Foo:\n    pass\n\n\nclass Bar(Foo):\n    pass\n\n\nclass Other:\n    pass\n\n\nclass SimpleModel(BaseModel):\n    just_subclasses: type[Foo]\n\n\nSimpleModel(just_subclasses=Foo)\nSimpleModel(just_subclasses=Bar)\ntry:\n    SimpleModel(just_subclasses=Other)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for SimpleModel\n    just_subclasses\n      Input should be a subclass of Foo [type=is_subclass_of, input_value= , input_type=type]\n    \"\"\" You may also use type to specify that any class is allowed. from pydantic import BaseModel, ValidationError\n\n\nclass Foo:\n    pass\n\n\nclass LenientSimpleModel(BaseModel):\n    any_class_goes: type\n\n\nLenientSimpleModel(any_class_goes=int)\nLenientSimpleModel(any_class_goes=Foo)\ntry:\n    LenientSimpleModel(any_class_goes=Foo())\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for LenientSimpleModel\n    any_class_goes\n      Input should be a type [type=is_type, input_value=<__main__.Foo object at 0x0123456789ab>, input_type=Foo]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#type","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#type","rank":-95},{"content":"is supported either unconstrained, constrained or with a bound. from typing import TypeVar\n\nfrom pydantic import BaseModel\n\nFoobar = TypeVar('Foobar')\nBoundFloat = TypeVar('BoundFloat', bound=float)\nIntStr = TypeVar('IntStr', int, str)\n\n\nclass Model(BaseModel):\n    a: Foobar  # equivalent of \": Any\"\n    b: BoundFloat  # equivalent of \": float\"\n    c: IntStr  # equivalent of \": Union[int, str]\"\n\n\nprint(Model(a=[1], b=4.2, c='x'))\n#> a=[1] b=4.2 c='x'\n\n# a may be None\nprint(Model(a=None, b=1, c=1))\n#> a=None b=1.0 c=1","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingtypevar","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#typingtypevar","rank":-100},{"content":", type(None) , or Literal[None] are all equivalent according to the typing specification .\nAllows only None value.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#none-types","title":"Standard Library Types - None Types","objectID":"/latest/api/standard_library_types/#none-types","rank":-105},{"content":": Strings are accepted as-is. and  are converted using the  method. Enums inheriting from  are converted using the  attribute. All other types cause an error. Strings aren't Sequences While instances of str are technically valid instances of the Sequence[str] protocol from a type-checker's point of\nview, this is frequently not intended as is a common source of bugs. As a result, Pydantic raises a ValidationError if you attempt to pass a str or bytes instance into a field of type Sequence[str] or Sequence[bytes] : Python 3.9 and above Python 3.10 and above from collections.abc import Sequence\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    sequence_of_strs: Optional[Sequence[str]] = None\n    sequence_of_bytes: Optional[Sequence[bytes]] = None\n\n\nprint(Model(sequence_of_strs=['a', 'bc']).sequence_of_strs)\n#> ['a', 'bc']\nprint(Model(sequence_of_strs=('a', 'bc')).sequence_of_strs)\n#> ('a', 'bc')\nprint(Model(sequence_of_bytes=[b'a', b'bc']).sequence_of_bytes)\n#> [b'a', b'bc']\nprint(Model(sequence_of_bytes=(b'a', b'bc')).sequence_of_bytes)\n#> (b'a', b'bc')\n\n\ntry:\n    Model(sequence_of_strs='abc')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    sequence_of_strs\n      'str' instances are not allowed as a Sequence value [type=sequence_str, input_value='abc', input_type=str]\n    \"\"\"\ntry:\n    Model(sequence_of_bytes=b'abc')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    sequence_of_bytes\n      'bytes' instances are not allowed as a Sequence value [type=sequence_str, input_value=b'abc', input_type=bytes]\n    \"\"\" from collections.abc import Sequence\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    sequence_of_strs: Sequence[str] | None = None\n    sequence_of_bytes: Sequence[bytes] | None = None\n\n\nprint(Model(sequence_of_strs=['a', 'bc']).sequence_of_strs)\n#> ['a', 'bc']\nprint(Model(sequence_of_strs=('a', 'bc')).sequence_of_strs)\n#> ('a', 'bc')\nprint(Model(sequence_of_bytes=[b'a', b'bc']).sequence_of_bytes)\n#> [b'a', b'bc']\nprint(Model(sequence_of_bytes=(b'a', b'bc')).sequence_of_bytes)\n#> (b'a', b'bc')\n\n\ntry:\n    Model(sequence_of_strs='abc')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    sequence_of_strs\n      'str' instances are not allowed as a Sequence value [type=sequence_str, input_value='abc', input_type=str]\n    \"\"\"\ntry:\n    Model(sequence_of_bytes=b'abc')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    sequence_of_bytes\n      'bytes' instances are not allowed as a Sequence value [type=sequence_str, input_value=b'abc', input_type=bytes]\n    \"\"\"","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#strings","title":"Standard Library Types - Strings","objectID":"/latest/api/standard_library_types/#strings","rank":-110},{"content":"are accepted as-is.  is converted using bytes(v) . str are converted using v.encode() . int , float , and Decimal are coerced using str(v).encode() . See ByteSize for more details.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#bytes","title":"Standard Library Types - Bytes","objectID":"/latest/api/standard_library_types/#bytes","rank":-115},{"content":"Pydantic supports the use of  as a lightweight way to specify that a field may accept only specific literal values: from typing import Literal\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Pie(BaseModel):\n    flavor: Literal['apple', 'pumpkin']\n\n\nPie(flavor='apple')\nPie(flavor='pumpkin')\ntry:\n    Pie(flavor='cherry')\nexcept ValidationError as e:\n    print(str(e))\n    \"\"\"\n    1 validation error for Pie\n    flavor\n      Input should be 'apple' or 'pumpkin' [type=literal_error, input_value='cherry', input_type=str]\n    \"\"\" One benefit of this field type is that it can be used to check for equality with one or more specific values\nwithout needing to declare custom validators: Python 3.9 and above Python 3.10 and above from typing import ClassVar, Literal, Union\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Cake(BaseModel):\n    kind: Literal['cake']\n    required_utensils: ClassVar[list[str]] = ['fork', 'knife']\n\n\nclass IceCream(BaseModel):\n    kind: Literal['icecream']\n    required_utensils: ClassVar[list[str]] = ['spoon']\n\n\nclass Meal(BaseModel):\n    dessert: Union[Cake, IceCream]\n\n\nprint(type(Meal(dessert={'kind': 'cake'}).dessert).__name__)\n#> Cake\nprint(type(Meal(dessert={'kind': 'icecream'}).dessert).__name__)\n#> IceCream\ntry:\n    Meal(dessert={'kind': 'pie'})\nexcept ValidationError as e:\n    print(str(e))\n    \"\"\"\n    2 validation errors for Meal\n    dessert.Cake.kind\n      Input should be 'cake' [type=literal_error, input_value='pie', input_type=str]\n    dessert.IceCream.kind\n      Input should be 'icecream' [type=literal_error, input_value='pie', input_type=str]\n    \"\"\" from typing import ClassVar, Literal\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Cake(BaseModel):\n    kind: Literal['cake']\n    required_utensils: ClassVar[list[str]] = ['fork', 'knife']\n\n\nclass IceCream(BaseModel):\n    kind: Literal['icecream']\n    required_utensils: ClassVar[list[str]] = ['spoon']\n\n\nclass Meal(BaseModel):\n    dessert: Cake | IceCream\n\n\nprint(type(Meal(dessert={'kind': 'cake'}).dessert).__name__)\n#> Cake\nprint(type(Meal(dessert={'kind': 'icecream'}).dessert).__name__)\n#> IceCream\ntry:\n    Meal(dessert={'kind': 'pie'})\nexcept ValidationError as e:\n    print(str(e))\n    \"\"\"\n    2 validation errors for Meal\n    dessert.Cake.kind\n      Input should be 'cake' [type=literal_error, input_value='pie', input_type=str]\n    dessert.IceCream.kind\n      Input should be 'icecream' [type=literal_error, input_value='pie', input_type=str]\n    \"\"\" With proper ordering in an annotated Union , you can use this to parse types of decreasing specificity: Python 3.9 and above Python 3.10 and above from typing import Literal, Optional, Union\n\nfrom pydantic import BaseModel\n\n\nclass Dessert(BaseModel):\n    kind: str\n\n\nclass Pie(Dessert):\n    kind: Literal['pie']\n    flavor: Optional[str]\n\n\nclass ApplePie(Pie):\n    flavor: Literal['apple']\n\n\nclass PumpkinPie(Pie):\n    flavor: Literal['pumpkin']\n\n\nclass Meal(BaseModel):\n    dessert: Union[ApplePie, PumpkinPie, Pie, Dessert]\n\n\nprint(type(Meal(dessert={'kind': 'pie', 'flavor': 'apple'}).dessert).__name__)\n#> ApplePie\nprint(type(Meal(dessert={'kind': 'pie', 'flavor': 'pumpkin'}).dessert).__name__)\n#> PumpkinPie\nprint(type(Meal(dessert={'kind': 'pie'}).dessert).__name__)\n#> Dessert\nprint(type(Meal(dessert={'kind': 'cake'}).dessert).__name__)\n#> Dessert from typing import Literal\n\nfrom pydantic import BaseModel\n\n\nclass Dessert(BaseModel):\n    kind: str\n\n\nclass Pie(Dessert):\n    kind: Literal['pie']\n    flavor: str | None\n\n\nclass ApplePie(Pie):\n    flavor: Literal['apple']\n\n\nclass PumpkinPie(Pie):\n    flavor: Literal['pumpkin']\n\n\nclass Meal(BaseModel):\n    dessert: ApplePie | PumpkinPie | Pie | Dessert\n\n\nprint(type(Meal(dessert={'kind': 'pie', 'flavor': 'apple'}).dessert).__name__)\n#> ApplePie\nprint(type(Meal(dessert={'kind': 'pie', 'flavor': 'pumpkin'}).dessert).__name__)\n#> PumpkinPie\nprint(type(Meal(dessert={'kind': 'pie'}).dessert).__name__)\n#> Dessert\nprint(type(Meal(dessert={'kind': 'cake'}).dessert).__name__)\n#> Dessert","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingliteral","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#typingliteral","rank":-120},{"content":"Allows any value, including None .","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingany","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#typingany","rank":-125},{"content":"From Python, supports any data that passes an isinstance(v, Hashable) check. From JSON, first loads the data via an Any validator, then checks if the data is hashable with isinstance(v, Hashable) .","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typinghashable","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#typinghashable","rank":-130},{"content":"Allows wrapping another type with arbitrary metadata, as per PEP-593 . The Annotated hint may contain a single call to the Field function , but otherwise the additional metadata is ignored and the root type is used.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingannotated","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#typingannotated","rank":-135},{"content":"Will cause the input value to be passed to re.compile(v) to create a regular expression pattern.","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#typingpattern","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#typingpattern","rank":-140},{"content":"Simply uses the type itself for validation by passing the value to Path(v) .","pageID":"Standard Library Types","abs_url":"/latest/api/standard_library_types/#pathlibpath","title":"Standard Library Types - ","objectID":"/latest/api/standard_library_types/#pathlibpath","rank":-145},{"content":"Bases: [] Usage Documentation TypeAdapter Type adapters provide a flexible way to perform validation and serialization based on a Python type. A TypeAdapter instance exposes some of the functionality from BaseModel instance methods\nfor types that do not have such methods (such as dataclasses, primitive types, and more). Note: TypeAdapter instances are not types, and cannot be used as type annotations for fields. Parameters: Name Type Description Default type The type associated with the TypeAdapter . required config | None Configuration for the TypeAdapter , should be a dictionary conforming to\n. Note You cannot provide a configuration when instantiating a TypeAdapter if the type you're using\nhas its own config that cannot be overridden (ex: BaseModel , TypedDict , and dataclass ). A type-adapter-config-unused error will\nbe raised in this case. None _parent_depth Depth at which to search for the . This frame is used when\nresolving forward annotations during schema building, by looking for the globals and locals of this\nframe. Defaults to 2, which will result in the frame where the TypeAdapter was instantiated. Note This parameter is named with an underscore to suggest its private nature and discourage use.\nIt may be deprecated in a minor version, so we only recommend using it if you're comfortable\nwith potential change in behavior/support. It's default value is 2 because internally,\nthe TypeAdapter class makes another call to fetch the frame. 2 module | None The module that passes to plugin if provided. None Attributes: Name Type Description The core schema for the type. | The schema validator for the type. The schema serializer for the type. Whether the core schema for the type is successfully built. rebuild ¶ rebuild(\n    *,\n    force:  = False,\n    raise_errors:  = True,\n    _parent_namespace_depth:  = 2,\n    _types_namespace:  | None = None\n) ->  | None Try to rebuild the pydantic-core schema for the adapter's type. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\nthe initial attempt to build the schema, and automatic rebuilding fails. Parameters: Name Type Description Default force Whether to force the rebuilding of the type adapter's schema, defaults to False . False raise_errors Whether to raise errors, defaults to True . True _parent_namespace_depth Depth at which to search for the . This\nframe is used when resolving forward annotations during schema rebuilding, by looking for\nthe locals of this frame. Defaults to 2, which will result in the frame where the method\nwas called. 2 _types_namespace | None An explicit types namespace to use, instead of using the local namespace\nfrom the parent frame. Defaults to None . None Returns: Type Description | None Returns None if the schema is already \"complete\" and rebuilding was not required. | None If rebuilding was required, returns True if rebuilding was successful, otherwise False . validate_python ¶ validate_python(\n    object: ,\n    /,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context: [, ] | None = None,\n    experimental_allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None,\n) -> Validate a Python object against the model. Parameters: Name Type Description Default object The Python object to validate against the model. required strict | None Whether to strictly check types. None from_attributes | None Whether to extract data from object attributes. None context [, ] | None Additional context to pass to the validator. None experimental_allow_partial | ['off', 'on', 'trailing-strings'] Experimental whether to enable partial validation , e.g. to process streams.\n* False / 'off': Default behavior, no partial validation.\n* True / 'on': Enable partial validation.\n* 'trailing-strings': Enable partial validation and allow trailing strings in the input. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Note When using TypeAdapter with a Pydantic dataclass , the use of the from_attributes argument is not supported. Returns: Type Description The validated object. validate_json ¶ validate_json(\n    data:  |  | ,\n    /,\n    *,\n    strict:  | None = None,\n    context: [, ] | None = None,\n    experimental_allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None,\n) -> Usage Documentation JSON Parsing Validate a JSON string or bytes against the model. Parameters: Name Type Description Default data |  | The JSON data to validate against the model. required strict | None Whether to strictly check types. None context [, ] | None Additional context to use during validation. None experimental_allow_partial | ['off', 'on', 'trailing-strings'] Experimental whether to enable partial validation , e.g. to process streams.\n* False / 'off': Default behavior, no partial validation.\n* True / 'on': Enable partial validation.\n* 'trailing-strings': Enable partial validation and allow trailing strings in the input. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated object. validate_strings ¶ validate_strings(\n    obj: ,\n    /,\n    *,\n    strict:  | None = None,\n    context: [, ] | None = None,\n    experimental_allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None,\n) -> Validate object contains string data against the model. Parameters: Name Type Description Default obj The object contains string data to validate. required strict | None Whether to strictly check types. None context [, ] | None Additional context to use during validation. None experimental_allow_partial | ['off', 'on', 'trailing-strings'] Experimental whether to enable partial validation , e.g. to process streams.\n* False / 'off': Default behavior, no partial validation.\n* True / 'on': Enable partial validation.\n* 'trailing-strings': Enable partial validation and allow trailing strings in the input. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated object. get_default_value ¶ get_default_value(\n    *,\n    strict:  | None = None,\n    context: [, ] | None = None\n) -> [] | None Get the default value for the wrapped type. Parameters: Name Type Description Default strict | None Whether to strictly check types. None context [, ] | None Additional context to pass to the validator. None Returns: Type Description [] | None The default value wrapped in a Some if there is one or None if not. dump_python ¶ dump_python(\n    instance: ,\n    /,\n    *,\n    mode: [\"json\", \"python\"] = \"python\",\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context: [, ] | None = None,\n) -> Dump an instance of the adapted type to a Python object. Parameters: Name Type Description Default instance The Python object to serialize. required mode ['json', 'python'] The output format. 'python' include | None Fields to include in the output. None exclude | None Fields to exclude from the output. None by_alias | None Whether to use alias names for field names. None exclude_unset Whether to exclude unset fields. False exclude_defaults Whether to exclude fields with default values. False exclude_none Whether to exclude fields with None values. False round_trip Whether to output the serialized data in a way that is compatible with deserialization. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context [, ] | None Additional context to pass to the serializer. None Returns: Type Description The serialized object. dump_json ¶ dump_json(\n    instance: ,\n    /,\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context: [, ] | None = None,\n) -> Usage Documentation JSON Serialization Serialize an instance of the adapted type to JSON. Parameters: Name Type Description Default instance The instance to be serialized. required indent | None Number of spaces for JSON indentation. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None Fields to include. None exclude | None Fields to exclude. None by_alias | None Whether to use alias names for field names. None exclude_unset Whether to exclude unset fields. False exclude_defaults Whether to exclude fields with default values. False exclude_none Whether to exclude fields with a value of None . False round_trip Whether to serialize and deserialize the instance to ensure round-tripping. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context [, ] | None Additional context to pass to the serializer. None Returns: Type Description The JSON representation of the given instance as bytes. json_schema ¶ json_schema(\n    *,\n    by_alias:  = True,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n    mode:  = \"validation\"\n) -> [, ] Generate a JSON schema for the adapted type. Parameters: Name Type Description Default by_alias Whether to use alias names for field names. True ref_template The format string used for generating $ref strings. schema_generator [] The generator class used for creating the schema. mode The mode to use for schema generation. 'validation' Returns: Type Description [, ] The JSON schema for the model as a dictionary. json_schemas staticmethod ¶ json_schemas(\n    inputs: [\n        [\n            , , []\n        ]\n    ],\n    /,\n    *,\n    by_alias:  = True,\n    title:  | None = None,\n    description:  | None = None,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n) -> [\n    [\n        [, ],\n        ,\n    ],\n    ,\n] Generate a JSON schema including definitions from multiple type adapters. Parameters: Name Type Description Default inputs [[, , []]] Inputs to schema generation. The first two items will form the keys of the (first)\noutput mapping; the type adapters will provide the core schemas that get converted into\ndefinitions in the output JSON schema. required by_alias Whether to use alias names. True title | None The title for the schema. None description | None The description for the schema. None ref_template The format string used for generating $ref strings. schema_generator [] The generator class used for creating the schema. Returns: Type Description [[[, ], ], ] A tuple where: The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n    JsonRef references to definitions that are defined in the second returned element.) The second element is a JSON schema containing all definitions referenced in the first returned\n    element, along with the optional title and description keys.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#TypeAdapter","title":"TypeAdapter","objectID":"/latest/api/type_adapter/#TypeAdapter","rank":100},{"content":"rebuild(\n    *,\n    force:  = False,\n    raise_errors:  = True,\n    _parent_namespace_depth:  = 2,\n    _types_namespace:  | None = None\n) ->  | None Try to rebuild the pydantic-core schema for the adapter's type. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\nthe initial attempt to build the schema, and automatic rebuilding fails. Parameters: Name Type Description Default force Whether to force the rebuilding of the type adapter's schema, defaults to False . False raise_errors Whether to raise errors, defaults to True . True _parent_namespace_depth Depth at which to search for the . This\nframe is used when resolving forward annotations during schema rebuilding, by looking for\nthe locals of this frame. Defaults to 2, which will result in the frame where the method\nwas called. 2 _types_namespace | None An explicit types namespace to use, instead of using the local namespace\nfrom the parent frame. Defaults to None . None Returns: Type Description | None Returns None if the schema is already \"complete\" and rebuilding was not required. | None If rebuilding was required, returns True if rebuilding was successful, otherwise False .","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.rebuild","title":"TypeAdapter - rebuild","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.rebuild","rank":95},{"content":"validate_python(\n    object: ,\n    /,\n    *,\n    strict:  | None = None,\n    from_attributes:  | None = None,\n    context: [, ] | None = None,\n    experimental_allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None,\n) -> Validate a Python object against the model. Parameters: Name Type Description Default object The Python object to validate against the model. required strict | None Whether to strictly check types. None from_attributes | None Whether to extract data from object attributes. None context [, ] | None Additional context to pass to the validator. None experimental_allow_partial | ['off', 'on', 'trailing-strings'] Experimental whether to enable partial validation , e.g. to process streams.\n* False / 'off': Default behavior, no partial validation.\n* True / 'on': Enable partial validation.\n* 'trailing-strings': Enable partial validation and allow trailing strings in the input. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Note When using TypeAdapter with a Pydantic dataclass , the use of the from_attributes argument is not supported. Returns: Type Description The validated object.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_python","title":"TypeAdapter - validate_python","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_python","rank":90},{"content":"validate_json(\n    data:  |  | ,\n    /,\n    *,\n    strict:  | None = None,\n    context: [, ] | None = None,\n    experimental_allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None,\n) -> Usage Documentation JSON Parsing Validate a JSON string or bytes against the model. Parameters: Name Type Description Default data |  | The JSON data to validate against the model. required strict | None Whether to strictly check types. None context [, ] | None Additional context to use during validation. None experimental_allow_partial | ['off', 'on', 'trailing-strings'] Experimental whether to enable partial validation , e.g. to process streams.\n* False / 'off': Default behavior, no partial validation.\n* True / 'on': Enable partial validation.\n* 'trailing-strings': Enable partial validation and allow trailing strings in the input. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated object.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json","title":"TypeAdapter - validate_json","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json","rank":85},{"content":"validate_strings(\n    obj: ,\n    /,\n    *,\n    strict:  | None = None,\n    context: [, ] | None = None,\n    experimental_allow_partial: (\n         | [\"off\", \"on\", \"trailing-strings\"]\n    ) = False,\n    by_alias:  | None = None,\n    by_name:  | None = None,\n) -> Validate object contains string data against the model. Parameters: Name Type Description Default obj The object contains string data to validate. required strict | None Whether to strictly check types. None context [, ] | None Additional context to use during validation. None experimental_allow_partial | ['off', 'on', 'trailing-strings'] Experimental whether to enable partial validation , e.g. to process streams.\n* False / 'off': Default behavior, no partial validation.\n* True / 'on': Enable partial validation.\n* 'trailing-strings': Enable partial validation and allow trailing strings in the input. False by_alias | None Whether to use the field's alias when validating against the provided input data. None by_name | None Whether to use the field's name when validating against the provided input data. None Returns: Type Description The validated object.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_strings","title":"TypeAdapter - validate_strings","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_strings","rank":80},{"content":"get_default_value(\n    *,\n    strict:  | None = None,\n    context: [, ] | None = None\n) -> [] | None Get the default value for the wrapped type. Parameters: Name Type Description Default strict | None Whether to strictly check types. None context [, ] | None Additional context to pass to the validator. None Returns: Type Description [] | None The default value wrapped in a Some if there is one or None if not.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.get_default_value","title":"TypeAdapter - get_default_value","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.get_default_value","rank":75},{"content":"dump_python(\n    instance: ,\n    /,\n    *,\n    mode: [\"json\", \"python\"] = \"python\",\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context: [, ] | None = None,\n) -> Dump an instance of the adapted type to a Python object. Parameters: Name Type Description Default instance The Python object to serialize. required mode ['json', 'python'] The output format. 'python' include | None Fields to include in the output. None exclude | None Fields to exclude from the output. None by_alias | None Whether to use alias names for field names. None exclude_unset Whether to exclude unset fields. False exclude_defaults Whether to exclude fields with default values. False exclude_none Whether to exclude fields with None values. False round_trip Whether to output the serialized data in a way that is compatible with deserialization. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context [, ] | None Additional context to pass to the serializer. None Returns: Type Description The serialized object.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.dump_python","title":"TypeAdapter - dump_python","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.dump_python","rank":70},{"content":"dump_json(\n    instance: ,\n    /,\n    *,\n    indent:  | None = None,\n    ensure_ascii:  = False,\n    include:  | None = None,\n    exclude:  | None = None,\n    by_alias:  | None = None,\n    exclude_unset:  = False,\n    exclude_defaults:  = False,\n    exclude_none:  = False,\n    round_trip:  = False,\n    warnings: (\n         | [\"none\", \"warn\", \"error\"]\n    ) = True,\n    fallback: [[], ] | None = None,\n    serialize_as_any:  = False,\n    context: [, ] | None = None,\n) -> Usage Documentation JSON Serialization Serialize an instance of the adapted type to JSON. Parameters: Name Type Description Default instance The instance to be serialized. required indent | None Number of spaces for JSON indentation. None ensure_ascii If True , the output is guaranteed to have all incoming non-ASCII characters escaped.\nIf False (the default), these characters will be output as-is. False include | None Fields to include. None exclude | None Fields to exclude. None by_alias | None Whether to use alias names for field names. None exclude_unset Whether to exclude unset fields. False exclude_defaults Whether to exclude fields with default values. False exclude_none Whether to exclude fields with a value of None . False round_trip Whether to serialize and deserialize the instance to ensure round-tripping. False warnings | ['none', 'warn', 'error'] How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n\"error\" raises a . True fallback [[], ] | None A function to call when an unknown value is encountered. If not provided,\na  error is raised. None serialize_as_any Whether to serialize fields with duck-typing serialization behavior. False context [, ] | None Additional context to pass to the serializer. None Returns: Type Description The JSON representation of the given instance as bytes.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.dump_json","title":"TypeAdapter - dump_json","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.dump_json","rank":65},{"content":"json_schema(\n    *,\n    by_alias:  = True,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n    mode:  = \"validation\"\n) -> [, ] Generate a JSON schema for the adapted type. Parameters: Name Type Description Default by_alias Whether to use alias names for field names. True ref_template The format string used for generating $ref strings. schema_generator [] The generator class used for creating the schema. mode The mode to use for schema generation. 'validation' Returns: Type Description [, ] The JSON schema for the model as a dictionary.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.json_schema","title":"TypeAdapter - json_schema","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.json_schema","rank":60},{"content":"json_schemas(\n    inputs: [\n        [\n            , , []\n        ]\n    ],\n    /,\n    *,\n    by_alias:  = True,\n    title:  | None = None,\n    description:  | None = None,\n    ref_template:  = ,\n    schema_generator: [\n        \n    ] = ,\n) -> [\n    [\n        [, ],\n        ,\n    ],\n    ,\n] Generate a JSON schema including definitions from multiple type adapters. Parameters: Name Type Description Default inputs [[, , []]] Inputs to schema generation. The first two items will form the keys of the (first)\noutput mapping; the type adapters will provide the core schemas that get converted into\ndefinitions in the output JSON schema. required by_alias Whether to use alias names. True title | None The title for the schema. None description | None The description for the schema. None ref_template The format string used for generating $ref strings. schema_generator [] The generator class used for creating the schema. Returns: Type Description [[[, ], ], ] A tuple where: The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n    JsonRef references to definitions that are defined in the second returned element.) The second element is a JSON schema containing all definitions referenced in the first returned\n    element, along with the optional title and description keys.","pageID":"TypeAdapter","abs_url":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.json_schemas","title":"TypeAdapter - json_schemas  staticmethod","objectID":"/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.json_schemas","rank":55},{"content":"pydantic.types ¶ The types module contains custom types used by pydantic. StrictBool module-attribute ¶ StrictBool = [, ()] A boolean that must be either True or False . PositiveInt module-attribute ¶ PositiveInt = [, (0)] An integer that must be greater than zero. from pydantic import BaseModel, PositiveInt, ValidationError\n\nclass Model(BaseModel):\n    positive_int: PositiveInt\n\nm = Model(positive_int=1)\nprint(repr(m))\n#> Model(positive_int=1)\n\ntry:\n    Model(positive_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_int',),\n            'msg': 'Input should be greater than 0',\n            'input': -1,\n            'ctx': {'gt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' NegativeInt module-attribute ¶ NegativeInt = [, (0)] An integer that must be less than zero. from pydantic import BaseModel, NegativeInt, ValidationError\n\nclass Model(BaseModel):\n    negative_int: NegativeInt\n\nm = Model(negative_int=-1)\nprint(repr(m))\n#> Model(negative_int=-1)\n\ntry:\n    Model(negative_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_int',),\n            'msg': 'Input should be less than 0',\n            'input': 1,\n            'ctx': {'lt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    ''' NonPositiveInt module-attribute ¶ NonPositiveInt = [, (0)] An integer that must be less than or equal to zero. from pydantic import BaseModel, NonPositiveInt, ValidationError\n\nclass Model(BaseModel):\n    non_positive_int: NonPositiveInt\n\nm = Model(non_positive_int=0)\nprint(repr(m))\n#> Model(non_positive_int=0)\n\ntry:\n    Model(non_positive_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_int',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1,\n            'ctx': {'le': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    ''' NonNegativeInt module-attribute ¶ NonNegativeInt = [, (0)] An integer that must be greater than or equal to zero. from pydantic import BaseModel, NonNegativeInt, ValidationError\n\nclass Model(BaseModel):\n    non_negative_int: NonNegativeInt\n\nm = Model(non_negative_int=0)\nprint(repr(m))\n#> Model(non_negative_int=0)\n\ntry:\n    Model(non_negative_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_int',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1,\n            'ctx': {'ge': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    ''' StrictInt module-attribute ¶ StrictInt = [, ()] An integer that must be validated in strict mode. from pydantic import BaseModel, StrictInt, ValidationError\n\nclass StrictIntModel(BaseModel):\n    strict_int: StrictInt\n\ntry:\n    StrictIntModel(strict_int=3.14159)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictIntModel\n    strict_int\n      Input should be a valid integer [type=int_type, input_value=3.14159, input_type=float]\n    ''' PositiveFloat module-attribute ¶ PositiveFloat = [, (0)] A float that must be greater than zero. from pydantic import BaseModel, PositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    positive_float: PositiveFloat\n\nm = Model(positive_float=1.0)\nprint(repr(m))\n#> Model(positive_float=1.0)\n\ntry:\n    Model(positive_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_float',),\n            'msg': 'Input should be greater than 0',\n            'input': -1.0,\n            'ctx': {'gt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' NegativeFloat module-attribute ¶ NegativeFloat = [, (0)] A float that must be less than zero. from pydantic import BaseModel, NegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    negative_float: NegativeFloat\n\nm = Model(negative_float=-1.0)\nprint(repr(m))\n#> Model(negative_float=-1.0)\n\ntry:\n    Model(negative_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_float',),\n            'msg': 'Input should be less than 0',\n            'input': 1.0,\n            'ctx': {'lt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    ''' NonPositiveFloat module-attribute ¶ NonPositiveFloat = [, (0)] A float that must be less than or equal to zero. from pydantic import BaseModel, NonPositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    non_positive_float: NonPositiveFloat\n\nm = Model(non_positive_float=0.0)\nprint(repr(m))\n#> Model(non_positive_float=0.0)\n\ntry:\n    Model(non_positive_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_float',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1.0,\n            'ctx': {'le': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    ''' NonNegativeFloat module-attribute ¶ NonNegativeFloat = [, (0)] A float that must be greater than or equal to zero. from pydantic import BaseModel, NonNegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    non_negative_float: NonNegativeFloat\n\nm = Model(non_negative_float=0.0)\nprint(repr(m))\n#> Model(non_negative_float=0.0)\n\ntry:\n    Model(non_negative_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_float',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1.0,\n            'ctx': {'ge': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    ''' StrictFloat module-attribute ¶ StrictFloat = [, (True)] A float that must be validated in strict mode. from pydantic import BaseModel, StrictFloat, ValidationError\n\nclass StrictFloatModel(BaseModel):\n    strict_float: StrictFloat\n\ntry:\n    StrictFloatModel(strict_float='1.0')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictFloatModel\n    strict_float\n      Input should be a valid number [type=float_type, input_value='1.0', input_type=str]\n    ''' FiniteFloat module-attribute ¶ FiniteFloat = [, (False)] A float that must be finite (not -inf , inf , or nan ). from pydantic import BaseModel, FiniteFloat\n\nclass Model(BaseModel):\n    finite: FiniteFloat\n\nm = Model(finite=1.0)\nprint(m)\n#> finite=1.0 StrictBytes module-attribute ¶ StrictBytes = [, ()] A bytes that must be validated in strict mode. StrictStr module-attribute ¶ StrictStr = [, ()] A string that must be validated in strict mode. UUID1 module-attribute ¶ UUID1 = [, (1)] A UUID that must be version 1. import uuid\n\nfrom pydantic import UUID1, BaseModel\n\nclass Model(BaseModel):\n    uuid1: UUID1\n\nModel(uuid1=uuid.uuid1()) UUID3 module-attribute ¶ UUID3 = [, (3)] A UUID that must be version 3. import uuid\n\nfrom pydantic import UUID3, BaseModel\n\nclass Model(BaseModel):\n    uuid3: UUID3\n\nModel(uuid3=uuid.uuid3(uuid.NAMESPACE_DNS, 'pydantic.org')) UUID4 module-attribute ¶ UUID4 = [, (4)] A UUID that must be version 4. import uuid\n\nfrom pydantic import UUID4, BaseModel\n\nclass Model(BaseModel):\n    uuid4: UUID4\n\nModel(uuid4=uuid.uuid4()) UUID5 module-attribute ¶ UUID5 = [, (5)] A UUID that must be version 5. import uuid\n\nfrom pydantic import UUID5, BaseModel\n\nclass Model(BaseModel):\n    uuid5: UUID5\n\nModel(uuid5=uuid.uuid5(uuid.NAMESPACE_DNS, 'pydantic.org')) UUID6 module-attribute ¶ UUID6 = [, (6)] A UUID that must be version 6. import uuid\n\nfrom pydantic import UUID6, BaseModel\n\nclass Model(BaseModel):\n    uuid6: UUID6\n\nModel(uuid6=uuid.UUID('1efea953-c2d6-6790-aa0a-69db8c87df97')) UUID7 module-attribute ¶ UUID7 = [, (7)] A UUID that must be version 7. import uuid\n\nfrom pydantic import UUID7, BaseModel\n\nclass Model(BaseModel):\n    uuid7: UUID7\n\nModel(uuid7=uuid.UUID('0194fdcb-1c47-7a09-b52c-561154de0b4a')) UUID8 module-attribute ¶ UUID8 = [, (8)] A UUID that must be version 8. import uuid\n\nfrom pydantic import UUID8, BaseModel\n\nclass Model(BaseModel):\n    uuid8: UUID8\n\nModel(uuid8=uuid.UUID('81a0b92e-6078-8551-9c81-8ccb666bdab8')) FilePath module-attribute ¶ FilePath = [, ('file')] A path that must point to a file. from pathlib import Path\n\nfrom pydantic import BaseModel, FilePath, ValidationError\n\nclass Model(BaseModel):\n    f: FilePath\n\npath = Path('text.txt')\npath.touch()\nm = Model(f='text.txt')\nprint(m.model_dump())\n#> {'f': PosixPath('text.txt')}\npath.unlink()\n\npath = Path('directory')\npath.mkdir(exist_ok=True)\ntry:\n    Model(f='directory')  # directory\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='directory', input_type=str]\n    '''\npath.rmdir()\n\ntry:\n    Model(f='not-exists-file')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='not-exists-file', input_type=str]\n    ''' DirectoryPath module-attribute ¶ DirectoryPath = [, ('dir')] A path that must point to a directory. from pathlib import Path\n\nfrom pydantic import BaseModel, DirectoryPath, ValidationError\n\nclass Model(BaseModel):\n    f: DirectoryPath\n\npath = Path('directory/')\npath.mkdir()\nm = Model(f='directory/')\nprint(m.model_dump())\n#> {'f': PosixPath('directory')}\npath.rmdir()\n\npath = Path('file.txt')\npath.touch()\ntry:\n    Model(f='file.txt')  # file\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='file.txt', input_type=str]\n    '''\npath.unlink()\n\ntry:\n    Model(f='not-exists-directory')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='not-exists-directory', input_type=str]\n    ''' NewPath module-attribute ¶ NewPath = [, ('new')] A path for a new file or directory that must not already exist. The parent directory must already exist. SocketPath module-attribute ¶ SocketPath = [, ('socket')] A path to an existing socket file Base64Bytes module-attribute ¶ Base64Bytes = [\n    , (=)\n] A bytes type that is encoded and decoded using the standard (non-URL-safe) base64 encoder. from pydantic import Base64Bytes, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_bytes: Base64Bytes\n\n# Initialize the model with base64 data\nm = Model(base64_bytes=b'VGhpcyBpcyB0aGUgd2F5')\n\n# Access decoded value\nprint(m.base64_bytes)\n#> b'This is the way'\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_bytes': b'VGhpcyBpcyB0aGUgd2F5'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_bytes=b'undecodable').base64_bytes)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_bytes\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value=b'undecodable', input_type=bytes]\n    ''' Base64Str module-attribute ¶ Base64Str = [\n    , (=)\n] A str type that is encoded and decoded using the standard (non-URL-safe) base64 encoder. from pydantic import Base64Str, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_str: Base64Str\n\n# Initialize the model with base64 data\nm = Model(base64_str='VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y')\n\n# Access decoded value\nprint(m.base64_str)\n#> These aren't the droids you're looking for\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_str': 'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_str='undecodable').base64_str)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_str\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value='undecodable', input_type=str]\n    ''' Base64UrlBytes module-attribute ¶ Base64UrlBytes = [\n    , (=)\n] A bytes type that is encoded and decoded using the URL-safe base64 encoder. from pydantic import Base64UrlBytes, BaseModel\n\nclass Model(BaseModel):\n    base64url_bytes: Base64UrlBytes\n\n# Initialize the model with base64 data\nm = Model(base64url_bytes=b'SHc_dHc-TXc==')\nprint(m)\n#> base64url_bytes=b'Hw?tw>Mw' Base64UrlStr module-attribute ¶ Base64UrlStr = [\n    , (=)\n] A str type that is encoded and decoded using the URL-safe base64 encoder. from pydantic import Base64UrlStr, BaseModel\n\nclass Model(BaseModel):\n    base64url_str: Base64UrlStr\n\n# Initialize the model with base64 data\nm = Model(base64url_str='SHc_dHc-TXc==')\nprint(m)\n#> base64url_str='Hw?tw>Mw' JsonValue module-attribute ¶ JsonValue:  = [\n    [\"JsonValue\"],\n    [, \"JsonValue\"],\n    ,\n    ,\n    ,\n    ,\n    None,\n] A JsonValue is used to represent a value that can be serialized to JSON. It may be one of: list['JsonValue'] dict[str, 'JsonValue'] str bool int float None The following example demonstrates how to use JsonValue to validate JSON data,\nand what kind of errors to expect when input data is not json serializable. import json\n\nfrom pydantic import BaseModel, JsonValue, ValidationError\n\nclass Model(BaseModel):\n    j: JsonValue\n\nvalid_json_data = {'j': {'a': {'b': {'c': 1, 'd': [2, None]}}}}\ninvalid_json_data = {'j': {'a': {'b': ...}}}\n\nprint(repr(Model.model_validate(valid_json_data)))\n#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\nprint(repr(Model.model_validate_json(json.dumps(valid_json_data))))\n#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\n\ntry:\n    Model.model_validate(invalid_json_data)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    j.dict.a.dict.b\n      input was not a valid JSON value [type=invalid-json-value, input_value=Ellipsis, input_type=ellipsis]\n    ''' OnErrorOmit module-attribute ¶ OnErrorOmit = [, ] When used as an item in a list, the key type in a dict, optional values of a TypedDict, etc.\nthis annotation omits the item from the iteration if there is any error validating it.\nThat is, instead of a  being propagated up and the entire iterable being discarded\nany invalid items are discarded and the valid ones are returned. Strict dataclass ¶ Bases: , Usage Documentation Strict Mode with Annotated Strict A field metadata class to indicate that a field should be validated in strict mode.\nUse this class as an annotation via Annotated , as seen below. Attributes: Name Type Description Whether to validate the field in strict mode. AllowInfNan dataclass ¶ Bases: A field metadata class to indicate that a field should allow -inf , inf , and nan . Use this class as an annotation via Annotated , as seen below. Attributes: Name Type Description Whether to allow -inf , inf , and nan . Defaults to True . StringConstraints dataclass ¶ Bases: Usage Documentation StringConstraints A field metadata class to apply constraints to str types.\nUse this class as an annotation via Annotated , as seen below. Attributes: Name Type Description | None Whether to remove leading and trailing whitespace. | None Whether to convert the string to uppercase. | None Whether to convert the string to lowercase. | None Whether to validate the string in strict mode. | None The minimum length of the string. | None The maximum length of the string. | [] | None A regex pattern that the string must match. ImportString ¶ A type that can be used to import a Python object from a string. ImportString expects a string and loads the Python object importable at that dotted path.\nAttributes of modules may be separated from the module by : or . , e.g. if 'math:cos' is provided,\nthe resulting field value would be the function cos . If a . is used and both an attribute and submodule\nare present at the same path, the module will be preferred. On model instantiation, pointers will be evaluated and imported. There is\nsome nuance to this behavior, demonstrated in the examples below. import math\n\nfrom pydantic import BaseModel, Field, ImportString, ValidationError\n\nclass ImportThings(BaseModel):\n    obj: ImportString\n\n# A string value will cause an automatic import\nmy_cos = ImportThings(obj='math.cos')\n\n# You can use the imported function as you would expect\ncos_of_0 = my_cos.obj(0)\nassert cos_of_0 == 1\n\n# A string whose value cannot be imported will raise an error\ntry:\n    ImportThings(obj='foo.bar')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ImportThings\n    obj\n      Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]\n    '''\n\n# Actual python objects can be assigned as well\nmy_cos = ImportThings(obj=math.cos)\nmy_cos_2 = ImportThings(obj='math.cos')\nmy_cos_3 = ImportThings(obj='math:cos')\nassert my_cos == my_cos_2 == my_cos_3\n\n# You can set default field value either as Python object:\nclass ImportThingsDefaultPyObj(BaseModel):\n    obj: ImportString = math.cos\n\n# or as a string value (but only if used with `validate_default=True`)\nclass ImportThingsDefaultString(BaseModel):\n    obj: ImportString = Field(default='math.cos', validate_default=True)\n\nmy_cos_default1 = ImportThingsDefaultPyObj()\nmy_cos_default2 = ImportThingsDefaultString()\nassert my_cos_default1.obj == my_cos_default2.obj == math.cos\n\n# note: this will not work!\nclass ImportThingsMissingValidateDefault(BaseModel):\n    obj: ImportString = 'math.cos'\n\nmy_cos_default3 = ImportThingsMissingValidateDefault()\nassert my_cos_default3.obj == 'math.cos'  # just string, not evaluated Serializing an ImportString type to json is also possible. from pydantic import BaseModel, ImportString\n\nclass ImportThings(BaseModel):\n    obj: ImportString\n\n# Create an instance\nm = ImportThings(obj='math.cos')\nprint(m)\n#> obj= print(m.model_dump_json())\n#> {\"obj\":\"math.cos\"} UuidVersion dataclass ¶ A field metadata class to indicate a UUID version. Use this class as an annotation via Annotated , as seen below. Attributes: Name Type Description [1, 3, 4, 5, 6, 7, 8] The version of the UUID. Must be one of 1, 3, 4, 5, 6, 7 or 8. Json ¶ A special type wrapper which loads JSON before parsing. You can use the Json data type to make Pydantic first load a raw JSON string before\nvalidating the loaded data into the parametrized type: from typing import Any\n\nfrom pydantic import BaseModel, Json, ValidationError\n\nclass AnyJsonModel(BaseModel):\n    json_obj: Json[Any]\n\nclass ConstrainedJsonModel(BaseModel):\n    json_obj: Json[list[int]]\n\nprint(AnyJsonModel(json_obj='{\"b\": 1}'))\n#> json_obj={'b': 1}\nprint(ConstrainedJsonModel(json_obj='[1, 2, 3]'))\n#> json_obj=[1, 2, 3]\n\ntry:\n    ConstrainedJsonModel(json_obj=12)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ConstrainedJsonModel\n    json_obj\n      JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]\n    '''\n\ntry:\n    ConstrainedJsonModel(json_obj='[a, b]')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ConstrainedJsonModel\n    json_obj\n      Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]\n    '''\n\ntry:\n    ConstrainedJsonModel(json_obj='[\"a\", \"b\"]')\nexcept ValidationError as e:\n    print(e)\n    '''\n    2 validation errors for ConstrainedJsonModel\n    json_obj.0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    json_obj.1\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]\n    ''' When you dump the model using model_dump or model_dump_json , the dumped value will be the result of validation,\nnot the original JSON string. However, you can use the argument round_trip=True to get the original JSON string back: from pydantic import BaseModel, Json\n\nclass ConstrainedJsonModel(BaseModel):\n    json_obj: Json[list[int]]\n\nprint(ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json())\n#> {\"json_obj\":[1,2,3]}\nprint(\n    ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json(round_trip=True)\n)\n#> {\"json_obj\":\"[1,2,3]\"} Secret ¶ Bases: [] A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks. You may either directly parametrize Secret with a type, or subclass from Secret with a parametrized type. The benefit of subclassing\nis that you can define a custom _display method, which will be used for repr() and str() methods. The examples below demonstrate both\nways of using Secret to create a new secret type. Directly parametrizing Secret with a type: from pydantic import BaseModel, Secret\n\nSecretBool = Secret[bool]\n\nclass Model(BaseModel):\n    secret_bool: SecretBool\n\nm = Model(secret_bool=True)\nprint(m.model_dump())\n#> {'secret_bool': Secret('**********')}\n\nprint(m.model_dump_json())\n#> {\"secret_bool\":\"**********\"}\n\nprint(m.secret_bool.get_secret_value())\n#> True Subclassing from parametrized Secret : from datetime import date\n\nfrom pydantic import BaseModel, Secret\n\nclass SecretDate(Secret[date]):\n    def _display(self) -> str:\n        return '****/**/**'\n\nclass Model(BaseModel):\n    secret_date: SecretDate\n\nm = Model(secret_date=date(2022, 1, 1))\nprint(m.model_dump())\n#> {'secret_date': SecretDate('****/**/**')}\n\nprint(m.model_dump_json())\n#> {\"secret_date\":\"****/**/**\"}\n\nprint(m.secret_date.get_secret_value())\n#> 2022-01-01 The value returned by the _display method will be used for repr() and str() . You can enforce constraints on the underlying type through annotations:\nFor example: from typing import Annotated\n\nfrom pydantic import BaseModel, Field, Secret, ValidationError\n\nSecretPosInt = Secret[Annotated[int, Field(gt=0, strict=True)]]\n\nclass Model(BaseModel):\n    sensitive_int: SecretPosInt\n\nm = Model(sensitive_int=42)\nprint(m.model_dump())\n#> {'sensitive_int': Secret('**********')}\n\ntry:\n    m = Model(sensitive_int=-42)  # (1)!\nexcept ValidationError as exc_info:\n    print(exc_info.errors(include_url=False, include_input=False))\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('sensitive_int',),\n            'msg': 'Input should be greater than 0',\n            'ctx': {'gt': 0},\n        }\n    ]\n    '''\n\ntry:\n    m = Model(sensitive_int='42')  # (2)!\nexcept ValidationError as exc_info:\n    print(exc_info.errors(include_url=False, include_input=False))\n    '''\n    [\n        {\n            'type': 'int_type',\n            'loc': ('sensitive_int',),\n            'msg': 'Input should be a valid integer',\n        }\n    ]\n    ''' The input value is not greater than 0, so it raises a validation error. The input value is not an integer, so it raises a validation error because the SecretPosInt type has strict mode enabled. SecretStr ¶ Bases: [] A string used for storing sensitive information that you do not want to be visible in logging or tracebacks. When the secret value is nonempty, it is displayed as '**********' instead of the underlying value in\ncalls to repr() and str() . If the value is empty, it is displayed as '' . from pydantic import BaseModel, SecretStr\n\nclass User(BaseModel):\n    username: str\n    password: SecretStr\n\nuser = User(username='scolvin', password='password1')\n\nprint(user)\n#> username='scolvin' password=SecretStr('**********')\nprint(user.password.get_secret_value())\n#> password1\nprint((SecretStr('password'), SecretStr('')))\n#> (SecretStr('**********'), SecretStr('')) As seen above, by default,  (and )\nwill be serialized as ********** when serializing to json. You can use the  to dump the\nsecret as plain-text when serializing to json. from pydantic import BaseModel, SecretBytes, SecretStr, field_serializer\n\nclass Model(BaseModel):\n    password: SecretStr\n    password_bytes: SecretBytes\n\n    @field_serializer('password', 'password_bytes', when_used='json')\n    def dump_secret(self, v):\n        return v.get_secret_value()\n\nmodel = Model(password='IAmSensitive', password_bytes=b'IAmSensitiveBytes')\nprint(model)\n#> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')\nprint(model.password)\n#> **********\nprint(model.model_dump())\n'''\n{\n    'password': SecretStr('**********'),\n    'password_bytes': SecretBytes(b'**********'),\n}\n'''\nprint(model.model_dump_json())\n#> {\"password\":\"IAmSensitive\",\"password_bytes\":\"IAmSensitiveBytes\"} SecretBytes ¶ Bases: [] A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks. It displays b'**********' instead of the string value on repr() and str() calls.\nWhen the secret value is nonempty, it is displayed as b'**********' instead of the underlying value in\ncalls to repr() and str() . If the value is empty, it is displayed as b'' . from pydantic import BaseModel, SecretBytes\n\nclass User(BaseModel):\n    username: str\n    password: SecretBytes\n\nuser = User(username='scolvin', password=b'password1')\n#> username='scolvin' password=SecretBytes(b'**********')\nprint(user.password.get_secret_value())\n#> b'password1'\nprint((SecretBytes(b'password'), SecretBytes(b'')))\n#> (SecretBytes(b'**********'), SecretBytes(b'')) PaymentCardNumber ¶ Bases: Based on: https://en.wikipedia.org/wiki/Payment_card_number. masked property ¶ masked: Mask all but the last 4 digits of the card number. Returns: Type Description A masked card number string. validate classmethod ¶ validate(\n    input_value: , /, _: \n) -> Validate the card number and return a PaymentCardNumber instance. validate_digits classmethod ¶ validate_digits(card_number: ) -> None Validate that the card number is all digits. validate_luhn_check_digit classmethod ¶ validate_luhn_check_digit(card_number: ) -> Based on: https://en.wikipedia.org/wiki/Luhn_algorithm. validate_brand staticmethod ¶ validate_brand(card_number: ) -> Validate length based on BIN for major brands:\nhttps://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN). ByteSize ¶ Bases: Converts a string representing a number of bytes with units (such as '1KB' or '11.5MiB' ) into an integer. You can use the ByteSize data type to (case-insensitively) convert a string representation of a number of bytes into\nan integer, and also to print out human-readable strings representing a number of bytes. In conformance with IEC 80000-13 Standard we interpret '1KB' to mean 1000 bytes,\nand '1KiB' to mean 1024 bytes. In general, including a middle 'i' will cause the unit to be interpreted as a power of 2,\nrather than a power of 10 (so, for example, '1 MB' is treated as 1_000_000 bytes, whereas '1 MiB' is treated as 1_048_576 bytes). Info Note that 1b will be parsed as \"1 byte\" and not \"1 bit\". from pydantic import BaseModel, ByteSize\n\nclass MyModel(BaseModel):\n    size: ByteSize\n\nprint(MyModel(size=52000).size)\n#> 52000\nprint(MyModel(size='3000 KiB').size)\n#> 3072000\n\nm = MyModel(size='50 PB')\nprint(m.size.human_readable())\n#> 44.4PiB\nprint(m.size.human_readable(decimal=True))\n#> 50.0PB\nprint(m.size.human_readable(separator=' '))\n#> 44.4 PiB\n\nprint(m.size.to('TiB'))\n#> 45474.73508864641 human_readable ¶ human_readable(\n    decimal:  = False, separator:  = \"\"\n) -> Converts a byte size to a human readable string. Parameters: Name Type Description Default decimal If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units\n(e.g. 1024 bytes per KiB). False separator A string used to split the value and unit. Defaults to an empty string (''). '' Returns: Type Description A human readable string representation of the byte size. to ¶ to(unit: ) -> Converts a byte size to another unit, including both byte and bit units. Parameters: Name Type Description Default unit The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,\nKiB, MiB, GiB, TiB, PiB, EiB (byte units) and\nbit, kbit, mbit, gbit, tbit, pbit, ebit,\nkibit, mibit, gibit, tibit, pibit, eibit (bit units). required Returns: Type Description The byte size in the new unit. PastDate ¶ A date in the past. FutureDate ¶ A date in the future. AwareDatetime ¶ A datetime that requires timezone info. NaiveDatetime ¶ A datetime that doesn't require timezone info. PastDatetime ¶ A datetime that must be in the past. FutureDatetime ¶ A datetime that must be in the future. EncoderProtocol ¶ Bases: Protocol for encoding and decoding data to and from bytes. decode classmethod ¶ decode(data: ) -> Decode the data using the encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data using the encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> Get the JSON format for the encoded data. Returns: Type Description The JSON format for the encoded data. Base64Encoder ¶ Bases: Standard (non-URL-safe) Base64 encoder. decode classmethod ¶ decode(data: ) -> Decode the data from base64 encoded bytes to original bytes data. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data from bytes to a base64 encoded bytes. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> ['base64'] Get the JSON format for the encoded data. Returns: Type Description ['base64'] The JSON format for the encoded data. Base64UrlEncoder ¶ Bases: URL-safe Base64 encoder. decode classmethod ¶ decode(data: ) -> Decode the data from base64 encoded bytes to original bytes data. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data from bytes to a base64 encoded bytes. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> ['base64url'] Get the JSON format for the encoded data. Returns: Type Description ['base64url'] The JSON format for the encoded data. EncodedBytes dataclass ¶ A bytes type that is encoded and decoded using the specified encoder. EncodedBytes needs an encoder that implements EncoderProtocol to operate. from typing import Annotated\n\nfrom pydantic import BaseModel, EncodedBytes, EncoderProtocol, ValidationError\n\nclass MyEncoder(EncoderProtocol):\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        if data == b'**undecodable**':\n            raise ValueError('Cannot decode data')\n        return data[13:]\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        return b'**encoded**: ' + value\n\n    @classmethod\n    def get_json_format(cls) -> str:\n        return 'my-encoder'\n\nMyEncodedBytes = Annotated[bytes, EncodedBytes(encoder=MyEncoder)]\n\nclass Model(BaseModel):\n    my_encoded_bytes: MyEncodedBytes\n\n# Initialize the model with encoded data\nm = Model(my_encoded_bytes=b'**encoded**: some bytes')\n\n# Access decoded value\nprint(m.my_encoded_bytes)\n#> b'some bytes'\n\n# Serialize into the encoded form\nprint(m.model_dump())\n#> {'my_encoded_bytes': b'**encoded**: some bytes'}\n\n# Validate encoded data\ntry:\n    Model(my_encoded_bytes=b'**undecodable**')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    my_encoded_bytes\n      Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]\n    ''' decode ¶ decode(data: , _: ) -> Decode the data using the specified encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode ¶ encode(value: ) -> Encode the data using the specified encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. EncodedStr dataclass ¶ A str type that is encoded and decoded using the specified encoder. EncodedStr needs an encoder that implements EncoderProtocol to operate. from typing import Annotated\n\nfrom pydantic import BaseModel, EncodedStr, EncoderProtocol, ValidationError\n\nclass MyEncoder(EncoderProtocol):\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        if data == b'**undecodable**':\n            raise ValueError('Cannot decode data')\n        return data[13:]\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        return b'**encoded**: ' + value\n\n    @classmethod\n    def get_json_format(cls) -> str:\n        return 'my-encoder'\n\nMyEncodedStr = Annotated[str, EncodedStr(encoder=MyEncoder)]\n\nclass Model(BaseModel):\n    my_encoded_str: MyEncodedStr\n\n# Initialize the model with encoded data\nm = Model(my_encoded_str='**encoded**: some str')\n\n# Access decoded value\nprint(m.my_encoded_str)\n#> some str\n\n# Serialize into the encoded form\nprint(m.model_dump())\n#> {'my_encoded_str': '**encoded**: some str'}\n\n# Validate encoded data\ntry:\n    Model(my_encoded_str='**undecodable**')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    my_encoded_str\n      Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]\n    ''' decode_str ¶ decode_str(data: , _: ) -> Decode the data using the specified encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode_str ¶ encode_str(value: ) -> Encode the data using the specified encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. GetPydanticSchema dataclass ¶ Usage Documentation Using GetPydanticSchema to Reduce Boilerplate A convenience class for creating an annotation that provides pydantic custom type hooks. This class is intended to eliminate the need to create a custom \"marker\" which defines the __get_pydantic_core_schema__ and __get_pydantic_json_schema__ custom hook methods. For example, to have a field treated by type checkers as int , but by pydantic as Any , you can do: from typing import Annotated, Any\n\nfrom pydantic import BaseModel, GetPydanticSchema\n\nHandleAsAny = GetPydanticSchema(lambda _s, h: h(Any))\n\nclass Model(BaseModel):\n    x: Annotated[int, HandleAsAny]  # pydantic sees `x: Any`\n\nprint(repr(Model(x='abc').x))\n#> 'abc' Tag dataclass ¶ Provides a way to specify the expected tag to use for a case of a (callable) discriminated union. Also provides a way to label a union case in error messages. When using a callable Discriminator , attach a Tag to each case in the Union to specify the tag that\nshould be used to identify that case. For example, in the below example, the Tag is used to specify that\nif get_discriminator_value returns 'apple' , the input should be validated as an ApplePie , and if it\nreturns 'pumpkin' , the input should be validated as a PumpkinPie . The primary role of the Tag here is to map the return value from the callable Discriminator function to\nthe appropriate member of the Union in question. from typing import Annotated, Any, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        Union[\n            Annotated[ApplePie, Tag('apple')],\n            Annotated[PumpkinPie, Tag('pumpkin')],\n        ],\n        Discriminator(get_discriminator_value),\n    ]\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n'''\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n'''\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n'''\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n''' Note You must specify a Tag for every case in a Tag that is associated with a\ncallable Discriminator . Failing to do so will result in a PydanticUserError with code callable-discriminator-no-tag . See the Discriminated Unions concepts docs for more details on how to use Tag s. Discriminator dataclass ¶ Usage Documentation Discriminated Unions with Callable Discriminator Provides a way to use a custom callable as the way to extract the value of a union discriminator. This allows you to get validation behavior like you'd get from Field(discriminator=<field_name>) ,\nbut without needing to have a single shared field across all the union choices. This also makes it\npossible to handle unions of models and primitive types with discriminated-union-style validation errors.\nFinally, this allows you to use a custom callable as the way to identify which member of a union a value\nbelongs to, while still seeing all the performance benefits of a discriminated union. Consider this example, which is much more performant with the use of Discriminator and thus a TaggedUnion than it would be as a normal Union . from typing import Annotated, Any, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        Union[\n            Annotated[ApplePie, Tag('apple')],\n            Annotated[PumpkinPie, Tag('pumpkin')],\n        ],\n        Discriminator(get_discriminator_value),\n    ]\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n'''\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n'''\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n'''\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n''' See the Discriminated Unions concepts docs for more details on how to use Discriminator s. discriminator instance-attribute ¶ discriminator:  | [[], ] The callable or field name for discriminating the type in a tagged union. A Callable discriminator must extract the value of the discriminator from the input.\nA str discriminator must be the name of a field to discriminate against. custom_error_type class-attribute instance-attribute ¶ custom_error_type:  | None = None Type to use in custom errors replacing the standard discriminated union\nvalidation errors. custom_error_message class-attribute instance-attribute ¶ custom_error_message:  | None = None Message to use in custom errors. custom_error_context class-attribute instance-attribute ¶ custom_error_context: (\n    [,  |  | ] | None\n) = None Context to use in custom errors. FailFast dataclass ¶ Bases: , A FailFast annotation can be used to specify that validation should stop at the first error. This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not. You might want to enable this setting if you want to validate your data faster (basically, if you use this,\nvalidation will be more performant with the caveat that you get less information). from typing import Annotated\n\nfrom pydantic import BaseModel, FailFast, ValidationError\n\nclass Model(BaseModel):\n    x: Annotated[list[int], FailFast()]\n\n# This will raise a single error for the first invalid value and stop validation\ntry:\n    obj = Model(x=[1, 2, 'a', 4, 5, 'b', 7, 8, 9, 'c'])\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    x.2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    ''' conint ¶ conint(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None,\n    multiple_of:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that conint returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, conint\n\nclass Foo(BaseModel):\n    bar: conint(strict=True, gt=0) from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[int, Field(strict=True, gt=0)] A wrapper around int that allows for additional constraints. Parameters: Name Type Description Default strict | None Whether to validate the integer in strict mode. Defaults to None . None gt | None The value must be greater than this. None ge | None The value must be greater than or equal to this. None lt | None The value must be less than this. None le | None The value must be less than or equal to this. None multiple_of | None The value must be a multiple of this. None Returns: Type Description [] The wrapped integer type. from pydantic import BaseModel, ValidationError, conint\n\nclass ConstrainedExample(BaseModel):\n    constrained_int: conint(gt=1)\n\nm = ConstrainedExample(constrained_int=2)\nprint(repr(m))\n#> ConstrainedExample(constrained_int=2)\n\ntry:\n    ConstrainedExample(constrained_int=0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_int',),\n            'msg': 'Input should be greater than 1',\n            'input': 0,\n            'ctx': {'gt': 1},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' confloat ¶ confloat(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None,\n    multiple_of:  | None = None,\n    allow_inf_nan:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that confloat returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, confloat\n\nclass Foo(BaseModel):\n    bar: confloat(strict=True, gt=0) from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[float, Field(strict=True, gt=0)] A wrapper around float that allows for additional constraints. Parameters: Name Type Description Default strict | None Whether to validate the float in strict mode. None gt | None The value must be greater than this. None ge | None The value must be greater than or equal to this. None lt | None The value must be less than this. None le | None The value must be less than or equal to this. None multiple_of | None The value must be a multiple of this. None allow_inf_nan | None Whether to allow -inf , inf , and nan . None Returns: Type Description [] The wrapped float type. from pydantic import BaseModel, ValidationError, confloat\n\nclass ConstrainedExample(BaseModel):\n    constrained_float: confloat(gt=1.0)\n\nm = ConstrainedExample(constrained_float=1.1)\nprint(repr(m))\n#> ConstrainedExample(constrained_float=1.1)\n\ntry:\n    ConstrainedExample(constrained_float=0.9)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_float',),\n            'msg': 'Input should be greater than 1',\n            'input': 0.9,\n            'ctx': {'gt': 1.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' conbytes ¶ conbytes(\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    strict:  | None = None\n) -> [] A wrapper around bytes that allows for additional constraints. Parameters: Name Type Description Default min_length | None The minimum length of the bytes. None max_length | None The maximum length of the bytes. None strict | None Whether to validate the bytes in strict mode. None Returns: Type Description [] The wrapped bytes type. constr ¶ constr(\n    *,\n    strip_whitespace:  | None = None,\n    to_upper:  | None = None,\n    to_lower:  | None = None,\n    strict:  | None = None,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    pattern:  | [] | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that constr returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, constr\n\nclass Foo(BaseModel):\n    bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$') from typing import Annotated\n\nfrom pydantic import BaseModel, StringConstraints\n\nclass Foo(BaseModel):\n    bar: Annotated[\n        str,\n        StringConstraints(\n            strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$'\n        ),\n    ] A wrapper around str that allows for additional constraints. from pydantic import BaseModel, constr\n\nclass Foo(BaseModel):\n    bar: constr(strip_whitespace=True, to_upper=True)\n\nfoo = Foo(bar='  hello  ')\nprint(foo)\n#> bar='HELLO' Parameters: Name Type Description Default strip_whitespace | None Whether to remove leading and trailing whitespace. None to_upper | None Whether to turn all characters to uppercase. None to_lower | None Whether to turn all characters to lowercase. None strict | None Whether to validate the string in strict mode. None min_length | None The minimum length of the string. None max_length | None The maximum length of the string. None pattern | [] | None A regex pattern to validate the string against. None Returns: Type Description [] The wrapped string type. conset ¶ conset(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None\n) -> [[]] A wrapper around typing.Set that allows for additional constraints. Parameters: Name Type Description Default item_type [] The type of the items in the set. required min_length | None The minimum length of the set. None max_length | None The maximum length of the set. None Returns: Type Description [[]] The wrapped set type. confrozenset ¶ confrozenset(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None\n) -> [[]] A wrapper around typing.FrozenSet that allows for additional constraints. Parameters: Name Type Description Default item_type [] The type of the items in the frozenset. required min_length | None The minimum length of the frozenset. None max_length | None The maximum length of the frozenset. None Returns: Type Description [[]] The wrapped frozenset type. conlist ¶ conlist(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    unique_items:  | None = None\n) -> [[]] A wrapper around  that adds validation. Parameters: Name Type Description Default item_type [] The type of the items in the list. required min_length | None The minimum length of the list. Defaults to None. None max_length | None The maximum length of the list. Defaults to None. None unique_items | None Whether the items in the list must be unique. Defaults to None. Warning The unique_items parameter is deprecated, use Set instead.\nSee this issue for more details. None Returns: Type Description [[]] The wrapped list type. condecimal ¶ condecimal(\n    *,\n    strict:  | None = None,\n    gt:  |  | None = None,\n    ge:  |  | None = None,\n    lt:  |  | None = None,\n    le:  |  | None = None,\n    multiple_of:  |  | None = None,\n    max_digits:  | None = None,\n    decimal_places:  | None = None,\n    allow_inf_nan:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that condecimal returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, condecimal\n\nclass Foo(BaseModel):\n    bar: condecimal(strict=True, allow_inf_nan=True) from decimal import Decimal\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[Decimal, Field(strict=True, allow_inf_nan=True)] A wrapper around Decimal that adds validation. Parameters: Name Type Description Default strict | None Whether to validate the value in strict mode. Defaults to None . None gt |  | None The value must be greater than this. Defaults to None . None ge |  | None The value must be greater than or equal to this. Defaults to None . None lt |  | None The value must be less than this. Defaults to None . None le |  | None The value must be less than or equal to this. Defaults to None . None multiple_of |  | None The value must be a multiple of this. Defaults to None . None max_digits | None The maximum number of digits. Defaults to None . None decimal_places | None The number of decimal places. Defaults to None . None allow_inf_nan | None Whether to allow infinity and NaN. Defaults to None . None from decimal import Decimal\n\nfrom pydantic import BaseModel, ValidationError, condecimal\n\nclass ConstrainedExample(BaseModel):\n    constrained_decimal: condecimal(gt=Decimal('1.0'))\n\nm = ConstrainedExample(constrained_decimal=Decimal('1.1'))\nprint(repr(m))\n#> ConstrainedExample(constrained_decimal=Decimal('1.1'))\n\ntry:\n    ConstrainedExample(constrained_decimal=Decimal('0.9'))\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_decimal',),\n            'msg': 'Input should be greater than 1.0',\n            'input': Decimal('0.9'),\n            'ctx': {'gt': Decimal('1.0')},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' condate ¶ condate(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None\n) -> [] A wrapper for date that adds constraints. Parameters: Name Type Description Default strict | None Whether to validate the date value in strict mode. Defaults to None . None gt | None The value must be greater than this. Defaults to None . None ge | None The value must be greater than or equal to this. Defaults to None . None lt | None The value must be less than this. Defaults to None . None le | None The value must be less than or equal to this. Defaults to None . None Returns: Type Description [] A date type with the specified constraints.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#Pydantic Types","title":"Pydantic Types","objectID":"/latest/api/types/#Pydantic Types","rank":100},{"content":"The types module contains custom types used by pydantic. StrictBool module-attribute ¶ StrictBool = [, ()] A boolean that must be either True or False . PositiveInt module-attribute ¶ PositiveInt = [, (0)] An integer that must be greater than zero. from pydantic import BaseModel, PositiveInt, ValidationError\n\nclass Model(BaseModel):\n    positive_int: PositiveInt\n\nm = Model(positive_int=1)\nprint(repr(m))\n#> Model(positive_int=1)\n\ntry:\n    Model(positive_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_int',),\n            'msg': 'Input should be greater than 0',\n            'input': -1,\n            'ctx': {'gt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' NegativeInt module-attribute ¶ NegativeInt = [, (0)] An integer that must be less than zero. from pydantic import BaseModel, NegativeInt, ValidationError\n\nclass Model(BaseModel):\n    negative_int: NegativeInt\n\nm = Model(negative_int=-1)\nprint(repr(m))\n#> Model(negative_int=-1)\n\ntry:\n    Model(negative_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_int',),\n            'msg': 'Input should be less than 0',\n            'input': 1,\n            'ctx': {'lt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    ''' NonPositiveInt module-attribute ¶ NonPositiveInt = [, (0)] An integer that must be less than or equal to zero. from pydantic import BaseModel, NonPositiveInt, ValidationError\n\nclass Model(BaseModel):\n    non_positive_int: NonPositiveInt\n\nm = Model(non_positive_int=0)\nprint(repr(m))\n#> Model(non_positive_int=0)\n\ntry:\n    Model(non_positive_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_int',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1,\n            'ctx': {'le': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    ''' NonNegativeInt module-attribute ¶ NonNegativeInt = [, (0)] An integer that must be greater than or equal to zero. from pydantic import BaseModel, NonNegativeInt, ValidationError\n\nclass Model(BaseModel):\n    non_negative_int: NonNegativeInt\n\nm = Model(non_negative_int=0)\nprint(repr(m))\n#> Model(non_negative_int=0)\n\ntry:\n    Model(non_negative_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_int',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1,\n            'ctx': {'ge': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    ''' StrictInt module-attribute ¶ StrictInt = [, ()] An integer that must be validated in strict mode. from pydantic import BaseModel, StrictInt, ValidationError\n\nclass StrictIntModel(BaseModel):\n    strict_int: StrictInt\n\ntry:\n    StrictIntModel(strict_int=3.14159)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictIntModel\n    strict_int\n      Input should be a valid integer [type=int_type, input_value=3.14159, input_type=float]\n    ''' PositiveFloat module-attribute ¶ PositiveFloat = [, (0)] A float that must be greater than zero. from pydantic import BaseModel, PositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    positive_float: PositiveFloat\n\nm = Model(positive_float=1.0)\nprint(repr(m))\n#> Model(positive_float=1.0)\n\ntry:\n    Model(positive_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_float',),\n            'msg': 'Input should be greater than 0',\n            'input': -1.0,\n            'ctx': {'gt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' NegativeFloat module-attribute ¶ NegativeFloat = [, (0)] A float that must be less than zero. from pydantic import BaseModel, NegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    negative_float: NegativeFloat\n\nm = Model(negative_float=-1.0)\nprint(repr(m))\n#> Model(negative_float=-1.0)\n\ntry:\n    Model(negative_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_float',),\n            'msg': 'Input should be less than 0',\n            'input': 1.0,\n            'ctx': {'lt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    ''' NonPositiveFloat module-attribute ¶ NonPositiveFloat = [, (0)] A float that must be less than or equal to zero. from pydantic import BaseModel, NonPositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    non_positive_float: NonPositiveFloat\n\nm = Model(non_positive_float=0.0)\nprint(repr(m))\n#> Model(non_positive_float=0.0)\n\ntry:\n    Model(non_positive_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_float',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1.0,\n            'ctx': {'le': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    ''' NonNegativeFloat module-attribute ¶ NonNegativeFloat = [, (0)] A float that must be greater than or equal to zero. from pydantic import BaseModel, NonNegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    non_negative_float: NonNegativeFloat\n\nm = Model(non_negative_float=0.0)\nprint(repr(m))\n#> Model(non_negative_float=0.0)\n\ntry:\n    Model(non_negative_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_float',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1.0,\n            'ctx': {'ge': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    ''' StrictFloat module-attribute ¶ StrictFloat = [, (True)] A float that must be validated in strict mode. from pydantic import BaseModel, StrictFloat, ValidationError\n\nclass StrictFloatModel(BaseModel):\n    strict_float: StrictFloat\n\ntry:\n    StrictFloatModel(strict_float='1.0')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictFloatModel\n    strict_float\n      Input should be a valid number [type=float_type, input_value='1.0', input_type=str]\n    ''' FiniteFloat module-attribute ¶ FiniteFloat = [, (False)] A float that must be finite (not -inf , inf , or nan ). from pydantic import BaseModel, FiniteFloat\n\nclass Model(BaseModel):\n    finite: FiniteFloat\n\nm = Model(finite=1.0)\nprint(m)\n#> finite=1.0 StrictBytes module-attribute ¶ StrictBytes = [, ()] A bytes that must be validated in strict mode. StrictStr module-attribute ¶ StrictStr = [, ()] A string that must be validated in strict mode. UUID1 module-attribute ¶ UUID1 = [, (1)] A UUID that must be version 1. import uuid\n\nfrom pydantic import UUID1, BaseModel\n\nclass Model(BaseModel):\n    uuid1: UUID1\n\nModel(uuid1=uuid.uuid1()) UUID3 module-attribute ¶ UUID3 = [, (3)] A UUID that must be version 3. import uuid\n\nfrom pydantic import UUID3, BaseModel\n\nclass Model(BaseModel):\n    uuid3: UUID3\n\nModel(uuid3=uuid.uuid3(uuid.NAMESPACE_DNS, 'pydantic.org')) UUID4 module-attribute ¶ UUID4 = [, (4)] A UUID that must be version 4. import uuid\n\nfrom pydantic import UUID4, BaseModel\n\nclass Model(BaseModel):\n    uuid4: UUID4\n\nModel(uuid4=uuid.uuid4()) UUID5 module-attribute ¶ UUID5 = [, (5)] A UUID that must be version 5. import uuid\n\nfrom pydantic import UUID5, BaseModel\n\nclass Model(BaseModel):\n    uuid5: UUID5\n\nModel(uuid5=uuid.uuid5(uuid.NAMESPACE_DNS, 'pydantic.org')) UUID6 module-attribute ¶ UUID6 = [, (6)] A UUID that must be version 6. import uuid\n\nfrom pydantic import UUID6, BaseModel\n\nclass Model(BaseModel):\n    uuid6: UUID6\n\nModel(uuid6=uuid.UUID('1efea953-c2d6-6790-aa0a-69db8c87df97')) UUID7 module-attribute ¶ UUID7 = [, (7)] A UUID that must be version 7. import uuid\n\nfrom pydantic import UUID7, BaseModel\n\nclass Model(BaseModel):\n    uuid7: UUID7\n\nModel(uuid7=uuid.UUID('0194fdcb-1c47-7a09-b52c-561154de0b4a')) UUID8 module-attribute ¶ UUID8 = [, (8)] A UUID that must be version 8. import uuid\n\nfrom pydantic import UUID8, BaseModel\n\nclass Model(BaseModel):\n    uuid8: UUID8\n\nModel(uuid8=uuid.UUID('81a0b92e-6078-8551-9c81-8ccb666bdab8')) FilePath module-attribute ¶ FilePath = [, ('file')] A path that must point to a file. from pathlib import Path\n\nfrom pydantic import BaseModel, FilePath, ValidationError\n\nclass Model(BaseModel):\n    f: FilePath\n\npath = Path('text.txt')\npath.touch()\nm = Model(f='text.txt')\nprint(m.model_dump())\n#> {'f': PosixPath('text.txt')}\npath.unlink()\n\npath = Path('directory')\npath.mkdir(exist_ok=True)\ntry:\n    Model(f='directory')  # directory\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='directory', input_type=str]\n    '''\npath.rmdir()\n\ntry:\n    Model(f='not-exists-file')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='not-exists-file', input_type=str]\n    ''' DirectoryPath module-attribute ¶ DirectoryPath = [, ('dir')] A path that must point to a directory. from pathlib import Path\n\nfrom pydantic import BaseModel, DirectoryPath, ValidationError\n\nclass Model(BaseModel):\n    f: DirectoryPath\n\npath = Path('directory/')\npath.mkdir()\nm = Model(f='directory/')\nprint(m.model_dump())\n#> {'f': PosixPath('directory')}\npath.rmdir()\n\npath = Path('file.txt')\npath.touch()\ntry:\n    Model(f='file.txt')  # file\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='file.txt', input_type=str]\n    '''\npath.unlink()\n\ntry:\n    Model(f='not-exists-directory')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='not-exists-directory', input_type=str]\n    ''' NewPath module-attribute ¶ NewPath = [, ('new')] A path for a new file or directory that must not already exist. The parent directory must already exist. SocketPath module-attribute ¶ SocketPath = [, ('socket')] A path to an existing socket file Base64Bytes module-attribute ¶ Base64Bytes = [\n    , (=)\n] A bytes type that is encoded and decoded using the standard (non-URL-safe) base64 encoder. from pydantic import Base64Bytes, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_bytes: Base64Bytes\n\n# Initialize the model with base64 data\nm = Model(base64_bytes=b'VGhpcyBpcyB0aGUgd2F5')\n\n# Access decoded value\nprint(m.base64_bytes)\n#> b'This is the way'\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_bytes': b'VGhpcyBpcyB0aGUgd2F5'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_bytes=b'undecodable').base64_bytes)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_bytes\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value=b'undecodable', input_type=bytes]\n    ''' Base64Str module-attribute ¶ Base64Str = [\n    , (=)\n] A str type that is encoded and decoded using the standard (non-URL-safe) base64 encoder. from pydantic import Base64Str, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_str: Base64Str\n\n# Initialize the model with base64 data\nm = Model(base64_str='VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y')\n\n# Access decoded value\nprint(m.base64_str)\n#> These aren't the droids you're looking for\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_str': 'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_str='undecodable').base64_str)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_str\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value='undecodable', input_type=str]\n    ''' Base64UrlBytes module-attribute ¶ Base64UrlBytes = [\n    , (=)\n] A bytes type that is encoded and decoded using the URL-safe base64 encoder. from pydantic import Base64UrlBytes, BaseModel\n\nclass Model(BaseModel):\n    base64url_bytes: Base64UrlBytes\n\n# Initialize the model with base64 data\nm = Model(base64url_bytes=b'SHc_dHc-TXc==')\nprint(m)\n#> base64url_bytes=b'Hw?tw>Mw' Base64UrlStr module-attribute ¶ Base64UrlStr = [\n    , (=)\n] A str type that is encoded and decoded using the URL-safe base64 encoder. from pydantic import Base64UrlStr, BaseModel\n\nclass Model(BaseModel):\n    base64url_str: Base64UrlStr\n\n# Initialize the model with base64 data\nm = Model(base64url_str='SHc_dHc-TXc==')\nprint(m)\n#> base64url_str='Hw?tw>Mw' JsonValue module-attribute ¶ JsonValue:  = [\n    [\"JsonValue\"],\n    [, \"JsonValue\"],\n    ,\n    ,\n    ,\n    ,\n    None,\n] A JsonValue is used to represent a value that can be serialized to JSON. It may be one of: list['JsonValue'] dict[str, 'JsonValue'] str bool int float None The following example demonstrates how to use JsonValue to validate JSON data,\nand what kind of errors to expect when input data is not json serializable. import json\n\nfrom pydantic import BaseModel, JsonValue, ValidationError\n\nclass Model(BaseModel):\n    j: JsonValue\n\nvalid_json_data = {'j': {'a': {'b': {'c': 1, 'd': [2, None]}}}}\ninvalid_json_data = {'j': {'a': {'b': ...}}}\n\nprint(repr(Model.model_validate(valid_json_data)))\n#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\nprint(repr(Model.model_validate_json(json.dumps(valid_json_data))))\n#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\n\ntry:\n    Model.model_validate(invalid_json_data)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    j.dict.a.dict.b\n      input was not a valid JSON value [type=invalid-json-value, input_value=Ellipsis, input_type=ellipsis]\n    ''' OnErrorOmit module-attribute ¶ OnErrorOmit = [, ] When used as an item in a list, the key type in a dict, optional values of a TypedDict, etc.\nthis annotation omits the item from the iteration if there is any error validating it.\nThat is, instead of a  being propagated up and the entire iterable being discarded\nany invalid items are discarded and the valid ones are returned. Strict dataclass ¶ Bases: , Usage Documentation Strict Mode with Annotated Strict A field metadata class to indicate that a field should be validated in strict mode.\nUse this class as an annotation via Annotated , as seen below. Attributes: Name Type Description Whether to validate the field in strict mode. AllowInfNan dataclass ¶ Bases: A field metadata class to indicate that a field should allow -inf , inf , and nan . Use this class as an annotation via Annotated , as seen below. Attributes: Name Type Description Whether to allow -inf , inf , and nan . Defaults to True . StringConstraints dataclass ¶ Bases: Usage Documentation StringConstraints A field metadata class to apply constraints to str types.\nUse this class as an annotation via Annotated , as seen below. Attributes: Name Type Description | None Whether to remove leading and trailing whitespace. | None Whether to convert the string to uppercase. | None Whether to convert the string to lowercase. | None Whether to validate the string in strict mode. | None The minimum length of the string. | None The maximum length of the string. | [] | None A regex pattern that the string must match. ImportString ¶ A type that can be used to import a Python object from a string. ImportString expects a string and loads the Python object importable at that dotted path.\nAttributes of modules may be separated from the module by : or . , e.g. if 'math:cos' is provided,\nthe resulting field value would be the function cos . If a . is used and both an attribute and submodule\nare present at the same path, the module will be preferred. On model instantiation, pointers will be evaluated and imported. There is\nsome nuance to this behavior, demonstrated in the examples below. import math\n\nfrom pydantic import BaseModel, Field, ImportString, ValidationError\n\nclass ImportThings(BaseModel):\n    obj: ImportString\n\n# A string value will cause an automatic import\nmy_cos = ImportThings(obj='math.cos')\n\n# You can use the imported function as you would expect\ncos_of_0 = my_cos.obj(0)\nassert cos_of_0 == 1\n\n# A string whose value cannot be imported will raise an error\ntry:\n    ImportThings(obj='foo.bar')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ImportThings\n    obj\n      Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]\n    '''\n\n# Actual python objects can be assigned as well\nmy_cos = ImportThings(obj=math.cos)\nmy_cos_2 = ImportThings(obj='math.cos')\nmy_cos_3 = ImportThings(obj='math:cos')\nassert my_cos == my_cos_2 == my_cos_3\n\n# You can set default field value either as Python object:\nclass ImportThingsDefaultPyObj(BaseModel):\n    obj: ImportString = math.cos\n\n# or as a string value (but only if used with `validate_default=True`)\nclass ImportThingsDefaultString(BaseModel):\n    obj: ImportString = Field(default='math.cos', validate_default=True)\n\nmy_cos_default1 = ImportThingsDefaultPyObj()\nmy_cos_default2 = ImportThingsDefaultString()\nassert my_cos_default1.obj == my_cos_default2.obj == math.cos\n\n# note: this will not work!\nclass ImportThingsMissingValidateDefault(BaseModel):\n    obj: ImportString = 'math.cos'\n\nmy_cos_default3 = ImportThingsMissingValidateDefault()\nassert my_cos_default3.obj == 'math.cos'  # just string, not evaluated Serializing an ImportString type to json is also possible. from pydantic import BaseModel, ImportString\n\nclass ImportThings(BaseModel):\n    obj: ImportString\n\n# Create an instance\nm = ImportThings(obj='math.cos')\nprint(m)\n#> obj= print(m.model_dump_json())\n#> {\"obj\":\"math.cos\"} UuidVersion dataclass ¶ A field metadata class to indicate a UUID version. Use this class as an annotation via Annotated , as seen below. Attributes: Name Type Description [1, 3, 4, 5, 6, 7, 8] The version of the UUID. Must be one of 1, 3, 4, 5, 6, 7 or 8. Json ¶ A special type wrapper which loads JSON before parsing. You can use the Json data type to make Pydantic first load a raw JSON string before\nvalidating the loaded data into the parametrized type: from typing import Any\n\nfrom pydantic import BaseModel, Json, ValidationError\n\nclass AnyJsonModel(BaseModel):\n    json_obj: Json[Any]\n\nclass ConstrainedJsonModel(BaseModel):\n    json_obj: Json[list[int]]\n\nprint(AnyJsonModel(json_obj='{\"b\": 1}'))\n#> json_obj={'b': 1}\nprint(ConstrainedJsonModel(json_obj='[1, 2, 3]'))\n#> json_obj=[1, 2, 3]\n\ntry:\n    ConstrainedJsonModel(json_obj=12)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ConstrainedJsonModel\n    json_obj\n      JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]\n    '''\n\ntry:\n    ConstrainedJsonModel(json_obj='[a, b]')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ConstrainedJsonModel\n    json_obj\n      Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]\n    '''\n\ntry:\n    ConstrainedJsonModel(json_obj='[\"a\", \"b\"]')\nexcept ValidationError as e:\n    print(e)\n    '''\n    2 validation errors for ConstrainedJsonModel\n    json_obj.0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    json_obj.1\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]\n    ''' When you dump the model using model_dump or model_dump_json , the dumped value will be the result of validation,\nnot the original JSON string. However, you can use the argument round_trip=True to get the original JSON string back: from pydantic import BaseModel, Json\n\nclass ConstrainedJsonModel(BaseModel):\n    json_obj: Json[list[int]]\n\nprint(ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json())\n#> {\"json_obj\":[1,2,3]}\nprint(\n    ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json(round_trip=True)\n)\n#> {\"json_obj\":\"[1,2,3]\"} Secret ¶ Bases: [] A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks. You may either directly parametrize Secret with a type, or subclass from Secret with a parametrized type. The benefit of subclassing\nis that you can define a custom _display method, which will be used for repr() and str() methods. The examples below demonstrate both\nways of using Secret to create a new secret type. Directly parametrizing Secret with a type: from pydantic import BaseModel, Secret\n\nSecretBool = Secret[bool]\n\nclass Model(BaseModel):\n    secret_bool: SecretBool\n\nm = Model(secret_bool=True)\nprint(m.model_dump())\n#> {'secret_bool': Secret('**********')}\n\nprint(m.model_dump_json())\n#> {\"secret_bool\":\"**********\"}\n\nprint(m.secret_bool.get_secret_value())\n#> True Subclassing from parametrized Secret : from datetime import date\n\nfrom pydantic import BaseModel, Secret\n\nclass SecretDate(Secret[date]):\n    def _display(self) -> str:\n        return '****/**/**'\n\nclass Model(BaseModel):\n    secret_date: SecretDate\n\nm = Model(secret_date=date(2022, 1, 1))\nprint(m.model_dump())\n#> {'secret_date': SecretDate('****/**/**')}\n\nprint(m.model_dump_json())\n#> {\"secret_date\":\"****/**/**\"}\n\nprint(m.secret_date.get_secret_value())\n#> 2022-01-01 The value returned by the _display method will be used for repr() and str() . You can enforce constraints on the underlying type through annotations:\nFor example: from typing import Annotated\n\nfrom pydantic import BaseModel, Field, Secret, ValidationError\n\nSecretPosInt = Secret[Annotated[int, Field(gt=0, strict=True)]]\n\nclass Model(BaseModel):\n    sensitive_int: SecretPosInt\n\nm = Model(sensitive_int=42)\nprint(m.model_dump())\n#> {'sensitive_int': Secret('**********')}\n\ntry:\n    m = Model(sensitive_int=-42)  # (1)!\nexcept ValidationError as exc_info:\n    print(exc_info.errors(include_url=False, include_input=False))\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('sensitive_int',),\n            'msg': 'Input should be greater than 0',\n            'ctx': {'gt': 0},\n        }\n    ]\n    '''\n\ntry:\n    m = Model(sensitive_int='42')  # (2)!\nexcept ValidationError as exc_info:\n    print(exc_info.errors(include_url=False, include_input=False))\n    '''\n    [\n        {\n            'type': 'int_type',\n            'loc': ('sensitive_int',),\n            'msg': 'Input should be a valid integer',\n        }\n    ]\n    ''' The input value is not greater than 0, so it raises a validation error. The input value is not an integer, so it raises a validation error because the SecretPosInt type has strict mode enabled. SecretStr ¶ Bases: [] A string used for storing sensitive information that you do not want to be visible in logging or tracebacks. When the secret value is nonempty, it is displayed as '**********' instead of the underlying value in\ncalls to repr() and str() . If the value is empty, it is displayed as '' . from pydantic import BaseModel, SecretStr\n\nclass User(BaseModel):\n    username: str\n    password: SecretStr\n\nuser = User(username='scolvin', password='password1')\n\nprint(user)\n#> username='scolvin' password=SecretStr('**********')\nprint(user.password.get_secret_value())\n#> password1\nprint((SecretStr('password'), SecretStr('')))\n#> (SecretStr('**********'), SecretStr('')) As seen above, by default,  (and )\nwill be serialized as ********** when serializing to json. You can use the  to dump the\nsecret as plain-text when serializing to json. from pydantic import BaseModel, SecretBytes, SecretStr, field_serializer\n\nclass Model(BaseModel):\n    password: SecretStr\n    password_bytes: SecretBytes\n\n    @field_serializer('password', 'password_bytes', when_used='json')\n    def dump_secret(self, v):\n        return v.get_secret_value()\n\nmodel = Model(password='IAmSensitive', password_bytes=b'IAmSensitiveBytes')\nprint(model)\n#> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')\nprint(model.password)\n#> **********\nprint(model.model_dump())\n'''\n{\n    'password': SecretStr('**********'),\n    'password_bytes': SecretBytes(b'**********'),\n}\n'''\nprint(model.model_dump_json())\n#> {\"password\":\"IAmSensitive\",\"password_bytes\":\"IAmSensitiveBytes\"} SecretBytes ¶ Bases: [] A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks. It displays b'**********' instead of the string value on repr() and str() calls.\nWhen the secret value is nonempty, it is displayed as b'**********' instead of the underlying value in\ncalls to repr() and str() . If the value is empty, it is displayed as b'' . from pydantic import BaseModel, SecretBytes\n\nclass User(BaseModel):\n    username: str\n    password: SecretBytes\n\nuser = User(username='scolvin', password=b'password1')\n#> username='scolvin' password=SecretBytes(b'**********')\nprint(user.password.get_secret_value())\n#> b'password1'\nprint((SecretBytes(b'password'), SecretBytes(b'')))\n#> (SecretBytes(b'**********'), SecretBytes(b'')) PaymentCardNumber ¶ Bases: Based on: https://en.wikipedia.org/wiki/Payment_card_number. masked property ¶ masked: Mask all but the last 4 digits of the card number. Returns: Type Description A masked card number string. validate classmethod ¶ validate(\n    input_value: , /, _: \n) -> Validate the card number and return a PaymentCardNumber instance. validate_digits classmethod ¶ validate_digits(card_number: ) -> None Validate that the card number is all digits. validate_luhn_check_digit classmethod ¶ validate_luhn_check_digit(card_number: ) -> Based on: https://en.wikipedia.org/wiki/Luhn_algorithm. validate_brand staticmethod ¶ validate_brand(card_number: ) -> Validate length based on BIN for major brands:\nhttps://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN). ByteSize ¶ Bases: Converts a string representing a number of bytes with units (such as '1KB' or '11.5MiB' ) into an integer. You can use the ByteSize data type to (case-insensitively) convert a string representation of a number of bytes into\nan integer, and also to print out human-readable strings representing a number of bytes. In conformance with IEC 80000-13 Standard we interpret '1KB' to mean 1000 bytes,\nand '1KiB' to mean 1024 bytes. In general, including a middle 'i' will cause the unit to be interpreted as a power of 2,\nrather than a power of 10 (so, for example, '1 MB' is treated as 1_000_000 bytes, whereas '1 MiB' is treated as 1_048_576 bytes). Info Note that 1b will be parsed as \"1 byte\" and not \"1 bit\". from pydantic import BaseModel, ByteSize\n\nclass MyModel(BaseModel):\n    size: ByteSize\n\nprint(MyModel(size=52000).size)\n#> 52000\nprint(MyModel(size='3000 KiB').size)\n#> 3072000\n\nm = MyModel(size='50 PB')\nprint(m.size.human_readable())\n#> 44.4PiB\nprint(m.size.human_readable(decimal=True))\n#> 50.0PB\nprint(m.size.human_readable(separator=' '))\n#> 44.4 PiB\n\nprint(m.size.to('TiB'))\n#> 45474.73508864641 human_readable ¶ human_readable(\n    decimal:  = False, separator:  = \"\"\n) -> Converts a byte size to a human readable string. Parameters: Name Type Description Default decimal If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units\n(e.g. 1024 bytes per KiB). False separator A string used to split the value and unit. Defaults to an empty string (''). '' Returns: Type Description A human readable string representation of the byte size. to ¶ to(unit: ) -> Converts a byte size to another unit, including both byte and bit units. Parameters: Name Type Description Default unit The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,\nKiB, MiB, GiB, TiB, PiB, EiB (byte units) and\nbit, kbit, mbit, gbit, tbit, pbit, ebit,\nkibit, mibit, gibit, tibit, pibit, eibit (bit units). required Returns: Type Description The byte size in the new unit. PastDate ¶ A date in the past. FutureDate ¶ A date in the future. AwareDatetime ¶ A datetime that requires timezone info. NaiveDatetime ¶ A datetime that doesn't require timezone info. PastDatetime ¶ A datetime that must be in the past. FutureDatetime ¶ A datetime that must be in the future. EncoderProtocol ¶ Bases: Protocol for encoding and decoding data to and from bytes. decode classmethod ¶ decode(data: ) -> Decode the data using the encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data using the encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> Get the JSON format for the encoded data. Returns: Type Description The JSON format for the encoded data. Base64Encoder ¶ Bases: Standard (non-URL-safe) Base64 encoder. decode classmethod ¶ decode(data: ) -> Decode the data from base64 encoded bytes to original bytes data. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data from bytes to a base64 encoded bytes. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> ['base64'] Get the JSON format for the encoded data. Returns: Type Description ['base64'] The JSON format for the encoded data. Base64UrlEncoder ¶ Bases: URL-safe Base64 encoder. decode classmethod ¶ decode(data: ) -> Decode the data from base64 encoded bytes to original bytes data. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data from bytes to a base64 encoded bytes. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> ['base64url'] Get the JSON format for the encoded data. Returns: Type Description ['base64url'] The JSON format for the encoded data. EncodedBytes dataclass ¶ A bytes type that is encoded and decoded using the specified encoder. EncodedBytes needs an encoder that implements EncoderProtocol to operate. from typing import Annotated\n\nfrom pydantic import BaseModel, EncodedBytes, EncoderProtocol, ValidationError\n\nclass MyEncoder(EncoderProtocol):\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        if data == b'**undecodable**':\n            raise ValueError('Cannot decode data')\n        return data[13:]\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        return b'**encoded**: ' + value\n\n    @classmethod\n    def get_json_format(cls) -> str:\n        return 'my-encoder'\n\nMyEncodedBytes = Annotated[bytes, EncodedBytes(encoder=MyEncoder)]\n\nclass Model(BaseModel):\n    my_encoded_bytes: MyEncodedBytes\n\n# Initialize the model with encoded data\nm = Model(my_encoded_bytes=b'**encoded**: some bytes')\n\n# Access decoded value\nprint(m.my_encoded_bytes)\n#> b'some bytes'\n\n# Serialize into the encoded form\nprint(m.model_dump())\n#> {'my_encoded_bytes': b'**encoded**: some bytes'}\n\n# Validate encoded data\ntry:\n    Model(my_encoded_bytes=b'**undecodable**')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    my_encoded_bytes\n      Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]\n    ''' decode ¶ decode(data: , _: ) -> Decode the data using the specified encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode ¶ encode(value: ) -> Encode the data using the specified encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. EncodedStr dataclass ¶ A str type that is encoded and decoded using the specified encoder. EncodedStr needs an encoder that implements EncoderProtocol to operate. from typing import Annotated\n\nfrom pydantic import BaseModel, EncodedStr, EncoderProtocol, ValidationError\n\nclass MyEncoder(EncoderProtocol):\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        if data == b'**undecodable**':\n            raise ValueError('Cannot decode data')\n        return data[13:]\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        return b'**encoded**: ' + value\n\n    @classmethod\n    def get_json_format(cls) -> str:\n        return 'my-encoder'\n\nMyEncodedStr = Annotated[str, EncodedStr(encoder=MyEncoder)]\n\nclass Model(BaseModel):\n    my_encoded_str: MyEncodedStr\n\n# Initialize the model with encoded data\nm = Model(my_encoded_str='**encoded**: some str')\n\n# Access decoded value\nprint(m.my_encoded_str)\n#> some str\n\n# Serialize into the encoded form\nprint(m.model_dump())\n#> {'my_encoded_str': '**encoded**: some str'}\n\n# Validate encoded data\ntry:\n    Model(my_encoded_str='**undecodable**')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    my_encoded_str\n      Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]\n    ''' decode_str ¶ decode_str(data: , _: ) -> Decode the data using the specified encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode_str ¶ encode_str(value: ) -> Encode the data using the specified encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. GetPydanticSchema dataclass ¶ Usage Documentation Using GetPydanticSchema to Reduce Boilerplate A convenience class for creating an annotation that provides pydantic custom type hooks. This class is intended to eliminate the need to create a custom \"marker\" which defines the __get_pydantic_core_schema__ and __get_pydantic_json_schema__ custom hook methods. For example, to have a field treated by type checkers as int , but by pydantic as Any , you can do: from typing import Annotated, Any\n\nfrom pydantic import BaseModel, GetPydanticSchema\n\nHandleAsAny = GetPydanticSchema(lambda _s, h: h(Any))\n\nclass Model(BaseModel):\n    x: Annotated[int, HandleAsAny]  # pydantic sees `x: Any`\n\nprint(repr(Model(x='abc').x))\n#> 'abc' Tag dataclass ¶ Provides a way to specify the expected tag to use for a case of a (callable) discriminated union. Also provides a way to label a union case in error messages. When using a callable Discriminator , attach a Tag to each case in the Union to specify the tag that\nshould be used to identify that case. For example, in the below example, the Tag is used to specify that\nif get_discriminator_value returns 'apple' , the input should be validated as an ApplePie , and if it\nreturns 'pumpkin' , the input should be validated as a PumpkinPie . The primary role of the Tag here is to map the return value from the callable Discriminator function to\nthe appropriate member of the Union in question. from typing import Annotated, Any, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        Union[\n            Annotated[ApplePie, Tag('apple')],\n            Annotated[PumpkinPie, Tag('pumpkin')],\n        ],\n        Discriminator(get_discriminator_value),\n    ]\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n'''\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n'''\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n'''\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n''' Note You must specify a Tag for every case in a Tag that is associated with a\ncallable Discriminator . Failing to do so will result in a PydanticUserError with code callable-discriminator-no-tag . See the Discriminated Unions concepts docs for more details on how to use Tag s. Discriminator dataclass ¶ Usage Documentation Discriminated Unions with Callable Discriminator Provides a way to use a custom callable as the way to extract the value of a union discriminator. This allows you to get validation behavior like you'd get from Field(discriminator=<field_name>) ,\nbut without needing to have a single shared field across all the union choices. This also makes it\npossible to handle unions of models and primitive types with discriminated-union-style validation errors.\nFinally, this allows you to use a custom callable as the way to identify which member of a union a value\nbelongs to, while still seeing all the performance benefits of a discriminated union. Consider this example, which is much more performant with the use of Discriminator and thus a TaggedUnion than it would be as a normal Union . from typing import Annotated, Any, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        Union[\n            Annotated[ApplePie, Tag('apple')],\n            Annotated[PumpkinPie, Tag('pumpkin')],\n        ],\n        Discriminator(get_discriminator_value),\n    ]\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n'''\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n'''\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n'''\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n''' See the Discriminated Unions concepts docs for more details on how to use Discriminator s. discriminator instance-attribute ¶ discriminator:  | [[], ] The callable or field name for discriminating the type in a tagged union. A Callable discriminator must extract the value of the discriminator from the input.\nA str discriminator must be the name of a field to discriminate against. custom_error_type class-attribute instance-attribute ¶ custom_error_type:  | None = None Type to use in custom errors replacing the standard discriminated union\nvalidation errors. custom_error_message class-attribute instance-attribute ¶ custom_error_message:  | None = None Message to use in custom errors. custom_error_context class-attribute instance-attribute ¶ custom_error_context: (\n    [,  |  | ] | None\n) = None Context to use in custom errors. FailFast dataclass ¶ Bases: , A FailFast annotation can be used to specify that validation should stop at the first error. This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not. You might want to enable this setting if you want to validate your data faster (basically, if you use this,\nvalidation will be more performant with the caveat that you get less information). from typing import Annotated\n\nfrom pydantic import BaseModel, FailFast, ValidationError\n\nclass Model(BaseModel):\n    x: Annotated[list[int], FailFast()]\n\n# This will raise a single error for the first invalid value and stop validation\ntry:\n    obj = Model(x=[1, 2, 'a', 4, 5, 'b', 7, 8, 9, 'c'])\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    x.2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    ''' conint ¶ conint(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None,\n    multiple_of:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that conint returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, conint\n\nclass Foo(BaseModel):\n    bar: conint(strict=True, gt=0) from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[int, Field(strict=True, gt=0)] A wrapper around int that allows for additional constraints. Parameters: Name Type Description Default strict | None Whether to validate the integer in strict mode. Defaults to None . None gt | None The value must be greater than this. None ge | None The value must be greater than or equal to this. None lt | None The value must be less than this. None le | None The value must be less than or equal to this. None multiple_of | None The value must be a multiple of this. None Returns: Type Description [] The wrapped integer type. from pydantic import BaseModel, ValidationError, conint\n\nclass ConstrainedExample(BaseModel):\n    constrained_int: conint(gt=1)\n\nm = ConstrainedExample(constrained_int=2)\nprint(repr(m))\n#> ConstrainedExample(constrained_int=2)\n\ntry:\n    ConstrainedExample(constrained_int=0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_int',),\n            'msg': 'Input should be greater than 1',\n            'input': 0,\n            'ctx': {'gt': 1},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' confloat ¶ confloat(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None,\n    multiple_of:  | None = None,\n    allow_inf_nan:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that confloat returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, confloat\n\nclass Foo(BaseModel):\n    bar: confloat(strict=True, gt=0) from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[float, Field(strict=True, gt=0)] A wrapper around float that allows for additional constraints. Parameters: Name Type Description Default strict | None Whether to validate the float in strict mode. None gt | None The value must be greater than this. None ge | None The value must be greater than or equal to this. None lt | None The value must be less than this. None le | None The value must be less than or equal to this. None multiple_of | None The value must be a multiple of this. None allow_inf_nan | None Whether to allow -inf , inf , and nan . None Returns: Type Description [] The wrapped float type. from pydantic import BaseModel, ValidationError, confloat\n\nclass ConstrainedExample(BaseModel):\n    constrained_float: confloat(gt=1.0)\n\nm = ConstrainedExample(constrained_float=1.1)\nprint(repr(m))\n#> ConstrainedExample(constrained_float=1.1)\n\ntry:\n    ConstrainedExample(constrained_float=0.9)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_float',),\n            'msg': 'Input should be greater than 1',\n            'input': 0.9,\n            'ctx': {'gt': 1.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' conbytes ¶ conbytes(\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    strict:  | None = None\n) -> [] A wrapper around bytes that allows for additional constraints. Parameters: Name Type Description Default min_length | None The minimum length of the bytes. None max_length | None The maximum length of the bytes. None strict | None Whether to validate the bytes in strict mode. None Returns: Type Description [] The wrapped bytes type. constr ¶ constr(\n    *,\n    strip_whitespace:  | None = None,\n    to_upper:  | None = None,\n    to_lower:  | None = None,\n    strict:  | None = None,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    pattern:  | [] | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that constr returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, constr\n\nclass Foo(BaseModel):\n    bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$') from typing import Annotated\n\nfrom pydantic import BaseModel, StringConstraints\n\nclass Foo(BaseModel):\n    bar: Annotated[\n        str,\n        StringConstraints(\n            strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$'\n        ),\n    ] A wrapper around str that allows for additional constraints. from pydantic import BaseModel, constr\n\nclass Foo(BaseModel):\n    bar: constr(strip_whitespace=True, to_upper=True)\n\nfoo = Foo(bar='  hello  ')\nprint(foo)\n#> bar='HELLO' Parameters: Name Type Description Default strip_whitespace | None Whether to remove leading and trailing whitespace. None to_upper | None Whether to turn all characters to uppercase. None to_lower | None Whether to turn all characters to lowercase. None strict | None Whether to validate the string in strict mode. None min_length | None The minimum length of the string. None max_length | None The maximum length of the string. None pattern | [] | None A regex pattern to validate the string against. None Returns: Type Description [] The wrapped string type. conset ¶ conset(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None\n) -> [[]] A wrapper around typing.Set that allows for additional constraints. Parameters: Name Type Description Default item_type [] The type of the items in the set. required min_length | None The minimum length of the set. None max_length | None The maximum length of the set. None Returns: Type Description [[]] The wrapped set type. confrozenset ¶ confrozenset(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None\n) -> [[]] A wrapper around typing.FrozenSet that allows for additional constraints. Parameters: Name Type Description Default item_type [] The type of the items in the frozenset. required min_length | None The minimum length of the frozenset. None max_length | None The maximum length of the frozenset. None Returns: Type Description [[]] The wrapped frozenset type. conlist ¶ conlist(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    unique_items:  | None = None\n) -> [[]] A wrapper around  that adds validation. Parameters: Name Type Description Default item_type [] The type of the items in the list. required min_length | None The minimum length of the list. Defaults to None. None max_length | None The maximum length of the list. Defaults to None. None unique_items | None Whether the items in the list must be unique. Defaults to None. Warning The unique_items parameter is deprecated, use Set instead.\nSee this issue for more details. None Returns: Type Description [[]] The wrapped list type. condecimal ¶ condecimal(\n    *,\n    strict:  | None = None,\n    gt:  |  | None = None,\n    ge:  |  | None = None,\n    lt:  |  | None = None,\n    le:  |  | None = None,\n    multiple_of:  |  | None = None,\n    max_digits:  | None = None,\n    decimal_places:  | None = None,\n    allow_inf_nan:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that condecimal returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, condecimal\n\nclass Foo(BaseModel):\n    bar: condecimal(strict=True, allow_inf_nan=True) from decimal import Decimal\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[Decimal, Field(strict=True, allow_inf_nan=True)] A wrapper around Decimal that adds validation. Parameters: Name Type Description Default strict | None Whether to validate the value in strict mode. Defaults to None . None gt |  | None The value must be greater than this. Defaults to None . None ge |  | None The value must be greater than or equal to this. Defaults to None . None lt |  | None The value must be less than this. Defaults to None . None le |  | None The value must be less than or equal to this. Defaults to None . None multiple_of |  | None The value must be a multiple of this. Defaults to None . None max_digits | None The maximum number of digits. Defaults to None . None decimal_places | None The number of decimal places. Defaults to None . None allow_inf_nan | None Whether to allow infinity and NaN. Defaults to None . None from decimal import Decimal\n\nfrom pydantic import BaseModel, ValidationError, condecimal\n\nclass ConstrainedExample(BaseModel):\n    constrained_decimal: condecimal(gt=Decimal('1.0'))\n\nm = ConstrainedExample(constrained_decimal=Decimal('1.1'))\nprint(repr(m))\n#> ConstrainedExample(constrained_decimal=Decimal('1.1'))\n\ntry:\n    ConstrainedExample(constrained_decimal=Decimal('0.9'))\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_decimal',),\n            'msg': 'Input should be greater than 1.0',\n            'input': Decimal('0.9'),\n            'ctx': {'gt': Decimal('1.0')},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    ''' condate ¶ condate(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None\n) -> [] A wrapper for date that adds constraints. Parameters: Name Type Description Default strict | None Whether to validate the date value in strict mode. Defaults to None . None gt | None The value must be greater than this. Defaults to None . None ge | None The value must be greater than or equal to this. Defaults to None . None lt | None The value must be less than this. Defaults to None . None le | None The value must be less than or equal to this. Defaults to None . None Returns: Type Description [] A date type with the specified constraints.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types","title":"Pydantic Types - pydantic.types","objectID":"/latest/api/types/#pydantic.types","rank":95},{"content":"StrictBool = [, ()] A boolean that must be either True or False .","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.StrictBool","title":"Pydantic Types - pydantic.types - StrictBool  module-attribute","objectID":"/latest/api/types/#pydantic.types.StrictBool","rank":90},{"content":"PositiveInt = [, (0)] An integer that must be greater than zero. from pydantic import BaseModel, PositiveInt, ValidationError\n\nclass Model(BaseModel):\n    positive_int: PositiveInt\n\nm = Model(positive_int=1)\nprint(repr(m))\n#> Model(positive_int=1)\n\ntry:\n    Model(positive_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_int',),\n            'msg': 'Input should be greater than 0',\n            'input': -1,\n            'ctx': {'gt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.PositiveInt","title":"Pydantic Types - pydantic.types - PositiveInt  module-attribute","objectID":"/latest/api/types/#pydantic.types.PositiveInt","rank":85},{"content":"NegativeInt = [, (0)] An integer that must be less than zero. from pydantic import BaseModel, NegativeInt, ValidationError\n\nclass Model(BaseModel):\n    negative_int: NegativeInt\n\nm = Model(negative_int=-1)\nprint(repr(m))\n#> Model(negative_int=-1)\n\ntry:\n    Model(negative_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_int',),\n            'msg': 'Input should be less than 0',\n            'input': 1,\n            'ctx': {'lt': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NegativeInt","title":"Pydantic Types - pydantic.types - NegativeInt  module-attribute","objectID":"/latest/api/types/#pydantic.types.NegativeInt","rank":80},{"content":"NonPositiveInt = [, (0)] An integer that must be less than or equal to zero. from pydantic import BaseModel, NonPositiveInt, ValidationError\n\nclass Model(BaseModel):\n    non_positive_int: NonPositiveInt\n\nm = Model(non_positive_int=0)\nprint(repr(m))\n#> Model(non_positive_int=0)\n\ntry:\n    Model(non_positive_int=1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_int',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1,\n            'ctx': {'le': 0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NonPositiveInt","title":"Pydantic Types - pydantic.types - NonPositiveInt  module-attribute","objectID":"/latest/api/types/#pydantic.types.NonPositiveInt","rank":75},{"content":"NonNegativeInt = [, (0)] An integer that must be greater than or equal to zero. from pydantic import BaseModel, NonNegativeInt, ValidationError\n\nclass Model(BaseModel):\n    non_negative_int: NonNegativeInt\n\nm = Model(non_negative_int=0)\nprint(repr(m))\n#> Model(non_negative_int=0)\n\ntry:\n    Model(non_negative_int=-1)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_int',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1,\n            'ctx': {'ge': 0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NonNegativeInt","title":"Pydantic Types - pydantic.types - NonNegativeInt  module-attribute","objectID":"/latest/api/types/#pydantic.types.NonNegativeInt","rank":70},{"content":"StrictInt = [, ()] An integer that must be validated in strict mode. from pydantic import BaseModel, StrictInt, ValidationError\n\nclass StrictIntModel(BaseModel):\n    strict_int: StrictInt\n\ntry:\n    StrictIntModel(strict_int=3.14159)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictIntModel\n    strict_int\n      Input should be a valid integer [type=int_type, input_value=3.14159, input_type=float]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.StrictInt","title":"Pydantic Types - pydantic.types - StrictInt  module-attribute","objectID":"/latest/api/types/#pydantic.types.StrictInt","rank":65},{"content":"PositiveFloat = [, (0)] A float that must be greater than zero. from pydantic import BaseModel, PositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    positive_float: PositiveFloat\n\nm = Model(positive_float=1.0)\nprint(repr(m))\n#> Model(positive_float=1.0)\n\ntry:\n    Model(positive_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('positive_float',),\n            'msg': 'Input should be greater than 0',\n            'input': -1.0,\n            'ctx': {'gt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.PositiveFloat","title":"Pydantic Types - pydantic.types - PositiveFloat  module-attribute","objectID":"/latest/api/types/#pydantic.types.PositiveFloat","rank":60},{"content":"NegativeFloat = [, (0)] A float that must be less than zero. from pydantic import BaseModel, NegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    negative_float: NegativeFloat\n\nm = Model(negative_float=-1.0)\nprint(repr(m))\n#> Model(negative_float=-1.0)\n\ntry:\n    Model(negative_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than',\n            'loc': ('negative_float',),\n            'msg': 'Input should be less than 0',\n            'input': 1.0,\n            'ctx': {'lt': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NegativeFloat","title":"Pydantic Types - pydantic.types - NegativeFloat  module-attribute","objectID":"/latest/api/types/#pydantic.types.NegativeFloat","rank":55},{"content":"NonPositiveFloat = [, (0)] A float that must be less than or equal to zero. from pydantic import BaseModel, NonPositiveFloat, ValidationError\n\nclass Model(BaseModel):\n    non_positive_float: NonPositiveFloat\n\nm = Model(non_positive_float=0.0)\nprint(repr(m))\n#> Model(non_positive_float=0.0)\n\ntry:\n    Model(non_positive_float=1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'less_than_equal',\n            'loc': ('non_positive_float',),\n            'msg': 'Input should be less than or equal to 0',\n            'input': 1.0,\n            'ctx': {'le': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/less_than_equal',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NonPositiveFloat","title":"Pydantic Types - pydantic.types - NonPositiveFloat  module-attribute","objectID":"/latest/api/types/#pydantic.types.NonPositiveFloat","rank":50},{"content":"NonNegativeFloat = [, (0)] A float that must be greater than or equal to zero. from pydantic import BaseModel, NonNegativeFloat, ValidationError\n\nclass Model(BaseModel):\n    non_negative_float: NonNegativeFloat\n\nm = Model(non_negative_float=0.0)\nprint(repr(m))\n#> Model(non_negative_float=0.0)\n\ntry:\n    Model(non_negative_float=-1.0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than_equal',\n            'loc': ('non_negative_float',),\n            'msg': 'Input should be greater than or equal to 0',\n            'input': -1.0,\n            'ctx': {'ge': 0.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NonNegativeFloat","title":"Pydantic Types - pydantic.types - NonNegativeFloat  module-attribute","objectID":"/latest/api/types/#pydantic.types.NonNegativeFloat","rank":45},{"content":"StrictFloat = [, (True)] A float that must be validated in strict mode. from pydantic import BaseModel, StrictFloat, ValidationError\n\nclass StrictFloatModel(BaseModel):\n    strict_float: StrictFloat\n\ntry:\n    StrictFloatModel(strict_float='1.0')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for StrictFloatModel\n    strict_float\n      Input should be a valid number [type=float_type, input_value='1.0', input_type=str]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.StrictFloat","title":"Pydantic Types - pydantic.types - StrictFloat  module-attribute","objectID":"/latest/api/types/#pydantic.types.StrictFloat","rank":40},{"content":"FiniteFloat = [, (False)] A float that must be finite (not -inf , inf , or nan ). from pydantic import BaseModel, FiniteFloat\n\nclass Model(BaseModel):\n    finite: FiniteFloat\n\nm = Model(finite=1.0)\nprint(m)\n#> finite=1.0","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.FiniteFloat","title":"Pydantic Types - pydantic.types - FiniteFloat  module-attribute","objectID":"/latest/api/types/#pydantic.types.FiniteFloat","rank":35},{"content":"StrictBytes = [, ()] A bytes that must be validated in strict mode.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.StrictBytes","title":"Pydantic Types - pydantic.types - StrictBytes  module-attribute","objectID":"/latest/api/types/#pydantic.types.StrictBytes","rank":30},{"content":"StrictStr = [, ()] A string that must be validated in strict mode.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.StrictStr","title":"Pydantic Types - pydantic.types - StrictStr  module-attribute","objectID":"/latest/api/types/#pydantic.types.StrictStr","rank":25},{"content":"UUID1 = [, (1)] A UUID that must be version 1. import uuid\n\nfrom pydantic import UUID1, BaseModel\n\nclass Model(BaseModel):\n    uuid1: UUID1\n\nModel(uuid1=uuid.uuid1())","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UUID1","title":"Pydantic Types - pydantic.types - UUID1  module-attribute","objectID":"/latest/api/types/#pydantic.types.UUID1","rank":20},{"content":"UUID3 = [, (3)] A UUID that must be version 3. import uuid\n\nfrom pydantic import UUID3, BaseModel\n\nclass Model(BaseModel):\n    uuid3: UUID3\n\nModel(uuid3=uuid.uuid3(uuid.NAMESPACE_DNS, 'pydantic.org'))","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UUID3","title":"Pydantic Types - pydantic.types - UUID3  module-attribute","objectID":"/latest/api/types/#pydantic.types.UUID3","rank":15},{"content":"UUID4 = [, (4)] A UUID that must be version 4. import uuid\n\nfrom pydantic import UUID4, BaseModel\n\nclass Model(BaseModel):\n    uuid4: UUID4\n\nModel(uuid4=uuid.uuid4())","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UUID4","title":"Pydantic Types - pydantic.types - UUID4  module-attribute","objectID":"/latest/api/types/#pydantic.types.UUID4","rank":10},{"content":"UUID5 = [, (5)] A UUID that must be version 5. import uuid\n\nfrom pydantic import UUID5, BaseModel\n\nclass Model(BaseModel):\n    uuid5: UUID5\n\nModel(uuid5=uuid.uuid5(uuid.NAMESPACE_DNS, 'pydantic.org'))","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UUID5","title":"Pydantic Types - pydantic.types - UUID5  module-attribute","objectID":"/latest/api/types/#pydantic.types.UUID5","rank":5},{"content":"UUID6 = [, (6)] A UUID that must be version 6. import uuid\n\nfrom pydantic import UUID6, BaseModel\n\nclass Model(BaseModel):\n    uuid6: UUID6\n\nModel(uuid6=uuid.UUID('1efea953-c2d6-6790-aa0a-69db8c87df97'))","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UUID6","title":"Pydantic Types - pydantic.types - UUID6  module-attribute","objectID":"/latest/api/types/#pydantic.types.UUID6","rank":0},{"content":"UUID7 = [, (7)] A UUID that must be version 7. import uuid\n\nfrom pydantic import UUID7, BaseModel\n\nclass Model(BaseModel):\n    uuid7: UUID7\n\nModel(uuid7=uuid.UUID('0194fdcb-1c47-7a09-b52c-561154de0b4a'))","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UUID7","title":"Pydantic Types - pydantic.types - UUID7  module-attribute","objectID":"/latest/api/types/#pydantic.types.UUID7","rank":-5},{"content":"UUID8 = [, (8)] A UUID that must be version 8. import uuid\n\nfrom pydantic import UUID8, BaseModel\n\nclass Model(BaseModel):\n    uuid8: UUID8\n\nModel(uuid8=uuid.UUID('81a0b92e-6078-8551-9c81-8ccb666bdab8'))","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UUID8","title":"Pydantic Types - pydantic.types - UUID8  module-attribute","objectID":"/latest/api/types/#pydantic.types.UUID8","rank":-10},{"content":"FilePath = [, ('file')] A path that must point to a file. from pathlib import Path\n\nfrom pydantic import BaseModel, FilePath, ValidationError\n\nclass Model(BaseModel):\n    f: FilePath\n\npath = Path('text.txt')\npath.touch()\nm = Model(f='text.txt')\nprint(m.model_dump())\n#> {'f': PosixPath('text.txt')}\npath.unlink()\n\npath = Path('directory')\npath.mkdir(exist_ok=True)\ntry:\n    Model(f='directory')  # directory\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='directory', input_type=str]\n    '''\npath.rmdir()\n\ntry:\n    Model(f='not-exists-file')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a file [type=path_not_file, input_value='not-exists-file', input_type=str]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.FilePath","title":"Pydantic Types - pydantic.types - FilePath  module-attribute","objectID":"/latest/api/types/#pydantic.types.FilePath","rank":-15},{"content":"DirectoryPath = [, ('dir')] A path that must point to a directory. from pathlib import Path\n\nfrom pydantic import BaseModel, DirectoryPath, ValidationError\n\nclass Model(BaseModel):\n    f: DirectoryPath\n\npath = Path('directory/')\npath.mkdir()\nm = Model(f='directory/')\nprint(m.model_dump())\n#> {'f': PosixPath('directory')}\npath.rmdir()\n\npath = Path('file.txt')\npath.touch()\ntry:\n    Model(f='file.txt')  # file\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='file.txt', input_type=str]\n    '''\npath.unlink()\n\ntry:\n    Model(f='not-exists-directory')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    f\n      Path does not point to a directory [type=path_not_directory, input_value='not-exists-directory', input_type=str]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.DirectoryPath","title":"Pydantic Types - pydantic.types - DirectoryPath  module-attribute","objectID":"/latest/api/types/#pydantic.types.DirectoryPath","rank":-20},{"content":"NewPath = [, ('new')] A path for a new file or directory that must not already exist. The parent directory must already exist.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NewPath","title":"Pydantic Types - pydantic.types - NewPath  module-attribute","objectID":"/latest/api/types/#pydantic.types.NewPath","rank":-25},{"content":"SocketPath = [, ('socket')] A path to an existing socket file","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.SocketPath","title":"Pydantic Types - pydantic.types - SocketPath  module-attribute","objectID":"/latest/api/types/#pydantic.types.SocketPath","rank":-30},{"content":"Base64Bytes = [\n    , (=)\n] A bytes type that is encoded and decoded using the standard (non-URL-safe) base64 encoder. from pydantic import Base64Bytes, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_bytes: Base64Bytes\n\n# Initialize the model with base64 data\nm = Model(base64_bytes=b'VGhpcyBpcyB0aGUgd2F5')\n\n# Access decoded value\nprint(m.base64_bytes)\n#> b'This is the way'\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_bytes': b'VGhpcyBpcyB0aGUgd2F5'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_bytes=b'undecodable').base64_bytes)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_bytes\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value=b'undecodable', input_type=bytes]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Base64Bytes","title":"Pydantic Types - pydantic.types - Base64Bytes  module-attribute","objectID":"/latest/api/types/#pydantic.types.Base64Bytes","rank":-35},{"content":"Base64Str = [\n    , (=)\n] A str type that is encoded and decoded using the standard (non-URL-safe) base64 encoder. from pydantic import Base64Str, BaseModel, ValidationError\n\nclass Model(BaseModel):\n    base64_str: Base64Str\n\n# Initialize the model with base64 data\nm = Model(base64_str='VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y')\n\n# Access decoded value\nprint(m.base64_str)\n#> These aren't the droids you're looking for\n\n# Serialize into the base64 form\nprint(m.model_dump())\n#> {'base64_str': 'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y'}\n\n# Validate base64 data\ntry:\n    print(Model(base64_str='undecodable').base64_str)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    base64_str\n      Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value='undecodable', input_type=str]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Base64Str","title":"Pydantic Types - pydantic.types - Base64Str  module-attribute","objectID":"/latest/api/types/#pydantic.types.Base64Str","rank":-40},{"content":"Base64UrlBytes = [\n    , (=)\n] A bytes type that is encoded and decoded using the URL-safe base64 encoder. from pydantic import Base64UrlBytes, BaseModel\n\nclass Model(BaseModel):\n    base64url_bytes: Base64UrlBytes\n\n# Initialize the model with base64 data\nm = Model(base64url_bytes=b'SHc_dHc-TXc==')\nprint(m)\n#> base64url_bytes=b'Hw?tw>Mw'","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Base64UrlBytes","title":"Pydantic Types - pydantic.types - Base64UrlBytes  module-attribute","objectID":"/latest/api/types/#pydantic.types.Base64UrlBytes","rank":-45},{"content":"Base64UrlStr = [\n    , (=)\n] A str type that is encoded and decoded using the URL-safe base64 encoder. from pydantic import Base64UrlStr, BaseModel\n\nclass Model(BaseModel):\n    base64url_str: Base64UrlStr\n\n# Initialize the model with base64 data\nm = Model(base64url_str='SHc_dHc-TXc==')\nprint(m)\n#> base64url_str='Hw?tw>Mw'","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Base64UrlStr","title":"Pydantic Types - pydantic.types - Base64UrlStr  module-attribute","objectID":"/latest/api/types/#pydantic.types.Base64UrlStr","rank":-50},{"content":"JsonValue:  = [\n    [\"JsonValue\"],\n    [, \"JsonValue\"],\n    ,\n    ,\n    ,\n    ,\n    None,\n] A JsonValue is used to represent a value that can be serialized to JSON. It may be one of: list['JsonValue'] dict[str, 'JsonValue'] str bool int float None The following example demonstrates how to use JsonValue to validate JSON data,\nand what kind of errors to expect when input data is not json serializable. import json\n\nfrom pydantic import BaseModel, JsonValue, ValidationError\n\nclass Model(BaseModel):\n    j: JsonValue\n\nvalid_json_data = {'j': {'a': {'b': {'c': 1, 'd': [2, None]}}}}\ninvalid_json_data = {'j': {'a': {'b': ...}}}\n\nprint(repr(Model.model_validate(valid_json_data)))\n#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\nprint(repr(Model.model_validate_json(json.dumps(valid_json_data))))\n#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})\n\ntry:\n    Model.model_validate(invalid_json_data)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    j.dict.a.dict.b\n      input was not a valid JSON value [type=invalid-json-value, input_value=Ellipsis, input_type=ellipsis]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.JsonValue","title":"Pydantic Types - pydantic.types - JsonValue  module-attribute","objectID":"/latest/api/types/#pydantic.types.JsonValue","rank":-55},{"content":"OnErrorOmit = [, ] When used as an item in a list, the key type in a dict, optional values of a TypedDict, etc.\nthis annotation omits the item from the iteration if there is any error validating it.\nThat is, instead of a  being propagated up and the entire iterable being discarded\nany invalid items are discarded and the valid ones are returned.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.OnErrorOmit","title":"Pydantic Types - pydantic.types - OnErrorOmit  module-attribute","objectID":"/latest/api/types/#pydantic.types.OnErrorOmit","rank":-60},{"content":"Bases: , Usage Documentation Strict Mode with Annotated Strict A field metadata class to indicate that a field should be validated in strict mode.\nUse this class as an annotation via Annotated , as seen below. Attributes: Name Type Description Whether to validate the field in strict mode.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Strict","title":"Pydantic Types - pydantic.types - Strict  dataclass","objectID":"/latest/api/types/#pydantic.types.Strict","rank":-65},{"content":"Bases: A field metadata class to indicate that a field should allow -inf , inf , and nan . Use this class as an annotation via Annotated , as seen below. Attributes: Name Type Description Whether to allow -inf , inf , and nan . Defaults to True .","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.AllowInfNan","title":"Pydantic Types - pydantic.types - AllowInfNan  dataclass","objectID":"/latest/api/types/#pydantic.types.AllowInfNan","rank":-70},{"content":"Bases: Usage Documentation StringConstraints A field metadata class to apply constraints to str types.\nUse this class as an annotation via Annotated , as seen below. Attributes: Name Type Description | None Whether to remove leading and trailing whitespace. | None Whether to convert the string to uppercase. | None Whether to convert the string to lowercase. | None Whether to validate the string in strict mode. | None The minimum length of the string. | None The maximum length of the string. | [] | None A regex pattern that the string must match.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.StringConstraints","title":"Pydantic Types - pydantic.types - StringConstraints  dataclass","objectID":"/latest/api/types/#pydantic.types.StringConstraints","rank":-75},{"content":"A type that can be used to import a Python object from a string. ImportString expects a string and loads the Python object importable at that dotted path.\nAttributes of modules may be separated from the module by : or . , e.g. if 'math:cos' is provided,\nthe resulting field value would be the function cos . If a . is used and both an attribute and submodule\nare present at the same path, the module will be preferred. On model instantiation, pointers will be evaluated and imported. There is\nsome nuance to this behavior, demonstrated in the examples below. import math\n\nfrom pydantic import BaseModel, Field, ImportString, ValidationError\n\nclass ImportThings(BaseModel):\n    obj: ImportString\n\n# A string value will cause an automatic import\nmy_cos = ImportThings(obj='math.cos')\n\n# You can use the imported function as you would expect\ncos_of_0 = my_cos.obj(0)\nassert cos_of_0 == 1\n\n# A string whose value cannot be imported will raise an error\ntry:\n    ImportThings(obj='foo.bar')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ImportThings\n    obj\n      Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]\n    '''\n\n# Actual python objects can be assigned as well\nmy_cos = ImportThings(obj=math.cos)\nmy_cos_2 = ImportThings(obj='math.cos')\nmy_cos_3 = ImportThings(obj='math:cos')\nassert my_cos == my_cos_2 == my_cos_3\n\n# You can set default field value either as Python object:\nclass ImportThingsDefaultPyObj(BaseModel):\n    obj: ImportString = math.cos\n\n# or as a string value (but only if used with `validate_default=True`)\nclass ImportThingsDefaultString(BaseModel):\n    obj: ImportString = Field(default='math.cos', validate_default=True)\n\nmy_cos_default1 = ImportThingsDefaultPyObj()\nmy_cos_default2 = ImportThingsDefaultString()\nassert my_cos_default1.obj == my_cos_default2.obj == math.cos\n\n# note: this will not work!\nclass ImportThingsMissingValidateDefault(BaseModel):\n    obj: ImportString = 'math.cos'\n\nmy_cos_default3 = ImportThingsMissingValidateDefault()\nassert my_cos_default3.obj == 'math.cos'  # just string, not evaluated Serializing an ImportString type to json is also possible. from pydantic import BaseModel, ImportString\n\nclass ImportThings(BaseModel):\n    obj: ImportString\n\n# Create an instance\nm = ImportThings(obj='math.cos')\nprint(m)\n#> obj= print(m.model_dump_json())\n#> {\"obj\":\"math.cos\"}","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.ImportString","title":"Pydantic Types - pydantic.types - ImportString","objectID":"/latest/api/types/#pydantic.types.ImportString","rank":-80},{"content":"A field metadata class to indicate a UUID version. Use this class as an annotation via Annotated , as seen below. Attributes: Name Type Description [1, 3, 4, 5, 6, 7, 8] The version of the UUID. Must be one of 1, 3, 4, 5, 6, 7 or 8.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.UuidVersion","title":"Pydantic Types - pydantic.types - UuidVersion  dataclass","objectID":"/latest/api/types/#pydantic.types.UuidVersion","rank":-85},{"content":"A special type wrapper which loads JSON before parsing. You can use the Json data type to make Pydantic first load a raw JSON string before\nvalidating the loaded data into the parametrized type: from typing import Any\n\nfrom pydantic import BaseModel, Json, ValidationError\n\nclass AnyJsonModel(BaseModel):\n    json_obj: Json[Any]\n\nclass ConstrainedJsonModel(BaseModel):\n    json_obj: Json[list[int]]\n\nprint(AnyJsonModel(json_obj='{\"b\": 1}'))\n#> json_obj={'b': 1}\nprint(ConstrainedJsonModel(json_obj='[1, 2, 3]'))\n#> json_obj=[1, 2, 3]\n\ntry:\n    ConstrainedJsonModel(json_obj=12)\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ConstrainedJsonModel\n    json_obj\n      JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]\n    '''\n\ntry:\n    ConstrainedJsonModel(json_obj='[a, b]')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for ConstrainedJsonModel\n    json_obj\n      Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]\n    '''\n\ntry:\n    ConstrainedJsonModel(json_obj='[\"a\", \"b\"]')\nexcept ValidationError as e:\n    print(e)\n    '''\n    2 validation errors for ConstrainedJsonModel\n    json_obj.0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    json_obj.1\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]\n    ''' When you dump the model using model_dump or model_dump_json , the dumped value will be the result of validation,\nnot the original JSON string. However, you can use the argument round_trip=True to get the original JSON string back: from pydantic import BaseModel, Json\n\nclass ConstrainedJsonModel(BaseModel):\n    json_obj: Json[list[int]]\n\nprint(ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json())\n#> {\"json_obj\":[1,2,3]}\nprint(\n    ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json(round_trip=True)\n)\n#> {\"json_obj\":\"[1,2,3]\"}","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Json","title":"Pydantic Types - pydantic.types - Json","objectID":"/latest/api/types/#pydantic.types.Json","rank":-90},{"content":"Bases: [] A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks. You may either directly parametrize Secret with a type, or subclass from Secret with a parametrized type. The benefit of subclassing\nis that you can define a custom _display method, which will be used for repr() and str() methods. The examples below demonstrate both\nways of using Secret to create a new secret type. Directly parametrizing Secret with a type: from pydantic import BaseModel, Secret\n\nSecretBool = Secret[bool]\n\nclass Model(BaseModel):\n    secret_bool: SecretBool\n\nm = Model(secret_bool=True)\nprint(m.model_dump())\n#> {'secret_bool': Secret('**********')}\n\nprint(m.model_dump_json())\n#> {\"secret_bool\":\"**********\"}\n\nprint(m.secret_bool.get_secret_value())\n#> True Subclassing from parametrized Secret : from datetime import date\n\nfrom pydantic import BaseModel, Secret\n\nclass SecretDate(Secret[date]):\n    def _display(self) -> str:\n        return '****/**/**'\n\nclass Model(BaseModel):\n    secret_date: SecretDate\n\nm = Model(secret_date=date(2022, 1, 1))\nprint(m.model_dump())\n#> {'secret_date': SecretDate('****/**/**')}\n\nprint(m.model_dump_json())\n#> {\"secret_date\":\"****/**/**\"}\n\nprint(m.secret_date.get_secret_value())\n#> 2022-01-01 The value returned by the _display method will be used for repr() and str() . You can enforce constraints on the underlying type through annotations:\nFor example: from typing import Annotated\n\nfrom pydantic import BaseModel, Field, Secret, ValidationError\n\nSecretPosInt = Secret[Annotated[int, Field(gt=0, strict=True)]]\n\nclass Model(BaseModel):\n    sensitive_int: SecretPosInt\n\nm = Model(sensitive_int=42)\nprint(m.model_dump())\n#> {'sensitive_int': Secret('**********')}\n\ntry:\n    m = Model(sensitive_int=-42)  # (1)!\nexcept ValidationError as exc_info:\n    print(exc_info.errors(include_url=False, include_input=False))\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('sensitive_int',),\n            'msg': 'Input should be greater than 0',\n            'ctx': {'gt': 0},\n        }\n    ]\n    '''\n\ntry:\n    m = Model(sensitive_int='42')  # (2)!\nexcept ValidationError as exc_info:\n    print(exc_info.errors(include_url=False, include_input=False))\n    '''\n    [\n        {\n            'type': 'int_type',\n            'loc': ('sensitive_int',),\n            'msg': 'Input should be a valid integer',\n        }\n    ]\n    ''' The input value is not greater than 0, so it raises a validation error. The input value is not an integer, so it raises a validation error because the SecretPosInt type has strict mode enabled.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Secret","title":"Pydantic Types - pydantic.types - Secret","objectID":"/latest/api/types/#pydantic.types.Secret","rank":-95},{"content":"Bases: [] A string used for storing sensitive information that you do not want to be visible in logging or tracebacks. When the secret value is nonempty, it is displayed as '**********' instead of the underlying value in\ncalls to repr() and str() . If the value is empty, it is displayed as '' . from pydantic import BaseModel, SecretStr\n\nclass User(BaseModel):\n    username: str\n    password: SecretStr\n\nuser = User(username='scolvin', password='password1')\n\nprint(user)\n#> username='scolvin' password=SecretStr('**********')\nprint(user.password.get_secret_value())\n#> password1\nprint((SecretStr('password'), SecretStr('')))\n#> (SecretStr('**********'), SecretStr('')) As seen above, by default,  (and )\nwill be serialized as ********** when serializing to json. You can use the  to dump the\nsecret as plain-text when serializing to json. from pydantic import BaseModel, SecretBytes, SecretStr, field_serializer\n\nclass Model(BaseModel):\n    password: SecretStr\n    password_bytes: SecretBytes\n\n    @field_serializer('password', 'password_bytes', when_used='json')\n    def dump_secret(self, v):\n        return v.get_secret_value()\n\nmodel = Model(password='IAmSensitive', password_bytes=b'IAmSensitiveBytes')\nprint(model)\n#> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')\nprint(model.password)\n#> **********\nprint(model.model_dump())\n'''\n{\n    'password': SecretStr('**********'),\n    'password_bytes': SecretBytes(b'**********'),\n}\n'''\nprint(model.model_dump_json())\n#> {\"password\":\"IAmSensitive\",\"password_bytes\":\"IAmSensitiveBytes\"}","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.SecretStr","title":"Pydantic Types - pydantic.types - SecretStr","objectID":"/latest/api/types/#pydantic.types.SecretStr","rank":-100},{"content":"Bases: [] A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks. It displays b'**********' instead of the string value on repr() and str() calls.\nWhen the secret value is nonempty, it is displayed as b'**********' instead of the underlying value in\ncalls to repr() and str() . If the value is empty, it is displayed as b'' . from pydantic import BaseModel, SecretBytes\n\nclass User(BaseModel):\n    username: str\n    password: SecretBytes\n\nuser = User(username='scolvin', password=b'password1')\n#> username='scolvin' password=SecretBytes(b'**********')\nprint(user.password.get_secret_value())\n#> b'password1'\nprint((SecretBytes(b'password'), SecretBytes(b'')))\n#> (SecretBytes(b'**********'), SecretBytes(b''))","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.SecretBytes","title":"Pydantic Types - pydantic.types - SecretBytes","objectID":"/latest/api/types/#pydantic.types.SecretBytes","rank":-105},{"content":"Bases: Based on: https://en.wikipedia.org/wiki/Payment_card_number. masked property ¶ masked: Mask all but the last 4 digits of the card number. Returns: Type Description A masked card number string. validate classmethod ¶ validate(\n    input_value: , /, _: \n) -> Validate the card number and return a PaymentCardNumber instance. validate_digits classmethod ¶ validate_digits(card_number: ) -> None Validate that the card number is all digits. validate_luhn_check_digit classmethod ¶ validate_luhn_check_digit(card_number: ) -> Based on: https://en.wikipedia.org/wiki/Luhn_algorithm. validate_brand staticmethod ¶ validate_brand(card_number: ) -> Validate length based on BIN for major brands:\nhttps://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.PaymentCardNumber","title":"Pydantic Types - pydantic.types - PaymentCardNumber","objectID":"/latest/api/types/#pydantic.types.PaymentCardNumber","rank":-110},{"content":"Bases: Converts a string representing a number of bytes with units (such as '1KB' or '11.5MiB' ) into an integer. You can use the ByteSize data type to (case-insensitively) convert a string representation of a number of bytes into\nan integer, and also to print out human-readable strings representing a number of bytes. In conformance with IEC 80000-13 Standard we interpret '1KB' to mean 1000 bytes,\nand '1KiB' to mean 1024 bytes. In general, including a middle 'i' will cause the unit to be interpreted as a power of 2,\nrather than a power of 10 (so, for example, '1 MB' is treated as 1_000_000 bytes, whereas '1 MiB' is treated as 1_048_576 bytes). Info Note that 1b will be parsed as \"1 byte\" and not \"1 bit\". from pydantic import BaseModel, ByteSize\n\nclass MyModel(BaseModel):\n    size: ByteSize\n\nprint(MyModel(size=52000).size)\n#> 52000\nprint(MyModel(size='3000 KiB').size)\n#> 3072000\n\nm = MyModel(size='50 PB')\nprint(m.size.human_readable())\n#> 44.4PiB\nprint(m.size.human_readable(decimal=True))\n#> 50.0PB\nprint(m.size.human_readable(separator=' '))\n#> 44.4 PiB\n\nprint(m.size.to('TiB'))\n#> 45474.73508864641 human_readable ¶ human_readable(\n    decimal:  = False, separator:  = \"\"\n) -> Converts a byte size to a human readable string. Parameters: Name Type Description Default decimal If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units\n(e.g. 1024 bytes per KiB). False separator A string used to split the value and unit. Defaults to an empty string (''). '' Returns: Type Description A human readable string representation of the byte size. to ¶ to(unit: ) -> Converts a byte size to another unit, including both byte and bit units. Parameters: Name Type Description Default unit The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,\nKiB, MiB, GiB, TiB, PiB, EiB (byte units) and\nbit, kbit, mbit, gbit, tbit, pbit, ebit,\nkibit, mibit, gibit, tibit, pibit, eibit (bit units). required Returns: Type Description The byte size in the new unit.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.ByteSize","title":"Pydantic Types - pydantic.types - ByteSize","objectID":"/latest/api/types/#pydantic.types.ByteSize","rank":-115},{"content":"A date in the past.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.PastDate","title":"Pydantic Types - pydantic.types - PastDate","objectID":"/latest/api/types/#pydantic.types.PastDate","rank":-120},{"content":"A date in the future.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.FutureDate","title":"Pydantic Types - pydantic.types - FutureDate","objectID":"/latest/api/types/#pydantic.types.FutureDate","rank":-125},{"content":"A datetime that requires timezone info.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.AwareDatetime","title":"Pydantic Types - pydantic.types - AwareDatetime","objectID":"/latest/api/types/#pydantic.types.AwareDatetime","rank":-130},{"content":"A datetime that doesn't require timezone info.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.NaiveDatetime","title":"Pydantic Types - pydantic.types - NaiveDatetime","objectID":"/latest/api/types/#pydantic.types.NaiveDatetime","rank":-135},{"content":"A datetime that must be in the past.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.PastDatetime","title":"Pydantic Types - pydantic.types - PastDatetime","objectID":"/latest/api/types/#pydantic.types.PastDatetime","rank":-140},{"content":"A datetime that must be in the future.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.FutureDatetime","title":"Pydantic Types - pydantic.types - FutureDatetime","objectID":"/latest/api/types/#pydantic.types.FutureDatetime","rank":-145},{"content":"Bases: Protocol for encoding and decoding data to and from bytes. decode classmethod ¶ decode(data: ) -> Decode the data using the encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data using the encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> Get the JSON format for the encoded data. Returns: Type Description The JSON format for the encoded data.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.EncoderProtocol","title":"Pydantic Types - pydantic.types - EncoderProtocol","objectID":"/latest/api/types/#pydantic.types.EncoderProtocol","rank":-150},{"content":"Bases: Standard (non-URL-safe) Base64 encoder. decode classmethod ¶ decode(data: ) -> Decode the data from base64 encoded bytes to original bytes data. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data from bytes to a base64 encoded bytes. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> ['base64'] Get the JSON format for the encoded data. Returns: Type Description ['base64'] The JSON format for the encoded data.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Base64Encoder","title":"Pydantic Types - pydantic.types - Base64Encoder","objectID":"/latest/api/types/#pydantic.types.Base64Encoder","rank":-155},{"content":"Bases: URL-safe Base64 encoder. decode classmethod ¶ decode(data: ) -> Decode the data from base64 encoded bytes to original bytes data. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode classmethod ¶ encode(value: ) -> Encode the data from bytes to a base64 encoded bytes. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data. get_json_format classmethod ¶ get_json_format() -> ['base64url'] Get the JSON format for the encoded data. Returns: Type Description ['base64url'] The JSON format for the encoded data.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Base64UrlEncoder","title":"Pydantic Types - pydantic.types - Base64UrlEncoder","objectID":"/latest/api/types/#pydantic.types.Base64UrlEncoder","rank":-160},{"content":"A bytes type that is encoded and decoded using the specified encoder. EncodedBytes needs an encoder that implements EncoderProtocol to operate. from typing import Annotated\n\nfrom pydantic import BaseModel, EncodedBytes, EncoderProtocol, ValidationError\n\nclass MyEncoder(EncoderProtocol):\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        if data == b'**undecodable**':\n            raise ValueError('Cannot decode data')\n        return data[13:]\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        return b'**encoded**: ' + value\n\n    @classmethod\n    def get_json_format(cls) -> str:\n        return 'my-encoder'\n\nMyEncodedBytes = Annotated[bytes, EncodedBytes(encoder=MyEncoder)]\n\nclass Model(BaseModel):\n    my_encoded_bytes: MyEncodedBytes\n\n# Initialize the model with encoded data\nm = Model(my_encoded_bytes=b'**encoded**: some bytes')\n\n# Access decoded value\nprint(m.my_encoded_bytes)\n#> b'some bytes'\n\n# Serialize into the encoded form\nprint(m.model_dump())\n#> {'my_encoded_bytes': b'**encoded**: some bytes'}\n\n# Validate encoded data\ntry:\n    Model(my_encoded_bytes=b'**undecodable**')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    my_encoded_bytes\n      Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]\n    ''' decode ¶ decode(data: , _: ) -> Decode the data using the specified encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode ¶ encode(value: ) -> Encode the data using the specified encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.EncodedBytes","title":"Pydantic Types - pydantic.types - EncodedBytes  dataclass","objectID":"/latest/api/types/#pydantic.types.EncodedBytes","rank":-165},{"content":"A str type that is encoded and decoded using the specified encoder. EncodedStr needs an encoder that implements EncoderProtocol to operate. from typing import Annotated\n\nfrom pydantic import BaseModel, EncodedStr, EncoderProtocol, ValidationError\n\nclass MyEncoder(EncoderProtocol):\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        if data == b'**undecodable**':\n            raise ValueError('Cannot decode data')\n        return data[13:]\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        return b'**encoded**: ' + value\n\n    @classmethod\n    def get_json_format(cls) -> str:\n        return 'my-encoder'\n\nMyEncodedStr = Annotated[str, EncodedStr(encoder=MyEncoder)]\n\nclass Model(BaseModel):\n    my_encoded_str: MyEncodedStr\n\n# Initialize the model with encoded data\nm = Model(my_encoded_str='**encoded**: some str')\n\n# Access decoded value\nprint(m.my_encoded_str)\n#> some str\n\n# Serialize into the encoded form\nprint(m.model_dump())\n#> {'my_encoded_str': '**encoded**: some str'}\n\n# Validate encoded data\ntry:\n    Model(my_encoded_str='**undecodable**')\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    my_encoded_str\n      Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]\n    ''' decode_str ¶ decode_str(data: , _: ) -> Decode the data using the specified encoder. Parameters: Name Type Description Default data The data to decode. required Returns: Type Description The decoded data. encode_str ¶ encode_str(value: ) -> Encode the data using the specified encoder. Parameters: Name Type Description Default value The data to encode. required Returns: Type Description The encoded data.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.EncodedStr","title":"Pydantic Types - pydantic.types - EncodedStr  dataclass","objectID":"/latest/api/types/#pydantic.types.EncodedStr","rank":-170},{"content":"Usage Documentation Using GetPydanticSchema to Reduce Boilerplate A convenience class for creating an annotation that provides pydantic custom type hooks. This class is intended to eliminate the need to create a custom \"marker\" which defines the __get_pydantic_core_schema__ and __get_pydantic_json_schema__ custom hook methods. For example, to have a field treated by type checkers as int , but by pydantic as Any , you can do: from typing import Annotated, Any\n\nfrom pydantic import BaseModel, GetPydanticSchema\n\nHandleAsAny = GetPydanticSchema(lambda _s, h: h(Any))\n\nclass Model(BaseModel):\n    x: Annotated[int, HandleAsAny]  # pydantic sees `x: Any`\n\nprint(repr(Model(x='abc').x))\n#> 'abc'","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.GetPydanticSchema","title":"Pydantic Types - pydantic.types - GetPydanticSchema  dataclass","objectID":"/latest/api/types/#pydantic.types.GetPydanticSchema","rank":-175},{"content":"Provides a way to specify the expected tag to use for a case of a (callable) discriminated union. Also provides a way to label a union case in error messages. When using a callable Discriminator , attach a Tag to each case in the Union to specify the tag that\nshould be used to identify that case. For example, in the below example, the Tag is used to specify that\nif get_discriminator_value returns 'apple' , the input should be validated as an ApplePie , and if it\nreturns 'pumpkin' , the input should be validated as a PumpkinPie . The primary role of the Tag here is to map the return value from the callable Discriminator function to\nthe appropriate member of the Union in question. from typing import Annotated, Any, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        Union[\n            Annotated[ApplePie, Tag('apple')],\n            Annotated[PumpkinPie, Tag('pumpkin')],\n        ],\n        Discriminator(get_discriminator_value),\n    ]\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n'''\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n'''\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n'''\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n''' Note You must specify a Tag for every case in a Tag that is associated with a\ncallable Discriminator . Failing to do so will result in a PydanticUserError with code callable-discriminator-no-tag . See the Discriminated Unions concepts docs for more details on how to use Tag s.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Tag","title":"Pydantic Types - pydantic.types - Tag  dataclass","objectID":"/latest/api/types/#pydantic.types.Tag","rank":-180},{"content":"Usage Documentation Discriminated Unions with Callable Discriminator Provides a way to use a custom callable as the way to extract the value of a union discriminator. This allows you to get validation behavior like you'd get from Field(discriminator=<field_name>) ,\nbut without needing to have a single shared field across all the union choices. This also makes it\npossible to handle unions of models and primitive types with discriminated-union-style validation errors.\nFinally, this allows you to use a custom callable as the way to identify which member of a union a value\nbelongs to, while still seeing all the performance benefits of a discriminated union. Consider this example, which is much more performant with the use of Discriminator and thus a TaggedUnion than it would be as a normal Union . from typing import Annotated, Any, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        Union[\n            Annotated[ApplePie, Tag('apple')],\n            Annotated[PumpkinPie, Tag('pumpkin')],\n        ],\n        Discriminator(get_discriminator_value),\n    ]\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n'''\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n'''\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n'''\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n''' See the Discriminated Unions concepts docs for more details on how to use Discriminator s. discriminator instance-attribute ¶ discriminator:  | [[], ] The callable or field name for discriminating the type in a tagged union. A Callable discriminator must extract the value of the discriminator from the input.\nA str discriminator must be the name of a field to discriminate against. custom_error_type class-attribute instance-attribute ¶ custom_error_type:  | None = None Type to use in custom errors replacing the standard discriminated union\nvalidation errors. custom_error_message class-attribute instance-attribute ¶ custom_error_message:  | None = None Message to use in custom errors. custom_error_context class-attribute instance-attribute ¶ custom_error_context: (\n    [,  |  | ] | None\n) = None Context to use in custom errors.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.Discriminator","title":"Pydantic Types - pydantic.types - Discriminator  dataclass","objectID":"/latest/api/types/#pydantic.types.Discriminator","rank":-185},{"content":"Bases: , A FailFast annotation can be used to specify that validation should stop at the first error. This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not. You might want to enable this setting if you want to validate your data faster (basically, if you use this,\nvalidation will be more performant with the caveat that you get less information). from typing import Annotated\n\nfrom pydantic import BaseModel, FailFast, ValidationError\n\nclass Model(BaseModel):\n    x: Annotated[list[int], FailFast()]\n\n# This will raise a single error for the first invalid value and stop validation\ntry:\n    obj = Model(x=[1, 2, 'a', 4, 5, 'b', 7, 8, 9, 'c'])\nexcept ValidationError as e:\n    print(e)\n    '''\n    1 validation error for Model\n    x.2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.FailFast","title":"Pydantic Types - pydantic.types - FailFast  dataclass","objectID":"/latest/api/types/#pydantic.types.FailFast","rank":-190},{"content":"conint(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None,\n    multiple_of:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that conint returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, conint\n\nclass Foo(BaseModel):\n    bar: conint(strict=True, gt=0) from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[int, Field(strict=True, gt=0)] A wrapper around int that allows for additional constraints. Parameters: Name Type Description Default strict | None Whether to validate the integer in strict mode. Defaults to None . None gt | None The value must be greater than this. None ge | None The value must be greater than or equal to this. None lt | None The value must be less than this. None le | None The value must be less than or equal to this. None multiple_of | None The value must be a multiple of this. None Returns: Type Description [] The wrapped integer type. from pydantic import BaseModel, ValidationError, conint\n\nclass ConstrainedExample(BaseModel):\n    constrained_int: conint(gt=1)\n\nm = ConstrainedExample(constrained_int=2)\nprint(repr(m))\n#> ConstrainedExample(constrained_int=2)\n\ntry:\n    ConstrainedExample(constrained_int=0)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_int',),\n            'msg': 'Input should be greater than 1',\n            'input': 0,\n            'ctx': {'gt': 1},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.conint","title":"Pydantic Types - pydantic.types - conint","objectID":"/latest/api/types/#pydantic.types.conint","rank":-195},{"content":"confloat(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None,\n    multiple_of:  | None = None,\n    allow_inf_nan:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that confloat returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, confloat\n\nclass Foo(BaseModel):\n    bar: confloat(strict=True, gt=0) from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[float, Field(strict=True, gt=0)] A wrapper around float that allows for additional constraints. Parameters: Name Type Description Default strict | None Whether to validate the float in strict mode. None gt | None The value must be greater than this. None ge | None The value must be greater than or equal to this. None lt | None The value must be less than this. None le | None The value must be less than or equal to this. None multiple_of | None The value must be a multiple of this. None allow_inf_nan | None Whether to allow -inf , inf , and nan . None Returns: Type Description [] The wrapped float type. from pydantic import BaseModel, ValidationError, confloat\n\nclass ConstrainedExample(BaseModel):\n    constrained_float: confloat(gt=1.0)\n\nm = ConstrainedExample(constrained_float=1.1)\nprint(repr(m))\n#> ConstrainedExample(constrained_float=1.1)\n\ntry:\n    ConstrainedExample(constrained_float=0.9)\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_float',),\n            'msg': 'Input should be greater than 1',\n            'input': 0.9,\n            'ctx': {'gt': 1.0},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.confloat","title":"Pydantic Types - pydantic.types - confloat","objectID":"/latest/api/types/#pydantic.types.confloat","rank":-200},{"content":"conbytes(\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    strict:  | None = None\n) -> [] A wrapper around bytes that allows for additional constraints. Parameters: Name Type Description Default min_length | None The minimum length of the bytes. None max_length | None The maximum length of the bytes. None strict | None Whether to validate the bytes in strict mode. None Returns: Type Description [] The wrapped bytes type.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.conbytes","title":"Pydantic Types - pydantic.types - conbytes","objectID":"/latest/api/types/#pydantic.types.conbytes","rank":-205},{"content":"constr(\n    *,\n    strip_whitespace:  | None = None,\n    to_upper:  | None = None,\n    to_lower:  | None = None,\n    strict:  | None = None,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    pattern:  | [] | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that constr returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, constr\n\nclass Foo(BaseModel):\n    bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$') from typing import Annotated\n\nfrom pydantic import BaseModel, StringConstraints\n\nclass Foo(BaseModel):\n    bar: Annotated[\n        str,\n        StringConstraints(\n            strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$'\n        ),\n    ] A wrapper around str that allows for additional constraints. from pydantic import BaseModel, constr\n\nclass Foo(BaseModel):\n    bar: constr(strip_whitespace=True, to_upper=True)\n\nfoo = Foo(bar='  hello  ')\nprint(foo)\n#> bar='HELLO' Parameters: Name Type Description Default strip_whitespace | None Whether to remove leading and trailing whitespace. None to_upper | None Whether to turn all characters to uppercase. None to_lower | None Whether to turn all characters to lowercase. None strict | None Whether to validate the string in strict mode. None min_length | None The minimum length of the string. None max_length | None The maximum length of the string. None pattern | [] | None A regex pattern to validate the string against. None Returns: Type Description [] The wrapped string type.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.constr","title":"Pydantic Types - pydantic.types - constr","objectID":"/latest/api/types/#pydantic.types.constr","rank":-210},{"content":"conset(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None\n) -> [[]] A wrapper around typing.Set that allows for additional constraints. Parameters: Name Type Description Default item_type [] The type of the items in the set. required min_length | None The minimum length of the set. None max_length | None The maximum length of the set. None Returns: Type Description [[]] The wrapped set type.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.conset","title":"Pydantic Types - pydantic.types - conset","objectID":"/latest/api/types/#pydantic.types.conset","rank":-215},{"content":"confrozenset(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None\n) -> [[]] A wrapper around typing.FrozenSet that allows for additional constraints. Parameters: Name Type Description Default item_type [] The type of the items in the frozenset. required min_length | None The minimum length of the frozenset. None max_length | None The maximum length of the frozenset. None Returns: Type Description [[]] The wrapped frozenset type.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.confrozenset","title":"Pydantic Types - pydantic.types - confrozenset","objectID":"/latest/api/types/#pydantic.types.confrozenset","rank":-220},{"content":"conlist(\n    item_type: [],\n    *,\n    min_length:  | None = None,\n    max_length:  | None = None,\n    unique_items:  | None = None\n) -> [[]] A wrapper around  that adds validation. Parameters: Name Type Description Default item_type [] The type of the items in the list. required min_length | None The minimum length of the list. Defaults to None. None max_length | None The maximum length of the list. Defaults to None. None unique_items | None Whether the items in the list must be unique. Defaults to None. Warning The unique_items parameter is deprecated, use Set instead.\nSee this issue for more details. None Returns: Type Description [[]] The wrapped list type.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.conlist","title":"Pydantic Types - pydantic.types - conlist","objectID":"/latest/api/types/#pydantic.types.conlist","rank":-225},{"content":"condecimal(\n    *,\n    strict:  | None = None,\n    gt:  |  | None = None,\n    ge:  |  | None = None,\n    lt:  |  | None = None,\n    le:  |  | None = None,\n    multiple_of:  |  | None = None,\n    max_digits:  | None = None,\n    decimal_places:  | None = None,\n    allow_inf_nan:  | None = None\n) -> [] Discouraged This function is discouraged in favor of using Annotated with\n instead. This function will be deprecated in Pydantic 3.0. The reason is that condecimal returns a type, which doesn't play well with static analysis tools. Don't do this Do this from pydantic import BaseModel, condecimal\n\nclass Foo(BaseModel):\n    bar: condecimal(strict=True, allow_inf_nan=True) from decimal import Decimal\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n    bar: Annotated[Decimal, Field(strict=True, allow_inf_nan=True)] A wrapper around Decimal that adds validation. Parameters: Name Type Description Default strict | None Whether to validate the value in strict mode. Defaults to None . None gt |  | None The value must be greater than this. Defaults to None . None ge |  | None The value must be greater than or equal to this. Defaults to None . None lt |  | None The value must be less than this. Defaults to None . None le |  | None The value must be less than or equal to this. Defaults to None . None multiple_of |  | None The value must be a multiple of this. Defaults to None . None max_digits | None The maximum number of digits. Defaults to None . None decimal_places | None The number of decimal places. Defaults to None . None allow_inf_nan | None Whether to allow infinity and NaN. Defaults to None . None from decimal import Decimal\n\nfrom pydantic import BaseModel, ValidationError, condecimal\n\nclass ConstrainedExample(BaseModel):\n    constrained_decimal: condecimal(gt=Decimal('1.0'))\n\nm = ConstrainedExample(constrained_decimal=Decimal('1.1'))\nprint(repr(m))\n#> ConstrainedExample(constrained_decimal=Decimal('1.1'))\n\ntry:\n    ConstrainedExample(constrained_decimal=Decimal('0.9'))\nexcept ValidationError as e:\n    print(e.errors())\n    '''\n    [\n        {\n            'type': 'greater_than',\n            'loc': ('constrained_decimal',),\n            'msg': 'Input should be greater than 1.0',\n            'input': Decimal('0.9'),\n            'ctx': {'gt': Decimal('1.0')},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        }\n    ]\n    '''","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.condecimal","title":"Pydantic Types - pydantic.types - condecimal","objectID":"/latest/api/types/#pydantic.types.condecimal","rank":-230},{"content":"condate(\n    *,\n    strict:  | None = None,\n    gt:  | None = None,\n    ge:  | None = None,\n    lt:  | None = None,\n    le:  | None = None\n) -> [] A wrapper for date that adds constraints. Parameters: Name Type Description Default strict | None Whether to validate the date value in strict mode. Defaults to None . None gt | None The value must be greater than this. Defaults to None . None ge | None The value must be greater than or equal to this. Defaults to None . None lt | None The value must be less than this. Defaults to None . None le | None The value must be less than or equal to this. Defaults to None . None Returns: Type Description [] A date type with the specified constraints.","pageID":"Pydantic Types","abs_url":"/latest/api/types/#pydantic.types.condate","title":"Pydantic Types - pydantic.types - condate","objectID":"/latest/api/types/#pydantic.types.condate","rank":-235},{"content":"Decorator for validating function calls. validate_call ¶ validate_call(\n    *,\n    config:  | None = None,\n    validate_return:  = False\n) -> [[], ] validate_call(func: ) -> validate_call(\n    func:  | None = None,\n    /,\n    *,\n    config:  | None = None,\n    validate_return:  = False,\n) ->  | [[], ] Usage Documentation Validation Decorator Returns a decorated wrapper around the function that validates the arguments and, optionally, the return value. Usage may be either as a plain decorator @validate_call or with arguments @validate_call(...) . Parameters: Name Type Description Default func | None The function to be decorated. None config | None The configuration dictionary. None validate_return Whether to validate the return value. False Returns: Type Description | [[], ] The decorated function.","pageID":"Validate Call","abs_url":"/latest/api/validate_call/#Validate Call","title":"Validate Call","objectID":"/latest/api/validate_call/#Validate Call","rank":100},{"content":"validate_call(\n    *,\n    config:  | None = None,\n    validate_return:  = False\n) -> [[], ] validate_call(func: ) -> validate_call(\n    func:  | None = None,\n    /,\n    *,\n    config:  | None = None,\n    validate_return:  = False,\n) ->  | [[], ] Usage Documentation Validation Decorator Returns a decorated wrapper around the function that validates the arguments and, optionally, the return value. Usage may be either as a plain decorator @validate_call or with arguments @validate_call(...) . Parameters: Name Type Description Default func | None The function to be decorated. None config | None The configuration dictionary. None validate_return Whether to validate the return value. False Returns: Type Description | [[], ] The decorated function.","pageID":"Validate Call","abs_url":"/latest/api/validate_call/#pydantic.validate_call_decorator.validate_call","title":"Validate Call - validate_call","objectID":"/latest/api/validate_call/#pydantic.validate_call_decorator.validate_call","rank":95},{"content":"pydantic.__version__ module-attribute ¶ __version__ = pydantic.version.version_info ¶ version_info() -> Return complete version information for Pydantic and its dependencies.","pageID":"Version Information","abs_url":"/latest/api/version/#Version Information","title":"Version Information","objectID":"/latest/api/version/#Version Information","rank":100},{"content":"__version__ =","pageID":"Version Information","abs_url":"/latest/api/version/#pydantic.__version__","title":"Version Information - pydantic.__version__  module-attribute","objectID":"/latest/api/version/#pydantic.__version__","rank":95},{"content":"version_info() -> Return complete version information for Pydantic and its dependencies.","pageID":"Version Information","abs_url":"/latest/api/version/#pydantic.version.version_info","title":"Version Information - pydantic.version.version_info","objectID":"/latest/api/version/#pydantic.version.version_info","rank":90},{"content":"An alias is an alternative name for a field, used when serializing and deserializing data. You can specify an alias in the following ways: alias on the must be a str validation_alias on the can be an instance of str , , or serialization_alias on the must be a str alias_generator on the can be a callable or an instance of For examples of how to use alias , validation_alias , and serialization_alias , see Field aliases .","pageID":"Alias","abs_url":"/latest/concepts/alias/#Alias","title":"Alias","objectID":"/latest/concepts/alias/#Alias","rank":100},{"content":"Pydantic provides two special types for convenience when using validation_alias : AliasPath and AliasChoices . The AliasPath is used to specify a path to a field using aliases. For example: from pydantic import BaseModel, Field, AliasPath\n\n\nclass User(BaseModel):\n    first_name: str = Field(validation_alias=AliasPath('names', 0))\n    last_name: str = Field(validation_alias=AliasPath('names', 1))\n\nuser = User.model_validate({'names': ['John', 'Doe']})  # (1)!\nprint(user)\n#> first_name='John' last_name='Doe' We are using model_validate to validate a dictionary using the field aliases. You can see more details about  in the API reference. In the 'first_name' field, we are using the alias 'names' and the index 0 to specify the path to the first name.\nIn the 'last_name' field, we are using the alias 'names' and the index 1 to specify the path to the last name. AliasChoices is used to specify a choice of aliases. For example: from pydantic import BaseModel, Field, AliasChoices\n\n\nclass User(BaseModel):\n    first_name: str = Field(validation_alias=AliasChoices('first_name', 'fname'))\n    last_name: str = Field(validation_alias=AliasChoices('last_name', 'lname'))\n\nuser = User.model_validate({'fname': 'John', 'lname': 'Doe'})  # (1)!\nprint(user)\n#> first_name='John' last_name='Doe'\nuser = User.model_validate({'first_name': 'John', 'lname': 'Doe'})  # (2)!\nprint(user)\n#> first_name='John' last_name='Doe' We are using the second alias choice for both fields. We are using the first alias choice for the field 'first_name' and the second alias choice\n   for the field 'last_name' . You can also use AliasChoices with AliasPath : from pydantic import BaseModel, Field, AliasPath, AliasChoices\n\n\nclass User(BaseModel):\n    first_name: str = Field(validation_alias=AliasChoices('first_name', AliasPath('names', 0)))\n    last_name: str = Field(validation_alias=AliasChoices('last_name', AliasPath('names', 1)))\n\n\nuser = User.model_validate({'first_name': 'John', 'last_name': 'Doe'})\nprint(user)\n#> first_name='John' last_name='Doe'\nuser = User.model_validate({'names': ['John', 'Doe']})\nprint(user)\n#> first_name='John' last_name='Doe'\nuser = User.model_validate({'names': ['John'], 'last_name': 'Doe'})\nprint(user)\n#> first_name='John' last_name='Doe'","pageID":"Alias","abs_url":"/latest/concepts/alias/#aliaspath-and-aliaschoices","title":"Alias - AliasPath and AliasChoices","objectID":"/latest/concepts/alias/#aliaspath-and-aliaschoices","rank":95},{"content":"You can use the alias_generator parameter of  to specify\na callable (or group of callables, via AliasGenerator ) that will generate aliases for all fields in a model.\nThis is useful if you want to use a consistent naming convention for all fields in a model, but do not\nwant to specify the alias for each field individually. Note Pydantic offers three built-in alias generators that you can use out of the box:","pageID":"Alias","abs_url":"/latest/concepts/alias/#using-alias-generators","title":"Alias - Using alias generators","objectID":"/latest/concepts/alias/#using-alias-generators","rank":90},{"content":"Here's a basic example using a callable: from pydantic import BaseModel, ConfigDict\n\n\nclass Tree(BaseModel):\n    model_config = ConfigDict(\n        alias_generator=lambda field_name: field_name.upper()\n    )\n\n    age: int\n    height: float\n    kind: str\n\n\nt = Tree.model_validate({'AGE': 12, 'HEIGHT': 1.2, 'KIND': 'oak'})\nprint(t.model_dump(by_alias=True))\n#> {'AGE': 12, 'HEIGHT': 1.2, 'KIND': 'oak'}","pageID":"Alias","abs_url":"/latest/concepts/alias/#using-a-callable","title":"Alias - Using alias generators - Using a callable","objectID":"/latest/concepts/alias/#using-a-callable","rank":85},{"content":"AliasGenerator is a class that allows you to specify multiple alias generators for a model.\nYou can use an AliasGenerator to specify different alias generators for validation and serialization. This is particularly useful if you need to use different naming conventions for loading and saving data,\nbut you don't want to specify the validation and serialization aliases for each field individually. For example: from pydantic import AliasGenerator, BaseModel, ConfigDict\n\n\nclass Tree(BaseModel):\n    model_config = ConfigDict(\n        alias_generator=AliasGenerator(\n            validation_alias=lambda field_name: field_name.upper(),\n            serialization_alias=lambda field_name: field_name.title(),\n        )\n    )\n\n    age: int\n    height: float\n    kind: str\n\n\nt = Tree.model_validate({'AGE': 12, 'HEIGHT': 1.2, 'KIND': 'oak'})\nprint(t.model_dump(by_alias=True))\n#> {'Age': 12, 'Height': 1.2, 'Kind': 'oak'}","pageID":"Alias","abs_url":"/latest/concepts/alias/#using-an-aliasgenerator","title":"Alias - Using alias generators - Using an AliasGenerator","objectID":"/latest/concepts/alias/#using-an-aliasgenerator","rank":80},{"content":"If you specify an alias on the , it will take precedence over the generated alias by default: from pydantic import BaseModel, ConfigDict, Field\n\n\ndef to_camel(string: str) -> str:\n    return ''.join(word.capitalize() for word in string.split('_'))\n\n\nclass Voice(BaseModel):\n    model_config = ConfigDict(alias_generator=to_camel)\n\n    name: str\n    language_code: str = Field(alias='lang')\n\n\nvoice = Voice(Name='Filiz', lang='tr-TR')\nprint(voice.language_code)\n#> tr-TR\nprint(voice.model_dump(by_alias=True))\n#> {'Name': 'Filiz', 'lang': 'tr-TR'}","pageID":"Alias","abs_url":"/latest/concepts/alias/#alias-precedence","title":"Alias - Alias Precedence","objectID":"/latest/concepts/alias/#alias-precedence","rank":75},{"content":"You may set alias_priority on a field to change this behavior: alias_priority=2 the alias will not be overridden by the alias generator. alias_priority=1 the alias will be overridden by the alias generator. alias_priority not set: alias is set: the alias will not be overridden by the alias generator. alias is not set: the alias will be overridden by the alias generator. The same precedence applies to validation_alias and serialization_alias .\nSee more about the different field aliases under field aliases .","pageID":"Alias","abs_url":"/latest/concepts/alias/#alias-priority","title":"Alias - Alias Precedence - Alias Priority","objectID":"/latest/concepts/alias/#alias-priority","rank":70},{"content":"You can use ConfigDict settings or runtime validation/serialization\nsettings to control whether or not aliases are used.","pageID":"Alias","abs_url":"/latest/concepts/alias/#alias-configuration","title":"Alias - Alias Configuration","objectID":"/latest/concepts/alias/#alias-configuration","rank":65},{"content":"You can use configuration settings to control, at the model level,\nwhether or not aliases are used for validation and serialization. If you would like to control\nthis behavior for nested models/surpassing the config-model boundary, use runtime settings . Validation ¶ When validating data, you can enable population of attributes by attribute name, alias, or both. By default , Pydantic uses aliases for validation. Further configuration is available via: : True by default : False by default validate_by_alias validate_by_name validate_by_alias and validate_by_name from pydantic import BaseModel, ConfigDict, Field\n\n\nclass Model(BaseModel):\n    my_field: str = Field(validation_alias='my_alias')\n\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=False)\n\n\nprint(repr(Model(my_alias='foo')))  # (1)!\n#> Model(my_field='foo') The alias my_alias is used for validation. from pydantic import BaseModel, ConfigDict, Field\n\n\nclass Model(BaseModel):\n    my_field: str = Field(validation_alias='my_alias')\n\n    model_config = ConfigDict(validate_by_alias=False, validate_by_name=True)\n\n\nprint(repr(Model(my_field='foo')))  # (1)!\n#> Model(my_field='foo') the attribute identifier my_field is used for validation. from pydantic import BaseModel, ConfigDict, Field\n\n\nclass Model(BaseModel):\n    my_field: str = Field(validation_alias='my_alias')\n\n    model_config = ConfigDict(validate_by_alias=True, validate_by_name=True)\n\n\nprint(repr(Model(my_alias='foo')))  # (1)!\n#> Model(my_field='foo')\n\nprint(repr(Model(my_field='foo')))  # (2)!\n#> Model(my_field='foo') The alias my_alias is used for validation. the attribute identifier my_field is used for validation. Warning You cannot set both validate_by_alias and validate_by_name to False .\nA user error is raised in this case. Serialization ¶ When serializing data, you can enable serialization by alias, which is disabled by default.\nSee the  API documentation\nfor more details. from pydantic import BaseModel, ConfigDict, Field\n\n\nclass Model(BaseModel):\n    my_field: str = Field(serialization_alias='my_alias')\n\n    model_config = ConfigDict(serialize_by_alias=True)\n\n\nm = Model(my_field='foo')\nprint(m.model_dump())  # (1)!\n#> {'my_alias': 'foo'} The alias my_alias is used for serialization. Note The fact that serialization by alias is disabled by default is notably inconsistent with the default for\nvalidation (where aliases are used by default). We anticipate changing this default in V3.","pageID":"Alias","abs_url":"/latest/concepts/alias/#configdict-settings","title":"Alias - Alias Configuration - ConfigDict Settings","objectID":"/latest/concepts/alias/#configdict-settings","rank":60},{"content":"You can use runtime alias flags to control alias use for validation and serialization\non a per-call basis. If you would like to control this behavior on a model level, use ConfigDict settings . Validation ¶ When validating data, you can enable population of attributes by attribute name, alias, or both. The by_alias and by_name flags are available on the ,\n, and  methods, as well as the  validation methods. By default: by_alias is True by_name is False by_alias by_name validate_by_alias and validate_by_name from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    my_field: str = Field(validation_alias='my_alias')\n\n\nm = Model.model_validate(\n    {'my_alias': 'foo'},  # (1)!\n    by_alias=True,\n    by_name=False,\n)\nprint(repr(m))\n#> Model(my_field='foo') The alias my_alias is used for validation. from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    my_field: str = Field(validation_alias='my_alias')\n\n\nm = Model.model_validate(\n    {'my_field': 'foo'}, by_alias=False, by_name=True  # (1)!\n)\nprint(repr(m))\n#> Model(my_field='foo') The attribute name my_field is used for validation. from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    my_field: str = Field(validation_alias='my_alias')\n\n\nm = Model.model_validate(\n    {'my_alias': 'foo'}, by_alias=True, by_name=True  # (1)!\n)\nprint(repr(m))\n#> Model(my_field='foo')\n\nm = Model.model_validate(\n    {'my_field': 'foo'}, by_alias=True, by_name=True  # (2)!\n)\nprint(repr(m))\n#> Model(my_field='foo') The alias my_alias is used for validation. The attribute name my_field is used for validation. Warning You cannot set both by_alias and by_name to False .\nA user error is raised in this case. Serialization ¶ When serializing data, you can enable serialization by alias via the by_alias flag\nwhich is available on the  and\n methods, as well as\nthe  ones. By default, by_alias is False . from pydantic import BaseModel , Field class Model ( BaseModel ): my_field : str = Field ( serialization_alias = 'my_alias' ) m = Model ( my_field = 'foo' ) print ( m . model_dump ( by_alias = True )) # (1)! #> {'my_alias': 'foo'} The alias my_alias is used for serialization. Note The fact that serialization by alias is disabled by default is notably inconsistent with the default for\nvalidation (where aliases are used by default). We anticipate changing this default in V3.","pageID":"Alias","abs_url":"/latest/concepts/alias/#runtime-settings","title":"Alias - Alias Configuration - Runtime Settings","objectID":"/latest/concepts/alias/#runtime-settings","rank":55},{"content":"The behaviour of Pydantic can be controlled via a variety of configuration values, documented\non the  class. This page describes how configuration can be\nspecified for Pydantic's supported types.","pageID":"Configuration","abs_url":"/latest/concepts/config/#Configuration","title":"Configuration","objectID":"/latest/concepts/config/#Configuration","rank":100},{"content":"On Pydantic models, configuration can be specified in two ways: Using the  class attribute: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    model_config = ConfigDict(str_max_length=5)  # (1)!\n\n    v: str\n\n\ntry:\n    m = Model(v='abcdef')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    v\n      String should have at most 5 characters [type=string_too_long, input_value='abcdef', input_type=str]\n    \"\"\" A plain dictionary (i.e. {'str_max_length': 5} ) can also be used. Note In Pydantic V1, the Config class was used. This is still supported, but deprecated . Using class arguments: from pydantic import BaseModel\n\n\nclass Model(BaseModel, frozen=True):\n    a: str  # (1)! Unlike the  class attribute,\n   static type checkers will recognize the frozen argument, and so any instance\n   mutation will be flagged as an type checking error.","pageID":"Configuration","abs_url":"/latest/concepts/config/#configuration-on-pydantic-models","title":"Configuration - Configuration on Pydantic models","objectID":"/latest/concepts/config/#configuration-on-pydantic-models","rank":95},{"content":"Pydantic dataclasses also support configuration (read more in the dedicated section ). from pydantic import ConfigDict, ValidationError\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass(config=ConfigDict(str_max_length=10, validate_assignment=True))\nclass User:\n    name: str\n\n\nuser = User(name='John Doe')\ntry:\n    user.name = 'x' * 20\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    name\n      String should have at most 10 characters [type=string_too_long, input_value='xxxxxxxxxxxxxxxxxxxx', input_type=str]\n    \"\"\"","pageID":"Configuration","abs_url":"/latest/concepts/config/#configuration-on-pydantic-dataclasses","title":"Configuration - Configuration on Pydantic dataclasses","objectID":"/latest/concepts/config/#configuration-on-pydantic-dataclasses","rank":90},{"content":"Type adapters (using the  class) support configuration,\nby providing a config argument. from pydantic import ConfigDict, TypeAdapter\n\nta = TypeAdapter(list[str], config=ConfigDict(coerce_numbers_to_str=True))\n\nprint(ta.validate_python([1, 2]))\n#> ['1', '2']","pageID":"Configuration","abs_url":"/latest/concepts/config/#configuration-on-typeadapter","title":"Configuration - Configuration on TypeAdapter","objectID":"/latest/concepts/config/#configuration-on-typeadapter","rank":85},{"content":"If you are using  or  classes,\nthe configuration can be set in two ways: Using the __pydantic_config__ class attribute: from dataclasses import dataclass\n\nfrom pydantic import ConfigDict\n\n\n@dataclass\nclass User:\n    __pydantic_config__ = ConfigDict(strict=True)\n\n    id: int\n    name: str = 'John Doe' Using the  decorator (this avoids static type checking errors with\n  ): from typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, with_config\n\n\n@with_config(ConfigDict(str_to_lower=True))\nclass Model(TypedDict):\n    x: str","pageID":"Configuration","abs_url":"/latest/concepts/config/#configuration-on-other-supported-types","title":"Configuration - Configuration on other supported types","objectID":"/latest/concepts/config/#configuration-on-other-supported-types","rank":80},{"content":"If you wish to change the behaviour of Pydantic globally, you can create your own custom parent class\nwith a custom configuration, as the configuration is inherited: from pydantic import BaseModel, ConfigDict\n\n\nclass Parent(BaseModel):\n    model_config = ConfigDict(extra='allow')\n\n\nclass Model(Parent):\n    x: str\n\n\nm = Model(x='foo', y='bar')\nprint(m.model_dump())\n#> {'x': 'foo', 'y': 'bar'} If you provide configuration to the subclasses, it will be merged with the parent configuration: from pydantic import BaseModel, ConfigDict\n\n\nclass Parent(BaseModel):\n    model_config = ConfigDict(extra='allow', str_to_lower=False)\n\n\nclass Model(Parent):\n    model_config = ConfigDict(str_to_lower=True)\n\n    x: str\n\n\nm = Model(x='FOO', y='bar')\nprint(m.model_dump())\n#> {'x': 'foo', 'y': 'bar'}\nprint(Model.model_config)\n#> {'extra': 'allow', 'str_to_lower': True} Warning If your model inherits from multiple bases, Pydantic currently doesn't follow the MRO . For more details, see this issue .","pageID":"Configuration","abs_url":"/latest/concepts/config/#change-behaviour-globally","title":"Configuration - Change behaviour globally","objectID":"/latest/concepts/config/#change-behaviour-globally","rank":75},{"content":"When using types that support configuration as field annotations, configuration may not be propagated: For Pydantic models and dataclasses, configuration will not be propagated, each model has its own\n  \"configuration boundary\": from pydantic import BaseModel, ConfigDict\n\n\nclass User(BaseModel):\n    name: str\n\n\nclass Parent(BaseModel):\n    user: User\n\n    model_config = ConfigDict(str_to_lower=True)\n\n\nprint(Parent(user={'name': 'JOHN'}))\n#> user=User(name='JOHN') For stdlib types (dataclasses and typed dictionaries), configuration will be propagated, unless\n  the type has its own configuration set: from dataclasses import dataclass\n\nfrom pydantic import BaseModel, ConfigDict, with_config\n\n\n@dataclass\nclass UserWithoutConfig:\n    name: str\n\n\n@dataclass\n@with_config(str_to_lower=False)\nclass UserWithConfig:\n    name: str\n\n\nclass Parent(BaseModel):\n    user_1: UserWithoutConfig\n    user_2: UserWithConfig\n\n    model_config = ConfigDict(str_to_lower=True)\n\n\nprint(Parent(user_1={'name': 'JOHN'}, user_2={'name': 'JOHN'}))\n#> user_1=UserWithoutConfig(name='john') user_2=UserWithConfig(name='JOHN')","pageID":"Configuration","abs_url":"/latest/concepts/config/#configuration-propagation","title":"Configuration - Configuration propagation","objectID":"/latest/concepts/config/#configuration-propagation","rank":70},{"content":"The following table provides details on how Pydantic converts data during validation in both strict and lax modes. The \"Strict\" column contains checkmarks for type conversions that are allowed when validating in Strict Mode . All JSON JSON - Strict Python Python - Strict Field Type Input Strict Input Source Conditions bool bool ✓ Python & JSON bool float Python & JSON Allowed values: 0.0, 1.0 . bool int Python & JSON Allowed values: 0, 1 . bool str Python & JSON Allowed values: 'f' , 'n' , 'no' , 'off' , 'false' , 'False' , 't' , 'y' , 'on' , 'yes' , 'true' , 'True' . bool Decimal Python Allowed values: Decimal(0), Decimal(1) . bytes bytearray Python bytes bytes ✓ Python bytes str ✓ JSON bytes str Python callable - JSON Never valid. callable Any ✓ Python callable() check must return True . date bytes Python Format: YYYY-MM-DD (UTF-8). date date ✓ Python date datetime Python Must be exact date, eg. no H , M , S , f . date float Python & JSON Interpreted as seconds or ms from epoch. See speedate . Must be exact date. date int Python & JSON Interpreted as seconds or ms from epoch. See speedate . Must be exact date. date str Python & JSON Format: YYYY-MM-DD . date Decimal Python Interpreted as seconds or ms from epoch. See speedate . Must be exact date. datetime bytes Python Format: YYYY-MM-DDTHH:MM:SS.f or YYYY-MM-DD . See speedate , (UTF-8). datetime date Python datetime datetime ✓ Python datetime float Python & JSON Interpreted as seconds or ms from epoch, see speedate . datetime int Python & JSON Interpreted as seconds or ms from epoch, see speedate . datetime str Python & JSON Format: YYYY-MM-DDTHH:MM:SS.f or YYYY-MM-DD . See speedate . datetime Decimal Python Interpreted as seconds or ms from epoch, see speedate . deque deque ✓ Python deque frozenset Python deque list Python deque set Python deque tuple Python deque Array ✓ JSON dict dict ✓ Python dict Mapping Python Must implement the mapping interface and have an items() method. dict Object ✓ JSON float bool Python & JSON float bytes Python Must match [0-9]+(\\.[0-9]+)? . float float ✓ Python & JSON bool is explicitly forbidden. float int ✓ Python & JSON float str Python & JSON Must match [0-9]+(\\.[0-9]+)? . float Decimal Python frozenset deque Python frozenset dict_keys Python frozenset dict_values Python frozenset frozenset ✓ Python frozenset list Python frozenset set Python frozenset tuple Python frozenset Array ✓ JSON int bool Python & JSON int bytes Python Must be numeric only, e.g. [0-9]+ . int float Python & JSON Must be exact int, e.g. val % 1 == 0 , raises error for nan , inf . int int ✓ Python & JSON bool is explicitly forbidden. int int Python & JSON int str Python & JSON Must be numeric only, e.g. [0-9]+ . int Decimal Python Must be exact int, e.g. val % 1 == 0 . list deque Python list dict_keys Python list dict_values Python list frozenset Python list list ✓ Python list set Python list tuple Python list Array ✓ JSON namedtuple dict ✓ Python namedtuple list ✓ Python namedtuple namedtuple ✓ Python namedtuple tuple ✓ Python namedtuple Array ✓ JSON namedtuple NamedTuple ✓ Python set deque Python set dict_keys Python set dict_values Python set frozenset Python set list Python set set ✓ Python set tuple Python set Array ✓ JSON str bytearray Python Assumes UTF-8, error on unicode decoding error. str bytes Python Assumes UTF-8, error on unicode decoding error. str str ✓ Python & JSON time bytes Python Format: HH:MM:SS.FFFFFF . See speedate . time float Python & JSON Interpreted as seconds, range 0 - 86399.9* . time int Python & JSON Interpreted as seconds, range 0 - 86399 . time str Python & JSON Format: HH:MM:SS.FFFFFF . See speedate . time time ✓ Python time Decimal Python Interpreted as seconds, range 0 - 86399.9* . timedelta bytes Python Format: ISO8601 . See speedate , (UTF-8). timedelta float Python & JSON Interpreted as seconds. timedelta int Python & JSON Interpreted as seconds. timedelta str Python & JSON Format: ISO8601 . See speedate . timedelta timedelta ✓ Python timedelta Decimal Python Interpreted as seconds. tuple deque Python tuple dict_keys Python tuple dict_values Python tuple frozenset Python tuple list Python tuple set Python tuple tuple ✓ Python tuple Array ✓ JSON type type ✓ Python Any Any ✓ Python & JSON ByteSize float ✓ Python & JSON ByteSize int ✓ Python & JSON ByteSize str ✓ Python & JSON ByteSize Decimal ✓ Python Decimal float ✓ JSON Decimal float Python & JSON Decimal int ✓ JSON Decimal int Python & JSON Decimal str ✓ JSON Decimal str Python & JSON Must match [0-9]+(\\.[0-9]+)? . Decimal Decimal ✓ Python Enum Any ✓ JSON Input value must be convertible to enum values. Enum Any Python Input value must be convertible to enum values. Enum Enum ✓ Python IPv4Address bytes Python IPv4Address int Python integer representing the IP address, must be less than 2**32 IPv4Address str ✓ JSON IPv4Address str Python & JSON IPv4Address IPv4Address ✓ Python IPv4Address IPv4Interface ✓ Python IPv4Interface bytes Python IPv4Interface int Python integer representing the IP address, must be less than 2**32 IPv4Interface str ✓ JSON IPv4Interface str Python & JSON IPv4Interface tuple Python IPv4Interface IPv4Address Python IPv4Interface IPv4Interface ✓ Python IPv4Network bytes Python IPv4Network int Python integer representing the IP network, must be less than 2**32 IPv4Network str ✓ JSON IPv4Network str Python & JSON IPv4Network IPv4Address Python IPv4Network IPv4Interface Python IPv4Network IPv4Network ✓ Python IPv6Address bytes Python IPv6Address int Python integer representing the IP address, must be less than 2**128 IPv6Address str ✓ JSON IPv6Address str Python & JSON IPv6Address IPv6Address ✓ Python IPv6Address IPv6Interface ✓ Python IPv6Interface bytes Python IPv6Interface int Python integer representing the IP address, must be less than 2**128 IPv6Interface str ✓ JSON IPv6Interface str Python & JSON IPv6Interface tuple Python IPv6Interface IPv6Address Python IPv6Interface IPv6Interface ✓ Python IPv6Network bytes Python IPv6Network int Python integer representing the IP address, must be less than 2**128 IPv6Network str ✓ JSON IPv6Network str Python & JSON IPv6Network IPv6Address Python IPv6Network IPv6Interface Python IPv6Network IPv6Network ✓ Python InstanceOf - JSON Never valid. InstanceOf Any ✓ Python isinstance() check must return True . IntEnum Any ✓ JSON Input value must be convertible to enum values. IntEnum Any Python Input value must be convertible to enum values. IntEnum IntEnum ✓ Python Iterable deque ✓ Python Iterable frozenset ✓ Python Iterable list ✓ Python Iterable set ✓ Python Iterable tuple ✓ Python Iterable Array ✓ JSON NamedTuple dict ✓ Python NamedTuple list ✓ Python NamedTuple namedtuple ✓ Python NamedTuple tuple ✓ Python NamedTuple Array ✓ JSON NamedTuple NamedTuple ✓ Python None None ✓ Python & JSON Path str ✓ JSON Path str Python Path Path ✓ Python Pattern bytes ✓ Python Input must be a valid pattern. Pattern str ✓ Python & JSON Input must be a valid pattern. Sequence deque Python Sequence list ✓ Python Sequence tuple Python Sequence Array ✓ JSON TypedDict dict ✓ Python TypedDict Any ✓ Python TypedDict Mapping Python Must implement the mapping interface and have an items() method. TypedDict Object ✓ JSON UUID str ✓ JSON UUID str Python UUID UUID ✓ Python Field Type Input Strict Input Source Conditions bool bool ✓ Python & JSON bool float Python & JSON Allowed values: 0.0, 1.0 . bool int Python & JSON Allowed values: 0, 1 . bool str Python & JSON Allowed values: 'f' , 'n' , 'no' , 'off' , 'false' , 'False' , 't' , 'y' , 'on' , 'yes' , 'true' , 'True' . bytes str ✓ JSON callable - JSON Never valid. date float Python & JSON Interpreted as seconds or ms from epoch. See speedate . Must be exact date. date int Python & JSON Interpreted as seconds or ms from epoch. See speedate . Must be exact date. date str Python & JSON Format: YYYY-MM-DD . datetime float Python & JSON Interpreted as seconds or ms from epoch, see speedate . datetime int Python & JSON Interpreted as seconds or ms from epoch, see speedate . datetime str Python & JSON Format: YYYY-MM-DDTHH:MM:SS.f or YYYY-MM-DD . See speedate . deque Array ✓ JSON dict Object ✓ JSON float bool Python & JSON float float ✓ Python & JSON bool is explicitly forbidden. float int ✓ Python & JSON float str Python & JSON Must match [0-9]+(\\.[0-9]+)? . frozenset Array ✓ JSON int bool Python & JSON int float Python & JSON Must be exact int, e.g. val % 1 == 0 , raises error for nan , inf . int int ✓ Python & JSON bool is explicitly forbidden. int int Python & JSON int str Python & JSON Must be numeric only, e.g. [0-9]+ . list Array ✓ JSON namedtuple Array ✓ JSON set Array ✓ JSON str str ✓ Python & JSON time float Python & JSON Interpreted as seconds, range 0 - 86399.9* . time int Python & JSON Interpreted as seconds, range 0 - 86399 . time str Python & JSON Format: HH:MM:SS.FFFFFF . See speedate . timedelta float Python & JSON Interpreted as seconds. timedelta int Python & JSON Interpreted as seconds. timedelta str Python & JSON Format: ISO8601 . See speedate . tuple Array ✓ JSON Any Any ✓ Python & JSON ByteSize float ✓ Python & JSON ByteSize int ✓ Python & JSON ByteSize str ✓ Python & JSON Decimal float ✓ JSON Decimal float Python & JSON Decimal int ✓ JSON Decimal int Python & JSON Decimal str ✓ JSON Decimal str Python & JSON Must match [0-9]+(\\.[0-9]+)? . Enum Any ✓ JSON Input value must be convertible to enum values. IPv4Address str ✓ JSON IPv4Address str Python & JSON IPv4Interface str ✓ JSON IPv4Interface str Python & JSON IPv4Network str ✓ JSON IPv4Network str Python & JSON IPv6Address str ✓ JSON IPv6Address str Python & JSON IPv6Interface str ✓ JSON IPv6Interface str Python & JSON IPv6Network str ✓ JSON IPv6Network str Python & JSON InstanceOf - JSON Never valid. IntEnum Any ✓ JSON Input value must be convertible to enum values. Iterable Array ✓ JSON NamedTuple Array ✓ JSON None None ✓ Python & JSON Path str ✓ JSON Pattern str ✓ Python & JSON Input must be a valid pattern. Sequence Array ✓ JSON TypedDict Object ✓ JSON UUID str ✓ JSON Field Type Input Strict Input Source Conditions bool bool ✓ Python & JSON bytes str ✓ JSON deque Array ✓ JSON dict Object ✓ JSON float float ✓ Python & JSON bool is explicitly forbidden. float int ✓ Python & JSON frozenset Array ✓ JSON int int ✓ Python & JSON bool is explicitly forbidden. list Array ✓ JSON namedtuple Array ✓ JSON set Array ✓ JSON str str ✓ Python & JSON tuple Array ✓ JSON Any Any ✓ Python & JSON ByteSize float ✓ Python & JSON ByteSize int ✓ Python & JSON ByteSize str ✓ Python & JSON Decimal float ✓ JSON Decimal int ✓ JSON Decimal str ✓ JSON Enum Any ✓ JSON Input value must be convertible to enum values. IPv4Address str ✓ JSON IPv4Interface str ✓ JSON IPv4Network str ✓ JSON IPv6Address str ✓ JSON IPv6Interface str ✓ JSON IPv6Network str ✓ JSON IntEnum Any ✓ JSON Input value must be convertible to enum values. Iterable Array ✓ JSON NamedTuple Array ✓ JSON None None ✓ Python & JSON Path str ✓ JSON Pattern str ✓ Python & JSON Input must be a valid pattern. Sequence Array ✓ JSON TypedDict Object ✓ JSON UUID str ✓ JSON Field Type Input Strict Input Source Conditions bool bool ✓ Python & JSON bool float Python & JSON Allowed values: 0.0, 1.0 . bool int Python & JSON Allowed values: 0, 1 . bool str Python & JSON Allowed values: 'f' , 'n' , 'no' , 'off' , 'false' , 'False' , 't' , 'y' , 'on' , 'yes' , 'true' , 'True' . bool Decimal Python Allowed values: Decimal(0), Decimal(1) . bytes bytearray Python bytes bytes ✓ Python bytes str Python callable Any ✓ Python callable() check must return True . date bytes Python Format: YYYY-MM-DD (UTF-8). date date ✓ Python date datetime Python Must be exact date, eg. no H , M , S , f . date float Python & JSON Interpreted as seconds or ms from epoch. See speedate . Must be exact date. date int Python & JSON Interpreted as seconds or ms from epoch. See speedate . Must be exact date. date str Python & JSON Format: YYYY-MM-DD . date Decimal Python Interpreted as seconds or ms from epoch. See speedate . Must be exact date. datetime bytes Python Format: YYYY-MM-DDTHH:MM:SS.f or YYYY-MM-DD . See speedate , (UTF-8). datetime date Python datetime datetime ✓ Python datetime float Python & JSON Interpreted as seconds or ms from epoch, see speedate . datetime int Python & JSON Interpreted as seconds or ms from epoch, see speedate . datetime str Python & JSON Format: YYYY-MM-DDTHH:MM:SS.f or YYYY-MM-DD . See speedate . datetime Decimal Python Interpreted as seconds or ms from epoch, see speedate . deque deque ✓ Python deque frozenset Python deque list Python deque set Python deque tuple Python dict dict ✓ Python dict Mapping Python Must implement the mapping interface and have an items() method. float bool Python & JSON float bytes Python Must match [0-9]+(\\.[0-9]+)? . float float ✓ Python & JSON bool is explicitly forbidden. float int ✓ Python & JSON float str Python & JSON Must match [0-9]+(\\.[0-9]+)? . float Decimal Python frozenset deque Python frozenset dict_keys Python frozenset dict_values Python frozenset frozenset ✓ Python frozenset list Python frozenset set Python frozenset tuple Python int bool Python & JSON int bytes Python Must be numeric only, e.g. [0-9]+ . int float Python & JSON Must be exact int, e.g. val % 1 == 0 , raises error for nan , inf . int int ✓ Python & JSON bool is explicitly forbidden. int int Python & JSON int str Python & JSON Must be numeric only, e.g. [0-9]+ . int Decimal Python Must be exact int, e.g. val % 1 == 0 . list deque Python list dict_keys Python list dict_values Python list frozenset Python list list ✓ Python list set Python list tuple Python namedtuple dict ✓ Python namedtuple list ✓ Python namedtuple namedtuple ✓ Python namedtuple tuple ✓ Python namedtuple NamedTuple ✓ Python set deque Python set dict_keys Python set dict_values Python set frozenset Python set list Python set set ✓ Python set tuple Python str bytearray Python Assumes UTF-8, error on unicode decoding error. str bytes Python Assumes UTF-8, error on unicode decoding error. str str ✓ Python & JSON time bytes Python Format: HH:MM:SS.FFFFFF . See speedate . time float Python & JSON Interpreted as seconds, range 0 - 86399.9* . time int Python & JSON Interpreted as seconds, range 0 - 86399 . time str Python & JSON Format: HH:MM:SS.FFFFFF . See speedate . time time ✓ Python time Decimal Python Interpreted as seconds, range 0 - 86399.9* . timedelta bytes Python Format: ISO8601 . See speedate , (UTF-8). timedelta float Python & JSON Interpreted as seconds. timedelta int Python & JSON Interpreted as seconds. timedelta str Python & JSON Format: ISO8601 . See speedate . timedelta timedelta ✓ Python timedelta Decimal Python Interpreted as seconds. tuple deque Python tuple dict_keys Python tuple dict_values Python tuple frozenset Python tuple list Python tuple set Python tuple tuple ✓ Python type type ✓ Python Any Any ✓ Python & JSON ByteSize float ✓ Python & JSON ByteSize int ✓ Python & JSON ByteSize str ✓ Python & JSON ByteSize Decimal ✓ Python Decimal float Python & JSON Decimal int Python & JSON Decimal str Python & JSON Must match [0-9]+(\\.[0-9]+)? . Decimal Decimal ✓ Python Enum Any Python Input value must be convertible to enum values. Enum Enum ✓ Python IPv4Address bytes Python IPv4Address int Python integer representing the IP address, must be less than 2**32 IPv4Address str Python & JSON IPv4Address IPv4Address ✓ Python IPv4Address IPv4Interface ✓ Python IPv4Interface bytes Python IPv4Interface int Python integer representing the IP address, must be less than 2**32 IPv4Interface str Python & JSON IPv4Interface tuple Python IPv4Interface IPv4Address Python IPv4Interface IPv4Interface ✓ Python IPv4Network bytes Python IPv4Network int Python integer representing the IP network, must be less than 2**32 IPv4Network str Python & JSON IPv4Network IPv4Address Python IPv4Network IPv4Interface Python IPv4Network IPv4Network ✓ Python IPv6Address bytes Python IPv6Address int Python integer representing the IP address, must be less than 2**128 IPv6Address str Python & JSON IPv6Address IPv6Address ✓ Python IPv6Address IPv6Interface ✓ Python IPv6Interface bytes Python IPv6Interface int Python integer representing the IP address, must be less than 2**128 IPv6Interface str Python & JSON IPv6Interface tuple Python IPv6Interface IPv6Address Python IPv6Interface IPv6Interface ✓ Python IPv6Network bytes Python IPv6Network int Python integer representing the IP address, must be less than 2**128 IPv6Network str Python & JSON IPv6Network IPv6Address Python IPv6Network IPv6Interface Python IPv6Network IPv6Network ✓ Python InstanceOf Any ✓ Python isinstance() check must return True . IntEnum Any Python Input value must be convertible to enum values. IntEnum IntEnum ✓ Python Iterable deque ✓ Python Iterable frozenset ✓ Python Iterable list ✓ Python Iterable set ✓ Python Iterable tuple ✓ Python NamedTuple dict ✓ Python NamedTuple list ✓ Python NamedTuple namedtuple ✓ Python NamedTuple tuple ✓ Python NamedTuple NamedTuple ✓ Python None None ✓ Python & JSON Path str Python Path Path ✓ Python Pattern bytes ✓ Python Input must be a valid pattern. Pattern str ✓ Python & JSON Input must be a valid pattern. Sequence deque Python Sequence list ✓ Python Sequence tuple Python TypedDict dict ✓ Python TypedDict Any ✓ Python TypedDict Mapping Python Must implement the mapping interface and have an items() method. UUID str Python UUID UUID ✓ Python Field Type Input Strict Input Source Conditions bool bool ✓ Python & JSON bytes bytes ✓ Python callable Any ✓ Python callable() check must return True . date date ✓ Python datetime datetime ✓ Python deque deque ✓ Python dict dict ✓ Python float float ✓ Python & JSON bool is explicitly forbidden. float int ✓ Python & JSON frozenset frozenset ✓ Python int int ✓ Python & JSON bool is explicitly forbidden. list list ✓ Python namedtuple dict ✓ Python namedtuple list ✓ Python namedtuple namedtuple ✓ Python namedtuple tuple ✓ Python namedtuple NamedTuple ✓ Python set set ✓ Python str str ✓ Python & JSON time time ✓ Python timedelta timedelta ✓ Python tuple tuple ✓ Python type type ✓ Python Any Any ✓ Python & JSON ByteSize float ✓ Python & JSON ByteSize int ✓ Python & JSON ByteSize str ✓ Python & JSON ByteSize Decimal ✓ Python Decimal Decimal ✓ Python Enum Enum ✓ Python IPv4Address IPv4Address ✓ Python IPv4Address IPv4Interface ✓ Python IPv4Interface IPv4Interface ✓ Python IPv4Network IPv4Network ✓ Python IPv6Address IPv6Address ✓ Python IPv6Address IPv6Interface ✓ Python IPv6Interface IPv6Interface ✓ Python IPv6Network IPv6Network ✓ Python InstanceOf Any ✓ Python isinstance() check must return True . IntEnum IntEnum ✓ Python Iterable deque ✓ Python Iterable frozenset ✓ Python Iterable list ✓ Python Iterable set ✓ Python Iterable tuple ✓ Python NamedTuple dict ✓ Python NamedTuple list ✓ Python NamedTuple namedtuple ✓ Python NamedTuple tuple ✓ Python NamedTuple NamedTuple ✓ Python None None ✓ Python & JSON Path Path ✓ Python Pattern bytes ✓ Python Input must be a valid pattern. Pattern str ✓ Python & JSON Input must be a valid pattern. Sequence list ✓ Python TypedDict dict ✓ Python TypedDict Any ✓ Python UUID UUID ✓ Python","pageID":"Conversion Table","abs_url":"/latest/concepts/conversion_table/#Conversion Table","title":"Conversion Table","objectID":"/latest/concepts/conversion_table/#Conversion Table","rank":100},{"content":"If you don't want to use Pydantic's  you can instead get the same data validation\non standard . Python 3.9 and above Python 3.10 and above from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    id: int\n    name: str = 'John Doe'\n    signup_ts: Optional[datetime] = None\n\n\nuser = User(id='42', signup_ts='2032-06-21T12:00')\nprint(user)\n\"\"\"\nUser(id=42, name='John Doe', signup_ts=datetime.datetime(2032, 6, 21, 12, 0))\n\"\"\" from datetime import datetime\n\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    id: int\n    name: str = 'John Doe'\n    signup_ts: datetime | None = None\n\n\nuser = User(id='42', signup_ts='2032-06-21T12:00')\nprint(user)\n\"\"\"\nUser(id=42, name='John Doe', signup_ts=datetime.datetime(2032, 6, 21, 12, 0))\n\"\"\" Note Keep in mind that Pydantic dataclasses are not a replacement for Pydantic models .\nThey provide a similar functionality to stdlib dataclasses with the addition of Pydantic validation. There are cases where subclassing using Pydantic models is the better choice. For more information and discussion see pydantic/pydantic#710 . Similarities between Pydantic dataclasses and models include support for: Configuration support Nested classes Generics Some differences between Pydantic dataclasses and models include: validators The behavior with the  configuration value Similarly to Pydantic models, arguments used to instantiate the dataclass are copied . To make use of the various methods to validate, dump and generate a JSON Schema,\nyou can wrap the dataclass with a  and make use of its methods. You can use both the Pydantic's  and the stdlib's  functions: Python 3.9 and above Python 3.10 and above import dataclasses\nfrom typing import Optional\n\nfrom pydantic import Field\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    id: int\n    name: str = 'John Doe'\n    friends: list[int] = dataclasses.field(default_factory=lambda: [0])\n    age: Optional[int] = dataclasses.field(\n        default=None,\n        metadata={'title': 'The age of the user', 'description': 'do not lie!'},\n    )\n    height: Optional[int] = Field(\n        default=None, title='The height in cm', ge=50, le=300\n    )\n\n\nuser = User(id='42', height='250')\nprint(user)\n#> User(id=42, name='John Doe', friends=[0], age=None, height=250) import dataclasses\n\nfrom pydantic import Field\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    id: int\n    name: str = 'John Doe'\n    friends: list[int] = dataclasses.field(default_factory=lambda: [0])\n    age: int | None = dataclasses.field(\n        default=None,\n        metadata={'title': 'The age of the user', 'description': 'do not lie!'},\n    )\n    height: int | None = Field(\n        default=None, title='The height in cm', ge=50, le=300\n    )\n\n\nuser = User(id='42', height='250')\nprint(user)\n#> User(id=42, name='John Doe', friends=[0], age=None, height=250) The Pydantic  decorator accepts the same arguments as the standard decorator,\nwith the addition of a config parameter.","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#Dataclasses","title":"Dataclasses","objectID":"/latest/concepts/dataclasses/#Dataclasses","rank":100},{"content":"If you want to modify the configuration like you would with a , you have two options: Use the config argument of the decorator. Define the configuration with the __pydantic_config__ attribute. from pydantic import ConfigDict\nfrom pydantic.dataclasses import dataclass\n\n\n# Option 1 -- using the decorator argument:\n@dataclass(config=ConfigDict(validate_assignment=True))  # (1)!\nclass MyDataclass1:\n    a: int\n\n\n# Option 2 -- using an attribute:\n@dataclass\nclass MyDataclass2:\n    a: int\n\n    __pydantic_config__ = ConfigDict(validate_assignment=True) You can read more about validate_assignment in the . Note While Pydantic dataclasses support the  configuration value, some default\nbehavior of stdlib dataclasses may prevail. For example, any extra fields present on a Pydantic dataclass with\n set to 'allow' are omitted in the dataclass' string representation.\nThere is also no way to provide validation using the __pydantic_extra__ attribute .","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#dataclass-config","title":"Dataclasses - Dataclass config","objectID":"/latest/concepts/dataclasses/#dataclass-config","rank":95},{"content":"The  function can be used to rebuild the core schema of the dataclass.\nSee the rebuilding model schema section for more details.","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#rebuilding-dataclass-schema","title":"Dataclasses - Rebuilding dataclass schema","objectID":"/latest/concepts/dataclasses/#rebuilding-dataclass-schema","rank":90},{"content":"","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#stdlib-dataclasses-and-pydantic-dataclasses","title":"Dataclasses - Stdlib dataclasses and Pydantic dataclasses","objectID":"/latest/concepts/dataclasses/#stdlib-dataclasses-and-pydantic-dataclasses","rank":85},{"content":"Stdlib dataclasses (nested or not) can also be inherited and Pydantic will automatically validate\nall the inherited fields. import dataclasses\n\nimport pydantic\n\n\n@dataclasses.dataclass\nclass Z:\n    z: int\n\n\n@dataclasses.dataclass\nclass Y(Z):\n    y: int = 0\n\n\n@pydantic.dataclasses.dataclass\nclass X(Y):\n    x: int = 0\n\n\nfoo = X(x=b'1', y='2', z='3')\nprint(foo)\n#> X(z=3, y=2, x=1)\n\ntry:\n    X(z='pika')\nexcept pydantic.ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for X\n    z\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='pika', input_type=str]\n    \"\"\" The decorator can also be applied directly on a stdlib dataclass, in which case a new subclass will be created: import dataclasses\n\nimport pydantic\n\n\n@dataclasses.dataclass\nclass A:\n    a: int\n\n\nPydanticA = pydantic.dataclasses.dataclass(A)\nprint(PydanticA(a='1'))\n#> A(a=1)","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#inherit-from-stdlib-dataclasses","title":"Dataclasses - Stdlib dataclasses and Pydantic dataclasses - Inherit from stdlib dataclasses","objectID":"/latest/concepts/dataclasses/#inherit-from-stdlib-dataclasses","rank":80},{"content":"When a standard library dataclass is used within a Pydantic model, a Pydantic dataclass or a ,\nvalidation will be applied (and the configuration stays the same). This means that using a stdlib or a Pydantic\ndataclass as a field annotation is functionally equivalent. Python 3.9 and above Python 3.10 and above import dataclasses\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\n\n@dataclasses.dataclass(frozen=True)\nclass User:\n    name: str\n\n\nclass Foo(BaseModel):\n    # Required so that pydantic revalidates the model attributes:\n    model_config = ConfigDict(revalidate_instances='always')\n\n    user: Optional[User] = None\n\n\n# nothing is validated as expected:\nuser = User(name=['not', 'a', 'string'])\nprint(user)\n#> User(name=['not', 'a', 'string'])\n\n\ntry:\n    Foo(user=user)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Foo\n    user.name\n      Input should be a valid string [type=string_type, input_value=['not', 'a', 'string'], input_type=list]\n    \"\"\"\n\nfoo = Foo(user=User(name='pika'))\ntry:\n    foo.user.name = 'bulbi'\nexcept dataclasses.FrozenInstanceError as e:\n    print(e)\n    #> cannot assign to field 'name' import dataclasses\n\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\n\n@dataclasses.dataclass(frozen=True)\nclass User:\n    name: str\n\n\nclass Foo(BaseModel):\n    # Required so that pydantic revalidates the model attributes:\n    model_config = ConfigDict(revalidate_instances='always')\n\n    user: User | None = None\n\n\n# nothing is validated as expected:\nuser = User(name=['not', 'a', 'string'])\nprint(user)\n#> User(name=['not', 'a', 'string'])\n\n\ntry:\n    Foo(user=user)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Foo\n    user.name\n      Input should be a valid string [type=string_type, input_value=['not', 'a', 'string'], input_type=list]\n    \"\"\"\n\nfoo = Foo(user=User(name='pika'))\ntry:\n    foo.user.name = 'bulbi'\nexcept dataclasses.FrozenInstanceError as e:\n    print(e)\n    #> cannot assign to field 'name'","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#usage-of-stdlib-dataclasses-with-basemodel","title":"Dataclasses - Stdlib dataclasses and Pydantic dataclasses - Usage of stdlib dataclasses with BaseModel","objectID":"/latest/concepts/dataclasses/#usage-of-stdlib-dataclasses-with-basemodel","rank":75},{"content":"As said above, validation is applied on standard library dataclasses. If you make use\nof custom types, you will get an error when trying to refer to the dataclass. To circumvent\nthe issue, you can set the \nconfiguration value on the dataclass: import dataclasses\n\nfrom pydantic import BaseModel, ConfigDict\nfrom pydantic.errors import PydanticSchemaGenerationError\n\n\nclass ArbitraryType:\n    def __init__(self, value):\n        self.value = value\n\n    def __repr__(self):\n        return f'ArbitraryType(value={self.value!r})'\n\n\n@dataclasses.dataclass\nclass DC:\n    a: ArbitraryType\n    b: str\n\n\n# valid as it is a stdlib dataclass without validation:\nmy_dc = DC(a=ArbitraryType(value=3), b='qwe')\n\ntry:\n\n    class Model(BaseModel):\n        dc: DC\n        other: str\n\n    # invalid as dc is now validated with pydantic, and ArbitraryType is not a known type\n    Model(dc=my_dc, other='other')\n\nexcept PydanticSchemaGenerationError as e:\n    print(e.message)\n    \"\"\"\n    Unable to generate pydantic-core schema for . Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\n    If you got this error by calling handler( ) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema( )` since we do not call `__get_pydantic_core_schema__` on ` ` otherwise to avoid infinite recursion.\n    \"\"\"\n\n\n# valid as we set arbitrary_types_allowed=True, and that config pushes down to the nested vanilla dataclass\nclass Model(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    dc: DC\n    other: str\n\n\nm = Model(dc=my_dc, other='other')\nprint(repr(m))\n#> Model(dc=DC(a=ArbitraryType(value=3), b='qwe'), other='other')","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#using-custom-types","title":"Dataclasses - Stdlib dataclasses and Pydantic dataclasses - Using custom types","objectID":"/latest/concepts/dataclasses/#using-custom-types","rank":70},{"content":"Pydantic dataclasses are still considered dataclasses, so using \nwill return True . To check if a type is specifically a Pydantic dataclass you can use the\n function. import dataclasses\n\nimport pydantic\n\n\n@dataclasses.dataclass\nclass StdLibDataclass:\n    id: int\n\n\nPydanticDataclass = pydantic.dataclasses.dataclass(StdLibDataclass)\n\nprint(dataclasses.is_dataclass(StdLibDataclass))\n#> True\nprint(pydantic.dataclasses.is_pydantic_dataclass(StdLibDataclass))\n#> False\n\nprint(dataclasses.is_dataclass(PydanticDataclass))\n#> True\nprint(pydantic.dataclasses.is_pydantic_dataclass(PydanticDataclass))\n#> True","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#checking-if-a-dataclass-is-a-pydantic-dataclass","title":"Dataclasses - Stdlib dataclasses and Pydantic dataclasses - Checking if a dataclass is a Pydantic dataclass","objectID":"/latest/concepts/dataclasses/#checking-if-a-dataclass-is-a-pydantic-dataclass","rank":65},{"content":"Validators also work with Pydantic dataclasses: from pydantic import field_validator\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass DemoDataclass:\n    product_id: str  # should be a five-digit string, may have leading zeros\n\n    @field_validator('product_id', mode='before')\n    @classmethod\n    def convert_int_serial(cls, v):\n        if isinstance(v, int):\n            v = str(v).zfill(5)\n        return v\n\n\nprint(DemoDataclass(product_id='01234'))\n#> DemoDataclass(product_id='01234')\nprint(DemoDataclass(product_id=2468))\n#> DemoDataclass(product_id='02468') The dataclass  method is also supported, and will\nbe called between the calls to before and after model validators.","pageID":"Dataclasses","abs_url":"/latest/concepts/dataclasses/#validators-and-initialization-hooks","title":"Dataclasses - Validators and initialization hooks","objectID":"/latest/concepts/dataclasses/#validators-and-initialization-hooks","rank":60},{"content":"In this section you will find documentation for new, experimental features in Pydantic. These features are subject to change or removal, and we are looking for feedback and suggestions before making them a permanent part of Pydantic. See our Version Policy for more information on experimental features.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#experimental-features","title":"Experimental","objectID":"/latest/concepts/experimental/#experimental-features","rank":100},{"content":"We welcome feedback on experimental features! Please open an issue on the Pydantic GitHub repository to share your thoughts, requests, or suggestions. We also encourage you to read through existing feedback and add your thoughts to existing issues.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#feedback","title":"Experimental - Feedback","objectID":"/latest/concepts/experimental/#feedback","rank":95},{"content":"When you import an experimental feature from the experimental module, you'll see a warning message that the feature is experimental. You can disable this warning with the following: import warnings\n\nfrom pydantic import PydanticExperimentalWarning\n\nwarnings.filterwarnings('ignore', category=PydanticExperimentalWarning)","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#warnings-on-import","title":"Experimental - Warnings on Import","objectID":"/latest/concepts/experimental/#warnings-on-import","rank":90},{"content":"Pydantic v2.8.0 introduced an experimental \"pipeline\" API that allows composing of parsing (validation), constraints and transformations in a more type-safe manner than existing APIs. This API is subject to change or removal, we are looking for feedback and suggestions before making it a permanent part of Pydantic. Generally, the pipeline API is used to define a sequence of steps to apply to incoming data during validation. The pipeline API is designed to be more type-safe and composable than the existing Pydantic API. Each step in the pipeline can be: A validation step that runs pydantic validation on the provided type A transformation step that modifies the data A constraint step that checks the data against a condition A predicate step that checks the data against a condition and raises an error if it returns False Note that the following example attempts to be exhaustive at the cost of complexity: if you find yourself writing this many transformations in type annotations you may want to consider having a UserIn and UserOut model (example below) or similar where you make the transformations via idiomatic plain Python code.\nThese APIs are meant for situations where the code savings are significant and the added complexity is relatively small. from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import Annotated\n\nfrom pydantic import BaseModel\nfrom pydantic.experimental.pipeline import validate_as\n\n\nclass User(BaseModel):\n    name: Annotated[str, validate_as(str).str_lower()]  # (1)!\n    age: Annotated[int, validate_as(int).gt(0)]  # (2)!\n    username: Annotated[str, validate_as(str).str_pattern(r'[a-z]+')]  # (3)!\n    password: Annotated[\n        str,\n        validate_as(str)\n        .transform(str.lower)\n        .predicate(lambda x: x != 'password'),  # (4)!\n    ]\n    favorite_number: Annotated[  # (5)!\n        int,\n        (validate_as(int) | validate_as(str).str_strip().validate_as(int)).gt(\n            0\n        ),\n    ]\n    friends: Annotated[list[User], validate_as(...).len(0, 100)]  # (6)!\n    bio: Annotated[\n        datetime,\n        validate_as(int)\n        .transform(lambda x: x / 1_000_000)\n        .validate_as(...),  # (8)!\n    ] Lowercase a string. Constrain an integer to be greater than zero. Constrain a string to match a regex pattern. You can also use the lower level transform, constrain and predicate methods. Use the | or & operators to combine steps (like a logical OR or AND). Calling validate_as(...) with Ellipsis , ... as the first positional argument implies validate_as(<field type>) . Use validate_as(Any) to accept any type. You can call validate_as() before or after other steps to do pre or post processing.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#pipeline-api","title":"Experimental - Pipeline API","objectID":"/latest/concepts/experimental/#pipeline-api","rank":85},{"content":"The validate_as method is a more type-safe way to define BeforeValidator , AfterValidator and WrapValidator : from typing import Annotated\n\nfrom pydantic.experimental.pipeline import transform, validate_as\n\n# BeforeValidator\nAnnotated[int, validate_as(str).str_strip().validate_as(...)]  # (1)!\n# AfterValidator\nAnnotated[int, transform(lambda x: x * 2)]  # (2)!\n# WrapValidator\nAnnotated[\n    int,\n    validate_as(str)\n    .str_strip()\n    .validate_as(...)\n    .transform(lambda x: x * 2),  # (3)!\n] Strip whitespace from a string before parsing it as an integer. Multiply an integer by 2 after parsing it. Strip whitespace from a string, validate it as an integer, then multiply it by 2.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#mapping-from-beforevalidator-aftervalidator-and-wrapvalidator","title":"Experimental - Pipeline API - Mapping from BeforeValidator, AfterValidator and WrapValidator","objectID":"/latest/concepts/experimental/#mapping-from-beforevalidator-aftervalidator-and-wrapvalidator","rank":80},{"content":"There are many alternative patterns to use depending on the scenario.\nJust as an example, consider the UserIn and UserOut pattern mentioned above: from __future__ import annotations\n\nfrom pydantic import BaseModel\n\n\nclass UserIn(BaseModel):\n    favorite_number: int | str\n\n\nclass UserOut(BaseModel):\n    favorite_number: int\n\n\ndef my_api(user: UserIn) -> UserOut:\n    favorite_number = user.favorite_number\n    if isinstance(favorite_number, str):\n        favorite_number = int(user.favorite_number.strip())\n\n    return UserOut(favorite_number=favorite_number)\n\n\nassert my_api(UserIn(favorite_number=' 1 ')).favorite_number == 1 This example uses plain idiomatic Python code that may be easier to understand, type-check, etc. than the examples above.\nThe approach you choose should really depend on your use case.\nYou will have to compare verbosity, performance, ease of returning meaningful errors to your users, etc. to choose the right pattern.\nJust be mindful of abusing advanced patterns like the pipeline API just because you can.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#alternative-patterns","title":"Experimental - Pipeline API - Alternative patterns","objectID":"/latest/concepts/experimental/#alternative-patterns","rank":75},{"content":"Pydantic v2.10.0 introduces experimental support for \"partial validation\". This allows you to validate an incomplete JSON string, or a Python object representing incomplete input data. Partial validation is particularly helpful when processing the output of an LLM, where the model streams structured responses, and you may wish to begin validating the stream while you're still receiving data (e.g. to show partial data to users). Warning Partial validation is an experimental feature and may change in future versions of Pydantic. The current implementation should be considered a proof of concept at this time and has a number of limitations . Partial validation can be enabled when using the three validation methods on TypeAdapter : , , and . This allows you to parse and validation incomplete JSON, but also to validate Python objects created by parsing incomplete data of any format. The experimental_allow_partial flag can be passed to these methods to enable partial validation.\nIt can take the following values (and is False , by default): False or 'off' - disable partial validation True or 'on' - enable partial validation, but don't support trailing strings 'trailing-strings' - enable partial validation and support trailing strings 'trailing-strings' mode 'trailing-strings' mode allows for trailing incomplete strings at the end of partial JSON to be included in the output.\nFor example, if you're validating against the following model: from typing import TypedDict\n\n\nclass Model(TypedDict):\n    a: str\n    b: str Then the following JSON input would be considered valid, despite the incomplete string at the end: ' { \"a\" : \"hello\" , \"b\" : \"wor' And would be validated as: {'a': 'hello', 'b': 'wor'} experiment_allow_partial in action: from typing import Annotated\n\nfrom annotated_types import MinLen\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom pydantic import TypeAdapter\n\n\nclass Foobar(TypedDict):  # (1)!\n    a: int\n    b: NotRequired[float]\n    c: NotRequired[Annotated[str, MinLen(5)]]\n\n\nta = TypeAdapter(list[Foobar])\n\nv = ta.validate_json('[{\"a\": 1, \"b\"', experimental_allow_partial=True)  # (2)!\nprint(v)\n#> [{'a': 1}]\n\nv = ta.validate_json(\n    '[{\"a\": 1, \"b\": 1.0, \"c\": \"abcd', experimental_allow_partial=True  # (3)!\n)\nprint(v)\n#> [{'a': 1, 'b': 1.0}]\n\nv = ta.validate_json(\n    '[{\"b\": 1.0, \"c\": \"abcde\"', experimental_allow_partial=True  # (4)!\n)\nprint(v)\n#> []\n\nv = ta.validate_json(\n    '[{\"a\": 1, \"b\": 1.0, \"c\": \"abcde\"},{\"a\": ', experimental_allow_partial=True\n)\nprint(v)\n#> [{'a': 1, 'b': 1.0, 'c': 'abcde'}]\n\nv = ta.validate_python([{'a': 1}], experimental_allow_partial=True)  # (5)!\nprint(v)\n#> [{'a': 1}]\n\nv = ta.validate_python(\n    [{'a': 1, 'b': 1.0, 'c': 'abcd'}], experimental_allow_partial=True  # (6)!\n)\nprint(v)\n#> [{'a': 1, 'b': 1.0}]\n\nv = ta.validate_json(\n    '[{\"a\": 1, \"b\": 1.0, \"c\": \"abcdefg',\n    experimental_allow_partial='trailing-strings',  # (7)!\n)\nprint(v)\n#> [{'a': 1, 'b': 1.0, 'c': 'abcdefg'}] The TypedDict Foobar has three field, but only a is required, that means that a valid instance of Foobar can be created even if the b and c fields are missing. Parsing JSON, the input is valid JSON up to the point where the string is truncated. In this case truncation of the input means the value of c ( abcd ) is invalid as input to c field, hence it's omitted. The a field is required, so validation on the only item in the list fails and is dropped. Partial validation also works with Python objects, it should have the same semantics as with JSON except of course you can't have a genuinely \"incomplete\" Python object. The same as above but with a Python object, c is dropped as it's not required and failed validation. The trailing-strings mode allows for incomplete strings at the end of partial JSON to be included in the output, in this case the input is valid JSON up to the point where the string is truncated, so the last string is included.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#partial-validation","title":"Experimental - Partial Validation","objectID":"/latest/concepts/experimental/#partial-validation","rank":70},{"content":"Partial validation follows the zen of Pydantic — it makes no guarantees about what the input data might have been, but it does guarantee to return a valid instance of the type you required, or raise a validation error. To do this, the experimental_allow_partial flag enables two pieces of behavior: 1. Partial JSON parsing ¶ The jiter JSON parser used by Pydantic already supports parsing partial JSON, experimental_allow_partial is simply passed to jiter via the allow_partial argument. Note If you just want pure JSON parsing with support for partial JSON, you can use the jiter Python library directly, or pass the allow_partial argument when calling . 2. Ignore errors in the last element of the input ¶ Only having access to part of the input data means errors can commonly occur in the last element of the input data. For example: if a string has a constraint MinLen(5) , when you only see part of the input, validation might fail because part of the string is missing (e.g. {\"name\": \"Sam instead of {\"name\": \"Samuel\"} ) if an int field has a constraint Ge(10) , when you only see part of the input, validation might fail because the number is too small (e.g. 1 instead of 10 ) if a TypedDict field has 3 required fields, but the partial input only has two of the fields, validation would fail because some field are missing etc. etc. — there are lost more cases like this The point is that if you only see part of some valid input data, validation errors can often occur in the last element of a sequence or last value of mapping. To avoid these errors breaking partial validation, Pydantic will ignore ALL errors in the last element of the input data. from typing import Annotated\n\nfrom annotated_types import MinLen\n\nfrom pydantic import BaseModel, TypeAdapter\n\n\nclass MyModel(BaseModel):\n    a: int\n    b: Annotated[str, MinLen(5)]\n\n\nta = TypeAdapter(list[MyModel])\nv = ta.validate_json(\n    '[{\"a\": 1, \"b\": \"12345\"}, {\"a\": 1,',\n    experimental_allow_partial=True,\n)\nprint(v)\n#> [MyModel(a=1, b='12345')]","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#how-partial-validation-works","title":"Experimental - Partial Validation - How Partial Validation Works","objectID":"/latest/concepts/experimental/#how-partial-validation-works","rank":65},{"content":"TypeAdapter only ¶ You can only pass experiment_allow_partial to  methods, it's not yet supported via other Pydantic entry points like . Types supported ¶ Right now only a subset of collection validators know how to handle partial validation: list set frozenset dict (as in dict[X, Y] ) TypedDict — only non-required fields may be missing, e.g. via  or ) While you can use experimental_allow_partial while validating against types that include other collection validators, those types will be validated \"all or nothing\", and partial validation will not work on more nested types. E.g. in the above example partial validation works although the second item in the list is dropped completely since BaseModel doesn't (yet) support partial validation. But partial validation won't work at all in the follow example because BaseModel doesn't support partial validation so it doesn't forward the allow_partial instruction down to the list validator in b : from typing import Annotated\n\nfrom annotated_types import MinLen\n\nfrom pydantic import BaseModel, TypeAdapter, ValidationError\n\n\nclass MyModel(BaseModel):\n    a: int = 1\n    b: list[Annotated[str, MinLen(5)]] = []  # (1)!\n\n\nta = TypeAdapter(MyModel)\ntry:\n    v = ta.validate_json(\n        '{\"a\": 1, \"b\": [\"12345\", \"12', experimental_allow_partial=True\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for MyModel\n    b.1\n      String should have at least 5 characters [type=string_too_short, input_value='12', input_type=str]\n    \"\"\" The list validator for b doesn't get the allow_partial instruction passed down to it by the model validator so it doesn't know to ignore errors in the last element of the input. Some invalid but complete JSON will be accepted ¶ The way jiter (the JSON parser used by Pydantic) works means it's currently not possible to differentiate between complete JSON like {\"a\": 1, \"b\": \"12\"} and incomplete JSON like {\"a\": 1, \"b\": \"12 . This means that some invalid JSON will be accepted by Pydantic when using experimental_allow_partial , e.g.: Python 3.9 and above Python 3.13 and above from typing import Annotated\n\nfrom annotated_types import MinLen\nfrom typing_extensions import TypedDict\n\nfrom pydantic import TypeAdapter\n\n\nclass Foobar(TypedDict, total=False):\n    a: int\n    b: Annotated[str, MinLen(5)]\n\n\nta = TypeAdapter(Foobar)\n\nv = ta.validate_json(\n    '{\"a\": 1, \"b\": \"12', experimental_allow_partial=True  # (1)!\n)\nprint(v)\n#> {'a': 1}\n\nv = ta.validate_json(\n    '{\"a\": 1, \"b\": \"12\"}', experimental_allow_partial=True  # (2)!\n)\nprint(v)\n#> {'a': 1} This will pass validation as expected although the last field will be omitted as it failed validation. This will also pass validation since the binary representation of the JSON data passed to pydantic-core is indistinguishable from the previous case. from typing import Annotated\n\nfrom annotated_types import MinLen\nfrom typing import TypedDict\n\nfrom pydantic import TypeAdapter\n\n\nclass Foobar(TypedDict, total=False):\n    a: int\n    b: Annotated[str, MinLen(5)]\n\n\nta = TypeAdapter(Foobar)\n\nv = ta.validate_json(\n    '{\"a\": 1, \"b\": \"12', experimental_allow_partial=True  # (1)!\n)\nprint(v)\n#> {'a': 1}\n\nv = ta.validate_json(\n    '{\"a\": 1, \"b\": \"12\"}', experimental_allow_partial=True  # (2)!\n)\nprint(v)\n#> {'a': 1} This will pass validation as expected although the last field will be omitted as it failed validation. This will also pass validation since the binary representation of the JSON data passed to pydantic-core is indistinguishable from the previous case. Any error in the last field of the input will be ignored ¶ As described above , many errors can result from truncating the input. Rather than trying to specifically ignore errors that could result from truncation, Pydantic ignores all errors in the last element of the input in partial validation mode. This means clearly invalid data will pass validation if the error is in the last field of the input: from typing import Annotated\n\nfrom annotated_types import Ge\n\nfrom pydantic import TypeAdapter\n\nta = TypeAdapter(list[Annotated[int, Ge(10)]])\nv = ta.validate_python([20, 30, 4], experimental_allow_partial=True)  # (1)!\nprint(v)\n#> [20, 30]\n\nta = TypeAdapter(list[int])\n\nv = ta.validate_python([1, 2, 'wrong'], experimental_allow_partial=True)  # (2)!\nprint(v)\n#> [1, 2] As you would expect, this will pass validation since Pydantic correctly ignores the error in the (truncated) last item. This will also pass validation since the error in the last item is ignored.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#limitations-of-partial-validation","title":"Experimental - Partial Validation - Limitations of Partial Validation","objectID":"/latest/concepts/experimental/#limitations-of-partial-validation","rank":60},{"content":"Pydantic provides the  decorator to perform validation on the provided\narguments (and additionally return type) of a callable. However, it only allows arguments to be provided\nby actually calling the decorated callable. In some situations, you may want to just validate the arguments,\nsuch as when loading from other data sources such as JSON data. For this reason, the experimental \nfunction can be used to construct a core schema, which can later be used with a . from pydantic_core import SchemaValidator\n\nfrom pydantic.experimental.arguments_schema import generate_arguments_schema\n\n\ndef func(p: bool, *args: str, **kwargs: int) -> None: ...\n\n\narguments_schema = generate_arguments_schema(func=func)\n\nval = SchemaValidator(arguments_schema, config={'coerce_numbers_to_str': True})\n\nargs, kwargs = val.validate_json(\n    '{\"p\": true, \"args\": [\"arg1\", 1], \"kwargs\": {\"extra\": 1}}'\n)\nprint(args, kwargs)  # (1)!\n#> (True, 'arg1', '1') {'extra': 1} If you want the validated arguments as a dictionary, you can use the \n   method: from inspect import signature\n\nsignature(func).bind(*args, **kwargs).arguments\n#> {'p': True, 'args': ('arg1', '1'), 'kwargs': {'extra': 1}} Note Unlike , this core schema will only validate the provided arguments;\nthe underlying callable will not be called. Additionally, you can ignore specific parameters by providing a callback, which is called for every parameter: from typing import Any\n\nfrom pydantic_core import SchemaValidator\n\nfrom pydantic.experimental.arguments_schema import generate_arguments_schema\n\n\ndef func(p: bool, *args: str, **kwargs: int) -> None: ...\n\n\ndef skip_first_parameter(index: int, name: str, annotation: Any) -> Any:\n    if index == 0:\n        return 'skip'\n\n\narguments_schema = generate_arguments_schema(\n    func=func,\n    parameters_callback=skip_first_parameter,\n)\n\nval = SchemaValidator(arguments_schema)\n\nargs, kwargs = val.validate_json('{\"args\": [\"arg1\"], \"kwargs\": {\"extra\": 1}}')\nprint(args, kwargs)\n#> ('arg1',) {'extra': 1}","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#validation-of-a-callables-arguments","title":"Experimental - Validation of a callable's arguments","objectID":"/latest/concepts/experimental/#validation-of-a-callables-arguments","rank":55},{"content":"The MISSING sentinel is a singleton indicating a field value was not provided during validation. This singleton can be used as a default value, as an alternative to None when it has an explicit\nmeaning. During serialization, any field with MISSING as a value is excluded from the output. Python 3.9 and above Python 3.10 and above from typing import Union\n\nfrom pydantic import BaseModel\nfrom pydantic.experimental.missing_sentinel import MISSING\n\n\nclass Configuration(BaseModel):\n    timeout: Union[int, None, MISSING] = MISSING\n\n\n# configuration defaults, stored somewhere else:\ndefaults = {'timeout': 200}\n\nconf = Configuration()\n\n# `timeout` is excluded from the serialization output:\nconf.model_dump()\n# {}\n\n# The `MISSING` value doesn't appear in the JSON Schema:\nConfiguration.model_json_schema()['properties']['timeout']\n#> {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'title': 'Timeout'}}\n\n\n# `is` can be used to discrimate between the sentinel and other values:\ntimeout = conf.timeout if conf.timeout is not MISSING else defaults['timeout'] from pydantic import BaseModel\nfrom pydantic.experimental.missing_sentinel import MISSING\n\n\nclass Configuration(BaseModel):\n    timeout: int | None | MISSING = MISSING\n\n\n# configuration defaults, stored somewhere else:\ndefaults = {'timeout': 200}\n\nconf = Configuration()\n\n# `timeout` is excluded from the serialization output:\nconf.model_dump()\n# {}\n\n# The `MISSING` value doesn't appear in the JSON Schema:\nConfiguration.model_json_schema()['properties']['timeout']\n#> {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'title': 'Timeout'}}\n\n\n# `is` can be used to discrimate between the sentinel and other values:\ntimeout = conf.timeout if conf.timeout is not MISSING else defaults['timeout'] This feature is marked as experimental because it relies on the draft PEP 661 , introducing sentinels in the standard library. As such, the following limitations currently apply: Static type checking of sentinels is only supported with Pyright 1.1.402 or greater, and the enableExperimentalFeatures type evaluation setting\n  should be enabled. Pickling of models containing MISSING as a value is not supported.","pageID":"Experimental","abs_url":"/latest/concepts/experimental/#missing-sentinel","title":"Experimental - MISSING sentinel","objectID":"/latest/concepts/experimental/#missing-sentinel","rank":50},{"content":"In this section, we will go through the available mechanisms to customize Pydantic model fields:\ndefault values, JSON Schema metadata, constraints, etc. To do so, the  function is used a lot, and behaves the same way as\nthe standard library  function for dataclasses: from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    name: str = Field(frozen=True) Note Even though name is assigned a value, it is still required and has no default value. If you want\nto emphasize on the fact that a value must be provided, you can use the : class Model(BaseModel):\n    name: str = Field(..., frozen=True) However, its usage is discouraged as it doesn't play well with static type checkers.","pageID":"Fields","abs_url":"/latest/concepts/fields/#Fields","title":"Fields","objectID":"/latest/concepts/fields/#Fields","rank":100},{"content":"To apply constraints or attach  functions to a model field, Pydantic\nsupports the  typing construct to attach metadata to an annotation: from typing import Annotated\n\nfrom pydantic import BaseModel, Field, WithJsonSchema\n\n\nclass Model(BaseModel):\n    name: Annotated[str, Field(strict=True), WithJsonSchema({'extra': 'data'})] As far as static type checkers are concerned, name is still typed as str , but Pydantic leverages\nthe available metadata to add validation logic, type constraints, etc. Using this pattern has some advantages: Using the f: <type> = Field(...) form can be confusing and might trick users into thinking f has a default value, while in reality it is still required. You can provide an arbitrary amount of metadata elements for a field. As shown in the example above,\n  the  function only supports a limited set of constraints/metadata,\n  and you may have to use different Pydantic utilities such as \n  in some cases. Types can be made reusable (see the documentation on custom types using this pattern). However, note that certain arguments to the  function (namely, default , default_factory , and alias ) are taken into account by static type checkers to synthesize a correct __init__ method. The annotated pattern is not understood by them, so you should use the normal\nassignment form instead. Tip The annotated pattern can also be used to add metadata to specific parts of the type. For instance, validation constraints can be added this way: from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    int_list: list[Annotated[int, Field(gt=0)]]\n    # Valid: [1, 3]\n    # Invalid: [-1, 2] Be careful not mixing field and type metadata: class Model(BaseModel):\n    field_bad: Annotated[int, Field(deprecated=True)] | None = None  # (1)!\n    field_ok: Annotated[int | None, Field(deprecated=True)] = None  # (2)! The  function is applied to int type, hence the deprecated flag won't have any effect. While this may be confusing given that the name of\n     the  function would imply it should apply to the field,\n     the API was designed when this function was the only way to provide metadata. You can\n     alternatively make use of the annotated_types library which is now supported by Pydantic. The  function is applied to the \"top-level\" union type,\n     hence the deprecated flag will be applied to the field.","pageID":"Fields","abs_url":"/latest/concepts/fields/#the-annotated-pattern","title":"Fields - The annotated pattern","objectID":"/latest/concepts/fields/#the-annotated-pattern","rank":95},{"content":"Default values for fields can be provided using the normal assignment syntax or by providing a value\nto the default argument: from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    # Both fields aren't required:\n    name: str = 'John Doe'\n    age: int = Field(default=20) Warning In Pydantic V1 , a type annotated as \nor wrapped by  would be given an implicit default of None even if no\ndefault was explicitly specified. This is no longer the case in Pydantic V2. You can also pass a callable to the default_factory argument that will be called to generate a default value: from uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex) The default factory can also take a single required argument, in which case the already validated data will be passed as a dictionary. from pydantic import BaseModel, EmailStr, Field\n\n\nclass User(BaseModel):\n    email: EmailStr\n    username: str = Field(default_factory=lambda data: data['email'])\n\n\nuser = User(email='user@example.com')\nprint(user.username)\n#> user@example.com The data argument will only contain the already validated data, based on the order of model fields (the above example would fail if username were to be defined before email ).","pageID":"Fields","abs_url":"/latest/concepts/fields/#default-values","title":"Fields - Default values","objectID":"/latest/concepts/fields/#default-values","rank":90},{"content":"By default, Pydantic will not validate default values. The validate_default field parameter\n(or the  configuration value) can be used\nto enable this behavior: from pydantic import BaseModel, Field, ValidationError\n\n\nclass User(BaseModel):\n    age: int = Field(default='twelve', validate_default=True)\n\n\ntry:\n    user = User()\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    age\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='twelve', input_type=str]\n    \"\"\"","pageID":"Fields","abs_url":"/latest/concepts/fields/#validate-default-values","title":"Fields - Validate default values","objectID":"/latest/concepts/fields/#validate-default-values","rank":85},{"content":"A common source of bugs in Python is to use a mutable object as a default value for a function or method argument,\nas the same instance ends up being reused in each call. The  module actually raises an error in this case, indicating that you should use\na default factory instead. While the same thing can be done in Pydantic, it is not required. In the event that the default value is not hashable,\nPydantic will create a deep copy of the default value when creating each instance of the model: from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    item_counts: list[dict[str, int]] = [{}]\n\n\nm1 = Model()\nm1.item_counts[0]['a'] = 1\nprint(m1.item_counts)\n#> [{'a': 1}]\n\nm2 = Model()\nprint(m2.item_counts)\n#> [{}]","pageID":"Fields","abs_url":"/latest/concepts/fields/#mutable-default-values","title":"Fields - Validate default values - Mutable default values","objectID":"/latest/concepts/fields/#mutable-default-values","rank":80},{"content":"Tip Read more about aliases in the dedicated section . For validation and serialization, you can define an alias for a field. There are three ways to define an alias: Field(alias='foo') Field(validation_alias='foo') Field(serialization_alias='foo') The alias parameter is used for both validation and serialization. If you want to use different aliases for validation and serialization respectively, you can use the validation_alias and serialization_alias parameters, which will apply only in their respective use cases. Here is an example of using the alias parameter: from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(alias='username')\n\n\nuser = User(username='johndoe')  # (1)!\nprint(user)\n#> name='johndoe'\nprint(user.model_dump(by_alias=True))  # (2)!\n#> {'username': 'johndoe'} The alias 'username' is used for instance creation and validation. We are using  to convert the model into a serializable format. Note that the by_alias keyword argument defaults to False , and must be specified explicitly to dump\nmodels using the field (serialization) aliases. You can also use  to\nconfigure this behavior at the model level. When by_alias=True , the alias 'username' used during serialization. If you want to use an alias only for validation, you can use the validation_alias parameter: from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(validation_alias='username')\n\n\nuser = User(username='johndoe')  # (1)!\nprint(user)\n#> name='johndoe'\nprint(user.model_dump(by_alias=True))  # (2)!\n#> {'name': 'johndoe'} The validation alias 'username' is used during validation. The field name 'name' is used during serialization. If you only want to define an alias for serialization , you can use the serialization_alias parameter: from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(serialization_alias='username')\n\n\nuser = User(name='johndoe')  # (1)!\nprint(user)\n#> name='johndoe'\nprint(user.model_dump(by_alias=True))  # (2)!\n#> {'username': 'johndoe'} The field name 'name' is used for validation. The serialization alias 'username' is used for serialization. Alias precedence and priority In case you use alias together with validation_alias or serialization_alias at the same time,\nthe validation_alias will have priority over alias for validation, and serialization_alias will have priority\nover alias for serialization. If you provide a value for the  model setting, you can control the order of precedence for field alias and generated aliases via the alias_priority field parameter. You can read more about alias precedence here .","pageID":"Fields","abs_url":"/latest/concepts/fields/#field-aliases","title":"Fields - Field aliases","objectID":"/latest/concepts/fields/#field-aliases","rank":75},{"content":"There are some keyword arguments that can be used to constrain numeric values: gt - greater than lt - less than ge - greater than or equal to le - less than or equal to multiple_of - a multiple of the given number allow_inf_nan - allow 'inf' , '-inf' , 'nan' values Here's an example: from pydantic import BaseModel, Field\n\n\nclass Foo(BaseModel):\n    positive: int = Field(gt=0)\n    non_negative: int = Field(ge=0)\n    negative: int = Field(lt=0)\n    non_positive: int = Field(le=0)\n    even: int = Field(multiple_of=2)\n    love_for_pydantic: float = Field(allow_inf_nan=True)\n\n\nfoo = Foo(\n    positive=1,\n    non_negative=0,\n    negative=-1,\n    non_positive=0,\n    even=2,\n    love_for_pydantic=float('inf'),\n)\nprint(foo)\n\"\"\"\npositive=1 non_negative=0 negative=-1 non_positive=0 even=2 love_for_pydantic=inf\n\"\"\" Constraints on compound types In case you use field constraints with compound types, an error can happen in some cases. To avoid potential issues,\nyou can use Annotated : from typing import Annotated, Optional\n\nfrom pydantic import BaseModel, Field\n\n\nclass Foo(BaseModel):\n    positive: Optional[Annotated[int, Field(gt=0)]]\n    # Can error in some cases, not recommended:\n    non_negative: Optional[int] = Field(ge=0)","pageID":"Fields","abs_url":"/latest/concepts/fields/#numeric-constraints","title":"Fields - Numeric Constraints","objectID":"/latest/concepts/fields/#numeric-constraints","rank":70},{"content":"There are fields that can be used to constrain strings: min_length : Minimum length of the string. max_length : Maximum length of the string. pattern : A regular expression that the string must match. Here's an example: from pydantic import BaseModel, Field\n\n\nclass Foo(BaseModel):\n    short: str = Field(min_length=3)\n    long: str = Field(max_length=10)\n    regex: str = Field(pattern=r'^\\d*$')  # (1)!\n\n\nfoo = Foo(short='foo', long='foobarbaz', regex='123')\nprint(foo)\n#> short='foo' long='foobarbaz' regex='123' Only digits are allowed.","pageID":"Fields","abs_url":"/latest/concepts/fields/#string-constraints","title":"Fields - String Constraints","objectID":"/latest/concepts/fields/#string-constraints","rank":65},{"content":"There are fields that can be used to constrain decimals: max_digits : Maximum number of digits within the Decimal . It does not include a zero before the decimal point or\n  trailing decimal zeroes. decimal_places : Maximum number of decimal places allowed. It does not include trailing decimal zeroes. Here's an example: from decimal import Decimal\n\nfrom pydantic import BaseModel, Field\n\n\nclass Foo(BaseModel):\n    precise: Decimal = Field(max_digits=5, decimal_places=2)\n\n\nfoo = Foo(precise=Decimal('123.45'))\nprint(foo)\n#> precise=Decimal('123.45')","pageID":"Fields","abs_url":"/latest/concepts/fields/#decimal-constraints","title":"Fields - Decimal Constraints","objectID":"/latest/concepts/fields/#decimal-constraints","rank":60},{"content":"There are fields that can be used to constrain dataclasses: init : Whether the field should be included in the __init__ of the dataclass. init_var : Whether the field should be seen as an init-only field in the dataclass. kw_only : Whether the field should be a keyword-only argument in the constructor of the dataclass. Here's an example: from pydantic import BaseModel, Field\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass Foo:\n    bar: str\n    baz: str = Field(init_var=True)\n    qux: str = Field(kw_only=True)\n\n\nclass Model(BaseModel):\n    foo: Foo\n\n\nmodel = Model(foo=Foo('bar', baz='baz', qux='qux'))\nprint(model.model_dump())  # (1)!\n#> {'foo': {'bar': 'bar', 'qux': 'qux'}} The baz field is not included in the model_dump() output, since it is an init-only field.","pageID":"Fields","abs_url":"/latest/concepts/fields/#dataclass-constraints","title":"Fields - Dataclass Constraints","objectID":"/latest/concepts/fields/#dataclass-constraints","rank":55},{"content":"The parameter repr can be used to control whether the field should be included in the string\nrepresentation of the model. from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(repr=True)  # (1)!\n    age: int = Field(repr=False)\n\n\nuser = User(name='John', age=42)\nprint(user)\n#> name='John' This is the default value.","pageID":"Fields","abs_url":"/latest/concepts/fields/#field-representation","title":"Fields - Field Representation","objectID":"/latest/concepts/fields/#field-representation","rank":50},{"content":"The parameter discriminator can be used to control the field that will be used to discriminate between different\nmodels in a union. It takes either the name of a field or a Discriminator instance. The Discriminator approach can be useful when the discriminator fields aren't the same for all the models in the Union . The following example shows how to use discriminator with a field name: Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    age: int\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    age: int\n\n\nclass Model(BaseModel):\n    pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n\n\nprint(Model.model_validate({'pet': {'pet_type': 'cat', 'age': 12}}))  # (1)!\n#> pet=Cat(pet_type='cat', age=12) See more about Validating data in the Models page. from typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    age: int\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    age: int\n\n\nclass Model(BaseModel):\n    pet: Cat | Dog = Field(discriminator='pet_type')\n\n\nprint(Model.model_validate({'pet': {'pet_type': 'cat', 'age': 12}}))  # (1)!\n#> pet=Cat(pet_type='cat', age=12) See more about Validating data in the Models page. The following example shows how to use the discriminator keyword argument with a Discriminator instance: Python 3.9 and above Python 3.10 and above from typing import Annotated, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Field, Tag\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    age: int\n\n\nclass Dog(BaseModel):\n    pet_kind: Literal['dog']\n    age: int\n\n\ndef pet_discriminator(v):\n    if isinstance(v, dict):\n        return v.get('pet_type', v.get('pet_kind'))\n    return getattr(v, 'pet_type', getattr(v, 'pet_kind', None))\n\n\nclass Model(BaseModel):\n    pet: Union[Annotated[Cat, Tag('cat')], Annotated[Dog, Tag('dog')]] = Field(\n        discriminator=Discriminator(pet_discriminator)\n    )\n\n\nprint(repr(Model.model_validate({'pet': {'pet_type': 'cat', 'age': 12}})))\n#> Model(pet=Cat(pet_type='cat', age=12))\n\nprint(repr(Model.model_validate({'pet': {'pet_kind': 'dog', 'age': 12}})))\n#> Model(pet=Dog(pet_kind='dog', age=12)) from typing import Annotated, Literal\n\nfrom pydantic import BaseModel, Discriminator, Field, Tag\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    age: int\n\n\nclass Dog(BaseModel):\n    pet_kind: Literal['dog']\n    age: int\n\n\ndef pet_discriminator(v):\n    if isinstance(v, dict):\n        return v.get('pet_type', v.get('pet_kind'))\n    return getattr(v, 'pet_type', getattr(v, 'pet_kind', None))\n\n\nclass Model(BaseModel):\n    pet: Annotated[Cat, Tag('cat')] | Annotated[Dog, Tag('dog')] = Field(\n        discriminator=Discriminator(pet_discriminator)\n    )\n\n\nprint(repr(Model.model_validate({'pet': {'pet_type': 'cat', 'age': 12}})))\n#> Model(pet=Cat(pet_type='cat', age=12))\n\nprint(repr(Model.model_validate({'pet': {'pet_kind': 'dog', 'age': 12}})))\n#> Model(pet=Dog(pet_kind='dog', age=12)) You can also take advantage of Annotated to define your discriminated unions.\nSee the Discriminated Unions docs for more details.","pageID":"Fields","abs_url":"/latest/concepts/fields/#discriminator","title":"Fields - Discriminator","objectID":"/latest/concepts/fields/#discriminator","rank":45},{"content":"The strict parameter on a  specifies whether the field should be validated in \"strict mode\".\nIn strict mode, Pydantic throws an error during validation instead of coercing data on the field where strict=True . from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(strict=True)\n    age: int = Field(strict=False)  # (1)!\n\n\nuser = User(name='John', age='42')  # (2)!\nprint(user)\n#> name='John' age=42 This is the default value. The age field is not validated in the strict mode. Therefore, it can be assigned a string. See Strict Mode for more details. See Conversion Table for more details on how Pydantic converts data in both strict and lax modes.","pageID":"Fields","abs_url":"/latest/concepts/fields/#strict-mode","title":"Fields - Strict Mode","objectID":"/latest/concepts/fields/#strict-mode","rank":40},{"content":"The parameter frozen is used to emulate the frozen dataclass behaviour. It is used to prevent the field from being\nassigned a new value after the model is created (immutability). See the frozen dataclass documentation for more details. from pydantic import BaseModel, Field, ValidationError\n\n\nclass User(BaseModel):\n    name: str = Field(frozen=True)\n    age: int\n\n\nuser = User(name='John', age=42)\n\ntry:\n    user.name = 'Jane'  # (1)!\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    name\n      Field is frozen [type=frozen_field, input_value='Jane', input_type=str]\n    \"\"\" Since name field is frozen, the assignment is not allowed.","pageID":"Fields","abs_url":"/latest/concepts/fields/#immutability","title":"Fields - Immutability","objectID":"/latest/concepts/fields/#immutability","rank":35},{"content":"The exclude parameter can be used to control which fields should be excluded from the\nmodel when exporting the model. See the following example: from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str\n    age: int = Field(exclude=True)\n\n\nuser = User(name='John', age=42)\nprint(user.model_dump())  # (1)!\n#> {'name': 'John'} The age field is not included in the model_dump() output, since it is excluded. See the [Serialization] section for more details.","pageID":"Fields","abs_url":"/latest/concepts/fields/#exclude","title":"Fields - Exclude","objectID":"/latest/concepts/fields/#exclude","rank":30},{"content":"The deprecated parameter can be used to mark a field as being deprecated. Doing so will result in: a runtime deprecation warning emitted when accessing the field. The deprecated keyword\n  being set in the generated JSON schema. This parameter accepts different types, described below.","pageID":"Fields","abs_url":"/latest/concepts/fields/#deprecated-fields","title":"Fields - Deprecated fields","objectID":"/latest/concepts/fields/#deprecated-fields","rank":25},{"content":"The value will be used as the deprecation message. from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    deprecated_field: Annotated[int, Field(deprecated='This is deprecated')]\n\n\nprint(Model.model_json_schema()['properties']['deprecated_field'])\n#> {'deprecated': True, 'title': 'Deprecated Field', 'type': 'integer'}","pageID":"Fields","abs_url":"/latest/concepts/fields/#deprecated-as-a-string","title":"Fields - Deprecated fields - deprecated as a string","objectID":"/latest/concepts/fields/#deprecated-as-a-string","rank":20},{"content":"The  decorator (or the\n on Python\n3.12 and lower) can be used as an instance. Python 3.9 and above Python 3.13 and above from typing import Annotated\n\nfrom typing_extensions import deprecated\n\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    deprecated_field: Annotated[int, deprecated('This is deprecated')]\n\n    # Or explicitly using `Field`:\n    alt_form: Annotated[int, Field(deprecated=deprecated('This is deprecated'))] from typing import Annotated\nfrom warnings import deprecated\n\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    deprecated_field: Annotated[int, deprecated('This is deprecated')]\n\n    # Or explicitly using `Field`:\n    alt_form: Annotated[int, Field(deprecated=deprecated('This is deprecated'))] Support for category and stacklevel The current implementation of this feature does not take into account the category and stacklevel arguments to the deprecated decorator. This might land in a future version of Pydantic.","pageID":"Fields","abs_url":"/latest/concepts/fields/#deprecated-via-the-warningsdeprecated-decorator","title":"Fields - Deprecated fields - deprecated via the @warnings.deprecated decorator","objectID":"/latest/concepts/fields/#deprecated-via-the-warningsdeprecated-decorator","rank":15},{"content":"from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    deprecated_field: Annotated[int, Field(deprecated=True)]\n\n\nprint(Model.model_json_schema()['properties']['deprecated_field'])\n#> {'deprecated': True, 'title': 'Deprecated Field', 'type': 'integer'} Accessing a deprecated field in validators When accessing a deprecated field inside a validator, the deprecation warning will be emitted. You can use\n to explicitly ignore it: import warnings\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass Model(BaseModel):\n    deprecated_field: int = Field(deprecated='This is deprecated')\n\n    @model_validator(mode='after')\n    def validate_model(self) -> Self:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', DeprecationWarning)\n            self.deprecated_field = self.deprecated_field * 2","pageID":"Fields","abs_url":"/latest/concepts/fields/#deprecated-as-a-boolean","title":"Fields - Deprecated fields - deprecated as a boolean","objectID":"/latest/concepts/fields/#deprecated-as-a-boolean","rank":10},{"content":"Some field parameters are used exclusively to customize the generated JSON schema. The parameters in question are: title description examples json_schema_extra Read more about JSON schema customization / modification with fields in the Customizing JSON Schema section of the JSON schema docs.","pageID":"Fields","abs_url":"/latest/concepts/fields/#customizing-json-schema","title":"Fields - Customizing JSON Schema","objectID":"/latest/concepts/fields/#customizing-json-schema","rank":5},{"content":"The  decorator can be used to include  or\n attributes when serializing a model or dataclass.\nThe property will also be taken into account in the JSON Schema (in serialization mode). Note Properties can be useful for fields that are computed from other fields, or for fields that\nare expensive to be computed (and thus, are cached if using ). However, note that Pydantic will not perform any additional logic on the wrapped property\n(validation, cache invalidation, etc.). Here's an example of the JSON schema (in serialization mode) generated for a model with a computed field: from pydantic import BaseModel, computed_field\n\n\nclass Box(BaseModel):\n    width: float\n    height: float\n    depth: float\n\n    @computed_field\n    @property  # (1)!\n    def volume(self) -> float:\n        return self.width * self.height * self.depth\n\n\nprint(Box.model_json_schema(mode='serialization'))\n\"\"\"\n{\n    'properties': {\n        'width': {'title': 'Width', 'type': 'number'},\n        'height': {'title': 'Height', 'type': 'number'},\n        'depth': {'title': 'Depth', 'type': 'number'},\n        'volume': {'readOnly': True, 'title': 'Volume', 'type': 'number'},\n    },\n    'required': ['width', 'height', 'depth', 'volume'],\n    'title': 'Box',\n    'type': 'object',\n}\n\"\"\" If not specified,  will implicitly convert the method\n   to a . However, it is preferable to explicitly use the  decorator\n   for type checking purposes. Here's an example using the model_dump method with a computed field: from pydantic import BaseModel, computed_field\n\n\nclass Box(BaseModel):\n    width: float\n    height: float\n    depth: float\n\n    @computed_field\n    @property\n    def volume(self) -> float:\n        return self.width * self.height * self.depth\n\n\nb = Box(width=1, height=2, depth=3)\nprint(b.model_dump())\n#> {'width': 1.0, 'height': 2.0, 'depth': 3.0, 'volume': 6.0} As with regular fields, computed fields can be marked as being deprecated: from typing_extensions import deprecated\n\nfrom pydantic import BaseModel, computed_field\n\n\nclass Box(BaseModel):\n    width: float\n    height: float\n    depth: float\n\n    @computed_field\n    @property\n    @deprecated(\"'volume' is deprecated\")\n    def volume(self) -> float:\n        return self.width * self.height * self.depth","pageID":"Fields","abs_url":"/latest/concepts/fields/#the-computed_field-decorator","title":"Fields - The computed_field decorator","objectID":"/latest/concepts/fields/#the-computed_field-decorator","rank":0},{"content":"Forward annotations (wrapped in quotes) or using the from __future__ import annotations future statement (as introduced in PEP563 ) are supported: from __future__ import annotations\n\nfrom pydantic import BaseModel\n\nMyInt = int\n\n\nclass Model(BaseModel):\n    a: MyInt\n    # Without the future import, equivalent to:\n    # a: 'MyInt'\n\n\nprint(Model(a='1'))\n#> a=1 As shown in the following sections, forward annotations are useful when you want to reference\na type that is not yet defined in your code. The internal logic to resolve forward annotations is described in detail in this section .","pageID":"Forward Annotations","abs_url":"/latest/concepts/forward_annotations/#Forward Annotations","title":"Forward Annotations","objectID":"/latest/concepts/forward_annotations/#Forward Annotations","rank":100},{"content":"Models with self-referencing fields are also supported. These annotations will be resolved during model creation. Within the model, you can either add the from __future__ import annotations import or wrap the annotation\nin a string: from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    a: int = 123\n    sibling: 'Optional[Foo]' = None\n\n\nprint(Foo())\n#> a=123 sibling=None\nprint(Foo(sibling={'a': '321'}))\n#> a=123 sibling=Foo(a=321, sibling=None)","pageID":"Forward Annotations","abs_url":"/latest/concepts/forward_annotations/#self-referencing-or-recursive-models","title":"Forward Annotations - Self-referencing (or \"Recursive\") Models","objectID":"/latest/concepts/forward_annotations/#self-referencing-or-recursive-models","rank":95},{"content":"When working with self-referencing recursive models, it is possible that you might encounter cyclic references\nin validation inputs. For example, this can happen when validating ORM instances with back-references from\nattributes. Rather than raising a  while attempting to validate data with cyclic references, Pydantic is able\nto detect the cyclic reference and raise an appropriate : Python 3.9 and above Python 3.10 and above from typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass ModelA(BaseModel):\n    b: 'Optional[ModelB]' = None\n\n\nclass ModelB(BaseModel):\n    a: Optional[ModelA] = None\n\n\ncyclic_data = {}\ncyclic_data['a'] = {'b': cyclic_data}\nprint(cyclic_data)\n#> {'a': {'b': {...}}}\n\ntry:\n    ModelB.model_validate(cyclic_data)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for ModelB\n    a.b\n      Recursion error - cyclic reference detected [type=recursion_loop, input_value={'a': {'b': {...}}}, input_type=dict]\n    \"\"\" from typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass ModelA(BaseModel):\n    b: 'Optional[ModelB]' = None\n\n\nclass ModelB(BaseModel):\n    a: ModelA | None = None\n\n\ncyclic_data = {}\ncyclic_data['a'] = {'b': cyclic_data}\nprint(cyclic_data)\n#> {'a': {'b': {...}}}\n\ntry:\n    ModelB.model_validate(cyclic_data)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for ModelB\n    a.b\n      Recursion error - cyclic reference detected [type=recursion_loop, input_value={'a': {'b': {...}}}, input_type=dict]\n    \"\"\" Because this error is raised without actually exceeding the maximum recursion depth, you can catch and\nhandle the raised  without needing to worry about the limited\nremaining recursion depth: from __future__ import annotations\n\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom dataclasses import field\n\nfrom pydantic import BaseModel, ValidationError, field_validator\n\n\ndef is_recursion_validation_error(exc: ValidationError) -> bool:\n    errors = exc.errors()\n    return len(errors) == 1 and errors[0]['type'] == 'recursion_loop'\n\n\n@contextmanager\ndef suppress_recursion_validation_error() -> Generator[None]:\n    try:\n        yield\n    except ValidationError as exc:\n        if not is_recursion_validation_error(exc):\n            raise exc\n\n\nclass Node(BaseModel):\n    id: int\n    children: list[Node] = field(default_factory=list)\n\n    @field_validator('children', mode='wrap')\n    @classmethod\n    def drop_cyclic_references(cls, children, h):\n        try:\n            return h(children)\n        except ValidationError as exc:\n            if not (\n                is_recursion_validation_error(exc)\n                and isinstance(children, list)\n            ):\n                raise exc\n\n            value_without_cyclic_refs = []\n            for child in children:\n                with suppress_recursion_validation_error():\n                    value_without_cyclic_refs.extend(h([child]))\n            return h(value_without_cyclic_refs)\n\n\n# Create data with cyclic references representing the graph 1 -> 2 -> 3 -> 1\nnode_data = {'id': 1, 'children': [{'id': 2, 'children': [{'id': 3}]}]}\nnode_data['children'][0]['children'][0]['children'] = [node_data]\n\nprint(Node.model_validate(node_data))\n#> id=1 children=[Node(id=2, children=[Node(id=3, children=[])])] Similarly, if Pydantic encounters a recursive reference during serialization , rather than waiting\nfor the maximum recursion depth to be exceeded, a  is raised immediately: from pydantic import TypeAdapter\n\n# Create data with cyclic references representing the graph 1 -> 2 -> 3 -> 1\nnode_data = {'id': 1, 'children': [{'id': 2, 'children': [{'id': 3}]}]}\nnode_data['children'][0]['children'][0]['children'] = [node_data]\n\ntry:\n    # Try serializing the circular reference as JSON\n    TypeAdapter(dict).dump_json(node_data)\nexcept ValueError as exc:\n    print(exc)\n    \"\"\"\n    Error serializing to JSON: ValueError: Circular reference detected (id repeated)\n    \"\"\" This can also be handled if desired: from dataclasses import field\nfrom typing import Any\n\nfrom pydantic import (\n    SerializerFunctionWrapHandler,\n    TypeAdapter,\n    field_serializer,\n)\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass NodeReference:\n    id: int\n\n\n@dataclass\nclass Node(NodeReference):\n    children: list['Node'] = field(default_factory=list)\n\n    @field_serializer('children', mode='wrap')\n    def serialize(\n        self, children: list['Node'], handler: SerializerFunctionWrapHandler\n    ) -> Any:\n        \"\"\"\n        Serialize a list of nodes, handling circular references by excluding the children.\n        \"\"\"\n        try:\n            return handler(children)\n        except ValueError as exc:\n            if not str(exc).startswith('Circular reference'):\n                raise exc\n\n            result = []\n            for node in children:\n                try:\n                    serialized = handler([node])\n                except ValueError as exc:\n                    if not str(exc).startswith('Circular reference'):\n                        raise exc\n                    result.append({'id': node.id})\n                else:\n                    result.append(serialized)\n            return result\n\n\n# Create a cyclic graph:\nnodes = [Node(id=1), Node(id=2), Node(id=3)]\nnodes[0].children.append(nodes[1])\nnodes[1].children.append(nodes[2])\nnodes[2].children.append(nodes[0])\n\nprint(nodes[0])\n#> Node(id=1, children=[Node(id=2, children=[Node(id=3, children=[...])])])\n\n# Serialize the cyclic graph:\nprint(TypeAdapter(Node).dump_python(nodes[0]))\n\"\"\"\n{\n    'id': 1,\n    'children': [{'id': 2, 'children': [{'id': 3, 'children': [{'id': 1}]}]}],\n}\n\"\"\"","pageID":"Forward Annotations","abs_url":"/latest/concepts/forward_annotations/#cyclic-references","title":"Forward Annotations - Self-referencing (or \"Recursive\") Models - Cyclic references","objectID":"/latest/concepts/forward_annotations/#cyclic-references","rank":90},{"content":"","pageID":"JSON","abs_url":"/latest/concepts/json/#json","title":"JSON","objectID":"/latest/concepts/json/#json","rank":100},{"content":"Pydantic provides builtin JSON parsing, which helps achieve: Significant performance improvements without the cost of using a 3rd party library Support for custom errors Support for strict specifications Here's an example of Pydantic's builtin JSON parsing via the  method, showcasing the support for strict specifications while parsing JSON data that doesn't match the model's type annotations: from datetime import date\n\nfrom pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Event(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n    when: date\n    where: tuple[int, int]\n\n\njson_data = '{\"when\": \"1987-01-28\", \"where\": [51, -1]}'\nprint(Event.model_validate_json(json_data))  # (1)!\n#> when=datetime.date(1987, 1, 28) where=(51, -1)\n\ntry:\n    Event.model_validate({'when': '1987-01-28', 'where': [51, -1]})  # (2)!\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for Event\n    when\n      Input should be a valid date [type=date_type, input_value='1987-01-28', input_type=str]\n    where\n      Input should be a valid tuple [type=tuple_type, input_value=[51, -1], input_type=list]\n    \"\"\" JSON has no date or tuple types, but Pydantic knows that so allows strings and arrays as inputs respectively when parsing JSON directly. If you pass the same values to the  method, Pydantic will raise a validation error because the strict configuration is enabled. In v2.5.0 and above, Pydantic uses jiter , a fast and iterable JSON parser, to parse JSON data.\nUsing jiter compared to serde results in modest performance improvements that will get even better in the future. The jiter JSON parser is almost entirely compatible with the serde JSON parser,\nwith one noticeable enhancement being that jiter supports deserialization of inf and NaN values.\nIn the future, jiter is intended to enable support validation errors to include the location\nin the original JSON input which contained the invalid value.","pageID":"JSON","abs_url":"/latest/concepts/json/#json-parsing","title":"JSON - Json Parsing","objectID":"/latest/concepts/json/#json-parsing","rank":95},{"content":"Starting in v2.7.0 , Pydantic's JSON parser offers support for partial JSON parsing, which is exposed via . Here's an example of this feature in action: from pydantic_core import from_json\n\npartial_json_data = '[\"aa\", \"bb\", \"c'  # (1)!\n\ntry:\n    result = from_json(partial_json_data, allow_partial=False)\nexcept ValueError as e:\n    print(e)  # (2)!\n    #> EOF while parsing a string at line 1 column 15\n\nresult = from_json(partial_json_data, allow_partial=True)\nprint(result)  # (3)!\n#> ['aa', 'bb'] The JSON list is incomplete - it's missing a closing \"] When allow_partial is set to False (the default), a parsing error occurs. When allow_partial is set to True , part of the input is deserialized successfully. This also works for deserializing partial dictionaries. For example: from pydantic_core import from_json\n\npartial_dog_json = '{\"breed\": \"lab\", \"name\": \"fluffy\", \"friends\": [\"buddy\", \"spot\", \"rufus\"], \"age'\ndog_dict = from_json(partial_dog_json, allow_partial=True)\nprint(dog_dict)\n#> {'breed': 'lab', 'name': 'fluffy', 'friends': ['buddy', 'spot', 'rufus']} Validating LLM Output This feature is particularly beneficial for validating LLM outputs.\nWe've written some blog posts about this topic, which you can find here . In future versions of Pydantic, we expect to expand support for this feature through either Pydantic's other JSON validation functions\n( and\n) or model configuration. Stay tuned 🚀! For now, you can use  in combination with  to achieve the same result. Here's an example: from pydantic_core import from_json\n\nfrom pydantic import BaseModel\n\n\nclass Dog(BaseModel):\n    breed: str\n    name: str\n    friends: list\n\n\npartial_dog_json = '{\"breed\": \"lab\", \"name\": \"fluffy\", \"friends\": [\"buddy\", \"spot\", \"rufus\"], \"age'\ndog = Dog.model_validate(from_json(partial_dog_json, allow_partial=True))\nprint(repr(dog))\n#> Dog(breed='lab', name='fluffy', friends=['buddy', 'spot', 'rufus']) Tip For partial JSON parsing to work reliably, all fields on the model should have default values. Check out the following example for a more in-depth look at how to use default values with partial JSON parsing: Using default values with partial JSON parsing from typing import Annotated, Any, Optional\n\nimport pydantic_core\n\nfrom pydantic import BaseModel, ValidationError, WrapValidator\n\n\ndef default_on_error(v, handler) -> Any:\n    \"\"\"\n    Raise a PydanticUseDefault exception if the value is missing.\n\n    This is useful for avoiding errors from partial\n    JSON preventing successful validation.\n    \"\"\"\n    try:\n        return handler(v)\n    except ValidationError as exc:\n        # there might be other types of errors resulting from partial JSON parsing\n        # that you allow here, feel free to customize as needed\n        if all(e['type'] == 'missing' for e in exc.errors()):\n            raise pydantic_core.PydanticUseDefault()\n        else:\n            raise\n\n\nclass NestedModel(BaseModel):\n    x: int\n    y: str\n\n\nclass MyModel(BaseModel):\n    foo: Optional[str] = None\n    bar: Annotated[\n        Optional[tuple[str, int]], WrapValidator(default_on_error)\n    ] = None\n    nested: Annotated[\n        Optional[NestedModel], WrapValidator(default_on_error)\n    ] = None\n\n\nm = MyModel.model_validate(\n    pydantic_core.from_json('{\"foo\": \"x\", \"bar\": [\"world\",', allow_partial=True)\n)\nprint(repr(m))\n#> MyModel(foo='x', bar=None, nested=None)\n\n\nm = MyModel.model_validate(\n    pydantic_core.from_json(\n        '{\"foo\": \"x\", \"bar\": [\"world\", 1], \"nested\": {\"x\":', allow_partial=True\n    )\n)\nprint(repr(m))\n#> MyModel(foo='x', bar=('world', 1), nested=None)","pageID":"JSON","abs_url":"/latest/concepts/json/#partial-json-parsing","title":"JSON - Json Parsing - Partial JSON Parsing","objectID":"/latest/concepts/json/#partial-json-parsing","rank":90},{"content":"Starting in v2.7.0 , Pydantic's JSON parser offers support for configuring how Python strings are cached during JSON parsing and validation (when Python strings are constructed from Rust strings during Python validation, e.g. after strip_whitespace=True ).\nThe cache_strings setting is exposed via both  and . The cache_strings setting can take any of the following values: True or 'all' (the default): cache all strings 'keys' : cache only dictionary keys, this only applies when used with  or when parsing JSON using False or 'none' : no caching Using the string caching feature results in performance improvements, but increases memory usage slightly. String Caching Details Strings are cached using a fully associative cache with a size of 16,384 . Only strings where len(string) < 64 are cached. There is some overhead to looking up the cache, which is normally worth it to avoid constructing strings.\nHowever, if you know there will be very few repeated strings in your data, you might get a performance boost by disabling this setting with cache_strings=False .","pageID":"JSON","abs_url":"/latest/concepts/json/#caching-strings","title":"JSON - Json Parsing - Caching Strings","objectID":"/latest/concepts/json/#caching-strings","rank":85},{"content":"For more information on JSON serialization, see the serialization concepts page.","pageID":"JSON","abs_url":"/latest/concepts/json/#json-serialization","title":"JSON - JSON Serialization","objectID":"/latest/concepts/json/#json-serialization","rank":80},{"content":"Pydantic allows automatic creation and customization of JSON schemas from models.\nThe generated JSON schemas are compliant with the following specifications: JSON Schema Draft 2020-12 OpenAPI Specification v3.1.0 .","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#JSON Schema","title":"JSON Schema","objectID":"/latest/concepts/json_schema/#JSON Schema","rank":100},{"content":"Use the following functions to generate JSON schema: returns a jsonable dict of a model's schema. returns a jsonable dict of an adapted type's schema. Note These methods are not to be confused with \nand , which serialize instances of the\nmodel or adapted type, respectively. These methods return JSON strings. In comparison,\n and\n return a jsonable dict\nrepresenting the JSON schema of the model or adapted type, respectively. on the \"jsonable\" nature of JSON schema Regarding the \"jsonable\" nature of the  results,\ncalling json.dumps(m.model_json_schema()) on some BaseModel m returns a valid JSON string. Similarly, for\n, calling json.dumps(TypeAdapter(<some_type>).json_schema()) returns a valid JSON string. Tip Pydantic offers support for both of: Customizing JSON Schema Customizing the JSON Schema Generation Process The first approach generally has a more narrow scope, allowing for customization of the JSON schema for\nmore specific cases and types. The second approach generally has a more broad scope, allowing for customization\nof the JSON schema generation process overall. The same effects can be achieved with either approach, but\ndepending on your use case, one approach might offer a more simple solution than the other. Here's an example of generating JSON schema from a BaseModel : Python 3.9 and above Python 3.10 and above import json\nfrom enum import Enum\nfrom typing import Annotated, Union\n\nfrom pydantic import BaseModel, Field\nfrom pydantic.config import ConfigDict\n\n\nclass FooBar(BaseModel):\n    count: int\n    size: Union[float, None] = None\n\n\nclass Gender(str, Enum):\n    male = 'male'\n    female = 'female'\n    other = 'other'\n    not_given = 'not_given'\n\n\nclass MainModel(BaseModel):\n    \"\"\"\n    This is the description of the main model\n    \"\"\"\n\n    model_config = ConfigDict(title='Main')\n\n    foo_bar: FooBar\n    gender: Annotated[Union[Gender, None], Field(alias='Gender')] = None\n    snap: int = Field(\n        default=42,\n        title='The Snap',\n        description='this is the value of snap',\n        gt=30,\n        lt=50,\n    )\n\n\nmain_model_schema = MainModel.model_json_schema()  # (1)!\nprint(json.dumps(main_model_schema, indent=2))  # (2)! JSON output: { \"$defs\" : { \"FooBar\" : { \"properties\" : { \"count\" : { \"title\" : \"Count\" , \"type\" : \"integer\" }, \"size\" : { \"anyOf\" : [ { \"type\" : \"number\" }, { \"type\" : \"null\" } ], \"default\" : null , \"title\" : \"Size\" } }, \"required\" : [ \"count\" ], \"title\" : \"FooBar\" , \"type\" : \"object\" }, \"Gender\" : { \"enum\" : [ \"male\" , \"female\" , \"other\" , \"not_given\" ], \"title\" : \"Gender\" , \"type\" : \"string\" } }, \"description\" : \"This is the description of the main model\" , \"properties\" : { \"foo_bar\" : { \"$ref\" : \"#/$defs/FooBar\" }, \"Gender\" : { \"anyOf\" : [ { \"$ref\" : \"#/$defs/Gender\" }, { \"type\" : \"null\" } ], \"default\" : null }, \"snap\" : { \"default\" : 42 , \"description\" : \"this is the value of snap\" , \"exclusiveMaximum\" : 50 , \"exclusiveMinimum\" : 30 , \"title\" : \"The Snap\" , \"type\" : \"integer\" } }, \"required\" : [ \"foo_bar\" ], \"title\" : \"Main\" , \"type\" : \"object\" } This produces a \"jsonable\" dict of MainModel 's schema. Calling json.dumps on the schema dict produces a JSON string. import json\nfrom enum import Enum\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field\nfrom pydantic.config import ConfigDict\n\n\nclass FooBar(BaseModel):\n    count: int\n    size: float | None = None\n\n\nclass Gender(str, Enum):\n    male = 'male'\n    female = 'female'\n    other = 'other'\n    not_given = 'not_given'\n\n\nclass MainModel(BaseModel):\n    \"\"\"\n    This is the description of the main model\n    \"\"\"\n\n    model_config = ConfigDict(title='Main')\n\n    foo_bar: FooBar\n    gender: Annotated[Gender | None, Field(alias='Gender')] = None\n    snap: int = Field(\n        default=42,\n        title='The Snap',\n        description='this is the value of snap',\n        gt=30,\n        lt=50,\n    )\n\n\nmain_model_schema = MainModel.model_json_schema()  # (1)!\nprint(json.dumps(main_model_schema, indent=2))  # (2)! JSON output: { \"$defs\" : { \"FooBar\" : { \"properties\" : { \"count\" : { \"title\" : \"Count\" , \"type\" : \"integer\" }, \"size\" : { \"anyOf\" : [ { \"type\" : \"number\" }, { \"type\" : \"null\" } ], \"default\" : null , \"title\" : \"Size\" } }, \"required\" : [ \"count\" ], \"title\" : \"FooBar\" , \"type\" : \"object\" }, \"Gender\" : { \"enum\" : [ \"male\" , \"female\" , \"other\" , \"not_given\" ], \"title\" : \"Gender\" , \"type\" : \"string\" } }, \"description\" : \"This is the description of the main model\" , \"properties\" : { \"foo_bar\" : { \"$ref\" : \"#/$defs/FooBar\" }, \"Gender\" : { \"anyOf\" : [ { \"$ref\" : \"#/$defs/Gender\" }, { \"type\" : \"null\" } ], \"default\" : null }, \"snap\" : { \"default\" : 42 , \"description\" : \"this is the value of snap\" , \"exclusiveMaximum\" : 50 , \"exclusiveMinimum\" : 30 , \"title\" : \"The Snap\" , \"type\" : \"integer\" } }, \"required\" : [ \"foo_bar\" ], \"title\" : \"Main\" , \"type\" : \"object\" } This produces a \"jsonable\" dict of MainModel 's schema. Calling json.dumps on the schema dict produces a JSON string. The  class lets you create an object with methods for validating, serializing,\nand producing JSON schemas for arbitrary types. This serves as a complete replacement for schema_of in\nPydantic V1 (which is now deprecated). Here's an example of generating JSON schema from a : from pydantic import TypeAdapter\n\nadapter = TypeAdapter(list[int])\nprint(adapter.json_schema())\n#> {'items': {'type': 'integer'}, 'type': 'array'} You can also generate JSON schemas for combinations of \nand , as shown in this example: import json\nfrom typing import Union\n\nfrom pydantic import BaseModel, TypeAdapter\n\n\nclass Cat(BaseModel):\n    name: str\n    color: str\n\n\nclass Dog(BaseModel):\n    name: str\n    breed: str\n\n\nta = TypeAdapter(Union[Cat, Dog])\nta_schema = ta.json_schema()\nprint(json.dumps(ta_schema, indent=2)) JSON output: { \"$defs\" : { \"Cat\" : { \"properties\" : { \"name\" : { \"title\" : \"Name\" , \"type\" : \"string\" }, \"color\" : { \"title\" : \"Color\" , \"type\" : \"string\" } }, \"required\" : [ \"name\" , \"color\" ], \"title\" : \"Cat\" , \"type\" : \"object\" }, \"Dog\" : { \"properties\" : { \"name\" : { \"title\" : \"Name\" , \"type\" : \"string\" }, \"breed\" : { \"title\" : \"Breed\" , \"type\" : \"string\" } }, \"required\" : [ \"name\" , \"breed\" ], \"title\" : \"Dog\" , \"type\" : \"object\" } }, \"anyOf\" : [ { \"$ref\" : \"#/$defs/Cat\" }, { \"$ref\" : \"#/$defs/Dog\" } ] }","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#generating-json-schema","title":"JSON Schema - Generating JSON Schema","objectID":"/latest/concepts/json_schema/#generating-json-schema","rank":95},{"content":"Specify the mode of JSON schema generation via the mode parameter in the\n and\n methods. By default, the mode is set to 'validation' , which produces a JSON schema corresponding to the model's validation schema. The  is a type alias that represents the available options for the mode parameter: 'validation' 'serialization' Here's an example of how to specify the mode parameter, and how it affects the generated JSON schema: from decimal import Decimal\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    a: Decimal = Decimal('12.34')\n\n\nprint(Model.model_json_schema(mode='validation'))\n\"\"\"\n{\n    'properties': {\n        'a': {\n            'anyOf': [\n                {'type': 'number'},\n                {\n                    'pattern': '^(?!^[-+.]*$)[+-]?0*\\\\d*\\\\.?\\\\d*$',\n                    'type': 'string',\n                },\n            ],\n            'default': '12.34',\n            'title': 'A',\n        }\n    },\n    'title': 'Model',\n    'type': 'object',\n}\n\"\"\"\n\nprint(Model.model_json_schema(mode='serialization'))\n\"\"\"\n{\n    'properties': {\n        'a': {\n            'default': '12.34',\n            'pattern': '^(?!^[-+.]*$)[+-]?0*\\\\d*\\\\.?\\\\d*$',\n            'title': 'A',\n            'type': 'string',\n        }\n    },\n    'title': 'Model',\n    'type': 'object',\n}\n\"\"\"","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#configuring-the-jsonschemamode","title":"JSON Schema - Generating JSON Schema - Configuring the JsonSchemaMode","objectID":"/latest/concepts/json_schema/#configuring-the-jsonschemamode","rank":90},{"content":"The generated JSON schema can be customized at both the field level and model level via: Field-level customization with the  constructor Model-level customization with At both the field and model levels, you can use the json_schema_extra option to add extra information to the JSON schema.\nThe Using json_schema_extra section below provides more details on this option. For custom types, Pydantic offers other tools for customizing JSON schema generation: WithJsonSchema annotation SkipJsonSchema annotation Implementing __get_pydantic_core_schema__ Implementing __get_pydantic_json_schema__","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#customizing-json-schema","title":"JSON Schema - Customizing JSON Schema","objectID":"/latest/concepts/json_schema/#customizing-json-schema","rank":85},{"content":"Optionally, the  function can be used to provide extra information about the field\nand validations. Some field parameters are used exclusively to customize the generated JSON Schema: title : The title of the field. description : The description of the field. examples : The examples of the field. json_schema_extra : Extra JSON Schema properties to be added to the field. field_title_generator : A function that programmatically sets the field's title, based on its name and info. Here's an example: import json\n\nfrom pydantic import BaseModel, EmailStr, Field, SecretStr\n\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    email: EmailStr = Field(examples=['marcelo@mail.com'])\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\n\nprint(json.dumps(User.model_json_schema(), indent=2)) JSON output: { \"properties\" : { \"age\" : { \"description\" : \"Age of the user\" , \"title\" : \"Age\" , \"type\" : \"integer\" }, \"email\" : { \"examples\" : [ \"marcelo@mail.com\" ], \"format\" : \"email\" , \"title\" : \"Email\" , \"type\" : \"string\" }, \"name\" : { \"title\" : \"Username\" , \"type\" : \"string\" }, \"password\" : { \"description\" : \"Password of the user\" , \"examples\" : [ \"123456\" ], \"format\" : \"password\" , \"title\" : \"Password\" , \"type\" : \"string\" , \"writeOnly\" : true } }, \"required\" : [ \"age\" , \"email\" , \"name\" , \"password\" ], \"title\" : \"User\" , \"type\" : \"object\" } Unenforced Field constraints ¶ If Pydantic finds constraints which are not being enforced, an error will be raised. If you want to force the\nconstraint to appear in the schema, even though it's not being checked upon parsing, you can use variadic arguments\nto  with the raw schema attribute name: from pydantic import BaseModel, Field, PositiveInt\n\ntry:\n    # this won't work since `PositiveInt` takes precedence over the\n    # constraints defined in `Field`, meaning they're ignored\n    class Model(BaseModel):\n        foo: PositiveInt = Field(lt=10)\n\nexcept ValueError as e:\n    print(e)\n\n\n# if you find yourself needing this, an alternative is to declare\n# the constraints in `Field` (or you could use `conint()`)\n# here both constraints will be enforced:\nclass ModelB(BaseModel):\n    # Here both constraints will be applied and the schema\n    # will be generated correctly\n    foo: int = Field(gt=0, lt=10)\n\n\nprint(ModelB.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'foo': {\n            'exclusiveMaximum': 10,\n            'exclusiveMinimum': 0,\n            'title': 'Foo',\n            'type': 'integer',\n        }\n    },\n    'required': ['foo'],\n    'title': 'ModelB',\n    'type': 'object',\n}\n\"\"\" You can specify JSON schema modifications via the  constructor via  as well: import json\nfrom typing import Annotated\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\n\nclass Foo(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n    name: Annotated[str, Field(max_length=256)] = Field(\n        'Bar', title='CustomName'\n    )\n\n\nprint(json.dumps(Foo.model_json_schema(), indent=2)) JSON output: { \"properties\" : { \"id\" : { \"title\" : \"Id\" , \"type\" : \"string\" }, \"name\" : { \"default\" : \"Bar\" , \"maxLength\" : 256 , \"title\" : \"CustomName\" , \"type\" : \"string\" } }, \"title\" : \"Foo\" , \"type\" : \"object\" }","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#field-level-customization","title":"JSON Schema - Customizing JSON Schema - Field-Level Customization","objectID":"/latest/concepts/json_schema/#field-level-customization","rank":80},{"content":"The field_title_generator parameter can be used to programmatically generate the title for a field based on its name and info. See the following example: import json\n\nfrom pydantic import BaseModel, Field\nfrom pydantic.fields import FieldInfo\n\n\ndef make_title(field_name: str, field_info: FieldInfo) -> str:\n    return field_name.upper()\n\n\nclass Person(BaseModel):\n    name: str = Field(field_title_generator=make_title)\n    age: int = Field(field_title_generator=make_title)\n\n\nprint(json.dumps(Person.model_json_schema(), indent=2))\n\"\"\"\n{\n  \"properties\": {\n    \"name\": {\n      \"title\": \"NAME\",\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"title\": \"AGE\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"age\"\n  ],\n  \"title\": \"Person\",\n  \"type\": \"object\"\n}\n\"\"\"","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#programmatic-field-title-generation","title":"JSON Schema - Customizing JSON Schema - Programmatic field title generation","objectID":"/latest/concepts/json_schema/#programmatic-field-title-generation","rank":75},{"content":"You can also use  to customize JSON schema generation on a model.\nSpecifically, the following config options are relevant:","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#model-level-customization","title":"JSON Schema - Customizing JSON Schema - Model-Level Customization","objectID":"/latest/concepts/json_schema/#model-level-customization","rank":70},{"content":"The json_schema_extra option can be used to add extra information to the JSON schema, either at the Field level or at the Model level .\nYou can pass a dict or a Callable to json_schema_extra . Using json_schema_extra with a dict ¶ You can pass a dict to json_schema_extra to add extra information to the JSON schema: import json\n\nfrom pydantic import BaseModel, ConfigDict\n\n\nclass Model(BaseModel):\n    a: str\n\n    model_config = ConfigDict(json_schema_extra={'examples': [{'a': 'Foo'}]})\n\n\nprint(json.dumps(Model.model_json_schema(), indent=2)) JSON output: { \"examples\" : [ { \"a\" : \"Foo\" } ], \"properties\" : { \"a\" : { \"title\" : \"A\" , \"type\" : \"string\" } }, \"required\" : [ \"a\" ], \"title\" : \"Model\" , \"type\" : \"object\" } Using json_schema_extra with a Callable ¶ You can pass a Callable to json_schema_extra to modify the JSON schema with a function: import json\n\nfrom pydantic import BaseModel, Field\n\n\ndef pop_default(s):\n    s.pop('default')\n\n\nclass Model(BaseModel):\n    a: int = Field(default=1, json_schema_extra=pop_default)\n\n\nprint(json.dumps(Model.model_json_schema(), indent=2)) JSON output: { \"properties\" : { \"a\" : { \"title\" : \"A\" , \"type\" : \"integer\" } }, \"title\" : \"Model\" , \"type\" : \"object\" } Merging json_schema_extra ¶ Starting in v2.9, Pydantic merges json_schema_extra dictionaries from annotated types.\nThis pattern offers a more additive approach to merging rather than the previous override behavior.\nThis can be quite helpful for cases of reusing json schema extra information across multiple types. We viewed this change largely as a bug fix, as it resolves unintentional differences in the json_schema_extra merging behavior\nbetween BaseModel and TypeAdapter instances - see this issue for more details. Python 3.9 and above Python 3.10 and above import json\nfrom typing import Annotated\n\nfrom typing_extensions import TypeAlias\n\nfrom pydantic import Field, TypeAdapter\n\nExternalType: TypeAlias = Annotated[\n    int, Field(json_schema_extra={'key1': 'value1'})\n]\n\nta = TypeAdapter(\n    Annotated[ExternalType, Field(json_schema_extra={'key2': 'value2'})]\n)\nprint(json.dumps(ta.json_schema(), indent=2))\n\"\"\"\n{\n  \"key1\": \"value1\",\n  \"key2\": \"value2\",\n  \"type\": \"integer\"\n}\n\"\"\" import json\nfrom typing import Annotated\n\nfrom typing import TypeAlias\n\nfrom pydantic import Field, TypeAdapter\n\nExternalType: TypeAlias = Annotated[\n    int, Field(json_schema_extra={'key1': 'value1'})\n]\n\nta = TypeAdapter(\n    Annotated[ExternalType, Field(json_schema_extra={'key2': 'value2'})]\n)\nprint(json.dumps(ta.json_schema(), indent=2))\n\"\"\"\n{\n  \"key1\": \"value1\",\n  \"key2\": \"value2\",\n  \"type\": \"integer\"\n}\n\"\"\" Note We no longer (and never fully did) support composing a mix of dict and callable type json_schema_extra specifications.\nIf this is a requirement for your use case, please open a pydantic issue and explain your situation - we'd be happy to reconsider this decision when presented with a compelling case.","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#using-json_schema_extra","title":"JSON Schema - Customizing JSON Schema - Using json_schema_extra","objectID":"/latest/concepts/json_schema/#using-json_schema_extra","rank":65},{"content":"Tip Using  is preferred over implementing __get_pydantic_json_schema__ for custom types,\nas it's more simple and less error-prone. The  annotation can be used to override the generated (base)\nJSON schema for a given type without the need to implement __get_pydantic_core_schema__ or __get_pydantic_json_schema__ on the type itself. Note that this overrides the whole JSON Schema generation process\nfor the field (in the following example, the 'type' also needs to be provided). import json\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, WithJsonSchema\n\nMyInt = Annotated[\n    int,\n    WithJsonSchema({'type': 'integer', 'examples': [1, 0, -1]}),\n]\n\n\nclass Model(BaseModel):\n    a: MyInt\n\n\nprint(json.dumps(Model.model_json_schema(), indent=2)) JSON output: { \"properties\" : { \"a\" : { \"examples\" : [ 1 , 0 , -1 ], \"title\" : \"A\" , \"type\" : \"integer\" } }, \"required\" : [ \"a\" ], \"title\" : \"Model\" , \"type\" : \"object\" } Note You might be tempted to use the  annotation\nto fine-tune the JSON Schema of fields having validators attached. Instead, it\nis recommended to use the json_schema_input_type argument .","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#withjsonschema-annotation","title":"JSON Schema - Customizing JSON Schema - WithJsonSchema annotation","objectID":"/latest/concepts/json_schema/#withjsonschema-annotation","rank":60},{"content":"The  annotation can be used to skip an included field (or part of a field's specifications)\nfrom the generated JSON schema. See the API docs for more details.","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#skipjsonschema-annotation","title":"JSON Schema - Customizing JSON Schema - SkipJsonSchema annotation","objectID":"/latest/concepts/json_schema/#skipjsonschema-annotation","rank":55},{"content":"Custom types (used as field_name: TheType or field_name: Annotated[TheType, ...] ) as well as Annotated metadata\n(used as field_name: Annotated[int, SomeMetadata] )\ncan modify or override the generated schema by implementing __get_pydantic_core_schema__ .\nThis method receives two positional arguments: The type annotation that corresponds to this type (so in the case of TheType[T][int] it would be TheType[int] ). A handler/callback to call the next implementer of __get_pydantic_core_schema__ . The handler system works just like wrap field validators .\nIn this case the input is the type and the output is a core_schema . Here is an example of a custom type that overrides the generated core_schema : from dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_core import core_schema\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler\n\n\n@dataclass\nclass CompressedString:\n    dictionary: dict[int, str]\n    text: list[int]\n\n    def build(self) -> str:\n        return ' '.join([self.dictionary[key] for key in self.text])\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: type[Any], handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        assert source is CompressedString\n        return core_schema.no_info_after_validator_function(\n            cls._validate,\n            core_schema.str_schema(),\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                cls._serialize,\n                info_arg=False,\n                return_schema=core_schema.str_schema(),\n            ),\n        )\n\n    @staticmethod\n    def _validate(value: str) -> 'CompressedString':\n        inverse_dictionary: dict[str, int] = {}\n        text: list[int] = []\n        for word in value.split(' '):\n            if word not in inverse_dictionary:\n                inverse_dictionary[word] = len(inverse_dictionary)\n            text.append(inverse_dictionary[word])\n        return CompressedString(\n            {v: k for k, v in inverse_dictionary.items()}, text\n        )\n\n    @staticmethod\n    def _serialize(value: 'CompressedString') -> str:\n        return value.build()\n\n\nclass MyModel(BaseModel):\n    value: CompressedString\n\n\nprint(MyModel.model_json_schema())\n\"\"\"\n{\n    'properties': {'value': {'title': 'Value', 'type': 'string'}},\n    'required': ['value'],\n    'title': 'MyModel',\n    'type': 'object',\n}\n\"\"\"\nprint(MyModel(value='fox fox fox dog fox'))\n\"\"\"\nvalue = CompressedString(dictionary={0: 'fox', 1: 'dog'}, text=[0, 0, 0, 1, 0])\n\"\"\"\n\nprint(MyModel(value='fox fox fox dog fox').model_dump(mode='json'))\n#> {'value': 'fox fox fox dog fox'} Since Pydantic would not know how to generate a schema for CompressedString , if you call handler(source) in its __get_pydantic_core_schema__ method you would get a pydantic.errors.PydanticSchemaGenerationError error.\nThis will be the case for most custom types, so you almost never want to call into handler for custom types. The process for Annotated metadata is much the same except that you can generally call into handler to have\nPydantic handle generating the schema. from collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom typing import Annotated, Any\n\nfrom pydantic_core import core_schema\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler, ValidationError\n\n\n@dataclass\nclass RestrictCharacters:\n    alphabet: Sequence[str]\n\n    def __get_pydantic_core_schema__(\n        self, source: type[Any], handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        if not self.alphabet:\n            raise ValueError('Alphabet may not be empty')\n        schema = handler(\n            source\n        )  # get the CoreSchema from the type / inner constraints\n        if schema['type'] != 'str':\n            raise TypeError('RestrictCharacters can only be applied to strings')\n        return core_schema.no_info_after_validator_function(\n            self.validate,\n            schema,\n        )\n\n    def validate(self, value: str) -> str:\n        if any(c not in self.alphabet for c in value):\n            raise ValueError(\n                f'{value!r} is not restricted to {self.alphabet!r}'\n            )\n        return value\n\n\nclass MyModel(BaseModel):\n    value: Annotated[str, RestrictCharacters('ABC')]\n\n\nprint(MyModel.model_json_schema())\n\"\"\"\n{\n    'properties': {'value': {'title': 'Value', 'type': 'string'}},\n    'required': ['value'],\n    'title': 'MyModel',\n    'type': 'object',\n}\n\"\"\"\nprint(MyModel(value='CBA'))\n#> value='CBA'\n\ntry:\n    MyModel(value='XYZ')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for MyModel\n    value\n      Value error, 'XYZ' is not restricted to 'ABC' [type=value_error, input_value='XYZ', input_type=str]\n    \"\"\" So far we have been wrapping the schema, but if you just want to modify it or ignore it you can as well. To modify the schema, first call the handler, then mutate the result: from typing import Annotated, Any\n\nfrom pydantic_core import ValidationError, core_schema\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler\n\n\nclass SmallString:\n    def __get_pydantic_core_schema__(\n        self,\n        source: type[Any],\n        handler: GetCoreSchemaHandler,\n    ) -> core_schema.CoreSchema:\n        schema = handler(source)\n        assert schema['type'] == 'str'\n        schema['max_length'] = 10  # modify in place\n        return schema\n\n\nclass MyModel(BaseModel):\n    value: Annotated[str, SmallString()]\n\n\ntry:\n    MyModel(value='too long!!!!!')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for MyModel\n    value\n      String should have at most 10 characters [type=string_too_long, input_value='too long!!!!!', input_type=str]\n    \"\"\" Tip Note that you must return a schema, even if you are just mutating it in place. To override the schema completely, do not call the handler and return your own CoreSchema : from typing import Annotated, Any\n\nfrom pydantic_core import ValidationError, core_schema\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler\n\n\nclass AllowAnySubclass:\n    def __get_pydantic_core_schema__(\n        self, source: type[Any], handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        # we can't call handler since it will fail for arbitrary types\n        def validate(value: Any) -> Any:\n            if not isinstance(value, source):\n                raise ValueError(\n                    f'Expected an instance of {source}, got an instance of {type(value)}'\n                )\n\n        return core_schema.no_info_plain_validator_function(validate)\n\n\nclass Foo:\n    pass\n\n\nclass Model(BaseModel):\n    f: Annotated[Foo, AllowAnySubclass()]\n\n\nprint(Model(f=Foo()))\n#> f=None\n\n\nclass NotFoo:\n    pass\n\n\ntry:\n    Model(f=NotFoo())\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    f\n      Value error, Expected an instance of , got an instance of [type=value_error, input_value=<__main__.NotFoo object at 0x0123456789ab>, input_type=NotFoo]\n    \"\"\"","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#implementing-__get_pydantic_core_schema__","title":"JSON Schema - Customizing JSON Schema - Implementing __get_pydantic_core_schema__","objectID":"/latest/concepts/json_schema/#implementing-__get_pydantic_core_schema__","rank":50},{"content":"You can also implement __get_pydantic_json_schema__ to modify or override the generated json schema.\nModifying this method only affects the JSON schema - it doesn't affect the core schema, which is used for validation and serialization. Here's an example of modifying the generated JSON schema: import json\nfrom typing import Any\n\nfrom pydantic_core import core_schema as cs\n\nfrom pydantic import GetCoreSchemaHandler, GetJsonSchemaHandler, TypeAdapter\nfrom pydantic.json_schema import JsonSchemaValue\n\n\nclass Person:\n    name: str\n    age: int\n\n    def __init__(self, name: str, age: int):\n        self.name = name\n        self.age = age\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source_type: Any, handler: GetCoreSchemaHandler\n    ) -> cs.CoreSchema:\n        return cs.typed_dict_schema(\n            {\n                'name': cs.typed_dict_field(cs.str_schema()),\n                'age': cs.typed_dict_field(cs.int_schema()),\n            },\n        )\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: cs.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)\n        json_schema = handler.resolve_ref_schema(json_schema)\n        json_schema['examples'] = [\n            {\n                'name': 'John Doe',\n                'age': 25,\n            }\n        ]\n        json_schema['title'] = 'Person'\n        return json_schema\n\n\nprint(json.dumps(TypeAdapter(Person).json_schema(), indent=2)) JSON output: { \"examples\" : [ { \"age\" : 25 , \"name\" : \"John Doe\" } ], \"properties\" : { \"name\" : { \"title\" : \"Name\" , \"type\" : \"string\" }, \"age\" : { \"title\" : \"Age\" , \"type\" : \"integer\" } }, \"required\" : [ \"name\" , \"age\" ], \"title\" : \"Person\" , \"type\" : \"object\" }","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#implementing-__get_pydantic_json_schema__","title":"JSON Schema - Customizing JSON Schema - Implementing __get_pydantic_json_schema__","objectID":"/latest/concepts/json_schema/#implementing-__get_pydantic_json_schema__","rank":45},{"content":"The field_title_generator parameter can be used to programmatically generate the title for a field based on its name and info.\nThis is similar to the field level field_title_generator , but the ConfigDict option will be applied to all fields of the class. See the following example: import json\n\nfrom pydantic import BaseModel, ConfigDict\n\n\nclass Person(BaseModel):\n    model_config = ConfigDict(\n        field_title_generator=lambda field_name, field_info: field_name.upper()\n    )\n    name: str\n    age: int\n\n\nprint(json.dumps(Person.model_json_schema(), indent=2))\n\"\"\"\n{\n  \"properties\": {\n    \"name\": {\n      \"title\": \"NAME\",\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"title\": \"AGE\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"age\"\n  ],\n  \"title\": \"Person\",\n  \"type\": \"object\"\n}\n\"\"\"","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#using-field_title_generator","title":"JSON Schema - Customizing JSON Schema - Using field_title_generator","objectID":"/latest/concepts/json_schema/#using-field_title_generator","rank":40},{"content":"The model_title_generator config option is similar to the field_title_generator option, but it applies to the title of the model itself,\nand accepts the model class as input. See the following example: import json\n\nfrom pydantic import BaseModel, ConfigDict\n\n\ndef make_title(model: type) -> str:\n    return f'Title-{model.__name__}'\n\n\nclass Person(BaseModel):\n    model_config = ConfigDict(model_title_generator=make_title)\n    name: str\n    age: int\n\n\nprint(json.dumps(Person.model_json_schema(), indent=2))\n\"\"\"\n{\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"title\": \"Age\",\n      \"type\": \"integer\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"age\"\n  ],\n  \"title\": \"Title-Person\",\n  \"type\": \"object\"\n}\n\"\"\"","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#using-model_title_generator","title":"JSON Schema - Customizing JSON Schema - Using model_title_generator","objectID":"/latest/concepts/json_schema/#using-model_title_generator","rank":35},{"content":"Types, custom field types, and constraints (like max_length ) are mapped to the corresponding spec formats in the\nfollowing priority order (when there is an equivalent available): JSON Schema Core JSON Schema Validation OpenAPI Data Types The standard format JSON field is used to define Pydantic extensions for more complex string sub-types. The field schema mapping from Python or Pydantic to JSON schema is done as follows: {{ schema_mappings_table }}","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#json-schema-types","title":"JSON Schema - JSON schema types","objectID":"/latest/concepts/json_schema/#json-schema-types","rank":30},{"content":"You can also generate a top-level JSON schema that only includes a list of models and related\nsub-models in its $defs : import json\n\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import models_json_schema\n\n\nclass Foo(BaseModel):\n    a: str = None\n\n\nclass Model(BaseModel):\n    b: Foo\n\n\nclass Bar(BaseModel):\n    c: int\n\n\n_, top_level_schema = models_json_schema(\n    [(Model, 'validation'), (Bar, 'validation')], title='My Schema'\n)\nprint(json.dumps(top_level_schema, indent=2)) JSON output: { \"$defs\" : { \"Bar\" : { \"properties\" : { \"c\" : { \"title\" : \"C\" , \"type\" : \"integer\" } }, \"required\" : [ \"c\" ], \"title\" : \"Bar\" , \"type\" : \"object\" }, \"Foo\" : { \"properties\" : { \"a\" : { \"default\" : null , \"title\" : \"A\" , \"type\" : \"string\" } }, \"title\" : \"Foo\" , \"type\" : \"object\" }, \"Model\" : { \"properties\" : { \"b\" : { \"$ref\" : \"#/$defs/Foo\" } }, \"required\" : [ \"b\" ], \"title\" : \"Model\" , \"type\" : \"object\" } }, \"title\" : \"My Schema\" }","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#top-level-schema-generation","title":"JSON Schema - Top-level schema generation","objectID":"/latest/concepts/json_schema/#top-level-schema-generation","rank":25},{"content":"If you need custom schema generation, you can use a schema_generator , modifying the\n class as necessary for your application. The various methods that can be used to produce JSON schema accept a keyword argument schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema , and you can pass your custom subclass to these methods in order to use your own approach to generating JSON schema. GenerateJsonSchema implements the translation of a type's pydantic-core schema into a JSON schema.\nBy design, this class breaks the JSON schema generation process into smaller methods that can be easily overridden in\nsubclasses to modify the \"global\" approach to generating JSON schema. from pydantic import BaseModel\nfrom pydantic.json_schema import GenerateJsonSchema\n\n\nclass MyGenerateJsonSchema(GenerateJsonSchema):\n    def generate(self, schema, mode='validation'):\n        json_schema = super().generate(schema, mode=mode)\n        json_schema['title'] = 'Customize title'\n        json_schema['$schema'] = self.schema_dialect\n        return json_schema\n\n\nclass MyModel(BaseModel):\n    x: int\n\n\nprint(MyModel.model_json_schema(schema_generator=MyGenerateJsonSchema))\n\"\"\"\n{\n    'properties': {'x': {'title': 'X', 'type': 'integer'}},\n    'required': ['x'],\n    'title': 'Customize title',\n    'type': 'object',\n    '$schema': 'https://json-schema.org/draft/2020-12/schema',\n}\n\"\"\" Below is an approach you can use to exclude any fields from the schema that don't have valid json schemas: Python 3.9 and above Python 3.10 and above from typing import Callable\n\nfrom pydantic_core import PydanticOmit, core_schema\n\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import GenerateJsonSchema, JsonSchemaValue\n\n\nclass MyGenerateJsonSchema(GenerateJsonSchema):\n    def handle_invalid_for_json_schema(\n        self, schema: core_schema.CoreSchema, error_info: str\n    ) -> JsonSchemaValue:\n        raise PydanticOmit\n\n\ndef example_callable():\n    return 1\n\n\nclass Example(BaseModel):\n    name: str = 'example'\n    function: Callable = example_callable\n\n\ninstance_example = Example()\n\nvalidation_schema = instance_example.model_json_schema(\n    schema_generator=MyGenerateJsonSchema, mode='validation'\n)\nprint(validation_schema)\n\"\"\"\n{\n    'properties': {\n        'name': {'default': 'example', 'title': 'Name', 'type': 'string'}\n    },\n    'title': 'Example',\n    'type': 'object',\n}\n\"\"\" from collections.abc import Callable\n\nfrom pydantic_core import PydanticOmit, core_schema\n\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import GenerateJsonSchema, JsonSchemaValue\n\n\nclass MyGenerateJsonSchema(GenerateJsonSchema):\n    def handle_invalid_for_json_schema(\n        self, schema: core_schema.CoreSchema, error_info: str\n    ) -> JsonSchemaValue:\n        raise PydanticOmit\n\n\ndef example_callable():\n    return 1\n\n\nclass Example(BaseModel):\n    name: str = 'example'\n    function: Callable = example_callable\n\n\ninstance_example = Example()\n\nvalidation_schema = instance_example.model_json_schema(\n    schema_generator=MyGenerateJsonSchema, mode='validation'\n)\nprint(validation_schema)\n\"\"\"\n{\n    'properties': {\n        'name': {'default': 'example', 'title': 'Name', 'type': 'string'}\n    },\n    'title': 'Example',\n    'type': 'object',\n}\n\"\"\"","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#customizing-the-json-schema-generation-process","title":"JSON Schema - Customizing the JSON Schema Generation Process","objectID":"/latest/concepts/json_schema/#customizing-the-json-schema-generation-process","rank":20},{"content":"By default, Pydantic recursively sorts JSON schemas by alphabetically sorting keys. Notably, Pydantic skips sorting the values of the properties key,\nto preserve the order of the fields as they were defined in the model. If you would like to customize this behavior, you can override the sort method in your custom GenerateJsonSchema subclass. The below example\nuses a no-op sort method to disable sorting entirely, which is reflected in the preserved order of the model fields and json_schema_extra keys: Python 3.9 and above Python 3.10 and above import json\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\nfrom pydantic.json_schema import GenerateJsonSchema, JsonSchemaValue\n\n\nclass MyGenerateJsonSchema(GenerateJsonSchema):\n    def sort(\n        self, value: JsonSchemaValue, parent_key: Optional[str] = None\n    ) -> JsonSchemaValue:\n        \"\"\"No-op, we don't want to sort schema values at all.\"\"\"\n        return value\n\n\nclass Bar(BaseModel):\n    c: str\n    b: str\n    a: str = Field(json_schema_extra={'c': 'hi', 'b': 'hello', 'a': 'world'})\n\n\njson_schema = Bar.model_json_schema(schema_generator=MyGenerateJsonSchema)\nprint(json.dumps(json_schema, indent=2))\n\"\"\"\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"c\": {\n      \"type\": \"string\",\n      \"title\": \"C\"\n    },\n    \"b\": {\n      \"type\": \"string\",\n      \"title\": \"B\"\n    },\n    \"a\": {\n      \"type\": \"string\",\n      \"c\": \"hi\",\n      \"b\": \"hello\",\n      \"a\": \"world\",\n      \"title\": \"A\"\n    }\n  },\n  \"required\": [\n    \"c\",\n    \"b\",\n    \"a\"\n  ],\n  \"title\": \"Bar\"\n}\n\"\"\" import json\n\nfrom pydantic import BaseModel, Field\nfrom pydantic.json_schema import GenerateJsonSchema, JsonSchemaValue\n\n\nclass MyGenerateJsonSchema(GenerateJsonSchema):\n    def sort(\n        self, value: JsonSchemaValue, parent_key: str | None = None\n    ) -> JsonSchemaValue:\n        \"\"\"No-op, we don't want to sort schema values at all.\"\"\"\n        return value\n\n\nclass Bar(BaseModel):\n    c: str\n    b: str\n    a: str = Field(json_schema_extra={'c': 'hi', 'b': 'hello', 'a': 'world'})\n\n\njson_schema = Bar.model_json_schema(schema_generator=MyGenerateJsonSchema)\nprint(json.dumps(json_schema, indent=2))\n\"\"\"\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"c\": {\n      \"type\": \"string\",\n      \"title\": \"C\"\n    },\n    \"b\": {\n      \"type\": \"string\",\n      \"title\": \"B\"\n    },\n    \"a\": {\n      \"type\": \"string\",\n      \"c\": \"hi\",\n      \"b\": \"hello\",\n      \"a\": \"world\",\n      \"title\": \"A\"\n    }\n  },\n  \"required\": [\n    \"c\",\n    \"b\",\n    \"a\"\n  ],\n  \"title\": \"Bar\"\n}\n\"\"\"","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#json-schema-sorting","title":"JSON Schema - Customizing the JSON Schema Generation Process - JSON schema sorting","objectID":"/latest/concepts/json_schema/#json-schema-sorting","rank":15},{"content":"The format of $ref s can be altered by calling \nor  with the ref_template keyword argument.\nThe definitions are always stored under the key $defs , but a specified prefix can be used for the references. This is useful if you need to extend or modify the JSON schema default definitions location. For example, with OpenAPI: import json\n\nfrom pydantic import BaseModel\nfrom pydantic.type_adapter import TypeAdapter\n\n\nclass Foo(BaseModel):\n    a: int\n\n\nclass Model(BaseModel):\n    a: Foo\n\n\nadapter = TypeAdapter(Model)\n\nprint(\n    json.dumps(\n        adapter.json_schema(ref_template='#/components/schemas/{model}'),\n        indent=2,\n    )\n) JSON output: { \"$defs\" : { \"Foo\" : { \"properties\" : { \"a\" : { \"title\" : \"A\" , \"type\" : \"integer\" } }, \"required\" : [ \"a\" ], \"title\" : \"Foo\" , \"type\" : \"object\" } }, \"properties\" : { \"a\" : { \"$ref\" : \"#/components/schemas/Foo\" } }, \"required\" : [ \"a\" ], \"title\" : \"Model\" , \"type\" : \"object\" }","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#customizing-the-refs-in-json-schema","title":"JSON Schema - Customizing the $refs in JSON Schema","objectID":"/latest/concepts/json_schema/#customizing-the-refs-in-json-schema","rank":10},{"content":"The JSON schema for Optional fields indicates that the value null is allowed. The Decimal type is exposed in JSON schema (and serialized) as a string. Since the namedtuple type doesn't exist in JSON, a model's JSON schema does not preserve namedtuple s as namedtuple s. Sub-models used are added to the $defs JSON attribute and referenced, as per the spec. Sub-models with modifications (via the Field class) like a custom title, description, or default value,\n    are recursively included instead of referenced. The description for models is taken from either the docstring of the class or the argument description to\n    the Field class. The schema is generated by default using aliases as keys, but it can be generated using model\n    property names instead by calling  or\n     with the by_alias=False keyword argument.","pageID":"JSON Schema","abs_url":"/latest/concepts/json_schema/#miscellaneous-notes-on-json-schema-generation","title":"JSON Schema - Miscellaneous Notes on JSON Schema Generation","objectID":"/latest/concepts/json_schema/#miscellaneous-notes-on-json-schema-generation","rank":5},{"content":"One of the primary ways of defining schema in Pydantic is via models. Models are simply classes which inherit from\n and define fields as annotated attributes. You can think of models as similar to structs in languages like C, or as the requirements of a single endpoint\nin an API. Models share many similarities with Python's , but have been designed with some subtle-yet-important\ndifferences that streamline certain workflows related to validation, serialization, and JSON schema generation.\nYou can find more discussion of this in the Dataclasses section of the docs. Untrusted data can be passed to a model and, after parsing and validation, Pydantic guarantees that the fields\nof the resultant model instance will conform to the field types defined on the model. Validation — a deliberate misnomer TL;DR We use the term \"validation\" to refer to the process of instantiating a model (or other type) that adheres to specified types and\nconstraints. This task, which Pydantic is well known for, is most widely recognized as \"validation\" in colloquial terms,\neven though in other contexts the term \"validation\" may be more restrictive. The long version The potential confusion around the term \"validation\" arises from the fact that, strictly speaking, Pydantic's\nprimary focus doesn't align precisely with the dictionary definition of \"validation\": validation noun the action of checking or proving the validity or accuracy of something. In Pydantic, the term \"validation\" refers to the process of instantiating a model (or other type) that adheres to specified\ntypes and constraints. Pydantic guarantees the types and constraints of the output, not the input data.\nThis distinction becomes apparent when considering that Pydantic's ValidationError is raised\nwhen data cannot be successfully parsed into a model instance. While this distinction may initially seem subtle, it holds practical significance.\nIn some cases, \"validation\" goes beyond just model creation, and can include the copying and coercion of data.\nThis can involve copying arguments passed to the constructor in order to perform coercion to a new type\nwithout mutating the original input data. For a more in-depth understanding of the implications for your usage,\nrefer to the Data Conversion and Attribute Copies sections below. In essence, Pydantic's primary goal is to assure that the resulting structure post-processing (termed \"validation\")\nprecisely conforms to the applied type hints. Given the widespread adoption of \"validation\" as the colloquial term\nfor this process, we will consistently use it in our documentation. While the terms \"parse\" and \"validation\" were previously used interchangeably, moving forward, we aim to exclusively employ \"validate\",\nwith \"parse\" reserved specifically for discussions related to JSON parsing .","pageID":"Models","abs_url":"/latest/concepts/models/#Models","title":"Models","objectID":"/latest/concepts/models/#Models","rank":100},{"content":"","pageID":"Models","abs_url":"/latest/concepts/models/","title":"Models - Models - TL;DR","objectID":"/latest/concepts/models/","rank":95},{"content":"","pageID":"Models","abs_url":"/latest/concepts/models/","title":"Models - Models - The long version","objectID":"/latest/concepts/models/","rank":90},{"content":"noun","pageID":"Models","abs_url":"/latest/concepts/models/","title":"Models - Models - validation","objectID":"/latest/concepts/models/","rank":85},{"content":"Note Pydantic relies heavily on the existing Python typing constructs to define models. If you are not familiar with those, the following resources\ncan be useful: The Type System Guides The mypy documentation from pydantic import BaseModel, ConfigDict\n\n\nclass User(BaseModel):\n    id: int\n    name: str = 'Jane Doe'\n\n    model_config = ConfigDict(str_max_length=10)  # (1)! Pydantic models support a variety of configuration values (see  for the available configuration values). In this example, User is a model with two fields: id , which is an integer and is required name , which is a string and is not required (it has a default value). Fields can be customized in a number of ways using the  function.\nSee the documentation on fields for more information. The model can then be instantiated: user = User(id='123') user is an instance of User . Initialization of the object will perform all parsing and validation.\nIf no  exception is raised,\nyou know the resulting model instance is valid. Fields of a model can be accessed as normal attributes of the user object: assert user.name == 'Jane Doe'  # (1)!\nassert user.id == 123  # (2)!\nassert isinstance(user.id, int) name wasn't set when user was initialized, so the default value was used.\n   The  attribute can be\n   inspected to check the field names explicitly set during instantiation. Note that the string '123' was coerced to an integer and its value is 123 .\n   More details on Pydantic's coercion logic can be found in the data conversion section. The model instance can be serialized using the  method: assert user.model_dump() == {'id': 123, 'name': 'Jane Doe'} Calling  on the instance will also provide a dictionary, but nested fields will not be\nrecursively converted into dictionaries.  also\nprovides numerous arguments to customize the serialization result. By default, models are mutable and field values can be changed through attribute assignment: user.id = 321\nassert user.id == 321 Warning When defining your models, watch out for naming collisions between your field name and its type annotation. For example, the following will not behave as expected and would yield a validation error: from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Boo(BaseModel):\n    int: Optional[int] = None\n\n\nm = Boo(int=123)  # Will fail to validate. Because of how Python evaluates , the statement is equivalent to int: None = None , thus\nleading to a validation error.","pageID":"Models","abs_url":"/latest/concepts/models/#basic-model-usage","title":"Models - Basic model usage","objectID":"/latest/concepts/models/#basic-model-usage","rank":80},{"content":"The example above only shows the tip of the iceberg of what models can do.\nModels possess the following methods and attributes: : Validates the given object against the Pydantic model. See Validating data . : Validates the given JSON data against the Pydantic model. See Validating data . : Creates models without running validation. See Creating models without validation . : Returns a dictionary of the model's fields and values. See Serialization . : Returns a JSON string representation of . See Serialization . : Returns a copy (by default, shallow copy) of the model. See Model copy . : Returns a jsonable dictionary representing the model's JSON Schema. See JSON Schema . : A mapping between field names and their definitions ( instances). : A mapping between computed field names and their definitions ( instances). : The extra fields set during validation. : The set of fields which were explicitly provided when the model was initialized. : Computes the class name for parametrizations of generic classes. : Performs additional actions after the model is instantiated and all field validators are applied. : Rebuilds the model schema, which also supports building recursive generic models.\n    See Rebuilding model schema . Note See the API documentation of  for the class definition including a full list of methods and attributes. Tip See Changes to pydantic.BaseModel in the Migration Guide for details on changes from Pydantic V1.","pageID":"Models","abs_url":"/latest/concepts/models/#model-methods-and-properties","title":"Models - Basic model usage - Model methods and properties","objectID":"/latest/concepts/models/#model-methods-and-properties","rank":75},{"content":"Pydantic may cast input data to force it to conform to model field types,\nand in some cases this may result in a loss of information.\nFor example: from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    a: int\n    b: float\n    c: str\n\n\nprint(Model(a=3.000, b='2.72', c=b'binary data').model_dump())\n#> {'a': 3, 'b': 2.72, 'c': 'binary data'} This is a deliberate decision of Pydantic, and is frequently the most useful approach. See here for a longer discussion on the subject. Nevertheless, Pydantic provides a strict mode , where no data conversion is performed.\nValues must be of the same type as the declared field type. This is also the case for collections. In most cases, you shouldn't make use of abstract container classes\nand just use a concrete type, such as : from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    items: list[int]  # (1)!\n\n\nprint(Model(items=(1, 2, 3)))\n#> items=[1, 2, 3] In this case, you might be tempted to use the abstract  type\n   to allow both lists and tuples. But Pydantic takes care of converting the tuple input to a list, so\n   in most cases this isn't necessary. Besides, using these abstract types can also lead to poor validation performance , and in general using concrete container types\nwill avoid unnecessary checks.","pageID":"Models","abs_url":"/latest/concepts/models/#data-conversion","title":"Models - Data conversion","objectID":"/latest/concepts/models/#data-conversion","rank":70},{"content":"By default, Pydantic models won't error when you provide extra data , and these values will simply be ignored: from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    x: int\n\n\nm = Model(x=1, y='a')\nassert m.model_dump() == {'x': 1} The  configuration value can be used to control this behavior: from pydantic import BaseModel, ConfigDict\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\nm = Model(x=1, y='a')  # (1)!\nassert m.model_dump() == {'x': 1, 'y': 'a'}\nassert m.__pydantic_extra__ == {'y': 'a'} If  was set to 'forbid' , this would fail. The configuration can take three values: 'ignore' : Providing extra data is ignored (the default). 'forbid' : Providing extra data is not permitted. 'allow' : Providing extra data is allowed and stored in the __pydantic_extra__ dictionary attribute.\n  The __pydantic_extra__ can explicitly be annotated to provide validation for extra fields. For more details, refer to the  API documentation. Pydantic dataclasses also support extra data (see the dataclass configuration section).","pageID":"Models","abs_url":"/latest/concepts/models/#extra-data","title":"Models - Extra data","objectID":"/latest/concepts/models/#extra-data","rank":65},{"content":"More complex hierarchical data structures can be defined using models themselves as types in annotations. Python 3.9 and above Python 3.10 and above from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    count: int\n    size: Optional[float] = None\n\n\nclass Bar(BaseModel):\n    apple: str = 'x'\n    banana: str = 'y'\n\n\nclass Spam(BaseModel):\n    foo: Foo\n    bars: list[Bar]\n\n\nm = Spam(foo={'count': 4}, bars=[{'apple': 'x1'}, {'apple': 'x2'}])\nprint(m)\n\"\"\"\nfoo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), Bar(apple='x2', banana='y')]\n\"\"\"\nprint(m.model_dump())\n\"\"\"\n{\n    'foo': {'count': 4, 'size': None},\n    'bars': [{'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}],\n}\n\"\"\" from pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    count: int\n    size: float | None = None\n\n\nclass Bar(BaseModel):\n    apple: str = 'x'\n    banana: str = 'y'\n\n\nclass Spam(BaseModel):\n    foo: Foo\n    bars: list[Bar]\n\n\nm = Spam(foo={'count': 4}, bars=[{'apple': 'x1'}, {'apple': 'x2'}])\nprint(m)\n\"\"\"\nfoo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), Bar(apple='x2', banana='y')]\n\"\"\"\nprint(m.model_dump())\n\"\"\"\n{\n    'foo': {'count': 4, 'size': None},\n    'bars': [{'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}],\n}\n\"\"\" Self-referencing models are supported. For more details, see  the documentation related to forward annotations .","pageID":"Models","abs_url":"/latest/concepts/models/#nested-models","title":"Models - Nested models","objectID":"/latest/concepts/models/#nested-models","rank":60},{"content":"When you define a model class in your code, Pydantic will analyze the body of the class to collect a variety of information\nrequired to perform validation and serialization, gathered in a core schema. Notably, the model's type annotations are evaluated to\nunderstand the valid types for each field (more information can be found in the Architecture documentation).\nHowever, it might be the case that annotations refer to symbols not defined when the model class is being created.\nTo circumvent this issue, the  method can be used: from pydantic import BaseModel, PydanticUserError\n\n\nclass Foo(BaseModel):\n    x: 'Bar'  # (1)!\n\n\ntry:\n    Foo.model_json_schema()\nexcept PydanticUserError as e:\n    print(e)\n    \"\"\"\n    `Foo` is not fully defined; you should define `Bar`, then call `Foo.model_rebuild()`.\n\n    For further information visit https://errors.pydantic.dev/2/u/class-not-fully-defined\n    \"\"\"\n\n\nclass Bar(BaseModel):\n    pass\n\n\nFoo.model_rebuild()\nprint(Foo.model_json_schema())\n\"\"\"\n{\n    '$defs': {'Bar': {'properties': {}, 'title': 'Bar', 'type': 'object'}},\n    'properties': {'x': {'$ref': '#/$defs/Bar'}},\n    'required': ['x'],\n    'title': 'Foo',\n    'type': 'object',\n}\n\"\"\" Bar is not yet defined when the Foo class is being created. For this reason,\n    a forward annotation is being used. Pydantic tries to determine when this is necessary automatically and error if it wasn't done, but you may want to\ncall  proactively when dealing with recursive models or generics. In V2,  replaced update_forward_refs() from V1. There are some slight differences with the new behavior.\nThe biggest change is that when calling  on the outermost model, it builds a core schema used for validation of the\nwhole model (nested models and all), so all types at all levels need to be ready before  is called.","pageID":"Models","abs_url":"/latest/concepts/models/#rebuilding-model-schema","title":"Models - Rebuilding model schema","objectID":"/latest/concepts/models/#rebuilding-model-schema","rank":55},{"content":"(Formerly known as \"ORM Mode\"/ from_orm ). Pydantic models can also be created from arbitrary class instances by reading the instance attributes corresponding\nto the model field names. One common application of this functionality is integration with object-relational mappings\n(ORMs). To do this, set the  config value to True (see the documentation on Configuration for more details). The example here uses SQLAlchemy , but the same approach should work for any ORM. from typing import Annotated\n\nfrom sqlalchemy import ARRAY, String\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n\nfrom pydantic import BaseModel, ConfigDict, StringConstraints\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass CompanyOrm(Base):\n    __tablename__ = 'companies'\n\n    id: Mapped[int] = mapped_column(primary_key=True, nullable=False)\n    public_key: Mapped[str] = mapped_column(\n        String(20), index=True, nullable=False, unique=True\n    )\n    domains: Mapped[list[str]] = mapped_column(ARRAY(String(255)))\n\n\nclass CompanyModel(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n\n    id: int\n    public_key: Annotated[str, StringConstraints(max_length=20)]\n    domains: list[Annotated[str, StringConstraints(max_length=255)]]\n\n\nco_orm = CompanyOrm(\n    id=123,\n    public_key='foobar',\n    domains=['example.com', 'foobar.com'],\n)\nprint(co_orm)\n#> <__main__.CompanyOrm object at 0x0123456789ab>\nco_model = CompanyModel.model_validate(co_orm)\nprint(co_model)\n#> id=123 public_key='foobar' domains=['example.com', 'foobar.com']","pageID":"Models","abs_url":"/latest/concepts/models/#arbitrary-class-instances","title":"Models - Arbitrary class instances","objectID":"/latest/concepts/models/#arbitrary-class-instances","rank":50},{"content":"When using attributes to parse models, model instances will be created from both top-level attributes and\ndeeper-nested attributes as appropriate. Here is an example demonstrating the principle: from pydantic import BaseModel, ConfigDict\n\n\nclass PetCls:\n    def __init__(self, *, name: str, species: str):\n        self.name = name\n        self.species = species\n\n\nclass PersonCls:\n    def __init__(self, *, name: str, age: float = None, pets: list[PetCls]):\n        self.name = name\n        self.age = age\n        self.pets = pets\n\n\nclass Pet(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n\n    name: str\n    species: str\n\n\nclass Person(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n\n    name: str\n    age: float = None\n    pets: list[Pet]\n\n\nbones = PetCls(name='Bones', species='dog')\norion = PetCls(name='Orion', species='cat')\nanna = PersonCls(name='Anna', age=20, pets=[bones, orion])\nanna_model = Person.model_validate(anna)\nprint(anna_model)\n\"\"\"\nname='Anna' age=20.0 pets=[Pet(name='Bones', species='dog'), Pet(name='Orion', species='cat')]\n\"\"\"","pageID":"Models","abs_url":"/latest/concepts/models/#nested-attributes","title":"Models - Arbitrary class instances - Nested attributes","objectID":"/latest/concepts/models/#nested-attributes","rank":45},{"content":"Pydantic will raise a  exception whenever it finds an error in the data it's validating. A single exception will be raised regardless of the number of errors found, and that validation error\nwill contain information about all of the errors and how they happened. See Error Handling for details on standard and custom errors. As a demonstration: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    list_of_ints: list[int]\n    a_float: float\n\n\ndata = dict(\n    list_of_ints=['1', 2, 'bad'],\n    a_float='not a float',\n)\n\ntry:\n    Model(**data)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for Model\n    list_of_ints.2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bad', input_type=str]\n    a_float\n      Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='not a float', input_type=str]\n    \"\"\"","pageID":"Models","abs_url":"/latest/concepts/models/#error-handling","title":"Models - Error handling","objectID":"/latest/concepts/models/#error-handling","rank":40},{"content":"Pydantic provides three methods on models classes for parsing data: : this is very similar to the __init__ method of the model,\n  except it takes a dictionary or an object rather than keyword arguments. If the object passed cannot be validated,\n  or if it's not a dictionary or instance of the model in question, a  will be raised. : this validates the provided data as a JSON string or bytes object.\n  If your incoming data is a JSON payload, this is generally considered faster (instead of manually parsing the data as a dictionary).\n  Learn more about JSON parsing in the JSON section of the docs. : this takes a dictionary (can be nested) with string keys and values and validates the data in JSON mode so that said strings can be coerced into the correct types. Python 3.9 and above Python 3.10 and above from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass User(BaseModel):\n    id: int\n    name: str = 'John Doe'\n    signup_ts: Optional[datetime] = None\n\n\nm = User.model_validate({'id': 123, 'name': 'James'})\nprint(m)\n#> id=123 name='James' signup_ts=None\n\ntry:\n    User.model_validate(['not', 'a', 'dict'])\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n      Input should be a valid dictionary or instance of User [type=model_type, input_value=['not', 'a', 'dict'], input_type=list]\n    \"\"\"\n\nm = User.model_validate_json('{\"id\": 123, \"name\": \"James\"}')\nprint(m)\n#> id=123 name='James' signup_ts=None\n\ntry:\n    m = User.model_validate_json('{\"id\": 123, \"name\": 123}')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    name\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    \"\"\"\n\ntry:\n    m = User.model_validate_json('invalid JSON')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n      Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='invalid JSON', input_type=str]\n    \"\"\"\n\nm = User.model_validate_strings({'id': '123', 'name': 'James'})\nprint(m)\n#> id=123 name='James' signup_ts=None\n\nm = User.model_validate_strings(\n    {'id': '123', 'name': 'James', 'signup_ts': '2024-04-01T12:00:00'}\n)\nprint(m)\n#> id=123 name='James' signup_ts=datetime.datetime(2024, 4, 1, 12, 0)\n\ntry:\n    m = User.model_validate_strings(\n        {'id': '123', 'name': 'James', 'signup_ts': '2024-04-01'}, strict=True\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    signup_ts\n      Input should be a valid datetime, invalid datetime separator, expected `T`, `t`, `_` or space [type=datetime_parsing, input_value='2024-04-01', input_type=str]\n    \"\"\" from datetime import datetime\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass User(BaseModel):\n    id: int\n    name: str = 'John Doe'\n    signup_ts: datetime | None = None\n\n\nm = User.model_validate({'id': 123, 'name': 'James'})\nprint(m)\n#> id=123 name='James' signup_ts=None\n\ntry:\n    User.model_validate(['not', 'a', 'dict'])\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n      Input should be a valid dictionary or instance of User [type=model_type, input_value=['not', 'a', 'dict'], input_type=list]\n    \"\"\"\n\nm = User.model_validate_json('{\"id\": 123, \"name\": \"James\"}')\nprint(m)\n#> id=123 name='James' signup_ts=None\n\ntry:\n    m = User.model_validate_json('{\"id\": 123, \"name\": 123}')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    name\n      Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    \"\"\"\n\ntry:\n    m = User.model_validate_json('invalid JSON')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n      Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='invalid JSON', input_type=str]\n    \"\"\"\n\nm = User.model_validate_strings({'id': '123', 'name': 'James'})\nprint(m)\n#> id=123 name='James' signup_ts=None\n\nm = User.model_validate_strings(\n    {'id': '123', 'name': 'James', 'signup_ts': '2024-04-01T12:00:00'}\n)\nprint(m)\n#> id=123 name='James' signup_ts=datetime.datetime(2024, 4, 1, 12, 0)\n\ntry:\n    m = User.model_validate_strings(\n        {'id': '123', 'name': 'James', 'signup_ts': '2024-04-01'}, strict=True\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    signup_ts\n      Input should be a valid datetime, invalid datetime separator, expected `T`, `t`, `_` or space [type=datetime_parsing, input_value='2024-04-01', input_type=str]\n    \"\"\" If you want to validate serialized data in a format other than JSON, you should load the data into a dictionary yourself and\nthen pass it to . Note Depending on the types and model configs involved, \nand  may have different validation behavior.\nIf you have data coming from a non-JSON source, but want the same validation\nbehavior and errors you'd get from ,\nour recommendation for now is to use either use model_validate_json(json.dumps(data)) , or use  if the data takes the form of a (potentially nested) dictionary with string keys and values. Note If you're passing in an instance of a model to , you will want to consider setting\n in the model's config.\nIf you don't set this value, then validation will be skipped on model instances. See the below example: revalidate_instances='never' revalidate_instances='always' from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    a: int\n\n\nm = Model(a=0)\n# note: setting `validate_assignment` to `True` in the config can prevent this kind of misbehavior.\nm.a = 'not an int'\n\n# doesn't raise a validation error even though m is invalid\nm2 = Model.model_validate(m) from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    a: int\n\n    model_config = ConfigDict(revalidate_instances='always')\n\n\nm = Model(a=0)\n# note: setting `validate_assignment` to `True` in the config can prevent this kind of misbehavior.\nm.a = 'not an int'\n\ntry:\n    m2 = Model.model_validate(m)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    a\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='not an int', input_type=str]\n    \"\"\"","pageID":"Models","abs_url":"/latest/concepts/models/#validating-data","title":"Models - Validating data","objectID":"/latest/concepts/models/#validating-data","rank":35},{"content":"Pydantic also provides the  method, which allows models to be created without validation .\nThis can be useful in at least a few cases: when working with complex data that is already known to be valid (for performance reasons) when one or more of the validator functions are non-idempotent when one or more of the validator functions have side effects that you don't want to be triggered. Warning does not do any validation, meaning it can create\nmodels which are invalid. You should only ever use the \nmethod with data which has already been validated, or that you definitely trust. Note In Pydantic V2, the performance gap between validation (either with direct instantiation or the model_validate* methods)\nand  has been narrowed\nconsiderably. For simple models, going with validation may even be faster. If you are using \nfor performance reasons, you may want to profile your use case before assuming it is actually faster. Note that for root models , the root value can be passed to\n positionally, instead of using a keyword argument. Here are some additional notes on the behavior of : When we say \"no validation is performed\" — this includes converting dictionaries to model instances. So if you have a field\n  referring to a model type, you will need to convert the inner dictionary to a model yourself. If you do not pass keyword arguments for fields with defaults, the default values will still be used. For models with private attributes, the __pydantic_private__ dictionary will be populated the same as it would be when\n  creating the model with validation. No __init__ method from the model or any of its parent classes will be called, even when a custom __init__ method is defined. On extra data behavior with For models with  set to 'allow' , data not corresponding to fields will be correctly stored in\nthe __pydantic_extra__ dictionary and saved to the model's __dict__ attribute. For models with  set to 'ignore' , data not corresponding to fields will be ignored — that is,\nnot stored in __pydantic_extra__ or __dict__ on the instance. Unlike when instantiating the model with validation, a call to  with  set to 'forbid' doesn't raise an error in the presence of data not corresponding to fields. Rather, said input data is simply ignored.","pageID":"Models","abs_url":"/latest/concepts/models/#creating-models-without-validation","title":"Models - Validating data - Creating models without validation","objectID":"/latest/concepts/models/#creating-models-without-validation","rank":30},{"content":"The  method allows models to be duplicated (with optional updates),\nwhich is particularly useful when working with frozen models. from pydantic import BaseModel\n\n\nclass BarModel(BaseModel):\n    whatever: int\n\n\nclass FooBarModel(BaseModel):\n    banana: float\n    foo: str\n    bar: BarModel\n\n\nm = FooBarModel(banana=3.14, foo='hello', bar={'whatever': 123})\n\nprint(m.model_copy(update={'banana': 0}))\n#> banana=0 foo='hello' bar=BarModel(whatever=123)\n\n# normal copy gives the same object reference for bar:\nprint(id(m.bar) == id(m.model_copy().bar))\n#> True\n# deep copy gives a new object reference for `bar`:\nprint(id(m.bar) == id(m.model_copy(deep=True).bar))\n#> False","pageID":"Models","abs_url":"/latest/concepts/models/#model-copy","title":"Models - Model copy","objectID":"/latest/concepts/models/#model-copy","rank":25},{"content":"Pydantic supports the creation of generic models to make it easier to reuse a common model structure. Both the new\n (introduced by PEP 695 in Python 3.12)\nand the old syntax are supported (refer to the Python documentation for more details). Here is an example using a generic Pydantic model to create an easily-reused HTTP response payload wrapper: Python 3.9 and above Python 3.12 and above (new syntax) from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel, ValidationError\n\nDataT = TypeVar('DataT')  # (1)!\n\n\nclass DataModel(BaseModel):\n    number: int\n\n\nclass Response(BaseModel, Generic[DataT]):  # (2)!\n    data: DataT  # (3)!\n\n\nprint(Response[int](data=1))\n#> data=1\nprint(Response[str](data='value'))\n#> data='value'\nprint(Response[str](data='value').model_dump())\n#> {'data': 'value'}\n\ndata = DataModel(number=1)\nprint(Response[DataModel](data=data).model_dump())\n#> {'data': {'number': 1}}\ntry:\n    Response[int](data='value')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response[int]\n    data\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='value', input_type=str]\n    \"\"\" Declare one or more  to use to parameterize your model. Declare a Pydantic model that inherits from  and \n   (in this specific order), and add the list of type variables you declared previously as parameters to the\n    parent. Use the type variables as annotations where you will want to replace them with other types. from pydantic import BaseModel, ValidationError\n\n\nclass DataModel(BaseModel):\n    number: int\n\n\nclass Response[DataT](BaseModel):  # (1)!\n    data: DataT  # (2)!\n\n\nprint(Response[int](data=1))\n#> data=1\nprint(Response[str](data='value'))\n#> data='value'\nprint(Response[str](data='value').model_dump())\n#> {'data': 'value'}\n\ndata = DataModel(number=1)\nprint(Response[DataModel](data=data).model_dump())\n#> {'data': {'number': 1}}\ntry:\n    Response[int](data='value')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response[int]\n    data\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='value', input_type=str]\n    \"\"\" Declare a Pydantic model and add the list of type variables as type parameters. Use the type variables as annotations where you will want to replace them with other types. Warning When parametrizing a model with a concrete type, Pydantic does not validate that the provided type\nis assignable to the type variable if it has an upper bound. Any configuration , validation or serialization logic\nset on the generic model will also be applied to the parametrized classes, in the same way as when inheriting from\na model class. Any custom methods or attributes will also be inherited. Generic models also integrate properly with type checkers, so you get all the type checking\nyou would expect if you were to declare a distinct type for each parametrization. Note Internally, Pydantic creates subclasses of the generic model at runtime when the generic model class is parametrized.\nThese classes are cached, so there should be minimal overhead introduced by the use of generics models. To inherit from a generic model and preserve the fact that it is generic, the subclass must also inherit from\n: from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel\n\nTypeX = TypeVar('TypeX')\n\n\nclass BaseClass(BaseModel, Generic[TypeX]):\n    X: TypeX\n\n\nclass ChildClass(BaseClass[TypeX], Generic[TypeX]):\n    pass\n\n\n# Parametrize `TypeX` with `int`:\nprint(ChildClass[int](X=1))\n#> X=1 You can also create a generic subclass of a model that partially or fully replaces the type variables in the\nsuperclass: from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel\n\nTypeX = TypeVar('TypeX')\nTypeY = TypeVar('TypeY')\nTypeZ = TypeVar('TypeZ')\n\n\nclass BaseClass(BaseModel, Generic[TypeX, TypeY]):\n    x: TypeX\n    y: TypeY\n\n\nclass ChildClass(BaseClass[int, TypeY], Generic[TypeY, TypeZ]):\n    z: TypeZ\n\n\n# Parametrize `TypeY` with `str`:\nprint(ChildClass[str, int](x='1', y='y', z='3'))\n#> x=1 y='y' z=3 If the name of the concrete subclasses is important, you can also override the default name generation\nby overriding the  method: from typing import Any, Generic, TypeVar\n\nfrom pydantic import BaseModel\n\nDataT = TypeVar('DataT')\n\n\nclass Response(BaseModel, Generic[DataT]):\n    data: DataT\n\n    @classmethod\n    def model_parametrized_name(cls, params: tuple[type[Any], ...]) -> str:\n        return f'{params[0].__name__.title()}Response'\n\n\nprint(repr(Response[int](data=1)))\n#> IntResponse(data=1)\nprint(repr(Response[str](data='a')))\n#> StrResponse(data='a') You can use parametrized generic models as types in other models: from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel\n\nT = TypeVar('T')\n\n\nclass ResponseModel(BaseModel, Generic[T]):\n    content: T\n\n\nclass Product(BaseModel):\n    name: str\n    price: float\n\n\nclass Order(BaseModel):\n    id: int\n    product: ResponseModel[Product]\n\n\nproduct = Product(name='Apple', price=0.5)\nresponse = ResponseModel[Product](content=product)\norder = Order(id=1, product=response)\nprint(repr(order))\n\"\"\"\nOrder(id=1, product=ResponseModel[Product](content=Product(name='Apple', price=0.5)))\n\"\"\" Using the same type variable in nested models allows you to enforce typing relationships at different points in your model: from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel, ValidationError\n\nT = TypeVar('T')\n\n\nclass InnerT(BaseModel, Generic[T]):\n    inner: T\n\n\nclass OuterT(BaseModel, Generic[T]):\n    outer: T\n    nested: InnerT[T]\n\n\nnested = InnerT[int](inner=1)\nprint(OuterT[int](outer=1, nested=nested))\n#> outer=1 nested=InnerT[int](inner=1)\ntry:\n    print(OuterT[int](outer='a', nested=InnerT(inner='a')))  # (1)!\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for OuterT[int]\n    outer\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    nested.inner\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\" The OuterT model is parametrized with int , but the data associated with the the T annotations during validation is of type str , leading to validation errors. Warning While it may not raise an error, we strongly advise against using parametrized generics in isinstance() checks. For example, you should not do isinstance(my_model, MyGenericModel[int]) . However, it is fine to do isinstance(my_model, MyGenericModel) (note that, for standard generics, it would raise an error to do a subclass check with a parameterized generic class). If you need to perform isinstance() checks against parametrized generics, you can do this by subclassing the parametrized generic class: class MyIntModel(MyGenericModel[int]): ...\n\nisinstance(my_model, MyIntModel)","pageID":"Models","abs_url":"/latest/concepts/models/#generic-models","title":"Models - Generic models","objectID":"/latest/concepts/models/#generic-models","rank":20},{"content":"When leaving type variables unparametrized, Pydantic treats generic models similarly to how it treats built-in generic\ntypes like  and : If the type variable is bound or constrained to a specific type,\n  it will be used. If the type variable has a default type (as specified by PEP 696 ), it will be used. For unbound or unconstrained type variables, Pydantic will fallback to . from typing import Generic\n\nfrom typing_extensions import TypeVar\n\nfrom pydantic import BaseModel, ValidationError\n\nT = TypeVar('T')\nU = TypeVar('U', bound=int)\nV = TypeVar('V', default=str)\n\n\nclass Model(BaseModel, Generic[T, U, V]):\n    t: T\n    u: U\n    v: V\n\n\nprint(Model(t='t', u=1, v='v'))\n#> t='t' u=1 v='v'\n\ntry:\n    Model(t='t', u='u', v=1)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    2 validation errors for Model\n    u\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='u', input_type=str]\n    v\n      Input should be a valid string [type=string_type, input_value=1, input_type=int]\n    \"\"\" Warning In some cases, validation against an unparametrized generic model can lead to data loss. Specifically, if a subtype of the type variable upper bound, constraints, or default is being used and the model isn't explicitly parametrized, the resulting type will not be the one being provided: from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel\n\nItemT = TypeVar('ItemT', bound='ItemBase')\n\n\nclass ItemBase(BaseModel): ...\n\n\nclass IntItem(ItemBase):\n    value: int\n\n\nclass ItemHolder(BaseModel, Generic[ItemT]):\n    item: ItemT\n\n\nloaded_data = {'item': {'value': 1}}\n\n\nprint(ItemHolder(**loaded_data))  # (1)!\n#> item=ItemBase()\n\nprint(ItemHolder[IntItem](**loaded_data))  # (2)!\n#> item=IntItem(value=1) When the generic isn't parametrized, the input data is validated against the ItemT upper bound.\n   Given that ItemBase has no fields, the item field information is lost. In this case, the type variable is explicitly parametrized, so the input data is validated against the IntItem class.","pageID":"Models","abs_url":"/latest/concepts/models/#validation-of-unparametrized-type-variables","title":"Models - Generic models - Validation of unparametrized type variables","objectID":"/latest/concepts/models/#validation-of-unparametrized-type-variables","rank":15},{"content":"The behavior of serialization differs when using type variables with upper bounds , constraints , or a default value: If a Pydantic model is used in a type variable upper bound and the type variable is never parametrized, then Pydantic will use the upper bound for validation but treat the value as  in terms of serialization: from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel\n\n\nclass ErrorDetails(BaseModel):\n    foo: str\n\n\nErrorDataT = TypeVar('ErrorDataT', bound=ErrorDetails)\n\n\nclass Error(BaseModel, Generic[ErrorDataT]):\n    message: str\n    details: ErrorDataT\n\n\nclass MyErrorDetails(ErrorDetails):\n    bar: str\n\n\n# serialized as Any\nerror = Error(\n    message='We just had an error',\n    details=MyErrorDetails(foo='var', bar='var2'),\n)\nassert error.model_dump() == {\n    'message': 'We just had an error',\n    'details': {\n        'foo': 'var',\n        'bar': 'var2',\n    },\n}\n\n# serialized using the concrete parametrization\n# note that `'bar': 'var2'` is missing\nerror = Error[ErrorDetails](\n    message='We just had an error',\n    details=ErrorDetails(foo='var'),\n)\nassert error.model_dump() == {\n    'message': 'We just had an error',\n    'details': {\n        'foo': 'var',\n    },\n} Here's another example of the above behavior, enumerating all permutations regarding bound specification and generic type parametrization: from typing import Generic, TypeVar\n\nfrom pydantic import BaseModel\n\nTBound = TypeVar('TBound', bound=BaseModel)\nTNoBound = TypeVar('TNoBound')\n\n\nclass IntValue(BaseModel):\n    value: int\n\n\nclass ItemBound(BaseModel, Generic[TBound]):\n    item: TBound\n\n\nclass ItemNoBound(BaseModel, Generic[TNoBound]):\n    item: TNoBound\n\n\nitem_bound_inferred = ItemBound(item=IntValue(value=3))\nitem_bound_explicit = ItemBound[IntValue](item=IntValue(value=3))\nitem_no_bound_inferred = ItemNoBound(item=IntValue(value=3))\nitem_no_bound_explicit = ItemNoBound[IntValue](item=IntValue(value=3))\n\n# calling `print(x.model_dump())` on any of the above instances results in the following:\n#> {'item': {'value': 3}} However, if constraints or a default value (as per PEP 696 ) is being used, then the default type or constraints\nwill be used for both validation and serialization if the type variable is not parametrized. You can override this behavior\nusing SerializeAsAny : Python 3.9 and above Python 3.13 and above from typing import Generic\n\nfrom typing_extensions import TypeVar\n\nfrom pydantic import BaseModel, SerializeAsAny\n\n\nclass ErrorDetails(BaseModel):\n    foo: str\n\n\nErrorDataT = TypeVar('ErrorDataT', default=ErrorDetails)\n\n\nclass Error(BaseModel, Generic[ErrorDataT]):\n    message: str\n    details: ErrorDataT\n\n\nclass MyErrorDetails(ErrorDetails):\n    bar: str\n\n\n# serialized using the default's serializer\nerror = Error(\n    message='We just had an error',\n    details=MyErrorDetails(foo='var', bar='var2'),\n)\nassert error.model_dump() == {\n    'message': 'We just had an error',\n    'details': {\n        'foo': 'var',\n    },\n}\n# If `ErrorDataT` was using an upper bound, `bar` would be present in `details`.\n\n\nclass SerializeAsAnyError(BaseModel, Generic[ErrorDataT]):\n    message: str\n    details: SerializeAsAny[ErrorDataT]\n\n\n# serialized as Any\nerror = SerializeAsAnyError(\n    message='We just had an error',\n    details=MyErrorDetails(foo='var', bar='baz'),\n)\nassert error.model_dump() == {\n    'message': 'We just had an error',\n    'details': {\n        'foo': 'var',\n        'bar': 'baz',\n    },\n} from typing import Generic\n\nfrom typing import TypeVar\n\nfrom pydantic import BaseModel, SerializeAsAny\n\n\nclass ErrorDetails(BaseModel):\n    foo: str\n\n\nErrorDataT = TypeVar('ErrorDataT', default=ErrorDetails)\n\n\nclass Error(BaseModel, Generic[ErrorDataT]):\n    message: str\n    details: ErrorDataT\n\n\nclass MyErrorDetails(ErrorDetails):\n    bar: str\n\n\n# serialized using the default's serializer\nerror = Error(\n    message='We just had an error',\n    details=MyErrorDetails(foo='var', bar='var2'),\n)\nassert error.model_dump() == {\n    'message': 'We just had an error',\n    'details': {\n        'foo': 'var',\n    },\n}\n# If `ErrorDataT` was using an upper bound, `bar` would be present in `details`.\n\n\nclass SerializeAsAnyError(BaseModel, Generic[ErrorDataT]):\n    message: str\n    details: SerializeAsAny[ErrorDataT]\n\n\n# serialized as Any\nerror = SerializeAsAnyError(\n    message='We just had an error',\n    details=MyErrorDetails(foo='var', bar='baz'),\n)\nassert error.model_dump() == {\n    'message': 'We just had an error',\n    'details': {\n        'foo': 'var',\n        'bar': 'baz',\n    },\n}","pageID":"Models","abs_url":"/latest/concepts/models/#serialization-of-unparametrized-type-variables","title":"Models - Generic models - Serialization of unparametrized type variables","objectID":"/latest/concepts/models/#serialization-of-unparametrized-type-variables","rank":10},{"content":"There are some occasions where it is desirable to create a model using runtime information to specify the fields.\nPydantic provides the  function to allow models to be created dynamically: from pydantic import BaseModel, create_model\n\nDynamicFoobarModel = create_model('DynamicFoobarModel', foo=str, bar=(int, 123))\n\n# Equivalent to:\n\n\nclass StaticFoobarModel(BaseModel):\n    foo: str\n    bar: int = 123 Field definitions are specified as keyword arguments, and should either be: A single element, representing the type annotation of the field. A two-tuple, the first element being the type and the second element the assigned value\n  (either a default or the  function). Here is a more advanced example: from typing import Annotated\n\nfrom pydantic import BaseModel, Field, PrivateAttr, create_model\n\nDynamicModel = create_model(\n    'DynamicModel',\n    foo=(str, Field(alias='FOO')),\n    bar=Annotated[str, Field(description='Bar field')],\n    _private=(int, PrivateAttr(default=1)),\n)\n\n\nclass StaticModel(BaseModel):\n    foo: str = Field(alias='FOO')\n    bar: Annotated[str, Field(description='Bar field')]\n    _private: int = PrivateAttr(default=1) The special keyword arguments __config__ and __base__ can be used to customize the new model.\nThis includes extending a base model with extra fields. from pydantic import BaseModel, create_model\n\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\nprint(BarModel)\n#> print(BarModel.model_fields.keys())\n#> dict_keys(['foo', 'bar', 'apple', 'banana']) You can also add validators by passing a dictionary to the __validators__ argument. from pydantic import ValidationError, create_model, field_validator\n\n\ndef alphanum(cls, v):\n    assert v.isalnum(), 'must be alphanumeric'\n    return v\n\n\nvalidators = {\n    'username_validator': field_validator('username')(alphanum)  # (1)!\n}\n\nUserModel = create_model(\n    'UserModel', username=(str, ...), __validators__=validators\n)\n\nuser = UserModel(username='scolvin')\nprint(user)\n#> username='scolvin'\n\ntry:\n    UserModel(username='scolvi%n')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserModel\n    username\n      Assertion failed, must be alphanumeric [type=assertion_error, input_value='scolvi%n', input_type=str]\n    \"\"\" Make sure that the validators names do not clash with any of the field names as\n   internally, Pydantic gathers all members into a namespace and mimics the normal\n   creation of a class using the types module utilities . Note To pickle a dynamically created model: the model must be defined globally the __module__ argument must be provided","pageID":"Models","abs_url":"/latest/concepts/models/#dynamic-model-creation","title":"Models - Dynamic model creation","objectID":"/latest/concepts/models/#dynamic-model-creation","rank":5},{"content":"Pydantic models can be defined with a \"custom root type\" by subclassing . The root type can be any type supported by Pydantic, and is specified by the generic parameter to RootModel .\nThe root value can be passed to the model __init__ or \nvia the first and only argument. Here's an example of how this works: from pydantic import RootModel\n\nPets = RootModel[list[str]]\nPetsByName = RootModel[dict[str, str]]\n\n\nprint(Pets(['dog', 'cat']))\n#> root=['dog', 'cat']\nprint(Pets(['dog', 'cat']).model_dump_json())\n#> [\"dog\",\"cat\"]\nprint(Pets.model_validate(['dog', 'cat']))\n#> root=['dog', 'cat']\nprint(Pets.model_json_schema())\n\"\"\"\n{'items': {'type': 'string'}, 'title': 'RootModel[list[str]]', 'type': 'array'}\n\"\"\"\n\nprint(PetsByName({'Otis': 'dog', 'Milo': 'cat'}))\n#> root={'Otis': 'dog', 'Milo': 'cat'}\nprint(PetsByName({'Otis': 'dog', 'Milo': 'cat'}).model_dump_json())\n#> {\"Otis\":\"dog\",\"Milo\":\"cat\"}\nprint(PetsByName.model_validate({'Otis': 'dog', 'Milo': 'cat'}))\n#> root={'Otis': 'dog', 'Milo': 'cat'} If you want to access items in the root field directly or to iterate over the items, you can implement\ncustom __iter__ and __getitem__ functions, as shown in the following example. from pydantic import RootModel\n\n\nclass Pets(RootModel):\n    root: list[str]\n\n    def __iter__(self):\n        return iter(self.root)\n\n    def __getitem__(self, item):\n        return self.root[item]\n\n\npets = Pets.model_validate(['dog', 'cat'])\nprint(pets[0])\n#> dog\nprint([pet for pet in pets])\n#> ['dog', 'cat'] You can also create subclasses of the parametrized root model directly: from pydantic import RootModel\n\n\nclass Pets(RootModel[list[str]]):\n    def describe(self) -> str:\n        return f'Pets: {\", \".join(self.root)}'\n\n\nmy_pets = Pets.model_validate(['dog', 'cat'])\n\nprint(my_pets.describe())\n#> Pets: dog, cat","pageID":"Models","abs_url":"/latest/concepts/models/#rootmodel-and-custom-root-types","title":"Models - RootModel and custom root types","objectID":"/latest/concepts/models/#rootmodel-and-custom-root-types","rank":0},{"content":"Models can be configured to be immutable via model_config['frozen'] = True . When this is set, attempting to change the\nvalues of instance attributes will raise errors. See the  for more details. Note This behavior was achieved in Pydantic V1 via the config setting allow_mutation = False .\nThis config flag is deprecated in Pydantic V2, and has been replaced with frozen . Warning In Python, immutability is not enforced. Developers have the ability to modify objects\nthat are conventionally considered \"immutable\" if they choose to do so. from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass FooBarModel(BaseModel):\n    model_config = ConfigDict(frozen=True)\n\n    a: str\n    b: dict\n\n\nfoobar = FooBarModel(a='hello', b={'apple': 'pear'})\n\ntry:\n    foobar.a = 'different'\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for FooBarModel\n    a\n      Instance is frozen [type=frozen_instance, input_value='different', input_type=str]\n    \"\"\"\n\nprint(foobar.a)\n#> hello\nprint(foobar.b)\n#> {'apple': 'pear'}\nfoobar.b['apple'] = 'grape'\nprint(foobar.b)\n#> {'apple': 'grape'} Trying to change a caused an error, and a remains unchanged. However, the dict b is mutable, and the\nimmutability of foobar doesn't stop b from being changed.","pageID":"Models","abs_url":"/latest/concepts/models/#faux-immutability","title":"Models - Faux immutability","objectID":"/latest/concepts/models/#faux-immutability","rank":-5},{"content":"Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). import abc\n\nfrom pydantic import BaseModel\n\n\nclass FooBarModel(BaseModel, abc.ABC):\n    a: str\n    b: int\n\n    @abc.abstractmethod\n    def my_abstract_method(self):\n        pass","pageID":"Models","abs_url":"/latest/concepts/models/#abstract-base-classes","title":"Models - Abstract base classes","objectID":"/latest/concepts/models/#abstract-base-classes","rank":-10},{"content":"Field order affects models in the following ways: field order is preserved in the model JSON Schema field order is preserved in validation errors field order is preserved when serializing data from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    a: int\n    b: int = 2\n    c: int = 1\n    d: int = 0\n    e: float\n\n\nprint(Model.model_fields.keys())\n#> dict_keys(['a', 'b', 'c', 'd', 'e'])\nm = Model(e=2, a=1)\nprint(m.model_dump())\n#> {'a': 1, 'b': 2, 'c': 1, 'd': 0, 'e': 2.0}\ntry:\n    Model(a='x', b='x', c='x', d='x', e='x')\nexcept ValidationError as err:\n    error_locations = [e['loc'] for e in err.errors()]\n\nprint(error_locations)\n#> [('a',), ('b',), ('c',), ('d',), ('e',)]","pageID":"Models","abs_url":"/latest/concepts/models/#field-ordering","title":"Models - Field ordering","objectID":"/latest/concepts/models/#field-ordering","rank":-15},{"content":"","pageID":"Models","abs_url":"/latest/concepts/models/#automatically-excluded-attributes","title":"Models - Automatically excluded attributes","objectID":"/latest/concepts/models/#automatically-excluded-attributes","rank":-20},{"content":"Attributes annotated with  are properly treated by Pydantic as class variables, and will not\nbecome fields on model instances: from typing import ClassVar\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    x: ClassVar[int] = 1\n\n    y: int = 2\n\n\nm = Model()\nprint(m)\n#> y=2\nprint(Model.x)\n#> 1","pageID":"Models","abs_url":"/latest/concepts/models/#class-variables","title":"Models - Automatically excluded attributes - Class variables","objectID":"/latest/concepts/models/#class-variables","rank":-25},{"content":"Attributes whose name has a leading underscore are not treated as fields by Pydantic, and are not included in the\nmodel schema. Instead, these are converted into a \"private attribute\" which is not validated or even set during\ncalls to __init__ , model_validate , etc. Here is an example of usage: from datetime import datetime\nfrom random import randint\nfrom typing import Any\n\nfrom pydantic import BaseModel, PrivateAttr\n\n\nclass TimeAwareModel(BaseModel):\n    _processed_at: datetime = PrivateAttr(default_factory=datetime.now)\n    _secret_value: str\n\n    def model_post_init(self, context: Any) -> None:\n        # this could also be done with `default_factory`:\n        self._secret_value = randint(1, 5)\n\n\nm = TimeAwareModel()\nprint(m._processed_at)\n#> 2032-01-02 03:04:05.000006\nprint(m._secret_value)\n#> 3 Private attribute names must start with underscore to prevent conflicts with model fields. However, dunder names\n(such as __attr__ ) are not supported, and will be completely ignored from the model definition.","pageID":"Models","abs_url":"/latest/concepts/models/#private-model-attributes","title":"Models - Automatically excluded attributes - Private model attributes","objectID":"/latest/concepts/models/#private-model-attributes","rank":-30},{"content":"All Pydantic models will have their signature generated based on their fields: import inspect\n\nfrom pydantic import BaseModel, Field\n\n\nclass FooModel(BaseModel):\n    id: int\n    name: str = None\n    description: str = 'Foo'\n    apple: int = Field(alias='pear')\n\n\nprint(inspect.signature(FooModel))\n#> (*, id: int, name: str = None, description: str = 'Foo', pear: int) -> None An accurate signature is useful for introspection purposes and libraries like FastAPI or hypothesis . The generated signature will also respect custom __init__ functions: import inspect\n\nfrom pydantic import BaseModel\n\n\nclass MyModel(BaseModel):\n    id: int\n    info: str = 'Foo'\n\n    def __init__(self, id: int = 1, *, bar: str, **data) -> None:\n        \"\"\"My custom init!\"\"\"\n        super().__init__(id=id, bar=bar, **data)\n\n\nprint(inspect.signature(MyModel))\n#> (id: int = 1, *, bar: str, info: str = 'Foo') -> None To be included in the signature, a field's alias or name must be a valid Python identifier.\nPydantic will prioritize a field's alias over its name when generating the signature, but may use the field name if the\nalias is not a valid Python identifier. If a field's alias and name are both not valid identifiers (which may be possible through exotic use of create_model ),\na **data argument will be added. In addition, the **data argument will always be present in the signature if model_config['extra'] == 'allow' .","pageID":"Models","abs_url":"/latest/concepts/models/#model-signature","title":"Models - Model signature","objectID":"/latest/concepts/models/#model-signature","rank":-35},{"content":"Pydantic supports structural pattern matching for models, as introduced by PEP 636 in Python 3.10. from pydantic import BaseModel\n\n\nclass Pet(BaseModel):\n    name: str\n    species: str\n\n\na = Pet(name='Bones', species='dog')\n\nmatch a:\n    # match `species` to 'dog', declare and initialize `dog_name`\n    case Pet(species='dog', name=dog_name):\n        print(f'{dog_name} is a dog')\n#> Bones is a dog\n    # default case\n    case _:\n        print('No dog matched') Note A match-case statement may seem as if it creates a new model, but don't be fooled;\nit is just syntactic sugar for getting an attribute and either comparing it or declaring and initializing it.","pageID":"Models","abs_url":"/latest/concepts/models/#structural-pattern-matching","title":"Models - Structural pattern matching","objectID":"/latest/concepts/models/#structural-pattern-matching","rank":-40},{"content":"In many cases, arguments passed to the constructor will be copied in order to perform validation and, where necessary,\ncoercion. In this example, note that the ID of the list changes after the class is constructed because it has been\ncopied during validation: from pydantic import BaseModel\n\n\nclass C1:\n    arr = []\n\n    def __init__(self, in_arr):\n        self.arr = in_arr\n\n\nclass C2(BaseModel):\n    arr: list[int]\n\n\narr_orig = [1, 9, 10, 3]\n\n\nc1 = C1(arr_orig)\nc2 = C2(arr=arr_orig)\nprint(f'{id(c1.arr) == id(c2.arr)=}')\n#> id(c1.arr) == id(c2.arr)=False Note There are some situations where Pydantic does not copy attributes, such as when passing models — we use the\nmodel as is. You can override this behaviour by setting model_config['revalidate_instances'] = 'always' .","pageID":"Models","abs_url":"/latest/concepts/models/#attribute-copies","title":"Models - Attribute copies","objectID":"/latest/concepts/models/#attribute-copies","rank":-45},{"content":"In most cases Pydantic won't be your bottle neck, only follow this if you're sure it's necessary.","pageID":"Performance","abs_url":"/latest/concepts/performance/#performance-tips","title":"Performance","objectID":"/latest/concepts/performance/#performance-tips","rank":100},{"content":"On model_validate(json.loads(...)) , the JSON is parsed in Python, then converted to a dict, then it's validated internally.\nOn the other hand, model_validate_json() already performs the validation internally. There are a few cases where model_validate(json.loads(...)) may be faster. Specifically, when using a 'before' or 'wrap' validator\non a model, validation may be faster with the two step method. You can read more about these special cases in this discussion . Many performance improvements are currently in the works for pydantic-core , as discussed here . Once these changes are merged, we should be at\nthe point where model_validate_json() is always faster than model_validate(json.loads(...)) .","pageID":"Performance","abs_url":"/latest/concepts/performance/#in-general-use-model_validate_json-not-model_validatejsonloads","title":"Performance - In general, use model_validate_json() not model_validate(json.loads(...))","objectID":"/latest/concepts/performance/#in-general-use-model_validate_json-not-model_validatejsonloads","rank":95},{"content":"The idea here is to avoid constructing validators and serializers more than necessary. Each time a TypeAdapter is instantiated,\nit will construct a new validator and serializer. If you're using a TypeAdapter in a function, it will be instantiated each time\nthe function is called. Instead, instantiate it once, and reuse it. Bad Good from pydantic import TypeAdapter\n\n\ndef my_func():\n    adapter = TypeAdapter(list[int])\n    # do something with adapter from pydantic import TypeAdapter\n\nadapter = TypeAdapter(list[int])\n\ndef my_func():\n    ...\n    # do something with adapter","pageID":"Performance","abs_url":"/latest/concepts/performance/#typeadapter-instantiated-once","title":"Performance - TypeAdapter instantiated once","objectID":"/latest/concepts/performance/#typeadapter-instantiated-once","rank":90},{"content":"When using Sequence , Pydantic calls isinstance(value, Sequence) to check if the value is a sequence.\nAlso, Pydantic will try to validate against different types of sequences, like list and tuple .\nIf you know the value is a list or tuple , use list or tuple instead of Sequence . The same applies to Mapping and dict .\nIf you know the value is a dict , use dict instead of Mapping .","pageID":"Performance","abs_url":"/latest/concepts/performance/#sequence-vs-list-or-tuple-with-mapping-vs-dict","title":"Performance - Sequence vs list or tuple with Mapping vs dict","objectID":"/latest/concepts/performance/#sequence-vs-list-or-tuple-with-mapping-vs-dict","rank":85},{"content":"If you don't need to validate a value, use Any to keep the value unchanged. from typing import Any\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    a: Any\n\n\nmodel = Model(a=1)","pageID":"Performance","abs_url":"/latest/concepts/performance/#dont-do-validation-when-you-dont-have-to-use-any-to-keep-the-value-unchanged","title":"Performance - Don't do validation when you don't have to, use Any to keep the value unchanged","objectID":"/latest/concepts/performance/#dont-do-validation-when-you-dont-have-to-use-any-to-keep-the-value-unchanged","rank":80},{"content":"Don't do this Do this class CompletedStr(str):\n    def __init__(self, s: str):\n        self.s = s\n        self.done = False from pydantic import BaseModel\n\n\nclass CompletedModel(BaseModel):\n    s: str\n    done: bool = False","pageID":"Performance","abs_url":"/latest/concepts/performance/#avoid-extra-information-via-subclasses-of-primitives","title":"Performance - Avoid extra information via subclasses of primitives","objectID":"/latest/concepts/performance/#avoid-extra-information-via-subclasses-of-primitives","rank":75},{"content":"Tagged union (or discriminated union) is a union with a field that indicates which type it is. from typing import Any, Literal\n\nfrom pydantic import BaseModel, Field\n\n\nclass DivModel(BaseModel):\n    el_type: Literal['div'] = 'div'\n    class_name: str | None = None\n    children: list[Any] | None = None\n\n\nclass SpanModel(BaseModel):\n    el_type: Literal['span'] = 'span'\n    class_name: str | None = None\n    contents: str | None = None\n\n\nclass ButtonModel(BaseModel):\n    el_type: Literal['button'] = 'button'\n    class_name: str | None = None\n    contents: str | None = None\n\n\nclass InputModel(BaseModel):\n    el_type: Literal['input'] = 'input'\n    class_name: str | None = None\n    value: str | None = None\n\n\nclass Html(BaseModel):\n    contents: DivModel | SpanModel | ButtonModel | InputModel = Field(\n        discriminator='el_type'\n    ) See Discriminated Unions for more details.","pageID":"Performance","abs_url":"/latest/concepts/performance/#use-tagged-union-not-union","title":"Performance - Use tagged union, not union","objectID":"/latest/concepts/performance/#use-tagged-union-not-union","rank":70},{"content":"Instead of using nested models, use TypedDict to define the structure of the data.","pageID":"Performance","abs_url":"/latest/concepts/performance/#use-typeddict-over-nested-models","title":"Performance - Use TypedDict over nested models","objectID":"/latest/concepts/performance/#use-typeddict-over-nested-models","rank":65},{"content":"Wrap validators are generally slower than other validators. This is because they require\nthat data is materialized in Python during validation. Wrap validators can be incredibly useful\nfor complex validation logic, but if you're looking for the best performance, you should avoid them.","pageID":"Performance","abs_url":"/latest/concepts/performance/#avoid-wrap-validators-if-you-really-care-about-performance","title":"Performance - Avoid wrap validators if you really care about performance","objectID":"/latest/concepts/performance/#avoid-wrap-validators-if-you-really-care-about-performance","rank":60},{"content":"Starting in v2.8+, you can apply the FailFast annotation to sequence types to fail early if any item in the sequence fails validation.\nIf you use this annotation, you won't get validation errors for the rest of the items in the sequence if one fails, so you're effectively\ntrading off visibility for performance. from typing import Annotated\n\nfrom pydantic import FailFast, TypeAdapter, ValidationError\n\nta = TypeAdapter(Annotated[list[bool], FailFast()])\ntry:\n    ta.validate_python([True, 'invalid', False, 'also invalid'])\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for list[bool]\n    1\n      Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value='invalid', input_type=str]\n    \"\"\" Read more about FailFast .","pageID":"Performance","abs_url":"/latest/concepts/performance/#failing-early-with-failfast","title":"Performance - Failing early with FailFast","objectID":"/latest/concepts/performance/#failing-early-with-failfast","rank":55},{"content":"Pydantic Settings provides optional Pydantic features for loading a settings or config class from environment variables or secrets files.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#settings-management","title":"Settings Management","objectID":"/latest/concepts/pydantic_settings/#settings-management","rank":100},{"content":"Installation is as simple as: pip install pydantic-settings","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#installation","title":"Settings Management - Installation","objectID":"/latest/concepts/pydantic_settings/#installation","rank":95},{"content":"If you create a model that inherits from BaseSettings , the model initialiser will attempt to determine\nthe values of any fields not passed as keyword arguments by reading from the environment. (Default values\nwill still be used if the matching environment variable is not set.) This makes it easy to: Create a clearly-defined, type-hinted application configuration class Automatically read modifications to the configuration from environment variables Manually override specific settings in the initialiser where desired (e.g. in unit tests) For example: from collections.abc import Callable from typing import Any from pydantic import ( AliasChoices , AmqpDsn , BaseModel , Field , ImportString , PostgresDsn , RedisDsn , ) from pydantic_settings import BaseSettings , SettingsConfigDict class SubModel ( BaseModel ): foo : str = 'bar' apple : int = 1 class Settings ( BaseSettings ): auth_key : str = Field ( validation_alias = 'my_auth_key' ) # (1)! api_key : str = Field ( alias = 'my_api_key' ) # (2)! redis_dsn : RedisDsn = Field ( 'redis://user:pass@localhost:6379/1' , validation_alias = AliasChoices ( 'service_redis_dsn' , 'redis_url' ), # (3)! ) pg_dsn : PostgresDsn = 'postgres://user:pass@localhost:5432/foobar' amqp_dsn : AmqpDsn = 'amqp://user:pass@localhost:5672/' special_function : ImportString [ Callable [[ Any ], Any ]] = 'math.cos' # (4)! # to override domains: # export my_prefix_domains='[\"foo.com\", \"bar.com\"]' domains : set [ str ] = set () # to override more_settings: # export my_prefix_more_settings='{\"foo\": \"x\", \"apple\": 1}' more_settings : SubModel = SubModel () model_config = SettingsConfigDict ( env_prefix = 'my_prefix_' ) # (5)! print ( Settings () . model_dump ()) \"\"\" { 'auth_key': 'xxx', 'api_key': 'xxx', 'redis_dsn': RedisDsn('redis://user:pass@localhost:6379/1'), 'pg_dsn': PostgresDsn('postgres://user:pass@localhost:5432/foobar'), 'amqp_dsn': AmqpDsn('amqp://user:pass@localhost:5672/'), 'special_function': math.cos, 'domains': set(), 'more_settings': {'foo': 'bar', 'apple': 1}, } \"\"\" The environment variable name is overridden using validation_alias . In this case, the environment variable my_auth_key will be read instead of auth_key . Check the Field documentation for more information. The environment variable name is overridden using alias . In this case, the environment variable my_api_key will be used for both validation and serialization instead of api_key . Check the Field documentation for more information. The  class allows to have multiple environment variable names for a single field.\n   The first environment variable that is found will be used. Check the documentation on alias choices for more information. The  class allows to import an object from a string.\n   In this case, the environment variable special_function will be read and the function  will be imported. The env_prefix config setting allows to set a prefix for all environment variables. Check the Environment variable names documentation for more information.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#usage","title":"Settings Management - Usage","objectID":"/latest/concepts/pydantic_settings/#usage","rank":90},{"content":"Unlike pydantic BaseModel , default values of BaseSettings fields are validated by default.\nYou can disable this behaviour by setting validate_default=False either in model_config or on field level by Field(validate_default=False) : from pydantic import Field from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( validate_default = False ) # default won't be validated foo : int = 'test' print ( Settings ()) #> foo='test' class Settings1 ( BaseSettings ): # default won't be validated foo : int = Field ( 'test' , validate_default = False ) print ( Settings1 ()) #> foo='test' Check the validation of default values for more information.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#validation-of-default-values","title":"Settings Management - Validation of default values","objectID":"/latest/concepts/pydantic_settings/#validation-of-default-values","rank":85},{"content":"By default, the environment variable name is the same as the field name. You can change the prefix for all environment variables by setting the env_prefix config setting,\nor via the _env_prefix keyword argument on instantiation: from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( env_prefix = 'my_prefix_' ) auth_key : str = 'xxx' # will be read from `my_prefix_auth_key` Note The default env_prefix is '' (empty string). env_prefix is not only for env settings but also for\ndotenv files, secrets, and other sources. If you want to change the environment variable name for a single field, you can use an alias. There are two ways to do this: Using Field(alias=...) (see api_key above) Using Field(validation_alias=...) (see auth_key above) Check the Field aliases documentation for more information about aliases. env_prefix does not apply to fields with alias. It means the environment variable name is the same\nas field alias: from pydantic import Field from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( env_prefix = 'my_prefix_' ) foo : str = Field ( 'xxx' , alias = 'FooAlias' ) # (1)! env_prefix will be ignored and the value will be read from FooAlias environment variable.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#environment-variable-names","title":"Settings Management - Environment variable names","objectID":"/latest/concepts/pydantic_settings/#environment-variable-names","rank":80},{"content":"By default, environment variable names are case-insensitive. If you want to make environment variable names case-sensitive, you can set the case_sensitive config setting: from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( case_sensitive = True ) redis_host : str = 'localhost' When case_sensitive is True , the environment variable names must match field names (optionally with a prefix),\nso in this example redis_host could only be modified via export redis_host . If you want to name environment variables\nall upper-case, you should name attribute all upper-case too. You can still name environment variables anything\nyou like through Field(validation_alias=...) . Case-sensitivity can also be set via the _case_sensitive keyword argument on instantiation. In case of nested models, the case_sensitive setting will be applied to all nested models. import os from pydantic import BaseModel , ValidationError from pydantic_settings import BaseSettings class RedisSettings ( BaseModel ): host : str port : int class Settings ( BaseSettings , case_sensitive = True ): redis : RedisSettings os . environ [ 'redis' ] = '{\"host\": \"localhost\", \"port\": 6379}' print ( Settings () . model_dump ()) #> {'redis': {'host': 'localhost', 'port': 6379}} os . environ [ 'redis' ] = '{\"HOST\": \"localhost\", \"port\": 6379}' # (1)! try : Settings () except ValidationError as e : print ( e ) \"\"\" 1 validation error for Settings redis.host Field required [type=missing, input_value={'HOST': 'localhost', 'port': 6379}, input_type=dict] For further information visit https://errors.pydantic.dev/2/v/missing \"\"\" Note that the host field is not found because the environment variable name is HOST (all upper-case). Note On Windows, Python's os module always treats environment variables as case-insensitive, so the case_sensitive config setting will have no effect - settings will always be updated ignoring case.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#case-sensitivity","title":"Settings Management - Environment variable names - Case-sensitivity","objectID":"/latest/concepts/pydantic_settings/#case-sensitivity","rank":75},{"content":"By default environment variables are parsed verbatim, including if the value is empty. You can choose to\nignore empty environment variables by setting the env_ignore_empty config setting to True . This can be\nuseful if you would prefer to use the default value for a field rather than an empty value from the\nenvironment. For most simple field types (such as int , float , str , etc.), the environment variable value is parsed\nthe same way it would be if passed directly to the initialiser (as a string). Complex types like list , set , dict , and sub-models are populated from the environment by treating the\nenvironment variable's value as a JSON-encoded string. Another way to populate nested complex variables is to configure your model with the env_nested_delimiter config setting, then use an environment variable with a name pointing to the nested module fields.\nWhat it does is simply explodes your variable into nested models or dicts.\nSo if you define a variable FOO__BAR__BAZ=123 it will convert it into FOO={'BAR': {'BAZ': 123}} If you have multiple variables with the same structure they will be merged. Note Sub model has to inherit from pydantic.BaseModel , Otherwise pydantic-settings will initialize sub model,\ncollects values for sub model fields separately, and you may get unexpected results. As an example, given the following environment variables: # your environment export V0 = 0 export SUB_MODEL = '{\"v1\": \"json-1\", \"v2\": \"json-2\"}' export SUB_MODEL__V2 = nested-2 export SUB_MODEL__V3 = 3 export SUB_MODEL__DEEP__V4 = v4 You could load them into the following settings model: from pydantic import BaseModel from pydantic_settings import BaseSettings , SettingsConfigDict class DeepSubModel ( BaseModel ): # (1)! v4 : str class SubModel ( BaseModel ): # (2)! v1 : str v2 : bytes v3 : int deep : DeepSubModel class Settings ( BaseSettings ): model_config = SettingsConfigDict ( env_nested_delimiter = '__' ) v0 : str sub_model : SubModel print ( Settings () . model_dump ()) \"\"\" { 'v0': '0', 'sub_model': {'v1': 'json-1', 'v2': b'nested-2', 'v3': 3, 'deep': {'v4': 'v4'}}, } \"\"\" Sub model has to inherit from pydantic.BaseModel . Sub model has to inherit from pydantic.BaseModel . env_nested_delimiter can be configured via the model_config as shown above, or via the _env_nested_delimiter keyword argument on instantiation. By default environment variables are split by env_nested_delimiter into arbitrarily deep nested fields. You can limit\nthe depth of the nested fields with the env_nested_max_split config setting. A common use case this is particularly useful\nis for two-level deep settings, where the env_nested_delimiter (usually a single _ ) may be a substring of model\nfield names. For example: # your environment export GENERATION_LLM_PROVIDER = 'anthropic' export GENERATION_LLM_API_KEY = 'your-api-key' export GENERATION_LLM_API_VERSION = '2024-03-15' You could load them into the following settings model: from pydantic import BaseModel from pydantic_settings import BaseSettings , SettingsConfigDict class LLMConfig ( BaseModel ): provider : str = 'openai' api_key : str api_type : str = 'azure' api_version : str = '2023-03-15-preview' class GenerationConfig ( BaseSettings ): model_config = SettingsConfigDict ( env_nested_delimiter = '_' , env_nested_max_split = 1 , env_prefix = 'GENERATION_' ) llm : LLMConfig ... print ( GenerationConfig () . model_dump ()) \"\"\" { 'llm': { 'provider': 'anthropic', 'api_key': 'your-api-key', 'api_type': 'azure', 'api_version': '2024-03-15', } } \"\"\" Without env_nested_max_split=1 set, GENERATION_LLM_API_KEY would be parsed as llm.api.key instead of llm.api_key and it would raise a ValidationError . Nested environment variables take precedence over the top-level environment variable JSON\n(e.g. in the example above, SUB_MODEL__V2 trumps SUB_MODEL ). You may also populate a complex type by providing your own source class. import json import os from typing import Any from pydantic.fields import FieldInfo from pydantic_settings import ( BaseSettings , EnvSettingsSource , PydanticBaseSettingsSource , ) class MyCustomSource ( EnvSettingsSource ): def prepare_field_value ( self , field_name : str , field : FieldInfo , value : Any , value_is_complex : bool ) -> Any : if field_name == 'numbers' : return [ int ( x ) for x in value . split ( ',' )] return json . loads ( value ) class Settings ( BaseSettings ): numbers : list [ int ] @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: return ( MyCustomSource ( settings_cls ),) os . environ [ 'numbers' ] = '1,2,3' print ( Settings () . model_dump ()) #> {'numbers': [1, 2, 3]}","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#parsing-environment-variable-values","title":"Settings Management - Parsing environment variable values","objectID":"/latest/concepts/pydantic_settings/#parsing-environment-variable-values","rank":70},{"content":"pydantic-settings by default parses complex types from environment variables as JSON strings. If you want to disable\nthis behavior for a field and parse the value in your own validator, you can annotate the field with NoDecode : import os from typing import Annotated from pydantic import field_validator from pydantic_settings import BaseSettings , NoDecode class Settings ( BaseSettings ): numbers : Annotated [ list [ int ], NoDecode ] # (1)! @field_validator ( 'numbers' , mode = 'before' ) @classmethod def decode_numbers ( cls , v : str ) -> list [ int ]: return [ int ( x ) for x in v . split ( ',' )] os . environ [ 'numbers' ] = '1,2,3' print ( Settings () . model_dump ()) #> {'numbers': [1, 2, 3]} The NoDecode annotation disables JSON parsing for the numbers field. The decode_numbers field validator\n   will be called to parse the value. You can also disable JSON parsing for all fields by setting the enable_decoding config setting to False : import os from pydantic import field_validator from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( enable_decoding = False ) numbers : list [ int ] @field_validator ( 'numbers' , mode = 'before' ) @classmethod def decode_numbers ( cls , v : str ) -> list [ int ]: return [ int ( x ) for x in v . split ( ',' )] os . environ [ 'numbers' ] = '1,2,3' print ( Settings () . model_dump ()) #> {'numbers': [1, 2, 3]} You can force JSON parsing for a field by annotating it with ForceDecode .\nThis will bypass the enable_decoding config setting: import os from typing import Annotated from pydantic import field_validator from pydantic_settings import BaseSettings , ForceDecode , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( enable_decoding = False ) numbers : Annotated [ list [ int ], ForceDecode ] numbers1 : list [ int ] # (1)! @field_validator ( 'numbers1' , mode = 'before' ) @classmethod def decode_numbers1 ( cls , v : str ) -> list [ int ]: return [ int ( x ) for x in v . split ( ',' )] os . environ [ 'numbers' ] = '[\"1\",\"2\",\"3\"]' os . environ [ 'numbers1' ] = '1,2,3' print ( Settings () . model_dump ()) #> {'numbers': [1, 2, 3], 'numbers1': [1, 2, 3]} The numbers1 field is not annotated with ForceDecode , so it will not be parsed as JSON.\n   and we have to provide a custom validator to parse the value.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#disabling-json-parsing","title":"Settings Management - Parsing environment variable values - Disabling JSON parsing","objectID":"/latest/concepts/pydantic_settings/#disabling-json-parsing","rank":65},{"content":"By default, Pydantic settings does not allow partial updates to nested model default objects. This behavior can be\noverriden by setting the nested_model_default_partial_update flag to True , which will allow partial updates on\nnested model default object fields. import os from pydantic import BaseModel from pydantic_settings import BaseSettings , SettingsConfigDict class SubModel ( BaseModel ): val : int = 0 flag : bool = False class SettingsPartialUpdate ( BaseSettings ): model_config = SettingsConfigDict ( env_nested_delimiter = '__' , nested_model_default_partial_update = True ) nested_model : SubModel = SubModel ( val = 1 ) class SettingsNoPartialUpdate ( BaseSettings ): model_config = SettingsConfigDict ( env_nested_delimiter = '__' , nested_model_default_partial_update = False ) nested_model : SubModel = SubModel ( val = 1 ) # Apply a partial update to the default object using environment variables os . environ [ 'NESTED_MODEL__FLAG' ] = 'True' # When partial update is enabled, the existing SubModel instance is updated # with nested_model.flag=True change assert SettingsPartialUpdate () . model_dump () == { 'nested_model' : { 'val' : 1 , 'flag' : True } } # When partial update is disabled, a new SubModel instance is instantiated # with nested_model.flag=True change assert SettingsNoPartialUpdate () . model_dump () == { 'nested_model' : { 'val' : 0 , 'flag' : True } }","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#nested-model-default-partial-updates","title":"Settings Management - Nested model default partial updates","objectID":"/latest/concepts/pydantic_settings/#nested-model-default-partial-updates","rank":60},{"content":"Dotenv files (generally named .env ) are a common pattern that make it easy to use environment variables in a\nplatform-independent manner. A dotenv file follows the same general principles of all environment variables, and it looks like this: .env # ignore comment ENVIRONMENT = \"production\" REDIS_ADDRESS = localhost:6379 MEANING_OF_LIFE = 42 MY_VAR = 'Hello world' Once you have your .env file filled with variables, pydantic supports loading it in two ways: Setting the env_file (and env_file_encoding if you don't want the default encoding of your OS) on model_config in the BaseSettings class: from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( env_file = '.env' , env_file_encoding = 'utf-8' ) Instantiating the BaseSettings derived class with the _env_file keyword argument\n(and the _env_file_encoding if needed): from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( env_file = '.env' , env_file_encoding = 'utf-8' ) settings = Settings ( _env_file = 'prod.env' , _env_file_encoding = 'utf-8' ) In either case, the value of the passed argument can be any valid path or filename, either absolute or relative to the\ncurrent working directory. From there, pydantic will handle everything for you by loading in your variables and\nvalidating them. Note If a filename is specified for env_file , Pydantic will only check the current working directory and\nwon't check any parent directories for the .env file. Even when using a dotenv file, pydantic will still read environment variables as well as the dotenv file, environment variables will always take priority over values loaded from a dotenv file . Passing a file path via the _env_file keyword argument on instantiation (method 2) will override\nthe value (if any) set on the model_config class. If the above snippets were used in conjunction, prod.env would be loaded\nwhile .env would be ignored. If you need to load multiple dotenv files, you can pass multiple file paths as a tuple or list. The files will be\nloaded in order, with each file overriding the previous one. from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( # `.env.prod` takes priority over `.env` env_file = ( '.env' , '.env.prod' ) ) You can also use the keyword argument override to tell Pydantic not to load any file at all (even if one is set in\nthe model_config class) by passing None as the instantiation keyword argument, e.g. settings = Settings(_env_file=None) . Because python-dotenv is used to parse the file, bash-like semantics such as export can be used which\n(depending on your OS and environment) may allow your dotenv file to also be used with source ,\nsee python-dotenv's documentation for more details. Pydantic settings consider extra config in case of dotenv file. It means if you set the extra=forbid ( default )\non model_config and your dotenv file contains an entry for a field that is not defined in settings model,\nit will raise ValidationError in settings construction. For compatibility with pydantic 1.x BaseSettings you should use extra=ignore : from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( env_file = '.env' , extra = 'ignore' ) Note Pydantic settings loads all the values from dotenv file and passes it to the model, regardless of the model's env_prefix .\nSo if you provide extra values in a dotenv file, whether they start with env_prefix or not,\na ValidationError will be raised.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#dotenv-env-support","title":"Settings Management - Dotenv (.env) support","objectID":"/latest/concepts/pydantic_settings/#dotenv-env-support","rank":55},{"content":"Pydantic settings provides integrated CLI support, making it easy to quickly define CLI applications using Pydantic\nmodels. There are two primary use cases for Pydantic settings CLI: When using a CLI to override fields in Pydantic models. When using Pydantic models to define CLIs. By default, the experience is tailored towards use case #1 and builds on the foundations established in parsing\nenvironment variables . If your use case primarily falls into #2, you will likely\nwant to enable most of the defaults outlined at the end of creating CLI applications .","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#command-line-support","title":"Settings Management - Command Line Support","objectID":"/latest/concepts/pydantic_settings/#command-line-support","rank":50},{"content":"To get started, let's revisit the example presented in parsing environment\nvariables but using a Pydantic settings CLI: import sys from pydantic import BaseModel from pydantic_settings import BaseSettings , SettingsConfigDict class DeepSubModel ( BaseModel ): v4 : str class SubModel ( BaseModel ): v1 : str v2 : bytes v3 : int deep : DeepSubModel class Settings ( BaseSettings ): model_config = SettingsConfigDict ( cli_parse_args = True ) v0 : str sub_model : SubModel sys . argv = [ 'example.py' , '--v0=0' , '--sub_model={\"v1\": \"json-1\", \"v2\": \"json-2\"}' , '--sub_model.v2=nested-2' , '--sub_model.v3=3' , '--sub_model.deep.v4=v4' , ] print ( Settings () . model_dump ()) \"\"\" { 'v0': '0', 'sub_model': {'v1': 'json-1', 'v2': b'nested-2', 'v3': 3, 'deep': {'v4': 'v4'}}, } \"\"\" To enable CLI parsing, we simply set the cli_parse_args flag to a valid value, which retains similar connotations as\ndefined in argparse . Note that a CLI settings source is the topmost source by default unless its priority value\nis customised : import os import sys from pydantic_settings import ( BaseSettings , CliSettingsSource , PydanticBaseSettingsSource , ) class Settings ( BaseSettings ): my_foo : str @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: return env_settings , CliSettingsSource ( settings_cls , cli_parse_args = True ) os . environ [ 'MY_FOO' ] = 'from environment' sys . argv = [ 'example.py' , '--my_foo=from cli' ] print ( Settings () . model_dump ()) #> {'my_foo': 'from environment'} Lists ¶ CLI argument parsing of lists supports intermixing of any of the below three styles: JSON style --field='[1,2]' Argparse style --field 1 --field 2 Lazy style --field=1,2 import sys from pydantic_settings import BaseSettings class Settings ( BaseSettings , cli_parse_args = True ): my_list : list [ int ] sys . argv = [ 'example.py' , '--my_list' , '[1,2]' ] print ( Settings () . model_dump ()) #> {'my_list': [1, 2]} sys . argv = [ 'example.py' , '--my_list' , '1' , '--my_list' , '2' ] print ( Settings () . model_dump ()) #> {'my_list': [1, 2]} sys . argv = [ 'example.py' , '--my_list' , '1,2' ] print ( Settings () . model_dump ()) #> {'my_list': [1, 2]} Dictionaries ¶ CLI argument parsing of dictionaries supports intermixing of any of the below two styles: JSON style --field='{\"k1\": 1, \"k2\": 2}' Environment variable style --field k1=1 --field k2=2 These can be used in conjunction with list forms as well, e.g: --field k1=1,k2=2 --field k3=3 --field '{\"k4\": 4}' etc. import sys from pydantic_settings import BaseSettings class Settings ( BaseSettings , cli_parse_args = True ): my_dict : dict [ str , int ] sys . argv = [ 'example.py' , '--my_dict' , '{\"k1\":1,\"k2\":2}' ] print ( Settings () . model_dump ()) #> {'my_dict': {'k1': 1, 'k2': 2}} sys . argv = [ 'example.py' , '--my_dict' , 'k1=1' , '--my_dict' , 'k2=2' ] print ( Settings () . model_dump ()) #> {'my_dict': {'k1': 1, 'k2': 2}} Literals and Enums ¶ CLI argument parsing of literals and enums are converted into CLI choices. import sys from enum import IntEnum from typing import Literal from pydantic_settings import BaseSettings class Fruit ( IntEnum ): pear = 0 kiwi = 1 lime = 2 class Settings ( BaseSettings , cli_parse_args = True ): fruit : Fruit pet : Literal [ 'dog' , 'cat' , 'bird' ] sys . argv = [ 'example.py' , '--fruit' , 'lime' , '--pet' , 'cat' ] print ( Settings () . model_dump ()) #> {'fruit': <Fruit.lime: 2>, 'pet': 'cat'} Aliases ¶ Pydantic field aliases are added as CLI argument aliases. Aliases of length one are converted into short options. import sys from pydantic import AliasChoices , AliasPath , Field from pydantic_settings import BaseSettings class User ( BaseSettings , cli_parse_args = True ): first_name : str = Field ( validation_alias = AliasChoices ( 'f' , 'fname' , AliasPath ( 'name' , 0 )) ) last_name : str = Field ( validation_alias = AliasChoices ( 'l' , 'lname' , AliasPath ( 'name' , 1 )) ) sys . argv = [ 'example.py' , '--fname' , 'John' , '--lname' , 'Doe' ] print ( User () . model_dump ()) #> {'first_name': 'John', 'last_name': 'Doe'} sys . argv = [ 'example.py' , '-f' , 'John' , '-l' , 'Doe' ] print ( User () . model_dump ()) #> {'first_name': 'John', 'last_name': 'Doe'} sys . argv = [ 'example.py' , '--name' , 'John,Doe' ] print ( User () . model_dump ()) #> {'first_name': 'John', 'last_name': 'Doe'} sys . argv = [ 'example.py' , '--name' , 'John' , '--lname' , 'Doe' ] print ( User () . model_dump ()) #> {'first_name': 'John', 'last_name': 'Doe'}","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#the-basics","title":"Settings Management - Command Line Support - The Basics","objectID":"/latest/concepts/pydantic_settings/#the-basics","rank":45},{"content":"Subcommands and positional arguments are expressed using the CliSubCommand and CliPositionalArg annotations. The\nsubcommand annotation can only be applied to required fields (i.e. fields that do not have a default value).\nFurthermore, subcommands must be a valid type derived from either a pydantic BaseModel or pydantic.dataclasses dataclass . Parsed subcommands can be retrieved from model instances using the get_subcommand utility function. If a subcommand is\nnot required, set the is_required flag to False to disable raising an error if no subcommand is found. Note CLI settings subcommands are limited to a single subparser per model. In other words, all subcommands for a model\nare grouped under a single subparser; it does not allow for multiple subparsers with each subparser having its own\nset of subcommands. For more information on subparsers, see argparse\nsubcommands . Note CliSubCommand and CliPositionalArg are always case sensitive. import sys from pydantic import BaseModel from pydantic_settings import ( BaseSettings , CliPositionalArg , CliSubCommand , SettingsError , get_subcommand , ) class Init ( BaseModel ): directory : CliPositionalArg [ str ] class Clone ( BaseModel ): repository : CliPositionalArg [ str ] directory : CliPositionalArg [ str ] class Git ( BaseSettings , cli_parse_args = True , cli_exit_on_error = False ): clone : CliSubCommand [ Clone ] init : CliSubCommand [ Init ] # Run without subcommands sys . argv = [ 'example.py' ] cmd = Git () assert cmd . model_dump () == { 'clone' : None , 'init' : None } try : # Will raise an error since no subcommand was provided get_subcommand ( cmd ) . model_dump () except SettingsError as err : assert str ( err ) == 'Error: CLI subcommand is required {clone, init}' # Will not raise an error since subcommand is not required assert get_subcommand ( cmd , is_required = False ) is None # Run the clone subcommand sys . argv = [ 'example.py' , 'clone' , 'repo' , 'dest' ] cmd = Git () assert cmd . model_dump () == { 'clone' : { 'repository' : 'repo' , 'directory' : 'dest' }, 'init' : None , } # Returns the subcommand model instance (in this case, 'clone') assert get_subcommand ( cmd ) . model_dump () == { 'directory' : 'dest' , 'repository' : 'repo' , } The CliSubCommand and CliPositionalArg annotations also support union operations and aliases. For unions of Pydantic\nmodels, it is important to remember the nuances that can arise\nduring validation. Specifically, for unions of subcommands that are identical in content, it is recommended to break\nthem out into separate CliSubCommand fields to avoid any complications. Lastly, the derived subcommand names from\nunions will be the names of the Pydantic model classes themselves. When assigning aliases to CliSubCommand or CliPositionalArg fields, only a single alias can be assigned. For\nnon-union subcommands, aliasing will change the displayed help text and subcommand name. Conversely, for union\nsubcommands, aliasing will have no tangible effect from the perspective of the CLI settings source. Lastly, for\npositional arguments, aliasing will change the CLI help text displayed for the field. import sys from typing import Union from pydantic import BaseModel , Field from pydantic_settings import ( BaseSettings , CliPositionalArg , CliSubCommand , get_subcommand , ) class Alpha ( BaseModel ): \"\"\"Apha Help\"\"\" cmd_alpha : CliPositionalArg [ str ] = Field ( alias = 'alpha-cmd' ) class Beta ( BaseModel ): \"\"\"Beta Help\"\"\" opt_beta : str = Field ( alias = 'opt-beta' ) class Gamma ( BaseModel ): \"\"\"Gamma Help\"\"\" opt_gamma : str = Field ( alias = 'opt-gamma' ) class Root ( BaseSettings , cli_parse_args = True , cli_exit_on_error = False ): alpha_or_beta : CliSubCommand [ Union [ Alpha , Beta ]] = Field ( alias = 'alpha-or-beta-cmd' ) gamma : CliSubCommand [ Gamma ] = Field ( alias = 'gamma-cmd' ) sys . argv = [ 'example.py' , 'Alpha' , 'hello' ] assert get_subcommand ( Root ()) . model_dump () == { 'cmd_alpha' : 'hello' } sys . argv = [ 'example.py' , 'Beta' , '--opt-beta=hey' ] assert get_subcommand ( Root ()) . model_dump () == { 'opt_beta' : 'hey' } sys . argv = [ 'example.py' , 'gamma-cmd' , '--opt-gamma=hi' ] assert get_subcommand ( Root ()) . model_dump () == { 'opt_gamma' : 'hi' }","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#subcommands-and-positional-arguments","title":"Settings Management - Command Line Support - Subcommands and Positional Arguments","objectID":"/latest/concepts/pydantic_settings/#subcommands-and-positional-arguments","rank":40},{"content":"The CliApp class provides two utility methods, CliApp.run and CliApp.run_subcommand , that can be used to run a\nPydantic BaseSettings , BaseModel , or pydantic.dataclasses.dataclass as a CLI application. Primarily, the methods\nprovide structure for running cli_cmd methods associated with models. CliApp.run can be used in directly providing the cli_args to be parsed, and will run the model cli_cmd method (if\ndefined) after instantiation: from pydantic_settings import BaseSettings , CliApp class Settings ( BaseSettings ): this_foo : str def cli_cmd ( self ) -> None : # Print the parsed data print ( self . model_dump ()) #> {'this_foo': 'is such a foo'} # Update the parsed data showing cli_cmd ran self . this_foo = 'ran the foo cli cmd' s = CliApp . run ( Settings , cli_args = [ '--this_foo' , 'is such a foo' ]) print ( s . model_dump ()) #> {'this_foo': 'ran the foo cli cmd'} Similarly, the CliApp.run_subcommand can be used in recursive fashion to run the cli_cmd method of a subcommand: from pydantic import BaseModel from pydantic_settings import CliApp , CliPositionalArg , CliSubCommand class Init ( BaseModel ): directory : CliPositionalArg [ str ] def cli_cmd ( self ) -> None : print ( f 'git init \" { self . directory } \"' ) #> git init \"dir\" self . directory = 'ran the git init cli cmd' class Clone ( BaseModel ): repository : CliPositionalArg [ str ] directory : CliPositionalArg [ str ] def cli_cmd ( self ) -> None : print ( f 'git clone from \" { self . repository } \" into \" { self . directory } \"' ) self . directory = 'ran the clone cli cmd' class Git ( BaseModel ): clone : CliSubCommand [ Clone ] init : CliSubCommand [ Init ] def cli_cmd ( self ) -> None : CliApp . run_subcommand ( self ) cmd = CliApp . run ( Git , cli_args = [ 'init' , 'dir' ]) assert cmd . model_dump () == { 'clone' : None , 'init' : { 'directory' : 'ran the git init cli cmd' }, } Note Unlike CliApp.run , CliApp.run_subcommand requires the subcommand model to have a defined cli_cmd method. For BaseModel and pydantic.dataclasses.dataclass types, CliApp.run will internally use the following BaseSettings configuration defaults: nested_model_default_partial_update=True case_sensitive=True cli_hide_none_type=True cli_avoid_json=True cli_enforce_required=True cli_implicit_flags=True cli_kebab_case=True","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#creating-cli-applications","title":"Settings Management - Command Line Support - Creating CLI Applications","objectID":"/latest/concepts/pydantic_settings/#creating-cli-applications","rank":35},{"content":"Pydantic settings supports running asynchronous CLI commands via CliApp.run and CliApp.run_subcommand . With this feature, you can define async def methods within your Pydantic models (including subcommands) and have them executed just like their synchronous counterparts. Specifically: Asynchronous methods are supported: You can now mark your cli_cmd or similar CLI entrypoint methods as async def and have CliApp execute them. Subcommands may also be asynchronous: If you have nested CLI subcommands, the final (lowest-level) subcommand methods can likewise be asynchronous. Limit asynchronous methods to final subcommands: Defining parent commands as asynchronous is not recommended, because it can result in additional threads and event loops being created. For best performance and to avoid unnecessary resource usage, only implement your deepest (child) subcommands as async def. Below is a simple example demonstrating an asynchronous top-level command: from pydantic_settings import BaseSettings , CliApp class AsyncSettings ( BaseSettings ): async def cli_cmd ( self ) -> None : print ( 'Hello from an async CLI method!' ) #> Hello from an async CLI method! # If an event loop is already running, a new thread will be used; # otherwise, asyncio.run() is used to execute this async method. assert CliApp . run ( AsyncSettings , cli_args = []) . model_dump () == {} Asynchronous Subcommands ¶ As mentioned above, you can also define subcommands as async. However, only do so for the leaf (lowest-level) subcommand to avoid spawning new threads and event loops unnecessarily in parent commands: from pydantic import BaseModel from pydantic_settings import ( BaseSettings , CliApp , CliPositionalArg , CliSubCommand , ) class Clone ( BaseModel ): repository : CliPositionalArg [ str ] directory : CliPositionalArg [ str ] async def cli_cmd ( self ) -> None : # Perform async tasks here, e.g. network or I/O operations print ( f 'Cloning async from \" { self . repository } \" into \" { self . directory } \"' ) #> Cloning async from \"repo\" into \"dir\" class Git ( BaseSettings ): clone : CliSubCommand [ Clone ] def cli_cmd ( self ) -> None : # Run the final subcommand (clone/init). It is recommended to define async methods only at the deepest level. CliApp . run_subcommand ( self ) CliApp . run ( Git , cli_args = [ 'clone' , 'repo' , 'dir' ]) . model_dump () == { 'repository' : 'repo' , 'directory' : 'dir' , } When executing a subcommand with an asynchronous cli_cmd, Pydantic settings automatically detects whether the current thread already has an active event loop. If so, the async command is run in a fresh thread to avoid conflicts. Otherwise, it uses asyncio.run() in the current thread. This handling ensures your asynchronous subcommands \"just work\" without additional manual setup.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#asynchronous-cli-commands","title":"Settings Management - Command Line Support - Asynchronous CLI Commands","objectID":"/latest/concepts/pydantic_settings/#asynchronous-cli-commands","rank":30},{"content":"An instantiated Pydantic model can be serialized into its CLI arguments using the CliApp.serialize method. from pydantic import BaseModel from pydantic_settings import CliApp class Nested ( BaseModel ): that : int class Settings ( BaseModel ): this : str nested : Nested print ( CliApp . serialize ( Settings ( this = 'hello' , nested = Nested ( that = 123 )))) #> ['--this', 'hello', '--nested.that', '123']","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#serializing-cli-arguments","title":"Settings Management - Command Line Support - Serializing CLI Arguments","objectID":"/latest/concepts/pydantic_settings/#serializing-cli-arguments","rank":25},{"content":"CLI mutually exclusive groups can be created by inheriting from the CliMutuallyExclusiveGroup class. Note A CliMutuallyExclusiveGroup cannot be used in a union or contain nested models. from typing import Optional from pydantic import BaseModel from pydantic_settings import CliApp , CliMutuallyExclusiveGroup , SettingsError class Circle ( CliMutuallyExclusiveGroup ): radius : Optional [ float ] = None diameter : Optional [ float ] = None perimeter : Optional [ float ] = None class Settings ( BaseModel ): circle : Circle try : CliApp . run ( Settings , cli_args = [ '--circle.radius=1' , '--circle.diameter=2' ], cli_exit_on_error = False , ) except SettingsError as e : print ( e ) \"\"\" error parsing CLI: argument --circle.diameter: not allowed with argument --circle.radius \"\"\"","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#mutually-exclusive-groups","title":"Settings Management - Command Line Support - Mutually Exclusive Groups","objectID":"/latest/concepts/pydantic_settings/#mutually-exclusive-groups","rank":20},{"content":"The below flags can be used to customise the CLI experience to your needs. Change the Displayed Program Name ¶ Change the default program name displayed in the help text usage by setting cli_prog_name . By default, it will derive\nthe name of the currently executing program from sys.argv[0] , just like argparse. import sys from pydantic_settings import BaseSettings class Settings ( BaseSettings , cli_parse_args = True , cli_prog_name = 'appdantic' ): pass try : sys . argv = [ 'example.py' , '--help' ] Settings () except SystemExit as e : print ( e ) #> 0 \"\"\" usage: appdantic [-h] options: -h, --help  show this help message and exit \"\"\" CLI Boolean Flags ¶ Change whether boolean fields should be explicit or implicit by default using the cli_implicit_flags setting. By\ndefault, boolean fields are \"explicit\", meaning a boolean value must be explicitly provided on the CLI, e.g. --flag=True . Conversely, boolean fields that are \"implicit\" derive the value from the flag itself, e.g. --flag,--no-flag , which removes the need for an explicit value to be passed. Additionally, the provided CliImplicitFlag and CliExplicitFlag annotations can be used for more granular control\nwhen necessary. from pydantic_settings import BaseSettings , CliExplicitFlag , CliImplicitFlag class ExplicitSettings ( BaseSettings , cli_parse_args = True ): \"\"\"Boolean fields are explicit by default.\"\"\" explicit_req : bool \"\"\" --explicit_req bool   (required) \"\"\" explicit_opt : bool = False \"\"\" --explicit_opt bool   (default: False) \"\"\" # Booleans are explicit by default, so must override implicit flags with annotation implicit_req : CliImplicitFlag [ bool ] \"\"\" --implicit_req, --no-implicit_req (required) \"\"\" implicit_opt : CliImplicitFlag [ bool ] = False \"\"\" --implicit_opt, --no-implicit_opt (default: False) \"\"\" class ImplicitSettings ( BaseSettings , cli_parse_args = True , cli_implicit_flags = True ): \"\"\"With cli_implicit_flags=True, boolean fields are implicit by default.\"\"\" # Booleans are implicit by default, so must override explicit flags with annotation explicit_req : CliExplicitFlag [ bool ] \"\"\" --explicit_req bool   (required) \"\"\" explicit_opt : CliExplicitFlag [ bool ] = False \"\"\" --explicit_opt bool   (default: False) \"\"\" implicit_req : bool \"\"\" --implicit_req, --no-implicit_req (required) \"\"\" implicit_opt : bool = False \"\"\" --implicit_opt, --no-implicit_opt (default: False) \"\"\" Ignore and Retrieve Unknown Arguments ¶ Change whether to ignore unknown CLI arguments and only parse known ones using cli_ignore_unknown_args . By default, the CLI\ndoes not ignore any args. Ignored arguments can then be retrieved using the CliUnknownArgs annotation. import sys from pydantic_settings import BaseSettings , CliUnknownArgs class Settings ( BaseSettings , cli_parse_args = True , cli_ignore_unknown_args = True ): good_arg : str ignored_args : CliUnknownArgs sys . argv = [ 'example.py' , '--bad-arg=bad' , 'ANOTHER_BAD_ARG' , '--good_arg=hello world' ] print ( Settings () . model_dump ()) #> {'good_arg': 'hello world', 'ignored_args': ['--bad-arg=bad', 'ANOTHER_BAD_ARG']} CLI Kebab Case for Arguments ¶ Change whether CLI arguments should use kebab case by enabling cli_kebab_case . import sys from pydantic import Field from pydantic_settings import BaseSettings class Settings ( BaseSettings , cli_parse_args = True , cli_kebab_case = True ): my_option : str = Field ( description = 'will show as kebab case on CLI' ) try : sys . argv = [ 'example.py' , '--help' ] Settings () except SystemExit as e : print ( e ) #> 0 \"\"\" usage: example.py [-h] [--my-option str] options: -h, --help       show this help message and exit --my-option str  will show as kebab case on CLI (required) \"\"\" Change Whether CLI Should Exit on Error ¶ Change whether the CLI internal parser will exit on error or raise a SettingsError exception by using cli_exit_on_error . By default, the CLI internal parser will exit on error. import sys from pydantic_settings import BaseSettings , SettingsError class Settings ( BaseSettings , cli_parse_args = True , cli_exit_on_error = False ): ... try : sys . argv = [ 'example.py' , '--bad-arg' ] Settings () except SettingsError as e : print ( e ) #> error parsing CLI: unrecognized arguments: --bad-arg Enforce Required Arguments at CLI ¶ Pydantic settings is designed to pull values in from various sources when instantating a model. This means a field that\nis required is not strictly required from any single source (e.g. the CLI). Instead, all that matters is that one of the\nsources provides the required value. However, if your use case aligns more with #2 , using Pydantic models to define CLIs, you will\nlikely want required fields to be strictly required at the CLI . We can enable this behavior by using cli_enforce_required . Note A required CliPositionalArg field is always strictly required (enforced) at the CLI. import os import sys from pydantic import Field from pydantic_settings import BaseSettings , SettingsError class Settings ( BaseSettings , cli_parse_args = True , cli_enforce_required = True , cli_exit_on_error = False , ): my_required_field : str = Field ( description = 'a top level required field' ) os . environ [ 'MY_REQUIRED_FIELD' ] = 'hello from environment' try : sys . argv = [ 'example.py' ] Settings () except SettingsError as e : print ( e ) #> error parsing CLI: the following arguments are required: --my_required_field Change the None Type Parse String ¶ Change the CLI string value that will be parsed (e.g. \"null\", \"void\", \"None\", etc.) into None by setting cli_parse_none_str . By default it will use the env_parse_none_str value if set. Otherwise, it will default to \"null\"\nif cli_avoid_json is False , and \"None\" if cli_avoid_json is True . import sys from typing import Optional from pydantic import Field from pydantic_settings import BaseSettings class Settings ( BaseSettings , cli_parse_args = True , cli_parse_none_str = 'void' ): v1 : Optional [ int ] = Field ( description = 'the top level v0 option' ) sys . argv = [ 'example.py' , '--v1' , 'void' ] print ( Settings () . model_dump ()) #> {'v1': None} Hide None Type Values ¶ Hide None values from the CLI help text by enabling cli_hide_none_type . import sys from typing import Optional from pydantic import Field from pydantic_settings import BaseSettings class Settings ( BaseSettings , cli_parse_args = True , cli_hide_none_type = True ): v0 : Optional [ str ] = Field ( description = 'the top level v0 option' ) try : sys . argv = [ 'example.py' , '--help' ] Settings () except SystemExit as e : print ( e ) #> 0 \"\"\" usage: example.py [-h] [--v0 str] options: -h, --help  show this help message and exit --v0 str    the top level v0 option (required) \"\"\" Avoid Adding JSON CLI Options ¶ Avoid adding complex fields that result in JSON strings at the CLI by enabling cli_avoid_json . import sys from pydantic import BaseModel , Field from pydantic_settings import BaseSettings class SubModel ( BaseModel ): v1 : int = Field ( description = 'the sub model v1 option' ) class Settings ( BaseSettings , cli_parse_args = True , cli_avoid_json = True ): sub_model : SubModel = Field ( description = 'The help summary for SubModel related options' ) try : sys . argv = [ 'example.py' , '--help' ] Settings () except SystemExit as e : print ( e ) #> 0 \"\"\" usage: example.py [-h] [--sub_model.v1 int] options: -h, --help          show this help message and exit sub_model options: The help summary for SubModel related options --sub_model.v1 int  the sub model v1 option (required) \"\"\" Use Class Docstring for Group Help Text ¶ By default, when populating the group help text for nested models it will pull from the field descriptions.\nAlternatively, we can also configure CLI settings to pull from the class docstring instead. Note If the field is a union of nested models the group help text will always be pulled from the field description;\neven if cli_use_class_docs_for_groups is set to True . import sys from pydantic import BaseModel , Field from pydantic_settings import BaseSettings class SubModel ( BaseModel ): \"\"\"The help text from the class docstring.\"\"\" v1 : int = Field ( description = 'the sub model v1 option' ) class Settings ( BaseSettings , cli_parse_args = True , cli_use_class_docs_for_groups = True ): \"\"\"My application help text.\"\"\" sub_model : SubModel = Field ( description = 'The help text from the field description' ) try : sys . argv = [ 'example.py' , '--help' ] Settings () except SystemExit as e : print ( e ) #> 0 \"\"\" usage: example.py [-h] [--sub_model JSON] [--sub_model.v1 int] My application help text. options: -h, --help          show this help message and exit sub_model options: The help text from the class docstring. --sub_model JSON    set sub_model from JSON string --sub_model.v1 int  the sub model v1 option (required) \"\"\" Change the CLI Flag Prefix Character ¶ Change The CLI flag prefix character used in CLI optional arguments by settings cli_flag_prefix_char . import sys from pydantic import AliasChoices , Field from pydantic_settings import BaseSettings class Settings ( BaseSettings , cli_parse_args = True , cli_flag_prefix_char = '+' ): my_arg : str = Field ( validation_alias = AliasChoices ( 'm' , 'my-arg' )) sys . argv = [ 'example.py' , '++my-arg' , 'hi' ] print ( Settings () . model_dump ()) #> {'my_arg': 'hi'} sys . argv = [ 'example.py' , '+m' , 'hi' ] print ( Settings () . model_dump ()) #> {'my_arg': 'hi'} Suppressing Fields from CLI Help Text ¶ To suppress a field from the CLI help text, the CliSuppress annotation can be used for field types, or the CLI_SUPPRESS string constant can be used for field descriptions. import sys from pydantic import Field from pydantic_settings import CLI_SUPPRESS , BaseSettings , CliSuppress class Settings ( BaseSettings , cli_parse_args = True ): \"\"\"Suppress fields from CLI help text.\"\"\" field_a : CliSuppress [ int ] = 0 field_b : str = Field ( default = 1 , description = CLI_SUPPRESS ) try : sys . argv = [ 'example.py' , '--help' ] Settings () except SystemExit as e : print ( e ) #> 0 \"\"\" usage: example.py [-h] Suppress fields from CLI help text. options: -h, --help          show this help message and exit \"\"\" CLI Shortcuts for Arguments ¶ Add alternative CLI argument names (shortcuts) for fields using the cli_shortcuts option in SettingsConfigDict . This allows you to define additional names for CLI arguments, which can be especially useful for providing more user-friendly or shorter aliases for deeply nested or verbose field names. The cli_shortcuts option takes a dictionary mapping the target field name (using dot notation for nested fields) to one or more shortcut names. If multiple fields share the same shortcut, the first matching field will take precedence. Flat Example: from pydantic import Field from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): option : str = Field ( default = 'foo' ) list_option : str = Field ( default = 'fizz' ) model_config = SettingsConfigDict ( cli_shortcuts = { 'option' : 'option2' , 'list_option' : [ 'list_option2' ]} ) # Now you can use the shortcuts on the CLI: # --option2 sets 'option', --list_option2 sets 'list_option' Nested Example: from pydantic import BaseModel , Field from pydantic_settings import BaseSettings , SettingsConfigDict class TwiceNested ( BaseModel ): option : str = Field ( default = 'foo' ) class Nested ( BaseModel ): twice_nested_option : TwiceNested = TwiceNested () option : str = Field ( default = 'foo' ) class Settings ( BaseSettings ): nested : Nested = Nested () model_config = SettingsConfigDict ( cli_shortcuts = { 'nested.option' : 'option2' , 'nested.twice_nested_option.option' : 'twice_nested_option' , } ) # Now you can use --option2 to set nested.option and --twice_nested_option to set nested.twice_nested_option.option If a shortcut collides (is mapped to multiple fields), it will apply to the first matching field in the model.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#customizing-the-cli-experience","title":"Settings Management - Command Line Support - Customizing the CLI Experience","objectID":"/latest/concepts/pydantic_settings/#customizing-the-cli-experience","rank":15},{"content":"A CLI settings source can be integrated with existing parsers by overriding the default CLI settings source with a user\ndefined one that specifies the root_parser object. import sys from argparse import ArgumentParser from pydantic_settings import BaseSettings , CliApp , CliSettingsSource parser = ArgumentParser () parser . add_argument ( '--food' , choices = [ 'pear' , 'kiwi' , 'lime' ]) class Settings ( BaseSettings ): name : str = 'Bob' # Set existing `parser` as the `root_parser` object for the user defined settings source cli_settings = CliSettingsSource ( Settings , root_parser = parser ) # Parse and load CLI settings from the command line into the settings source. sys . argv = [ 'example.py' , '--food' , 'kiwi' , '--name' , 'waldo' ] s = CliApp . run ( Settings , cli_settings_source = cli_settings ) print ( s . model_dump ()) #> {'name': 'waldo'} # Load CLI settings from pre-parsed arguments. i.e., the parsing occurs elsewhere and we # just need to load the pre-parsed args into the settings source. parsed_args = parser . parse_args ([ '--food' , 'kiwi' , '--name' , 'ralph' ]) s = CliApp . run ( Settings , cli_args = parsed_args , cli_settings_source = cli_settings ) print ( s . model_dump ()) #> {'name': 'ralph'} A CliSettingsSource connects with a root_parser object by using parser methods to add settings_cls fields as\ncommand line arguments. The CliSettingsSource internal parser representation is based on the argparse library, and\ntherefore, requires parser methods that support the same attributes as their argparse counterparts. The available\nparser methods that can be customised, along with their argparse counterparts (the defaults), are listed below: parse_args_method - ( argparse.ArgumentParser.parse_args ) add_argument_method - ( argparse.ArgumentParser.add_argument ) add_argument_group_method - ( argparse.ArgumentParser.add_argument_group ) add_parser_method - ( argparse._SubParsersAction.add_parser ) add_subparsers_method - ( argparse.ArgumentParser.add_subparsers ) formatter_class - ( argparse.RawDescriptionHelpFormatter ) For a non-argparse parser the parser methods can be set to None if not supported. The CLI settings will only raise an\nerror when connecting to the root parser if a parser method is necessary but set to None . Note The formatter_class is only applied to subcommands. The CliSettingsSource never touches or modifies any of the\nexternal parser settings to avoid breaking changes. Since subcommands reside on their own internal parser trees, we\ncan safely apply the formatter_class settings without breaking the external parser logic.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#integrating-with-existing-parsers","title":"Settings Management - Command Line Support - Integrating with Existing Parsers","objectID":"/latest/concepts/pydantic_settings/#integrating-with-existing-parsers","rank":10},{"content":"Placing secret values in files is a common pattern to provide sensitive configuration to an application. A secret file follows the same principal as a dotenv file except it only contains a single value and the file name\nis used as the key. A secret file will look like the following: /var/run/database_password super_secret_database_password Once you have your secret files, pydantic supports loading it in two ways: Setting the secrets_dir on model_config in a BaseSettings class to the directory where your secret files are stored. from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( secrets_dir = '/var/run' ) database_password : str Instantiating the BaseSettings derived class with the _secrets_dir keyword argument: settings = Settings(_secrets_dir='/var/run') In either case, the value of the passed argument can be any valid directory, either absolute or relative to the\ncurrent working directory. Note that a non existent directory will only generate a warning .\nFrom there, pydantic will handle everything for you by loading in your variables and validating them. Even when using a secrets directory, pydantic will still read environment variables from a dotenv file or the environment, a dotenv file and environment variables will always take priority over values loaded from the secrets directory . Passing a file path via the _secrets_dir keyword argument on instantiation (method 2) will override\nthe value (if any) set on the model_config class. If you need to load settings from multiple secrets directories, you can pass multiple paths as a tuple or list. Just like for env_file , values from subsequent paths override previous ones. from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    # files in '/run/secrets' take priority over '/var/run'\n    model_config = SettingsConfigDict(secrets_dir=('/var/run', '/run/secrets'))\n\n    database_password: str If any of secrets_dir is missing, it is ignored, and warning is shown. If any of secrets_dir is a file, error is raised.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#secrets","title":"Settings Management - Secrets","objectID":"/latest/concepts/pydantic_settings/#secrets","rank":5},{"content":"Docker Secrets can be used to provide sensitive configuration to an application running in a Docker container.\nTo use these secrets in a pydantic application the process is simple. More information regarding creating, managing\nand using secrets in Docker see the official Docker documentation . First, define your Settings class with a SettingsConfigDict that specifies the secrets directory. from pydantic_settings import BaseSettings , SettingsConfigDict class Settings ( BaseSettings ): model_config = SettingsConfigDict ( secrets_dir = '/run/secrets' ) my_secret_data : str Note By default Docker uses /run/secrets as the target mount point. If you want to use a different location, change Config.secrets_dir accordingly. Then, create your secret via the Docker CLI printf \"This is a secret\" | docker secret create my_secret_data - Last, run your application inside a Docker container and supply your newly created secret docker service create --name pydantic-with-secrets --secret my_secret_data pydantic-app:latest","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#use-case-docker-secrets","title":"Settings Management - Secrets - Use Case: Docker Secrets","objectID":"/latest/concepts/pydantic_settings/#use-case-docker-secrets","rank":0},{"content":"You must set one parameter: secret_id : The AWS secret id You must have the same naming convention in the key value in secret as in the field name. For example, if the key in secret is named SqlServerPassword , the field name must be the same. You can use an alias too. In AWS Secrets Manager, nested models are supported with the -- separator in the key name. For example, SqlServer--Password . Arrays (e.g. MySecret--0 , MySecret--1 ) are not supported. import os from pydantic import BaseModel from pydantic_settings import ( AWSSecretsManagerSettingsSource , BaseSettings , PydanticBaseSettingsSource , ) class SubModel ( BaseModel ): a : str class AWSSecretsManagerSettings ( BaseSettings ): foo : str bar : int sub : SubModel @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: aws_secrets_manager_settings = AWSSecretsManagerSettingsSource ( settings_cls , os . environ [ 'AWS_SECRETS_MANAGER_SECRET_ID' ], ) return ( init_settings , env_settings , dotenv_settings , file_secret_settings , aws_secrets_manager_settings , )","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#aws-secrets-manager","title":"Settings Management - AWS Secrets Manager","objectID":"/latest/concepts/pydantic_settings/#aws-secrets-manager","rank":-5},{"content":"You must set two parameters: url : For example, https://my-resource.vault.azure.net/ . credential : If you use DefaultAzureCredential , in local you can execute az login to get your identity credentials. The identity must have a role assignment (the recommended one is Key Vault Secrets User ), so you can access the secrets. You must have the same naming convention in the field name as in the Key Vault secret name. For example, if the secret is named SqlServerPassword , the field name must be the same. You can use an alias too. In Key Vault, nested models are supported with the -- separator. For example, SqlServer--Password . Key Vault arrays (e.g. MySecret--0 , MySecret--1 ) are not supported. import os from azure.identity import DefaultAzureCredential from pydantic import BaseModel from pydantic_settings import ( AzureKeyVaultSettingsSource , BaseSettings , PydanticBaseSettingsSource , ) class SubModel ( BaseModel ): a : str class AzureKeyVaultSettings ( BaseSettings ): foo : str bar : int sub : SubModel @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: az_key_vault_settings = AzureKeyVaultSettingsSource ( settings_cls , os . environ [ 'AZURE_KEY_VAULT_URL' ], DefaultAzureCredential (), ) return ( init_settings , env_settings , dotenv_settings , file_secret_settings , az_key_vault_settings , )","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#azure-key-vault","title":"Settings Management - Azure Key Vault","objectID":"/latest/concepts/pydantic_settings/#azure-key-vault","rank":-10},{"content":"The Azure Key Vault source accepts a dash_to_underscore option, disabled by default, to support Key Vault kebab-case secret names by mapping them to Python's snake_case field names. When enabled, dashes ( - ) in secret names are mapped to underscores ( _ ) in field names during validation. This mapping applies only to field names , not to aliases. import os from azure.identity import DefaultAzureCredential from pydantic import Field from pydantic_settings import ( AzureKeyVaultSettingsSource , BaseSettings , PydanticBaseSettingsSource , ) class AzureKeyVaultSettings ( BaseSettings ): field_with_underscore : str field_with_alias : str = Field ( ... , alias = 'Alias-With-Dashes' ) @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: az_key_vault_settings = AzureKeyVaultSettingsSource ( settings_cls , os . environ [ 'AZURE_KEY_VAULT_URL' ], DefaultAzureCredential (), dash_to_underscore = True , ) return ( az_key_vault_settings ,) This setup will load Azure Key Vault secrets named field-with-underscore and Alias-With-Dashes , mapping them to the field_with_underscore and field_with_alias fields, respectively. Tip Alternatively, you can configure an alias_generator to map PascalCase secrets.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#dash-to-underscore-mapping","title":"Settings Management - Azure Key Vault - Dash to underscore mapping","objectID":"/latest/concepts/pydantic_settings/#dash-to-underscore-mapping","rank":-15},{"content":"Google Cloud Secret Manager allows you to store, manage, and access sensitive information as secrets in Google Cloud Platform. This integration lets you retrieve secrets directly from GCP Secret Manager for use in your Pydantic settings.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#google-cloud-secret-manager","title":"Settings Management - Google Cloud Secret Manager","objectID":"/latest/concepts/pydantic_settings/#google-cloud-secret-manager","rank":-20},{"content":"The Google Cloud Secret Manager integration requires additional dependencies: pip install \"pydantic-settings[gcp-secret-manager]\"","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#installation_1","title":"Settings Management - Google Cloud Secret Manager - Installation","objectID":"/latest/concepts/pydantic_settings/#installation_1","rank":-25},{"content":"To use Google Cloud Secret Manager, you need to: Create a GoogleSecretManagerSettingsSource . (See GCP Authentication for authentication options.) Add this source to your settings customization pipeline from pydantic import BaseModel from pydantic_settings import ( BaseSettings , GoogleSecretManagerSettingsSource , PydanticBaseSettingsSource , SettingsConfigDict , ) class Database ( BaseModel ): password : str user : str class Settings ( BaseSettings ): database : Database model_config = SettingsConfigDict ( env_nested_delimiter = '__' ) @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: # Create the GCP Secret Manager settings source gcp_settings = GoogleSecretManagerSettingsSource ( settings_cls , # If not provided, will use google.auth.default() # to get credentials from the environemnt # credentials=your_credentials, # If not provided, will use google.auth.default() # to get project_id from the environemnt project_id = 'your-gcp-project-id' , ) return ( init_settings , env_settings , dotenv_settings , file_secret_settings , gcp_settings , )","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#basic-usage","title":"Settings Management - Google Cloud Secret Manager - Basic Usage","objectID":"/latest/concepts/pydantic_settings/#basic-usage","rank":-30},{"content":"The GoogleSecretManagerSettingsSource supports several authentication methods: Default credentials - If you don't provide credentials or project ID, it will use google.auth.default() to obtain them. This works with: Service account credentials from GOOGLE_APPLICATION_CREDENTIALS environment variable User credentials from gcloud auth application-default login Compute Engine, GKE, Cloud Run, or Cloud Functions default service accounts Explicit credentials - You can also provide credentials directly. e.g. sa_credentials = google.oauth2.service_account.Credentials.from_service_account_file('path/to/service-account.json') and then GoogleSecretManagerSettingsSource(credentials=sa_credentials)","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#gcp-authentication","title":"Settings Management - Google Cloud Secret Manager - GCP Authentication","objectID":"/latest/concepts/pydantic_settings/#gcp-authentication","rank":-35},{"content":"For nested models, Secret Manager supports the env_nested_delimiter setting as long as it complies with the naming rules . In the example above, you would create secrets named database__password and database__user in Secret Manager.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#nested-models","title":"Settings Management - Google Cloud Secret Manager - Nested Models","objectID":"/latest/concepts/pydantic_settings/#nested-models","rank":-40},{"content":"Case Sensitivity : By default, secret names are case-sensitive. Secret Naming : Create secrets in Google Secret Manager with names that match your field names (including any prefix). According the Secret Manager documentation , a secret name can contain uppercase and lowercase letters, numerals, hyphens, and underscores. The maximum allowed length for a name is 255 characters. Secret Versions : The GoogleSecretManagerSettingsSource uses the \"latest\" version of secrets. For more details on creating and managing secrets in Google Cloud Secret Manager, see the official Google Cloud documentation .","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#important-notes","title":"Settings Management - Google Cloud Secret Manager - Important Notes","objectID":"/latest/concepts/pydantic_settings/#important-notes","rank":-45},{"content":"Other settings sources are available for common configuration files: JsonConfigSettingsSource using json_file and json_file_encoding arguments PyprojectTomlConfigSettingsSource using (optional) pyproject_toml_depth and (optional) pyproject_toml_table_header arguments TomlConfigSettingsSource using toml_file argument YamlConfigSettingsSource using yaml_file and yaml_file_encoding arguments You can also provide multiple files by providing a list of path: toml_file = [ 'config.default.toml' , 'config.custom.toml' ] To use them, you can use the same mechanism described here from pydantic import BaseModel from pydantic_settings import ( BaseSettings , PydanticBaseSettingsSource , SettingsConfigDict , TomlConfigSettingsSource , ) class Nested ( BaseModel ): nested_field : str class Settings ( BaseSettings ): foobar : str nested : Nested model_config = SettingsConfigDict ( toml_file = 'config.toml' ) @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: return ( TomlConfigSettingsSource ( settings_cls ),) This will be able to read the following \"config.toml\" file, located in your working directory: foobar = \"Hello\" [nested] nested_field = \"world!\"","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#other-settings-source","title":"Settings Management - Other settings source","objectID":"/latest/concepts/pydantic_settings/#other-settings-source","rank":-50},{"content":"\"pyproject.toml\" is a standardized file for providing configuration values in Python projects. PEP 518 defines a [tool] table that can be used to provide arbitrary tool configuration.\nWhile encouraged to use the [tool] table, PyprojectTomlConfigSettingsSource can be used to load variables from any location with in \"pyproject.toml\" file. This is controlled by providing SettingsConfigDict(pyproject_toml_table_header=tuple[str, ...]) where the value is a tuple of header parts.\nBy default, pyproject_toml_table_header=('tool', 'pydantic-settings') which will load variables from the [tool.pydantic-settings] table. from pydantic_settings import (\n    BaseSettings,\n    PydanticBaseSettingsSource,\n    PyprojectTomlConfigSettingsSource,\n    SettingsConfigDict,\n)\n\n\nclass Settings(BaseSettings):\n    \"\"\"Example loading values from the table used by default.\"\"\"\n\n    field: str\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -> tuple[PydanticBaseSettingsSource, ...]:\n        return (PyprojectTomlConfigSettingsSource(settings_cls),)\n\n\nclass SomeTableSettings(Settings):\n    \"\"\"Example loading values from a user defined table.\"\"\"\n\n    model_config = SettingsConfigDict(\n        pyproject_toml_table_header=('tool', 'some-table')\n    )\n\n\nclass RootSettings(Settings):\n    \"\"\"Example loading values from the root of a pyproject.toml file.\"\"\"\n\n    model_config = SettingsConfigDict(extra='ignore', pyproject_toml_table_header=()) This will be able to read the following \"pyproject.toml\" file, located in your working directory, resulting in Settings(field='default-table') , SomeTableSettings(field='some-table') , & RootSettings(field='root') : field = \"root\" [tool.pydantic-settings] field = \"default-table\" [tool.some-table] field = \"some-table\" By default, PyprojectTomlConfigSettingsSource will only look for a \"pyproject.toml\" in the your current working directory.\nHowever, there are two options to change this behavior. SettingsConfigDict(pyproject_toml_depth=<int>) can be provided to check <int> number of directories up in the directory tree for a \"pyproject.toml\" if one is not found in the current working directory.\n  By default, no parent directories are checked. An explicit file path can be provided to the source when it is instantiated (e.g. PyprojectTomlConfigSettingsSource(settings_cls, Path('~/.config').resolve() / 'pyproject.toml') ).\n  If a file path is provided this way, it will be treated as absolute (no other locations are checked). from pathlib import Path\n\nfrom pydantic_settings import (\n    BaseSettings,\n    PydanticBaseSettingsSource,\n    PyprojectTomlConfigSettingsSource,\n    SettingsConfigDict,\n)\n\n\nclass DiscoverSettings(BaseSettings):\n    \"\"\"Example of discovering a pyproject.toml in parent directories in not in `Path.cwd()`.\"\"\"\n\n    model_config = SettingsConfigDict(pyproject_toml_depth=2)\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -> tuple[PydanticBaseSettingsSource, ...]:\n        return (PyprojectTomlConfigSettingsSource(settings_cls),)\n\n\nclass ExplicitFilePathSettings(BaseSettings):\n    \"\"\"Example of explicitly providing the path to the file to load.\"\"\"\n\n    field: str\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,\n        file_secret_settings: PydanticBaseSettingsSource,\n    ) -> tuple[PydanticBaseSettingsSource, ...]:\n        return (\n            PyprojectTomlConfigSettingsSource(\n                settings_cls, Path('~/.config').resolve() / 'pyproject.toml'\n            ),\n        )","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#pyprojecttoml","title":"Settings Management - Other settings source - pyproject.toml","objectID":"/latest/concepts/pydantic_settings/#pyprojecttoml","rank":-55},{"content":"In the case where a value is specified for the same Settings field in multiple ways,\nthe selected value is determined as follows (in descending order of priority): If cli_parse_args is enabled, arguments passed in at the CLI. Arguments passed to the Settings class initialiser. Environment variables, e.g. my_prefix_special_function as described above. Variables loaded from a dotenv ( .env ) file. Variables loaded from the secrets directory. The default field values for the Settings model.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#field-value-priority","title":"Settings Management - Field value priority","objectID":"/latest/concepts/pydantic_settings/#field-value-priority","rank":-60},{"content":"If the default order of priority doesn't match your needs, it's possible to change it by overriding\nthe settings_customise_sources method of your Settings . settings_customise_sources takes four callables as arguments and returns any number of callables as a tuple.\nIn turn these callables are called to build the inputs to the fields of the settings class. Each callable should take an instance of the settings class as its sole argument and return a dict .","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#customise-settings-sources","title":"Settings Management - Customise settings sources","objectID":"/latest/concepts/pydantic_settings/#customise-settings-sources","rank":-65},{"content":"The order of the returned callables decides the priority of inputs; first item is the highest priority. from pydantic import PostgresDsn from pydantic_settings import BaseSettings , PydanticBaseSettingsSource class Settings ( BaseSettings ): database_dsn : PostgresDsn @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: return env_settings , init_settings , file_secret_settings print ( Settings ( database_dsn = 'postgres://postgres@localhost:5432/kwargs_db' )) #> database_dsn=PostgresDsn('postgres://postgres@localhost:5432/kwargs_db') By flipping env_settings and init_settings , environment variables now have precedence over __init__ kwargs.","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#changing-priority","title":"Settings Management - Customise settings sources - Changing Priority","objectID":"/latest/concepts/pydantic_settings/#changing-priority","rank":-70},{"content":"As explained earlier, pydantic ships with multiples built-in settings sources. However, you may occasionally\nneed to add your own custom sources, settings_customise_sources makes this very easy: import json from pathlib import Path from typing import Any from pydantic.fields import FieldInfo from pydantic_settings import ( BaseSettings , PydanticBaseSettingsSource , SettingsConfigDict , ) class JsonConfigSettingsSource ( PydanticBaseSettingsSource ): \"\"\" A simple settings source class that loads variables from a JSON file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading `config.json` \"\"\" def get_field_value ( self , field : FieldInfo , field_name : str ) -> tuple [ Any , str , bool ]: encoding = self . config . get ( 'env_file_encoding' ) file_content_json = json . loads ( Path ( 'tests/example_test_config.json' ) . read_text ( encoding ) ) field_value = file_content_json . get ( field_name ) return field_value , field_name , False def prepare_field_value ( self , field_name : str , field : FieldInfo , value : Any , value_is_complex : bool ) -> Any : return value def __call__ ( self ) -> dict [ str , Any ]: d : dict [ str , Any ] = {} for field_name , field in self . settings_cls . model_fields . items (): field_value , field_key , value_is_complex = self . get_field_value ( field , field_name ) field_value = self . prepare_field_value ( field_name , field , field_value , value_is_complex ) if field_value is not None : d [ field_key ] = field_value return d class Settings ( BaseSettings ): model_config = SettingsConfigDict ( env_file_encoding = 'utf-8' ) foobar : str @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: return ( init_settings , JsonConfigSettingsSource ( settings_cls ), env_settings , file_secret_settings , ) print ( Settings ()) #> foobar='test' Accessing the result of previous sources ¶ Each source of settings can access the output of the previous ones. from typing import Any\n\nfrom pydantic.fields import FieldInfo\n\nfrom pydantic_settings import PydanticBaseSettingsSource\n\n\nclass MyCustomSource(PydanticBaseSettingsSource):\n    def get_field_value(\n        self, field: FieldInfo, field_name: str\n    ) -> tuple[Any, str, bool]: ...\n\n    def __call__(self) -> dict[str, Any]:\n        # Retrieve the aggregated settings from previous sources\n        current_state = self.current_state\n        current_state.get('some_setting')\n\n        # Retrive settings from all sources individually\n        # self.settings_sources_data[\"SettingsSourceName\"]: dict[str, Any]\n        settings_sources_data = self.settings_sources_data\n        settings_sources_data['SomeSettingsSource'].get('some_setting')\n\n        # Your code here...","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#adding-sources","title":"Settings Management - Customise settings sources - Adding sources","objectID":"/latest/concepts/pydantic_settings/#adding-sources","rank":-75},{"content":"You might also want to disable a source: from pydantic import ValidationError from pydantic_settings import BaseSettings , PydanticBaseSettingsSource class Settings ( BaseSettings ): my_api_key : str @classmethod def settings_customise_sources ( cls , settings_cls : type [ BaseSettings ], init_settings : PydanticBaseSettingsSource , env_settings : PydanticBaseSettingsSource , dotenv_settings : PydanticBaseSettingsSource , file_secret_settings : PydanticBaseSettingsSource , ) -> tuple [ PydanticBaseSettingsSource , ... ]: # here we choose to ignore arguments from init_settings return env_settings , file_secret_settings try : Settings ( my_api_key = 'this is ignored' ) except ValidationError as exc_info : print ( exc_info ) \"\"\" 1 validation error for Settings my_api_key Field required [type=missing, input_value={}, input_type=dict] For further information visit https://errors.pydantic.dev/2/v/missing \"\"\"","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#removing-sources","title":"Settings Management - Customise settings sources - Removing sources","objectID":"/latest/concepts/pydantic_settings/#removing-sources","rank":-80},{"content":"In case you want to reload in-place an existing setting, you can do it by using its __init__ method : import os from pydantic import Field from pydantic_settings import BaseSettings class Settings ( BaseSettings ): foo : str = Field ( 'foo' ) mutable_settings = Settings () print ( mutable_settings . foo ) #> foo os . environ [ 'foo' ] = 'bar' print ( mutable_settings . foo ) #> foo mutable_settings . __init__ () print ( mutable_settings . foo ) #> bar os . environ . pop ( 'foo' ) mutable_settings . __init__ () print ( mutable_settings . foo ) #> foo","pageID":"Settings Management","abs_url":"/latest/concepts/pydantic_settings/#in-place-reloading","title":"Settings Management - In-place reloading","objectID":"/latest/concepts/pydantic_settings/#in-place-reloading","rank":-85},{"content":"Beyond accessing model attributes directly via their field names (e.g. model.foobar ), models can be converted, dumped,\nserialized, and exported in a number of ways. Serialization can be customized for the whole model, or on a per-field\nor per-type basis. Tip Want to quickly jump to the relevant serializer section? Field serializer field plain serializer field wrap serializer Model serializer model plain serializer model wrap serializer","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#Serialization","title":"Serialization","objectID":"/latest/concepts/serialization/#Serialization","rank":100},{"content":"Pydantic allows models (and any other type using type adapters ) to be serialized in two modes: Python and JSON . The Python output may contain non-JSON serializable data (although this\ncan be emulated).","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#serializing-data","title":"Serialization - Serializing data","objectID":"/latest/concepts/serialization/#serializing-data","rank":95},{"content":"When using the Python mode, Pydantic models (and model-like types such as ) (1) will be (recursively) converted to dictionaries. This is achievable by using the  method: With the exception of root models , where the root value is dumped directly. Python 3.9 and above Python 3.10 and above from typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n\nclass BarModel(BaseModel):\n    whatever: tuple[int, ...]\n\n\nclass FooBarModel(BaseModel):\n    banana: Optional[float] = 1.1\n    foo: str = Field(serialization_alias='foo_alias')\n    bar: BarModel\n\n\nm = FooBarModel(banana=3.14, foo='hello', bar={'whatever': (1, 2)})\n\n# returns a dictionary:\nprint(m.model_dump())\n#> {'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': (1, 2)}}\n\nprint(m.model_dump(by_alias=True))\n#> {'banana': 3.14, 'foo_alias': 'hello', 'bar': {'whatever': (1, 2)}} from pydantic import BaseModel, Field\n\n\nclass BarModel(BaseModel):\n    whatever: tuple[int, ...]\n\n\nclass FooBarModel(BaseModel):\n    banana: float | None = 1.1\n    foo: str = Field(serialization_alias='foo_alias')\n    bar: BarModel\n\n\nm = FooBarModel(banana=3.14, foo='hello', bar={'whatever': (1, 2)})\n\n# returns a dictionary:\nprint(m.model_dump())\n#> {'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': (1, 2)}}\n\nprint(m.model_dump(by_alias=True))\n#> {'banana': 3.14, 'foo_alias': 'hello', 'bar': {'whatever': (1, 2)}} Notice that the value of whatever was dumped as tuple, which isn't a known JSON type. The mode argument can be set to 'json' to ensure JSON-compatible types are used: print(m.model_dump(mode='json'))\n#> {'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': [1, 2]}} See also The  method, useful when not dealing with Pydantic models.","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#python-mode","title":"Serialization - Serializing data - Python mode","objectID":"/latest/concepts/serialization/#python-mode","rank":90},{"content":"Pydantic allows data to be serialized directly to a JSON-encoded string, by trying its best to convert Python values to valid\nJSON data. This is achievable by using the  method: from datetime import datetime\n\nfrom pydantic import BaseModel\n\n\nclass BarModel(BaseModel):\n    whatever: tuple[int, ...]\n\n\nclass FooBarModel(BaseModel):\n    foo: datetime\n    bar: BarModel\n\n\nm = FooBarModel(foo=datetime(2032, 6, 1, 12, 13, 14), bar={'whatever': (1, 2)})\n\nprint(m.model_dump_json(indent=2))\n\"\"\"\n{\n  \"foo\": \"2032-06-01T12:13:14\",\n  \"bar\": {\n    \"whatever\": [\n      1,\n      2\n    ]\n  }\n}\n\"\"\" In addition to the  by the standard library  module, Pydantic supports a wide\nvariety of types (,  objects, , etc). If an unsupported type\nis used and can't be serialized to JSON, a  exception\nis raised. See also The  method, useful when not dealing with Pydantic models.","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#json-mode","title":"Serialization - Serializing data - JSON mode","objectID":"/latest/concepts/serialization/#json-mode","rank":85},{"content":"Pydantic models can also be iterated over, yielding (field_name, field_value) pairs. Note that field values\nare left as is, so sub-models will not be converted to dictionaries: from pydantic import BaseModel\n\n\nclass BarModel(BaseModel):\n    whatever: int\n\n\nclass FooBarModel(BaseModel):\n    banana: float\n    foo: str\n    bar: BarModel\n\n\nm = FooBarModel(banana=3.14, foo='hello', bar={'whatever': 123})\n\nfor name, value in m:\n    print(f'{name}: {value}')\n    #> banana: 3.14\n    #> foo: hello\n    #> bar: whatever=123 This means that calling  on a model can be used to construct a dictionary of the model: print(dict(m))\n#> {'banana': 3.14, 'foo': 'hello', 'bar': BarModel(whatever=123)} Note Root models does get converted to a dictionary with the key 'root' .","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#iterating-over-models","title":"Serialization - Iterating over models","objectID":"/latest/concepts/serialization/#iterating-over-models","rank":80},{"content":"Pydantic models support efficient pickling and unpickling. import pickle\n\nfrom pydantic import BaseModel\n\n\nclass FooBarModel(BaseModel):\n    a: str\n    b: int\n\n\nm = FooBarModel(a='hello', b=123)\nprint(m)\n#> a='hello' b=123\ndata = pickle.dumps(m)\nprint(data[:20])\n#> b'\\x80\\x04\\x95\\x95\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main_'\nm2 = pickle.loads(data)\nprint(m2)\n#> a='hello' b=123","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#pickling-support","title":"Serialization - Pickling support","objectID":"/latest/concepts/serialization/#pickling-support","rank":75},{"content":"Similar to custom validators , you can leverage custom serializers at the field and model levels to further\ncontrol the serialization behavior. Warning Only one serializer can be defined per field/model. It is not possible to combine multiple serializers together\n(including plain and wrap serializers).","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#serializers","title":"Serialization - Serializers","objectID":"/latest/concepts/serialization/#serializers","rank":70},{"content":"In its simplest form, a field serializer is a callable taking the value to be serialized as an argument and returning the serialized value . If the return_type argument is provided to the serializer (or if a return type annotation is available on the serializer function),\nit will be used to build an extra serializer, to ensure that the serialized field value complies with this return type. Two different types of serializers can be used. They can all be defined using the annotated pattern or using the\n decorator, applied on instance or . Plain serializers : are called unconditionally to serialize a field. The serialization logic for types supported\n  by Pydantic will not be called. Using such serializers is also useful to specify the logic for arbitrary types. Annotated pattern Decorator from typing import Annotated, Any\n\nfrom pydantic import BaseModel, PlainSerializer\n\n\ndef ser_number(value: Any) -> Any:\n    if isinstance(value, int):\n        return value * 2\n    else:\n        return value\n\n\nclass Model(BaseModel):\n    number: Annotated[int, PlainSerializer(ser_number)]\n\n\nprint(Model(number=4).model_dump())\n#> {'number': 8}\nm = Model(number=1)\nm.number = 'invalid'\nprint(m.model_dump())  # (1)!\n#> {'number': 'invalid'} Pydantic will not validate that the serialized value complies with the int type. from typing import Any\n\nfrom pydantic import BaseModel, field_serializer\n\n\nclass Model(BaseModel):\n    number: int\n\n    @field_serializer('number', mode='plain')  # (1)!\n    def ser_number(self, value: Any) -> Any:\n        if isinstance(value, int):\n            return value * 2\n        else:\n            return value\n\n\nprint(Model(number=4).model_dump())\n#> {'number': 8}\nm = Model(number=1)\nm.number = 'invalid'\nprint(m.model_dump())  # (2)!\n#> {'number': 'invalid'} 'plain' is the default mode for the decorator, and can be omitted. Pydantic will not validate that the serialized value complies with the int type. Wrap serializers : give more flexibility to customize the serialization behavior. You can run code before or after\n  the Pydantic serialization logic. Such serializers must be defined with a mandatory extra handler parameter: a callable taking the value to be serialized\nas an argument. Internally, this handler will delegate serialization of the value to Pydantic. You are free to not call the\nhandler at all. Annotated pattern Decorator from typing import Annotated, Any\n\nfrom pydantic import BaseModel, SerializerFunctionWrapHandler, WrapSerializer\n\n\ndef ser_number(value: Any, handler: SerializerFunctionWrapHandler) -> int:\n    return handler(value) + 1\n\n\nclass Model(BaseModel):\n    number: Annotated[int, WrapSerializer(ser_number)]\n\n\nprint(Model(number=4).model_dump())\n#> {'number': 5} from typing import Any\n\nfrom pydantic import BaseModel, SerializerFunctionWrapHandler, field_serializer\n\n\nclass Model(BaseModel):\n    number: int\n\n    @field_serializer('number', mode='wrap')\n    def ser_number(\n        self, value: Any, handler: SerializerFunctionWrapHandler\n    ) -> int:\n        return handler(value) + 1\n\n\nprint(Model(number=4).model_dump())\n#> {'number': 5} Which serializer pattern to use ¶ While both approaches can achieve the same thing, each pattern provides different benefits. Using the annotated pattern ¶ One of the key benefits of using the annotated pattern is to make\nserializers reusable: from typing import Annotated\n\nfrom pydantic import BaseModel, Field, PlainSerializer\n\nDoubleNumber = Annotated[int, PlainSerializer(lambda v: v * 2)]\n\n\nclass Model1(BaseModel):\n    my_number: DoubleNumber\n\n\nclass Model2(BaseModel):\n    other_number: Annotated[DoubleNumber, Field(description='My other number')]\n\n\nclass Model3(BaseModel):\n    list_of_even_numbers: list[DoubleNumber]  # (1)! As mentioned in the annotated pattern documentation,\n   we can also make use of serializers for specific parts of the annotation (in this case,\n   serialization is applied for list items, but not the whole list). It is also easier to understand which serializers are applied to a type, by just looking at the field annotation. Using the decorator pattern ¶ One of the key benefits of using the  decorator is to apply\nthe function to multiple fields: from pydantic import BaseModel, field_serializer\n\n\nclass Model(BaseModel):\n    f1: str\n    f2: str\n\n    @field_serializer('f1', 'f2', mode='plain')\n    def capitalize(self, value: str) -> str:\n        return value.capitalize() Here are a couple additional notes about the decorator usage: If you want the serializer to apply to all fields (including the ones defined in subclasses), you can pass '*' as the field name argument. By default, the decorator will ensure the provided field name(s) are defined on the model. If you want to\n  disable this check during class creation, you can do so by passing False to the check_fields argument.\n  This is useful when the field serializer is defined on a base class, and the field is expected to exist on\n  subclasses.","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#field-serializers","title":"Serialization - Serializers - Field serializers","objectID":"/latest/concepts/serialization/#field-serializers","rank":65},{"content":"Serialization can also be customized on the entire model using the \ndecorator. If the return_type argument is provided to the  decorator\n(or if a return type annotation is available on the serializer function), it will be used to build an extra serializer,\nto ensure that the serialized model value complies with this return type. As with field serializers , two different types of model serializers can be used: Plain serializers : are called unconditionally to serialize the model. from pydantic import BaseModel, model_serializer\n\n\nclass UserModel(BaseModel):\n    username: str\n    password: str\n\n    @model_serializer(mode='plain')  # (1)!\n    def serialize_model(self) -> str:  # (2)!\n        return f'{self.username} - {self.password}'\n\n\nprint(UserModel(username='foo', password='bar').model_dump())\n#> foo - bar 'plain' is the default mode for the decorator, and can be omitted. You are free to return a value that isn't a dictionary. Wrap serializers : give more flexibility to customize the serialization behavior. You can run code before or after\n  the Pydantic serialization logic. Such serializers must be defined with a mandatory extra handler parameter: a callable taking the instance of the model\nas an argument. Internally, this handler will delegate serialization of the model to Pydantic. You are free to not call the\nhandler at all. from pydantic import BaseModel, SerializerFunctionWrapHandler, model_serializer\n\n\nclass UserModel(BaseModel):\n    username: str\n    password: str\n\n    @model_serializer(mode='wrap')\n    def serialize_model(\n        self, handler: SerializerFunctionWrapHandler\n    ) -> dict[str, object]:\n        serialized = handler(self)\n        serialized['fields'] = list(serialized)\n        return serialized\n\n\nprint(UserModel(username='foo', password='bar').model_dump())\n#> {'username': 'foo', 'password': 'bar', 'fields': ['username', 'password']}","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#model-serializers","title":"Serialization - Serializers - Model serializers","objectID":"/latest/concepts/serialization/#model-serializers","rank":60},{"content":"Both the field and model serializers callables (in all modes) can optionally take an extra info argument,\nproviding useful extra information, such as: user defined context the current serialization mode: either 'python' or 'json' (see the  property) the various parameters set during serialization using the serialization methods (e.g. , ) the current field name, if using a field serializer (see the\n   property).","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#serialization-info","title":"Serialization - Serialization info","objectID":"/latest/concepts/serialization/#serialization-info","rank":55},{"content":"You can pass a context object to the serialization methods , which can be accessed\ninside the serializer functions using the  property: from pydantic import BaseModel, FieldSerializationInfo, field_serializer\n\n\nclass Model(BaseModel):\n    text: str\n\n    @field_serializer('text', mode='plain')\n    @classmethod\n    def remove_stopwords(cls, v: str, info: FieldSerializationInfo) -> str:\n        if isinstance(info.context, dict):\n            stopwords = info.context.get('stopwords', set())\n            v = ' '.join(w for w in v.split() if w.lower() not in stopwords)\n        return v\n\n\nmodel = Model(text='This is an example document')\nprint(model.model_dump())  # no context\n#> {'text': 'This is an example document'}\nprint(model.model_dump(context={'stopwords': ['this', 'is', 'an']}))\n#> {'text': 'example document'} Similarly, you can use a context for validation .","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#serialization-context","title":"Serialization - Serialization info - Serialization context","objectID":"/latest/concepts/serialization/#serialization-context","rank":50},{"content":"","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#serializing-subclasses","title":"Serialization - Serializing subclasses","objectID":"/latest/concepts/serialization/#serializing-subclasses","rank":45},{"content":"Subclasses of supported types are serialized according to their super class: from datetime import date\n\nfrom pydantic import BaseModel\n\n\nclass MyDate(date):\n    @property\n    def my_date_format(self) -> str:\n        return self.strftime('%d/%m/%Y')\n\n\nclass FooModel(BaseModel):\n    date: date\n\n\nm = FooModel(date=MyDate(2023, 1, 1))\nprint(m.model_dump_json())\n#> {\"date\":\"2023-01-01\"}","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#subclasses-of-supported-types","title":"Serialization - Serializing subclasses - Subclasses of supported types","objectID":"/latest/concepts/serialization/#subclasses-of-supported-types","rank":40},{"content":"When using model-like classes (Pydantic models, dataclasses, etc.) as field annotations, the default behavior is to\nserializer the field value as though it was an instance of the class, even if it is a subclass. More specifically,\nonly the fields declared on the type annotation will be included in the serialization result: from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n\n\nclass UserLogin(User):\n    password: str\n\n\nclass OuterModel(BaseModel):\n    user: User\n\n\nuser = UserLogin(name='pydantic', password='hunter2')\n\nm = OuterModel(user=user)\nprint(m)\n#> user=UserLogin(name='pydantic', password='hunter2')\nprint(m.model_dump())  # (1)!\n#> {'user': {'name': 'pydantic'}} Note: the password field is not included Migration Warning This behavior is different from how things worked in Pydantic V1, where we would always include\nall (subclass) fields when recursively serializing models to dictionaries. The motivation behind this change\nin behavior is that it helps ensure that you know precisely which fields could be included when serializing,\neven if subclasses get passed when instantiating the object. In particular, this can help prevent surprises\nwhen adding sensitive information like secrets as fields of subclasses. To enable the old V1 behavior, refer\nto the next section.","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#subclasses-of-model-like-types","title":"Serialization - Serializing subclasses - Subclasses of model-like types","objectID":"/latest/concepts/serialization/#subclasses-of-model-like-types","rank":35},{"content":"Duck typing serialization is the behavior of serializing a model instance based on the actual field values, rather\nthan the field definitions. This means that for a field annotated with a model-like class, all the fields present\nin subclasses of such class will be included in the serialized output. This behavior can be configured at the field level and at runtime, for a specific serialization call: Field level: use the  annotation. Runtime level: use the serialize_as_any argument when calling the serialization methods . We discuss these options below in more detail: SerializeAsAny annotation ¶ If you want duck typing serialization behavior, this can be done using the\n annotation\non a type: from pydantic import BaseModel, SerializeAsAny\n\n\nclass User(BaseModel):\n    name: str\n\n\nclass UserLogin(User):\n    password: str\n\n\nclass OuterModel(BaseModel):\n    as_any: SerializeAsAny[User]\n    as_user: User\n\n\nuser = UserLogin(name='pydantic', password='password')\n\nprint(OuterModel(as_any=user, as_user=user).model_dump())\n\"\"\"\n{\n    'as_any': {'name': 'pydantic', 'password': 'password'},\n    'as_user': {'name': 'pydantic'},\n}\n\"\"\" When a type is annotated as SerializeAsAny[<type>] , the validation behavior will be the same as if it was\nannotated as <type> , and static type checkers will treat the annotation as if it was simply <type> .\nWhen serializing, the field will be serialized as though the type hint for the field was ,\nwhich is where the name comes from. serialize_as_any runtime setting ¶ The serialize_as_any runtime setting can be used to serialize model data with or without duck typed serialization behavior. serialize_as_any can be passed as a keyword argument to the various serialization methods (such as\n and  on Pydantic models). from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n\n\nclass UserLogin(User):\n    password: str\n\n\nclass OuterModel(BaseModel):\n    user1: User\n    user2: User\n\n\nuser = UserLogin(name='pydantic', password='password')\n\nouter_model = OuterModel(user1=user, user2=user)\nprint(outer_model.model_dump(serialize_as_any=True))  # (1)!\n\"\"\"\n{\n    'user1': {'name': 'pydantic', 'password': 'password'},\n    'user2': {'name': 'pydantic', 'password': 'password'},\n}\n\"\"\"\n\nprint(outer_model.model_dump(serialize_as_any=False))  # (2)!\n#> {'user1': {'name': 'pydantic'}, 'user2': {'name': 'pydantic'}} With serialize_as_any set to True , the result matches that of V1. With serialize_as_any set to False (the V2 default), fields present on the subclass,\n   but not the base class, are not included in serialization.","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#serializing-with-duck-typing","title":"Serialization - Serializing subclasses - Serializing with duck typing 🦆","objectID":"/latest/concepts/serialization/#serializing-with-duck-typing","rank":30},{"content":"For serialization, field inclusion and exclusion can be configured in two ways: at the field level, using the exclude and exclude_if parameters on the Field() function . using the various serialization parameters on the serialization methods .","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#field-inclusion-and-exclusion","title":"Serialization - Field inclusion and exclusion","objectID":"/latest/concepts/serialization/#field-inclusion-and-exclusion","rank":25},{"content":"At the field level, the exclude and exclude_if parameters can be used: from pydantic import BaseModel, Field\n\n\nclass Transaction(BaseModel):\n    id: int\n    private_id: int = Field(exclude=True)\n    value: int = Field(ge=0, exclude_if=lambda v: v == 0)\n\n\nprint(Transaction(id=1, private_id=2, value=0).model_dump())\n#> {'id': 1} Exclusion at the field level takes priority over the include serialization parameter described below.","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#at-the-field-level","title":"Serialization - Field inclusion and exclusion - At the field level","objectID":"/latest/concepts/serialization/#at-the-field-level","rank":20},{"content":"When using the serialization methods (such as ),\nseveral parameters can be used to exclude or include fields. Excluding and including specific fields ¶ Consider the following models: from pydantic import BaseModel, Field, SecretStr\n\n\nclass User(BaseModel):\n    id: int\n    username: str\n    password: SecretStr\n\n\nclass Transaction(BaseModel):\n    id: str\n    private_id: str = Field(exclude=True)\n    user: User\n    value: int\n\n\nt = Transaction(\n    id='1234567890',\n    private_id='123',\n    user=User(id=42, username='JohnDoe', password='hashedpassword'),\n    value=9876543210,\n) The exclude parameter can be used to specify which fields should be excluded (including the others), and vice-versa\nusing the include parameter. # using a set:\nprint(t.model_dump(exclude={'user', 'value'}))\n#> {'id': '1234567890'}\n\n# using a dictionary:\nprint(t.model_dump(exclude={'user': {'username', 'password'}, 'value': True}))\n#> {'id': '1234567890', 'user': {'id': 42}}\n\n# same configuration using `include`:\nprint(t.model_dump(include={'id': True, 'user': {'id'}}))\n#> {'id': '1234567890', 'user': {'id': 42}} Note that using False to include a field in exclude (or to exclude a field in include ) is not supported. It is also possible to exclude or include specific items from sequence and dictionaries: from pydantic import BaseModel\n\n\nclass Hobby(BaseModel):\n    name: str\n    info: str\n\n\nclass User(BaseModel):\n    hobbies: list[Hobby]\n\n\nuser = User(\n    hobbies=[\n        Hobby(name='Programming', info='Writing code and stuff'),\n        Hobby(name='Gaming', info='Hell Yeah!!!'),\n    ],\n)\n\nprint(user.model_dump(exclude={'hobbies': {-1: {'info'}}}))  # (1)!\n\"\"\"\n{\n    'hobbies': [\n        {'name': 'Programming', 'info': 'Writing code and stuff'},\n        {'name': 'Gaming'},\n    ]\n}\n\"\"\" The equivalent call with include would be: user.model_dump(\n   include={'hobbies': {0: True, -1: {'name'}}}\n) The special key '__all__' can be used to apply an exclusion/inclusion pattern to all members: print(user.model_dump(exclude={'hobbies': {'__all__': {'info'}}}))\n#> {'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}]} Excluding and including fields based on their value ¶ When using the serialization methods , it is possible to exclude fields based on their value,\nusing the following parameters: exclude_defaults : Exclude all fields whose value compares equal to the default value\n  (using the equality ( == ) comparison operator). exclude_none : Exclude all fields whose value is None . exclude_unset : Pydantic keeps track of fields that were explicitly set during instantiation (using the\n   property). Using exclude_unset , any field that\n  was not explicitly provided will be excluded: from pydantic import BaseModel\n\n\nclass UserModel(BaseModel):\n    name: str\n    age: int = 18\n\n\nuser = UserModel(name='John')\nprint(user.model_fields_set)\n#> {'name'}\n\nprint(user.model_dump(exclude_unset=True))\n#> {'name': 'John'} Note that altering a field after the instance has been created will remove it from the unset fields: user.age = 21\n\nprint(user.model_dump(exclude_unset=True))\n#> {'name': 'John', 'age': 21} Tip The experimental MISSING sentinel can be used as an alternative to exclude_unset .\nAny field with MISSING as a value is automatically excluded from the serialization output.","pageID":"Serialization","abs_url":"/latest/concepts/serialization/#as-parameters-to-the-serialization-methods","title":"Serialization - Field inclusion and exclusion - As parameters to the serialization methods","objectID":"/latest/concepts/serialization/#as-parameters-to-the-serialization-methods","rank":15},{"content":"By default, Pydantic will attempt to coerce values to the desired type when possible.\nFor example, you can pass the string \"123\" as the input to an int field, and it will be converted to 123 .\nThis coercion behavior is useful in many scenarios — think: UUIDs, URL parameters, HTTP headers, environment variables,\nuser input, etc. However, there are also situations where this is not desirable, and you want Pydantic to error instead of coercing data. To better support this use case, Pydantic provides a \"strict mode\" that can be enabled on a per-model, per-field, or\neven per-validation-call basis. When strict mode is enabled, Pydantic will be much less lenient when coercing data,\nand will instead error if the data is not of the correct type. Here is a brief example showing the difference between validation behavior in strict and the default/\"lax\" mode: from pydantic import BaseModel, ValidationError\n\n\nclass MyModel(BaseModel):\n    x: int\n\n\nprint(MyModel.model_validate({'x': '123'}))  # lax mode\n#> x=123\n\ntry:\n    MyModel.model_validate({'x': '123'}, strict=True)  # strict mode\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for MyModel\n    x\n      Input should be a valid integer [type=int_type, input_value='123', input_type=str]\n    \"\"\" There are various ways to get strict-mode validation while using Pydantic, which will be discussed in more detail below: Passing strict=True to the validation methods , such as BaseModel.model_validate , TypeAdapter.validate_python , and similar for JSON Using Field(strict=True) with fields of a BaseModel , dataclass , or TypedDict Using pydantic.types.Strict as a type annotation on a field Pydantic provides some type aliases that are already annotated with Strict , such as pydantic.types.StrictInt Using ConfigDict(strict=True)","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#Strict Mode","title":"Strict Mode","objectID":"/latest/concepts/strict_mode/#Strict Mode","rank":100},{"content":"For most types, when validating data from python in strict mode, only the instances of the exact types are accepted.\nFor example, when validating an int field, only instances of int are accepted; passing instances of float or str will result in raising a ValidationError . Note that we are looser when validating data from JSON in strict mode. For example, when validating a UUID field,\ninstances of str will be accepted when validating from JSON, but not from python: import json\nfrom uuid import UUID\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass MyModel(BaseModel):\n    guid: UUID\n\n\ndata = {'guid': '12345678-1234-1234-1234-123456789012'}\n\nprint(MyModel.model_validate(data))  # OK: lax\n#> guid=UUID('12345678-1234-1234-1234-123456789012')\n\nprint(\n    MyModel.model_validate_json(json.dumps(data), strict=True)\n)  # OK: strict, but from json\n#> guid=UUID('12345678-1234-1234-1234-123456789012')\n\ntry:\n    MyModel.model_validate(data, strict=True)  # Not OK: strict, from python\nexcept ValidationError as exc:\n    print(exc.errors(include_url=False))\n    \"\"\"\n    [\n        {\n            'type': 'is_instance_of',\n            'loc': ('guid',),\n            'msg': 'Input should be an instance of UUID',\n            'input': '12345678-1234-1234-1234-123456789012',\n            'ctx': {'class': 'UUID'},\n        }\n    ]\n    \"\"\" For more details about what types are allowed as inputs in strict mode, you can review the Conversion Table .","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#type-coercions-in-strict-mode","title":"Strict Mode - Type coercions in strict mode","objectID":"/latest/concepts/strict_mode/#type-coercions-in-strict-mode","rank":95},{"content":"All the examples included so far get strict-mode validation through the use of strict=True as a keyword argument to\nthe validation methods. While we have shown this for BaseModel.model_validate , this also works with arbitrary types\nthrough the use of TypeAdapter : from pydantic import TypeAdapter, ValidationError\n\nprint(TypeAdapter(bool).validate_python('yes'))  # OK: lax\n#> True\n\ntry:\n    TypeAdapter(bool).validate_python('yes', strict=True)  # Not OK: strict\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for bool\n      Input should be a valid boolean [type=bool_type, input_value='yes', input_type=str]\n    \"\"\" Note this also works even when using more \"complex\" types in TypeAdapter : from dataclasses import dataclass\n\nfrom pydantic import TypeAdapter, ValidationError\n\n\n@dataclass\nclass MyDataclass:\n    x: int\n\n\ntry:\n    TypeAdapter(MyDataclass).validate_python({'x': '123'}, strict=True)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for MyDataclass\n      Input should be an instance of MyDataclass [type=dataclass_exact_type, input_value={'x': '123'}, input_type=dict]\n    \"\"\" This also works with the TypeAdapter.validate_json and BaseModel.model_validate_json methods: import json\nfrom uuid import UUID\n\nfrom pydantic import BaseModel, TypeAdapter, ValidationError\n\ntry:\n    TypeAdapter(list[int]).validate_json('[\"1\", 2, \"3\"]', strict=True)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    2 validation errors for list[int]\n    0\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    2\n      Input should be a valid integer [type=int_type, input_value='3', input_type=str]\n    \"\"\"\n\n\nclass Model(BaseModel):\n    x: int\n    y: UUID\n\n\ndata = {'x': '1', 'y': '12345678-1234-1234-1234-123456789012'}\ntry:\n    Model.model_validate(data, strict=True)\nexcept ValidationError as exc:\n    # Neither x nor y are valid in strict mode from python:\n    print(exc)\n    \"\"\"\n    2 validation errors for Model\n    x\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    y\n      Input should be an instance of UUID [type=is_instance_of, input_value='12345678-1234-1234-1234-123456789012', input_type=str]\n    \"\"\"\n\njson_data = json.dumps(data)\ntry:\n    Model.model_validate_json(json_data, strict=True)\nexcept ValidationError as exc:\n    # From JSON, x is still not valid in strict mode, but y is:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    x\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    \"\"\"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#strict-mode-in-method-calls","title":"Strict Mode - Strict mode in method calls","objectID":"/latest/concepts/strict_mode/#strict-mode-in-method-calls","rank":90},{"content":"For individual fields on a model, you can set strict=True on the field .\nThis will cause strict-mode validation to be used for that field, even when the validation methods are called without strict=True . Only the fields for which strict=True is set will be affected: from pydantic import BaseModel, Field, ValidationError\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    n_pets: int\n\n\nuser = User(name='John', age='42', n_pets='1')\nprint(user)\n#> name='John' age=42 n_pets=1\n\n\nclass AnotherUser(BaseModel):\n    name: str\n    age: int = Field(strict=True)\n    n_pets: int\n\n\ntry:\n    anotheruser = AnotherUser(name='John', age='42', n_pets='1')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for AnotherUser\n    age\n      Input should be a valid integer [type=int_type, input_value='42', input_type=str]\n    \"\"\" Note that making fields strict will also affect the validation performed when instantiating the model class: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: int = Field(strict=True)\n    y: int = Field(strict=False)\n\n\ntry:\n    Model(x='1', y='2')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Model\n    x\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    \"\"\"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#strict-mode-with-field","title":"Strict Mode - Strict mode with Field","objectID":"/latest/concepts/strict_mode/#strict-mode-with-field","rank":85},{"content":"Note that Field(strict=True) (or with any other keyword arguments) can be used as an annotation if necessary, e.g.,\nwhen working with TypedDict : Python 3.9 and above Python 3.13 and above from typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom pydantic import Field, TypeAdapter, ValidationError\n\n\nclass MyDict(TypedDict):\n    x: Annotated[int, Field(strict=True)]\n\n\ntry:\n    TypeAdapter(MyDict).validate_python({'x': '1'})\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for MyDict\n    x\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    \"\"\" from typing import Annotated\n\nfrom typing import TypedDict\n\nfrom pydantic import Field, TypeAdapter, ValidationError\n\n\nclass MyDict(TypedDict):\n    x: Annotated[int, Field(strict=True)]\n\n\ntry:\n    TypeAdapter(MyDict).validate_python({'x': '1'})\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for MyDict\n    x\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    \"\"\"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#using-field-as-an-annotation","title":"Strict Mode - Strict mode with Field - Using Field as an annotation","objectID":"/latest/concepts/strict_mode/#using-field-as-an-annotation","rank":80},{"content":"Pydantic also provides the Strict class, which is intended for use as\nmetadata with  class; this annotation indicates that the annotated field should be validated in\nstrict mode: from typing import Annotated\n\nfrom pydantic import BaseModel, Strict, ValidationError\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    is_active: Annotated[bool, Strict()]\n\n\nUser(name='David', age=33, is_active=True)\ntry:\n    User(name='David', age=33, is_active='True')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for User\n    is_active\n      Input should be a valid boolean [type=bool_type, input_value='True', input_type=str]\n    \"\"\" This is, in fact, the method used to implement some of the strict-out-of-the-box types provided by Pydantic,\nsuch as StrictInt .","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#strict-mode-with-annotated-strict","title":"Strict Mode - Strict mode with Annotated[..., Strict()]","objectID":"/latest/concepts/strict_mode/#strict-mode-with-annotated-strict","rank":75},{"content":"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#strict-mode-with-configdict","title":"Strict Mode - Strict mode with ConfigDict","objectID":"/latest/concepts/strict_mode/#strict-mode-with-configdict","rank":70},{"content":"If you want to enable strict mode for all fields on a complex input type, you can use ConfigDict(strict=True) in the model_config : from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass User(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n    name: str\n    age: int\n    is_active: bool\n\n\ntry:\n    User(name='David', age='33', is_active='yes')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    2 validation errors for User\n    age\n      Input should be a valid integer [type=int_type, input_value='33', input_type=str]\n    is_active\n      Input should be a valid boolean [type=bool_type, input_value='yes', input_type=str]\n    \"\"\" Note When using strict=True through a model's model_config , you can still override the strictness\nof individual fields by setting strict=False on individual fields: from pydantic import BaseModel, ConfigDict, Field\n\n\nclass User(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n    name: str\n    age: int = Field(strict=False) Note that strict mode is not recursively applied to nested model fields: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Inner(BaseModel):\n    y: int\n\n\nclass Outer(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n    x: int\n    inner: Inner\n\n\nprint(Outer(x=1, inner=Inner(y='2')))\n#> x=1 inner=Inner(y=2)\n\ntry:\n    Outer(x='1', inner=Inner(y='2'))\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Outer\n    x\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    \"\"\" (This is also the case for dataclasses and TypedDict .) If this is undesirable, you should make sure that strict mode is enabled for all the types involved.\nFor example, this can be done for model classes by using a shared base class with model_config = ConfigDict(strict=True) : from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass MyBaseModel(BaseModel):\n    model_config = ConfigDict(strict=True)\n\n\nclass Inner(MyBaseModel):\n    y: int\n\n\nclass Outer(MyBaseModel):\n    x: int\n    inner: Inner\n\n\ntry:\n    Outer.model_validate({'x': 1, 'inner': {'y': '2'}})\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Outer\n    inner.y\n      Input should be a valid integer [type=int_type, input_value='2', input_type=str]\n    \"\"\"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#basemodel","title":"Strict Mode - Strict mode with ConfigDict - BaseModel","objectID":"/latest/concepts/strict_mode/#basemodel","rank":65},{"content":"Pydantic dataclasses behave similarly to the examples shown above with BaseModel , just that instead of model_config you should use the config keyword argument to the @pydantic.dataclasses.dataclass decorator. When possible, you can achieve nested strict mode for vanilla dataclasses or TypedDict subclasses by annotating fields\nwith the pydantic.types.Strict annotation . However, if this is not possible (e.g., when working with third-party types), you can set the config that Pydantic\nshould use for the type by setting the __pydantic_config__ attribute on the type: Python 3.9 and above Python 3.13 and above from typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, ValidationError\n\n\nclass Inner(TypedDict):\n    y: int\n\n\nInner.__pydantic_config__ = ConfigDict(strict=True)\n\n\nclass Outer(TypedDict):\n    x: int\n    inner: Inner\n\n\nadapter = TypeAdapter(Outer)\nprint(adapter.validate_python({'x': '1', 'inner': {'y': 2}}))\n#> {'x': 1, 'inner': {'y': 2}}\n\n\ntry:\n    adapter.validate_python({'x': '1', 'inner': {'y': '2'}})\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Outer\n    inner.y\n      Input should be a valid integer [type=int_type, input_value='2', input_type=str]\n    \"\"\" from typing import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter, ValidationError\n\n\nclass Inner(TypedDict):\n    y: int\n\n\nInner.__pydantic_config__ = ConfigDict(strict=True)\n\n\nclass Outer(TypedDict):\n    x: int\n    inner: Inner\n\n\nadapter = TypeAdapter(Outer)\nprint(adapter.validate_python({'x': '1', 'inner': {'y': 2}}))\n#> {'x': 1, 'inner': {'y': 2}}\n\n\ntry:\n    adapter.validate_python({'x': '1', 'inner': {'y': '2'}})\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for Outer\n    inner.y\n      Input should be a valid integer [type=int_type, input_value='2', input_type=str]\n    \"\"\"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#dataclasses-and-typeddict","title":"Strict Mode - Strict mode with ConfigDict - Dataclasses and TypedDict","objectID":"/latest/concepts/strict_mode/#dataclasses-and-typeddict","rank":60},{"content":"You can also get strict mode through the use of the config keyword argument to the TypeAdapter class: from pydantic import ConfigDict, TypeAdapter, ValidationError\n\nadapter = TypeAdapter(bool, config=ConfigDict(strict=True))\n\ntry:\n    adapter.validate_python('yes')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for bool\n      Input should be a valid boolean [type=bool_type, input_value='yes', input_type=str]\n    \"\"\"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#typeadapter","title":"Strict Mode - Strict mode with ConfigDict - TypeAdapter","objectID":"/latest/concepts/strict_mode/#typeadapter","rank":55},{"content":"Strict mode is also usable with the @validate_call decorator by passing the config keyword argument: from pydantic import ConfigDict, ValidationError, validate_call\n\n\n@validate_call(config=ConfigDict(strict=True))\ndef foo(x: int) -> int:\n    return x\n\n\ntry:\n    foo('1')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for foo\n    0\n      Input should be a valid integer [type=int_type, input_value='1', input_type=str]\n    \"\"\"","pageID":"Strict Mode","abs_url":"/latest/concepts/strict_mode/#validate_call","title":"Strict Mode - Strict mode with ConfigDict - @validate_call","objectID":"/latest/concepts/strict_mode/#validate_call","rank":50},{"content":"You may have types that are not BaseModel s that you want to validate data against.\nOr you may want to validate a list[SomeModel] , or dump it to JSON. For use cases like this, Pydantic provides ,\nwhich can be used for type validation, serialization, and JSON schema generation without needing to create a\n. A  instance exposes some of the functionality from\n instance methods for types that do not have such methods\n(such as dataclasses, primitive types, and more): Python 3.9 and above Python 3.13 and above from typing_extensions import TypedDict\n\nfrom pydantic import TypeAdapter, ValidationError\n\n\nclass User(TypedDict):\n    name: str\n    id: int\n\n\nuser_list_adapter = TypeAdapter(list[User])\nuser_list = user_list_adapter.validate_python([{'name': 'Fred', 'id': '3'}])\nprint(repr(user_list))\n#> [{'name': 'Fred', 'id': 3}]\n\ntry:\n    user_list_adapter.validate_python(\n        [{'name': 'Fred', 'id': 'wrong', 'other': 'no'}]\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for list[User]\n    0.id\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='wrong', input_type=str]\n    \"\"\"\n\nprint(repr(user_list_adapter.dump_json(user_list)))\n#> b'[{\"name\":\"Fred\",\"id\":3}]' from typing import TypedDict\n\nfrom pydantic import TypeAdapter, ValidationError\n\n\nclass User(TypedDict):\n    name: str\n    id: int\n\n\nuser_list_adapter = TypeAdapter(list[User])\nuser_list = user_list_adapter.validate_python([{'name': 'Fred', 'id': '3'}])\nprint(repr(user_list))\n#> [{'name': 'Fred', 'id': 3}]\n\ntry:\n    user_list_adapter.validate_python(\n        [{'name': 'Fred', 'id': 'wrong', 'other': 'no'}]\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for list[User]\n    0.id\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='wrong', input_type=str]\n    \"\"\"\n\nprint(repr(user_list_adapter.dump_json(user_list)))\n#> b'[{\"name\":\"Fred\",\"id\":3}]' dump_json returns bytes TypeAdapter 's dump_json methods returns a bytes object, unlike the corresponding method for BaseModel , model_dump_json , which returns a str .\nThe reason for this discrepancy is that in V1, model dumping returned a str type, so this behavior is retained in V2 for backwards compatibility.\nFor the BaseModel case, bytes are coerced to str types, but bytes are often the desired end type.\nHence, for the new TypeAdapter class in V2, the return type is simply bytes , which can easily be coerced to a str type if desired. Note Despite some overlap in use cases with ,\n should not be used as a type annotation for\nspecifying fields of a BaseModel , etc.","pageID":"Type Adapter","abs_url":"/latest/concepts/type_adapter/#Type Adapter","title":"Type Adapter","objectID":"/latest/concepts/type_adapter/#Type Adapter","rank":100},{"content":"can be used to apply the parsing logic to populate Pydantic models\nin a more ad-hoc way. This function behaves similarly to\n,\nbut works with arbitrary Pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of\n. For example: from pydantic import BaseModel, TypeAdapter\n\n\nclass Item(BaseModel):\n    id: int\n    name: str\n\n\n# `item_data` could come from an API call, eg., via something like:\n# item_data = requests.get('https://my-api.com/items').json()\nitem_data = [{'id': 1, 'name': 'My Item'}]\n\nitems = TypeAdapter(list[Item]).validate_python(item_data)\nprint(items)\n#> [Item(id=1, name='My Item')] is capable of parsing data into any of the types Pydantic can\nhandle as fields of a . Performance considerations When creating an instance of , the provided type must be analyzed and converted into a pydantic-core\nschema. This comes with some non-trivial overhead, so it is recommended to create a TypeAdapter for a given type\njust once and reuse it in loops or other performance-critical code.","pageID":"Type Adapter","abs_url":"/latest/concepts/type_adapter/#parsing-data-into-a-specified-type","title":"Type Adapter - Parsing data into a specified type","objectID":"/latest/concepts/type_adapter/#parsing-data-into-a-specified-type","rank":95},{"content":"In v2.10+, 's support deferred schema building and manual rebuilds. This is helpful for the case of: Types with forward references Types for which core schema builds are expensive When you initialize a  with a type, Pydantic analyzes the type and creates a core schema for it.\nThis core schema contains the information needed to validate and serialize data for that type.\nSee the architecture documentation for more information on core schemas. If you set  to True when initializing a TypeAdapter ,\nPydantic will defer building the core schema until the first time it is needed (for validation or serialization). In order to manually trigger the building of the core schema, you can call the\n method on the  instance: from pydantic import ConfigDict, TypeAdapter\n\nta = TypeAdapter('MyInt', config=ConfigDict(defer_build=True))\n\n# some time later, the forward reference is defined\nMyInt = int\n\nta.rebuild()\nassert ta.validate_python(1) == 1","pageID":"Type Adapter","abs_url":"/latest/concepts/type_adapter/#rebuilding-a-typeadapters-schema","title":"Type Adapter - Rebuilding a TypeAdapter's schema","objectID":"/latest/concepts/type_adapter/#rebuilding-a-typeadapters-schema","rank":90},{"content":"Where possible Pydantic uses standard library types to define fields, thus smoothing\nthe learning curve. For many useful applications, however, no standard library type exists,\nso Pydantic implements many commonly used types. There are also more complex types that can be found in the Pydantic Extra Types package. If no existing type suits your purpose you can also implement your own Pydantic-compatible types with custom properties and validation. The following sections describe the types supported by Pydantic. Standard Library Types — types from the Python standard library. Strict Types — types that enable you to prevent coercion from compatible types. Custom Data Types — create your own custom data types. Field Type Conversions — strict and lax conversion between different field types.","pageID":"Types","abs_url":"/latest/concepts/types/#Types","title":"Types","objectID":"/latest/concepts/types/#Types","rank":100},{"content":"During validation, Pydantic can coerce data into expected types. There are two modes of coercion: strict and lax. See Conversion Table for more details on how Pydantic converts data in both strict and lax modes. See Strict mode and Strict Types for details on enabling strict coercion.","pageID":"Types","abs_url":"/latest/concepts/types/#type-conversion","title":"Types - Type conversion","objectID":"/latest/concepts/types/#type-conversion","rank":95},{"content":"Pydantic provides the following strict types: These types will only pass validation when the validated value is of the respective type or is a subtype of that type.","pageID":"Types","abs_url":"/latest/concepts/types/#strict-types","title":"Types - Strict Types","objectID":"/latest/concepts/types/#strict-types","rank":90},{"content":"This behavior is also exposed via the strict field of the constrained types and can be combined with a multitude of complex validation rules. See the individual type signatures for supported arguments. The following caveats apply: StrictBytes (and the strict option of conbytes() ) will accept both bytes ,\n   and bytearray types. StrictInt (and the strict option of conint() ) will not accept bool types,\n    even though bool is a subclass of int in Python. Other subclasses will work. StrictFloat (and the strict option of confloat() ) will not accept int . Besides the above, you can also have a  type that will only accept finite values (i.e. not inf , -inf or nan ).","pageID":"Types","abs_url":"/latest/concepts/types/#constrained-types","title":"Types - Strict Types - Constrained types","objectID":"/latest/concepts/types/#constrained-types","rank":85},{"content":"You can also define your own custom data types. There are several ways to achieve it.","pageID":"Types","abs_url":"/latest/concepts/types/#custom-types","title":"Types - Custom Types","objectID":"/latest/concepts/types/#custom-types","rank":80},{"content":"The annotated pattern can be used to make types reusable across your code base.\nFor example, to create a type representing a positive integer: from typing import Annotated\n\nfrom pydantic import Field, TypeAdapter, ValidationError\n\nPositiveInt = Annotated[int, Field(gt=0)]  # (1)!\n\nta = TypeAdapter(PositiveInt)\n\nprint(ta.validate_python(1))\n#> 1\n\ntry:\n    ta.validate_python(-1)\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for constrained-int\n      Input should be greater than 0 [type=greater_than, input_value=-1, input_type=int]\n    \"\"\" Note that you can also use constraints from the annotated-types library to make this Pydantic-agnostic: from annotated_types import Gt\n\nPositiveInt = Annotated[int, Gt(0)] Adding validation and serialization ¶ You can add or override validation, serialization, and JSON schemas to an arbitrary type using the markers that\nPydantic exports: from typing import Annotated\n\nfrom pydantic import (\n    AfterValidator,\n    PlainSerializer,\n    TypeAdapter,\n    WithJsonSchema,\n)\n\nTruncatedFloat = Annotated[\n    float,\n    AfterValidator(lambda x: round(x, 1)),\n    PlainSerializer(lambda x: f'{x:.1e}', return_type=str),\n    WithJsonSchema({'type': 'string'}, mode='serialization'),\n]\n\n\nta = TypeAdapter(TruncatedFloat)\n\ninput = 1.02345\nassert input != 1.0\n\nassert ta.validate_python(input) == 1.0\n\nassert ta.dump_json(input) == b'\"1.0e+00\"'\n\nassert ta.json_schema(mode='validation') == {'type': 'number'}\nassert ta.json_schema(mode='serialization') == {'type': 'string'} Generics ¶ can be used within the  type: from typing import Annotated, TypeVar\n\nfrom annotated_types import Gt, Len\n\nfrom pydantic import TypeAdapter, ValidationError\n\nT = TypeVar('T')\n\n\nShortList = Annotated[list[T], Len(max_length=4)]\n\n\nta = TypeAdapter(ShortList[int])\n\nv = ta.validate_python([1, 2, 3, 4])\nassert v == [1, 2, 3, 4]\n\ntry:\n    ta.validate_python([1, 2, 3, 4, 5])\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for list[int]\n      List should have at most 4 items after validation, not 5 [type=too_long, input_value=[1, 2, 3, 4, 5], input_type=list]\n    \"\"\"\n\n\nPositiveList = list[Annotated[T, Gt(0)]]\n\nta = TypeAdapter(PositiveList[float])\n\nv = ta.validate_python([1.0])\nassert type(v[0]) is float\n\n\ntry:\n    ta.validate_python([-1.0])\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for list[constrained-float]\n    0\n      Input should be greater than 0 [type=greater_than, input_value=-1.0, input_type=float]\n    \"\"\"","pageID":"Types","abs_url":"/latest/concepts/types/#using-the-annotated-pattern","title":"Types - Custom Types - Using the annotated pattern","objectID":"/latest/concepts/types/#using-the-annotated-pattern","rank":75},{"content":"The above examples make use of implicit type aliases, assigned to a variable. At runtime, Pydantic\nhas no way of knowing the name of the variable it was assigned to, and this can be problematic for\ntwo reasons: The JSON Schema of the alias won't be converted into a definition .\n  This is mostly useful when you are using the alias more than once in a model definition. In most cases, recursive type aliases won't work. By leveraging the new type statement (introduced in PEP 695 ), you can define aliases as follows: Python 3.9 and above Python 3.12 and above (new syntax) from typing import Annotated\n\nfrom annotated_types import Gt\nfrom typing_extensions import TypeAliasType\n\nfrom pydantic import BaseModel\n\nPositiveIntList = TypeAliasType('PositiveIntList', list[Annotated[int, Gt(0)]])\n\n\nclass Model(BaseModel):\n    x: PositiveIntList\n    y: PositiveIntList\n\n\nprint(Model.model_json_schema())  # (1)!\n\"\"\"\n{\n    '$defs': {\n        'PositiveIntList': {\n            'items': {'exclusiveMinimum': 0, 'type': 'integer'},\n            'type': 'array',\n        }\n    },\n    'properties': {\n        'x': {'$ref': '#/$defs/PositiveIntList'},\n        'y': {'$ref': '#/$defs/PositiveIntList'},\n    },\n    'required': ['x', 'y'],\n    'title': 'Model',\n    'type': 'object',\n}\n\"\"\" If PositiveIntList were to be defined as an implicit type alias, its definition\n   would have been duplicated in both 'x' and 'y' . from typing import Annotated\n\nfrom annotated_types import Gt\n\nfrom pydantic import BaseModel\n\ntype PositiveIntList = list[Annotated[int, Gt(0)]]\n\n\nclass Model(BaseModel):\n    x: PositiveIntList\n    y: PositiveIntList\n\n\nprint(Model.model_json_schema())  # (1)!\n\"\"\"\n{\n    '$defs': {\n        'PositiveIntList': {\n            'items': {'exclusiveMinimum': 0, 'type': 'integer'},\n            'type': 'array',\n        }\n    },\n    'properties': {\n        'x': {'$ref': '#/$defs/PositiveIntList'},\n        'y': {'$ref': '#/$defs/PositiveIntList'},\n    },\n    'required': ['x', 'y'],\n    'title': 'Model',\n    'type': 'object',\n}\n\"\"\" If PositiveIntList were to be defined as an implicit type alias, its definition\n   would have been duplicated in both 'x' and 'y' . When to use named type aliases While (named) PEP 695 and implicit type aliases are meant to be equivalent for static type checkers,\nPydantic will not understand field-specific metadata inside named aliases. That is, metadata such as alias , default , deprecated , cannot be used: Python 3.9 and above Python 3.12 and above (new syntax) from typing import Annotated\n\nfrom typing_extensions import TypeAliasType\n\nfrom pydantic import BaseModel, Field\n\nMyAlias = TypeAliasType('MyAlias', Annotated[int, Field(default=1)])\n\n\nclass Model(BaseModel):\n    x: MyAlias  # This is not allowed from typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\ntype MyAlias = Annotated[int, Field(default=1)]\n\n\nclass Model(BaseModel):\n    x: MyAlias  # This is not allowed Only metadata that can be applied to the annotated type itself is allowed\n(e.g. validation constraints and JSON metadata).\nTrying to support field-specific metadata would require eagerly inspecting the\ntype alias's , and as such Pydantic\nwouldn't be able to have the alias stored as a JSON Schema definition. Note As with implicit type aliases,  can also be used inside the generic alias: Python 3.9 and above Python 3.12 and above (new syntax) from typing import Annotated, TypeVar\n\nfrom annotated_types import Len\nfrom typing_extensions import TypeAliasType\n\nT = TypeVar('T')\n\nShortList = TypeAliasType(\n    'ShortList', Annotated[list[T], Len(max_length=4)], type_params=(T,)\n) from typing import Annotated, TypeVar\n\nfrom annotated_types import Len\n\ntype ShortList[T] = Annotated[list[T], Len(max_length=4)] Named recursive types ¶ Named type aliases should be used whenever you need to define recursive type aliases (1). For several reasons, Pydantic isn't able to support implicit recursive aliases. For\n   instance, it won't be able to resolve forward annotations across modules. For instance, here is an example definition of a JSON type: Python 3.9 and above Python 3.12 and above (new syntax) from typing import Union\n\nfrom typing_extensions import TypeAliasType\n\nfrom pydantic import TypeAdapter\n\nJson = TypeAliasType(\n    'Json',\n    'Union[dict[str, Json], list[Json], str, int, float, bool, None]',  # (1)!\n)\n\nta = TypeAdapter(Json)\nprint(ta.json_schema())\n\"\"\"\n{\n    '$defs': {\n        'Json': {\n            'anyOf': [\n                {\n                    'additionalProperties': {'$ref': '#/$defs/Json'},\n                    'type': 'object',\n                },\n                {'items': {'$ref': '#/$defs/Json'}, 'type': 'array'},\n                {'type': 'string'},\n                {'type': 'integer'},\n                {'type': 'number'},\n                {'type': 'boolean'},\n                {'type': 'null'},\n            ]\n        }\n    },\n    '$ref': '#/$defs/Json',\n}\n\"\"\" Wrapping the annotation in quotes is necessary as it is eagerly evaluated\n   (and Json has yet to be defined). from pydantic import TypeAdapter\n\ntype Json = dict[str, Json] | list[Json] | str | int | float | bool | None  # (1)!\n\nta = TypeAdapter(Json)\nprint(ta.json_schema())\n\"\"\"\n{\n    '$defs': {\n        'Json': {\n            'anyOf': [\n                {\n                    'additionalProperties': {'$ref': '#/$defs/Json'},\n                    'type': 'object',\n                },\n                {'items': {'$ref': '#/$defs/Json'}, 'type': 'array'},\n                {'type': 'string'},\n                {'type': 'integer'},\n                {'type': 'number'},\n                {'type': 'boolean'},\n                {'type': 'null'},\n            ]\n        }\n    },\n    '$ref': '#/$defs/Json',\n}\n\"\"\" The value of a named type alias is lazily evaluated, so there's no need to use forward annotations. Tip Pydantic defines a  type as a convenience.","pageID":"Types","abs_url":"/latest/concepts/types/#named-type-aliases","title":"Types - Custom Types - Named type aliases","objectID":"/latest/concepts/types/#named-type-aliases","rank":70},{"content":"To do more extensive customization of how Pydantic handles custom classes, and in particular when you have access to the\nclass or can subclass it, you can implement a special __get_pydantic_core_schema__ to tell Pydantic how to generate the pydantic-core schema. While pydantic uses pydantic-core internally to handle validation and serialization, it is a new API for Pydantic V2,\nthus it is one of the areas most likely to be tweaked in the future and you should try to stick to the built-in\nconstructs like those provided by annotated-types , pydantic.Field , or BeforeValidator and so on. You can implement __get_pydantic_core_schema__ both on a custom type and on metadata intended to be put in Annotated .\nIn both cases the API is middleware-like and similar to that of \"wrap\" validators: you get a source_type (which isn't\nnecessarily the same as the class, in particular for generics) and a handler that you can call with a type to either\ncall the next metadata in Annotated or call into Pydantic's internal schema generation. The simplest no-op implementation calls the handler with the type you are given, then returns that as the result. You can\nalso choose to modify the type before calling the handler, modify the core schema returned by the handler, or not call the\nhandler at all. As a method on a custom type ¶ The following is an example of a type that uses __get_pydantic_core_schema__ to customize how it gets validated.\nThis is equivalent to implementing __get_validators__ in Pydantic V1. from typing import Any\n\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom pydantic import GetCoreSchemaHandler, TypeAdapter\n\n\nclass Username(str):\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source_type: Any, handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(cls, handler(str))\n\n\nta = TypeAdapter(Username)\nres = ta.validate_python('abc')\nassert isinstance(res, Username)\nassert res == 'abc' See JSON Schema for more details on how to customize JSON schemas for custom types. As an annotation ¶ Often you'll want to parametrize your custom type by more than just generic type parameters (which you can do via the type system and will be discussed later). Or you may not actually care (or want to) make an instance of your subclass; you actually want the original type, just with some extra validation done. For example, if you were to implement pydantic.AfterValidator (see Adding validation and serialization ) yourself, you'd do something similar to the following: Python 3.9 and above Python 3.10 and above from dataclasses import dataclass\nfrom typing import Annotated, Any, Callable\n\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler\n\n\n@dataclass(frozen=True)  # (1)!\nclass MyAfterValidator:\n    func: Callable[[Any], Any]\n\n    def __get_pydantic_core_schema__(\n        self, source_type: Any, handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            self.func, handler(source_type)\n        )\n\n\nUsername = Annotated[str, MyAfterValidator(str.lower)]\n\n\nclass Model(BaseModel):\n    name: Username\n\n\nassert Model(name='ABC').name == 'abc'  # (2)! The frozen=True specification makes MyAfterValidator hashable. Without this, a union such as Username | None will raise an error. Notice that type checkers will not complain about assigning 'ABC' to Username like they did in the previous example because they do not consider Username to be a distinct type from str . from dataclasses import dataclass\nfrom typing import Annotated, Any\nfrom collections.abc import Callable\n\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler\n\n\n@dataclass(frozen=True)  # (1)!\nclass MyAfterValidator:\n    func: Callable[[Any], Any]\n\n    def __get_pydantic_core_schema__(\n        self, source_type: Any, handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            self.func, handler(source_type)\n        )\n\n\nUsername = Annotated[str, MyAfterValidator(str.lower)]\n\n\nclass Model(BaseModel):\n    name: Username\n\n\nassert Model(name='ABC').name == 'abc'  # (2)! The frozen=True specification makes MyAfterValidator hashable. Without this, a union such as Username | None will raise an error. Notice that type checkers will not complain about assigning 'ABC' to Username like they did in the previous example because they do not consider Username to be a distinct type from str . Handling third-party types ¶ Another use case for the pattern in the previous section is to handle third party types. from typing import Annotated, Any\n\nfrom pydantic_core import core_schema\n\nfrom pydantic import (\n    BaseModel,\n    GetCoreSchemaHandler,\n    GetJsonSchemaHandler,\n    ValidationError,\n)\nfrom pydantic.json_schema import JsonSchemaValue\n\n\nclass ThirdPartyType:\n    \"\"\"\n    This is meant to represent a type from a third-party library that wasn't designed with Pydantic\n    integration in mind, and so doesn't have a `pydantic_core.CoreSchema` or anything.\n    \"\"\"\n\n    x: int\n\n    def __init__(self):\n        self.x = 0\n\n\nclass _ThirdPartyTypePydanticAnnotation:\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls,\n        _source_type: Any,\n        _handler: GetCoreSchemaHandler,\n    ) -> core_schema.CoreSchema:\n        \"\"\"\n        We return a pydantic_core.CoreSchema that behaves in the following ways:\n\n        * ints will be parsed as `ThirdPartyType` instances with the int as the x attribute\n        * `ThirdPartyType` instances will be parsed as `ThirdPartyType` instances without any changes\n        * Nothing else will pass validation\n        * Serialization will always return just an int\n        \"\"\"\n\n        def validate_from_int(value: int) -> ThirdPartyType:\n            result = ThirdPartyType()\n            result.x = value\n            return result\n\n        from_int_schema = core_schema.chain_schema(\n            [\n                core_schema.int_schema(),\n                core_schema.no_info_plain_validator_function(validate_from_int),\n            ]\n        )\n\n        return core_schema.json_or_python_schema(\n            json_schema=from_int_schema,\n            python_schema=core_schema.union_schema(\n                [\n                    # check if it's an instance first before doing any further work\n                    core_schema.is_instance_schema(ThirdPartyType),\n                    from_int_schema,\n                ]\n            ),\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                lambda instance: instance.x\n            ),\n        )\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, _core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        # Use the same schema that would be used for `int`\n        return handler(core_schema.int_schema())\n\n\n# We now create an `Annotated` wrapper that we'll use as the annotation for fields on `BaseModel`s, etc.\nPydanticThirdPartyType = Annotated[\n    ThirdPartyType, _ThirdPartyTypePydanticAnnotation\n]\n\n\n# Create a model class that uses this annotation as a field\nclass Model(BaseModel):\n    third_party_type: PydanticThirdPartyType\n\n\n# Demonstrate that this field is handled correctly, that ints are parsed into `ThirdPartyType`, and that\n# these instances are also \"dumped\" directly into ints as expected.\nm_int = Model(third_party_type=1)\nassert isinstance(m_int.third_party_type, ThirdPartyType)\nassert m_int.third_party_type.x == 1\nassert m_int.model_dump() == {'third_party_type': 1}\n\n# Do the same thing where an instance of ThirdPartyType is passed in\ninstance = ThirdPartyType()\nassert instance.x == 0\ninstance.x = 10\n\nm_instance = Model(third_party_type=instance)\nassert isinstance(m_instance.third_party_type, ThirdPartyType)\nassert m_instance.third_party_type.x == 10\nassert m_instance.model_dump() == {'third_party_type': 10}\n\n# Demonstrate that validation errors are raised as expected for invalid inputs\ntry:\n    Model(third_party_type='a')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for Model\n    third_party_type.is-instance[ThirdPartyType]\n      Input should be an instance of ThirdPartyType [type=is_instance_of, input_value='a', input_type=str]\n    third_party_type.chain[int,function-plain[validate_from_int()]]\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\"\n\n\nassert Model.model_json_schema() == {\n    'properties': {\n        'third_party_type': {'title': 'Third Party Type', 'type': 'integer'}\n    },\n    'required': ['third_party_type'],\n    'title': 'Model',\n    'type': 'object',\n} You can use this approach to e.g. define behavior for Pandas or Numpy types. Using GetPydanticSchema to reduce boilerplate ¶ You may notice that the above examples where we create a marker class require a good amount of boilerplate.\nFor many simple cases you can greatly minimize this by using pydantic.GetPydanticSchema : from typing import Annotated\n\nfrom pydantic_core import core_schema\n\nfrom pydantic import BaseModel, GetPydanticSchema\n\n\nclass Model(BaseModel):\n    y: Annotated[\n        str,\n        GetPydanticSchema(\n            lambda tp, handler: core_schema.no_info_after_validator_function(\n                lambda x: x * 2, handler(tp)\n            )\n        ),\n    ]\n\n\nassert Model(y='ab').y == 'abab' Summary ¶ Let's recap: Pydantic provides high level hooks to customize types via Annotated like AfterValidator and Field . Use these when possible. Under the hood these use pydantic-core to customize validation, and you can hook into that directly using GetPydanticSchema or a marker class with __get_pydantic_core_schema__ . If you really want a custom type you can implement __get_pydantic_core_schema__ on the type itself.","pageID":"Types","abs_url":"/latest/concepts/types/#customizing-validation-with-__get_pydantic_core_schema__","title":"Types - Custom Types - Customizing validation with __get_pydantic_core_schema__","objectID":"/latest/concepts/types/#customizing-validation-with-__get_pydantic_core_schema__","rank":65},{"content":"Warning This is an advanced technique that you might not need in the beginning. In most of\nthe cases you will probably be fine with standard Pydantic models. You can use Generic Classes as\nfield types and perform custom validation based on the \"type parameters\" (or sub-types)\nwith __get_pydantic_core_schema__ . If the Generic class that you are using as a sub-type has a classmethod __get_pydantic_core_schema__ , you don't need to use\n for it to work. Because the source_type parameter is not the same as the cls parameter, you can use typing.get_args (or typing_extensions.get_args ) to extract the generic parameters.\nThen you can use the handler to generate a schema for them by calling handler.generate_schema .\nNote that we do not do something like handler(get_args(source_type)[0]) because we want to generate an unrelated\nschema for that generic parameter, not one that is influenced by the current context of Annotated metadata and such.\nThis is less important for custom types, but crucial for annotated metadata that modifies schema building. Python 3.9 and above Python 3.10 and above from dataclasses import dataclass\nfrom typing import Any, Generic, TypeVar\n\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing_extensions import get_args, get_origin\n\nfrom pydantic import (\n    BaseModel,\n    GetCoreSchemaHandler,\n    ValidationError,\n    ValidatorFunctionWrapHandler,\n)\n\nItemType = TypeVar('ItemType')\n\n\n# This is not a pydantic model, it's an arbitrary generic class\n@dataclass\nclass Owner(Generic[ItemType]):\n    name: str\n    item: ItemType\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source_type: Any, handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        origin = get_origin(source_type)\n        if origin is None:  # used as `x: Owner` without params\n            origin = source_type\n            item_tp = Any\n        else:\n            item_tp = get_args(source_type)[0]\n        # both calling handler(...) and handler.generate_schema(...)\n        # would work, but prefer the latter for conceptual and consistency reasons\n        item_schema = handler.generate_schema(item_tp)\n\n        def val_item(\n            v: Owner[Any], handler: ValidatorFunctionWrapHandler\n        ) -> Owner[Any]:\n            v.item = handler(v.item)\n            return v\n\n        python_schema = core_schema.chain_schema(\n            # `chain_schema` means do the following steps in order:\n            [\n                # Ensure the value is an instance of Owner\n                core_schema.is_instance_schema(cls),\n                # Use the item_schema to validate `items`\n                core_schema.no_info_wrap_validator_function(\n                    val_item, item_schema\n                ),\n            ]\n        )\n\n        return core_schema.json_or_python_schema(\n            # for JSON accept an object with name and item keys\n            json_schema=core_schema.chain_schema(\n                [\n                    core_schema.typed_dict_schema(\n                        {\n                            'name': core_schema.typed_dict_field(\n                                core_schema.str_schema()\n                            ),\n                            'item': core_schema.typed_dict_field(item_schema),\n                        }\n                    ),\n                    # after validating the json data convert it to python\n                    core_schema.no_info_before_validator_function(\n                        lambda data: Owner(\n                            name=data['name'], item=data['item']\n                        ),\n                        # note that we reuse the same schema here as below\n                        python_schema,\n                    ),\n                ]\n            ),\n            python_schema=python_schema,\n        )\n\n\nclass Car(BaseModel):\n    color: str\n\n\nclass House(BaseModel):\n    rooms: int\n\n\nclass Model(BaseModel):\n    car_owner: Owner[Car]\n    home_owner: Owner[House]\n\n\nmodel = Model(\n    car_owner=Owner(name='John', item=Car(color='black')),\n    home_owner=Owner(name='James', item=House(rooms=3)),\n)\nprint(model)\n\"\"\"\ncar_owner=Owner(name='John', item=Car(color='black')) home_owner=Owner(name='James', item=House(rooms=3))\n\"\"\"\n\ntry:\n    # If the values of the sub-types are invalid, we get an error\n    Model(\n        car_owner=Owner(name='John', item=House(rooms=3)),\n        home_owner=Owner(name='James', item=Car(color='black')),\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for Model\n    wine\n      Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='Kinda good', input_type=str]\n    cheese\n      Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value='yeah', input_type=str]\n    \"\"\"\n\n# Similarly with JSON\nmodel = Model.model_validate_json(\n    '{\"car_owner\":{\"name\":\"John\",\"item\":{\"color\":\"black\"}},\"home_owner\":{\"name\":\"James\",\"item\":{\"rooms\":3}}}'\n)\nprint(model)\n\"\"\"\ncar_owner=Owner(name='John', item=Car(color='black')) home_owner=Owner(name='James', item=House(rooms=3))\n\"\"\"\n\ntry:\n    Model.model_validate_json(\n        '{\"car_owner\":{\"name\":\"John\",\"item\":{\"rooms\":3}},\"home_owner\":{\"name\":\"James\",\"item\":{\"color\":\"black\"}}}'\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for Model\n    car_owner.item.color\n      Field required [type=missing, input_value={'rooms': 3}, input_type=dict]\n    home_owner.item.rooms\n      Field required [type=missing, input_value={'color': 'black'}, input_type=dict]\n    \"\"\" from dataclasses import dataclass\nfrom typing import Any, Generic, TypeVar\n\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing import get_args, get_origin\n\nfrom pydantic import (\n    BaseModel,\n    GetCoreSchemaHandler,\n    ValidationError,\n    ValidatorFunctionWrapHandler,\n)\n\nItemType = TypeVar('ItemType')\n\n\n# This is not a pydantic model, it's an arbitrary generic class\n@dataclass\nclass Owner(Generic[ItemType]):\n    name: str\n    item: ItemType\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source_type: Any, handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        origin = get_origin(source_type)\n        if origin is None:  # used as `x: Owner` without params\n            origin = source_type\n            item_tp = Any\n        else:\n            item_tp = get_args(source_type)[0]\n        # both calling handler(...) and handler.generate_schema(...)\n        # would work, but prefer the latter for conceptual and consistency reasons\n        item_schema = handler.generate_schema(item_tp)\n\n        def val_item(\n            v: Owner[Any], handler: ValidatorFunctionWrapHandler\n        ) -> Owner[Any]:\n            v.item = handler(v.item)\n            return v\n\n        python_schema = core_schema.chain_schema(\n            # `chain_schema` means do the following steps in order:\n            [\n                # Ensure the value is an instance of Owner\n                core_schema.is_instance_schema(cls),\n                # Use the item_schema to validate `items`\n                core_schema.no_info_wrap_validator_function(\n                    val_item, item_schema\n                ),\n            ]\n        )\n\n        return core_schema.json_or_python_schema(\n            # for JSON accept an object with name and item keys\n            json_schema=core_schema.chain_schema(\n                [\n                    core_schema.typed_dict_schema(\n                        {\n                            'name': core_schema.typed_dict_field(\n                                core_schema.str_schema()\n                            ),\n                            'item': core_schema.typed_dict_field(item_schema),\n                        }\n                    ),\n                    # after validating the json data convert it to python\n                    core_schema.no_info_before_validator_function(\n                        lambda data: Owner(\n                            name=data['name'], item=data['item']\n                        ),\n                        # note that we reuse the same schema here as below\n                        python_schema,\n                    ),\n                ]\n            ),\n            python_schema=python_schema,\n        )\n\n\nclass Car(BaseModel):\n    color: str\n\n\nclass House(BaseModel):\n    rooms: int\n\n\nclass Model(BaseModel):\n    car_owner: Owner[Car]\n    home_owner: Owner[House]\n\n\nmodel = Model(\n    car_owner=Owner(name='John', item=Car(color='black')),\n    home_owner=Owner(name='James', item=House(rooms=3)),\n)\nprint(model)\n\"\"\"\ncar_owner=Owner(name='John', item=Car(color='black')) home_owner=Owner(name='James', item=House(rooms=3))\n\"\"\"\n\ntry:\n    # If the values of the sub-types are invalid, we get an error\n    Model(\n        car_owner=Owner(name='John', item=House(rooms=3)),\n        home_owner=Owner(name='James', item=Car(color='black')),\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for Model\n    wine\n      Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='Kinda good', input_type=str]\n    cheese\n      Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value='yeah', input_type=str]\n    \"\"\"\n\n# Similarly with JSON\nmodel = Model.model_validate_json(\n    '{\"car_owner\":{\"name\":\"John\",\"item\":{\"color\":\"black\"}},\"home_owner\":{\"name\":\"James\",\"item\":{\"rooms\":3}}}'\n)\nprint(model)\n\"\"\"\ncar_owner=Owner(name='John', item=Car(color='black')) home_owner=Owner(name='James', item=House(rooms=3))\n\"\"\"\n\ntry:\n    Model.model_validate_json(\n        '{\"car_owner\":{\"name\":\"John\",\"item\":{\"rooms\":3}},\"home_owner\":{\"name\":\"James\",\"item\":{\"color\":\"black\"}}}'\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for Model\n    car_owner.item.color\n      Field required [type=missing, input_value={'rooms': 3}, input_type=dict]\n    home_owner.item.rooms\n      Field required [type=missing, input_value={'color': 'black'}, input_type=dict]\n    \"\"\" Generic containers ¶ The same idea can be applied to create generic container types, like a custom Sequence type: Python 3.9 and above Python 3.10 and above from collections.abc import Sequence\nfrom typing import Any, TypeVar\n\nfrom pydantic_core import ValidationError, core_schema\nfrom typing_extensions import get_args\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler\n\nT = TypeVar('T')\n\n\nclass MySequence(Sequence[T]):\n    def __init__(self, v: Sequence[T]):\n        self.v = v\n\n    def __getitem__(self, i):\n        return self.v[i]\n\n    def __len__(self):\n        return len(self.v)\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: Any, handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        instance_schema = core_schema.is_instance_schema(cls)\n\n        args = get_args(source)\n        if args:\n            # replace the type and rely on Pydantic to generate the right schema\n            # for `Sequence`\n            sequence_t_schema = handler.generate_schema(Sequence[args[0]])\n        else:\n            sequence_t_schema = handler.generate_schema(Sequence)\n\n        non_instance_schema = core_schema.no_info_after_validator_function(\n            MySequence, sequence_t_schema\n        )\n        return core_schema.union_schema([instance_schema, non_instance_schema])\n\n\nclass M(BaseModel):\n    model_config = dict(validate_default=True)\n\n    s1: MySequence = [3]\n\n\nm = M()\nprint(m)\n#> s1=<__main__.MySequence object at 0x0123456789ab>\nprint(m.s1.v)\n#> [3]\n\n\nclass M(BaseModel):\n    s1: MySequence[int]\n\n\nM(s1=[1])\ntry:\n    M(s1=['a'])\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    2 validation errors for M\n    s1.is-instance[MySequence]\n      Input should be an instance of MySequence [type=is_instance_of, input_value=['a'], input_type=list]\n    s1.function-after[MySequence(), json-or-python[json=list[int],python=chain[is-instance[Sequence],function-wrap[sequence_validator()]]]].0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\" from collections.abc import Sequence\nfrom typing import Any, TypeVar\n\nfrom pydantic_core import ValidationError, core_schema\nfrom typing import get_args\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler\n\nT = TypeVar('T')\n\n\nclass MySequence(Sequence[T]):\n    def __init__(self, v: Sequence[T]):\n        self.v = v\n\n    def __getitem__(self, i):\n        return self.v[i]\n\n    def __len__(self):\n        return len(self.v)\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: Any, handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        instance_schema = core_schema.is_instance_schema(cls)\n\n        args = get_args(source)\n        if args:\n            # replace the type and rely on Pydantic to generate the right schema\n            # for `Sequence`\n            sequence_t_schema = handler.generate_schema(Sequence[args[0]])\n        else:\n            sequence_t_schema = handler.generate_schema(Sequence)\n\n        non_instance_schema = core_schema.no_info_after_validator_function(\n            MySequence, sequence_t_schema\n        )\n        return core_schema.union_schema([instance_schema, non_instance_schema])\n\n\nclass M(BaseModel):\n    model_config = dict(validate_default=True)\n\n    s1: MySequence = [3]\n\n\nm = M()\nprint(m)\n#> s1=<__main__.MySequence object at 0x0123456789ab>\nprint(m.s1.v)\n#> [3]\n\n\nclass M(BaseModel):\n    s1: MySequence[int]\n\n\nM(s1=[1])\ntry:\n    M(s1=['a'])\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    2 validation errors for M\n    s1.is-instance[MySequence]\n      Input should be an instance of MySequence [type=is_instance_of, input_value=['a'], input_type=list]\n    s1.function-after[MySequence(), json-or-python[json=list[int],python=chain[is-instance[Sequence],function-wrap[sequence_validator()]]]].0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    \"\"\"","pageID":"Types","abs_url":"/latest/concepts/types/#handling-custom-generic-classes","title":"Types - Custom Types - Handling custom generic classes","objectID":"/latest/concepts/types/#handling-custom-generic-classes","rank":60},{"content":"Note This was not possible with Pydantic V2 to V2.3, it was re-added in Pydantic V2.4. As of Pydantic V2.4, you can access the field name via the handler.field_name within __get_pydantic_core_schema__ and thereby set the field name which will be available from info.field_name . from typing import Any\n\nfrom pydantic_core import core_schema\n\nfrom pydantic import BaseModel, GetCoreSchemaHandler, ValidationInfo\n\n\nclass CustomType:\n    \"\"\"Custom type that stores the field it was used in.\"\"\"\n\n    def __init__(self, value: int, field_name: str):\n        self.value = value\n        self.field_name = field_name\n\n    def __repr__(self):\n        return f'CustomType<{self.value} {self.field_name!r}>'\n\n    @classmethod\n    def validate(cls, value: int, info: ValidationInfo):\n        return cls(value, info.field_name)\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source_type: Any, handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        return core_schema.with_info_after_validator_function(\n            cls.validate, handler(int)\n        )\n\n\nclass MyModel(BaseModel):\n    my_field: CustomType\n\n\nm = MyModel(my_field=1)\nprint(m.my_field)\n#> CustomType<1 'my_field'> You can also access field_name from the markers used with Annotated , like . from typing import Annotated\n\nfrom pydantic import AfterValidator, BaseModel, ValidationInfo\n\n\ndef my_validators(value: int, info: ValidationInfo):\n    return f'<{value} {info.field_name!r}>'\n\n\nclass MyModel(BaseModel):\n    my_field: Annotated[int, AfterValidator(my_validators)]\n\n\nm = MyModel(my_field=1)\nprint(m.my_field)\n#> <1 'my_field'>","pageID":"Types","abs_url":"/latest/concepts/types/#access-to-field-name","title":"Types - Custom Types - Access to field name","objectID":"/latest/concepts/types/#access-to-field-name","rank":55},{"content":"Unions are fundamentally different to all other types Pydantic validates - instead of requiring all fields/items/values to be valid, unions require only one member to be valid. This leads to some nuance around how to validate unions: which member(s) of the union should you validate data against, and in which order? which errors to raise when validation fails? Validating unions feels like adding another orthogonal dimension to the validation process. To solve these problems, Pydantic supports three fundamental approaches to validating unions: left to right mode - the simplest approach, each member of the union is tried in order and the first match is returned smart mode - similar to \"left to right mode\" members are tried in order; however, validation will proceed past the first match to attempt to find a better match, this is the default mode for most union validation discriminated unions - only one member of the union is tried, based on a discriminator Tip In general, we recommend using discriminated unions . They are both more performant and more predictable than untagged unions, as they allow you to control which member of the union to validate against. For complex cases, if you're using untagged unions, it's recommended to use union_mode='left_to_right' if you need guarantees about the order of validation attempts against the union members. If you're looking for incredibly specialized behavior, you can use a custom validator .","pageID":"Unions","abs_url":"/latest/concepts/unions/#Unions","title":"Unions","objectID":"/latest/concepts/unions/#Unions","rank":100},{"content":"","pageID":"Unions","abs_url":"/latest/concepts/unions/#union-modes","title":"Unions - Union Modes","objectID":"/latest/concepts/unions/#union-modes","rank":95},{"content":"Note Because this mode often leads to unexpected validation results, it is not the default in Pydantic >=2, instead union_mode='smart' is the default. With this approach, validation is attempted against each member of the union in their order they're defined, and the first successful validation is accepted as input. If validation fails on all members, the validation error includes the errors from all members of the union. union_mode='left_to_right' must be set as a Field parameter on union fields where you want to use it. Python 3.9 and above Python 3.10 and above from typing import Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass User(BaseModel):\n    id: Union[str, int] = Field(union_mode='left_to_right')\n\n\nprint(User(id=123))\n#> id=123\nprint(User(id='hello'))\n#> id='hello'\n\ntry:\n    User(id=[])\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for User\n    id.str\n      Input should be a valid string [type=string_type, input_value=[], input_type=list]\n    id.int\n      Input should be a valid integer [type=int_type, input_value=[], input_type=list]\n    \"\"\" from pydantic import BaseModel, Field, ValidationError\n\n\nclass User(BaseModel):\n    id: str | int = Field(union_mode='left_to_right')\n\n\nprint(User(id=123))\n#> id=123\nprint(User(id='hello'))\n#> id='hello'\n\ntry:\n    User(id=[])\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for User\n    id.str\n      Input should be a valid string [type=string_type, input_value=[], input_type=list]\n    id.int\n      Input should be a valid integer [type=int_type, input_value=[], input_type=list]\n    \"\"\" The order of members is very important in this case, as demonstrated by tweak the above example: Python 3.9 and above Python 3.10 and above from typing import Union\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: Union[int, str] = Field(union_mode='left_to_right')\n\n\nprint(User(id=123))  # (1)\n#> id=123\nprint(User(id='456'))  # (2)\n#> id=456 As expected the input is validated against the int member and the result is as expected. We're in lax mode and the numeric string '123' is valid as input to the first member of the union, int .\n   Since that is tried first, we get the surprising result of id being an int instead of a str . from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: int | str = Field(union_mode='left_to_right')\n\n\nprint(User(id=123))  # (1)\n#> id=123\nprint(User(id='456'))  # (2)\n#> id=456 As expected the input is validated against the int member and the result is as expected. We're in lax mode and the numeric string '123' is valid as input to the first member of the union, int .\n   Since that is tried first, we get the surprising result of id being an int instead of a str .","pageID":"Unions","abs_url":"/latest/concepts/unions/#left-to-right-mode","title":"Unions - Union Modes - Left to Right Mode","objectID":"/latest/concepts/unions/#left-to-right-mode","rank":90},{"content":"Because of the potentially surprising results of union_mode='left_to_right' , in Pydantic >=2 the default mode for Union validation is union_mode='smart' . In this mode, pydantic attempts to select the best match for the input from the union members. The exact algorithm may change between Pydantic minor releases to allow for improvements in both performance and accuracy. Note We reserve the right to tweak the internal smart matching algorithm in future versions of Pydantic. If you rely on very specific\nmatching behavior, it's recommended to use union_mode='left_to_right' or discriminated unions . Python 3.9 and above Python 3.10 and above from typing import Union\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: Union[int, str, UUID]\n    name: str\n\n\nuser_01 = User(id=123, name='John Doe')\nprint(user_01)\n#> id=123 name='John Doe'\nprint(user_01.id)\n#> 123\nuser_02 = User(id='1234', name='John Doe')\nprint(user_02)\n#> id='1234' name='John Doe'\nprint(user_02.id)\n#> 1234\nuser_03_uuid = UUID('cf57432e-809e-4353-adbd-9d5c0d733868')\nuser_03 = User(id=user_03_uuid, name='John Doe')\nprint(user_03)\n#> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe'\nprint(user_03.id)\n#> cf57432e-809e-4353-adbd-9d5c0d733868\nprint(user_03_uuid.int)\n#> 275603287559914445491632874575877060712 from uuid import UUID\n\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: int | str | UUID\n    name: str\n\n\nuser_01 = User(id=123, name='John Doe')\nprint(user_01)\n#> id=123 name='John Doe'\nprint(user_01.id)\n#> 123\nuser_02 = User(id='1234', name='John Doe')\nprint(user_02)\n#> id='1234' name='John Doe'\nprint(user_02.id)\n#> 1234\nuser_03_uuid = UUID('cf57432e-809e-4353-adbd-9d5c0d733868')\nuser_03 = User(id=user_03_uuid, name='John Doe')\nprint(user_03)\n#> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe'\nprint(user_03.id)\n#> cf57432e-809e-4353-adbd-9d5c0d733868\nprint(user_03_uuid.int)\n#> 275603287559914445491632874575877060712","pageID":"Unions","abs_url":"/latest/concepts/unions/#smart-mode","title":"Unions - Union Modes - Smart Mode","objectID":"/latest/concepts/unions/#smart-mode","rank":85},{"content":"Discriminated unions are sometimes referred to as \"Tagged Unions\". We can use discriminated unions to more efficiently validate Union types, by choosing which member of the union to validate against. This makes validation more efficient and also avoids a proliferation of errors when validation fails. Adding discriminator to unions also means the generated JSON schema implements the associated OpenAPI specification .","pageID":"Unions","abs_url":"/latest/concepts/unions/#discriminated-unions","title":"Unions - Discriminated Unions","objectID":"/latest/concepts/unions/#discriminated-unions","rank":80},{"content":"Frequently, in the case of a Union with multiple models,\nthere is a common field to all members of the union that can be used to distinguish\nwhich union case the data should be validated against; this is referred to as the \"discriminator\" in OpenAPI . To validate models based on that information you can set the same field - let's call it my_discriminator -\nin each of the models with a discriminated value, which is one (or many) Literal value(s).\nFor your Union , you can set the discriminator in its value: Field(discriminator='my_discriminator') . Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    meows: int\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    barks: float\n\n\nclass Lizard(BaseModel):\n    pet_type: Literal['reptile', 'lizard']\n    scales: bool\n\n\nclass Model(BaseModel):\n    pet: Union[Cat, Dog, Lizard] = Field(discriminator='pet_type')\n    n: int\n\n\nprint(Model(pet={'pet_type': 'dog', 'barks': 3.14}, n=1))\n#> pet=Dog(pet_type='dog', barks=3.14) n=1\ntry:\n    Model(pet={'pet_type': 'dog'}, n=1)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    pet.dog.barks\n      Field required [type=missing, input_value={'pet_type': 'dog'}, input_type=dict]\n    \"\"\" from typing import Literal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    meows: int\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    barks: float\n\n\nclass Lizard(BaseModel):\n    pet_type: Literal['reptile', 'lizard']\n    scales: bool\n\n\nclass Model(BaseModel):\n    pet: Cat | Dog | Lizard = Field(discriminator='pet_type')\n    n: int\n\n\nprint(Model(pet={'pet_type': 'dog', 'barks': 3.14}, n=1))\n#> pet=Dog(pet_type='dog', barks=3.14) n=1\ntry:\n    Model(pet={'pet_type': 'dog'}, n=1)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    pet.dog.barks\n      Field required [type=missing, input_value={'pet_type': 'dog'}, input_type=dict]\n    \"\"\"","pageID":"Unions","abs_url":"/latest/concepts/unions/#discriminated-unions-with-str-discriminators","title":"Unions - Discriminated Unions - Discriminated Unions with str discriminators","objectID":"/latest/concepts/unions/#discriminated-unions-with-str-discriminators","rank":75},{"content":"In the case of a Union with multiple models, sometimes there isn't a single uniform field\nacross all models that you can use as a discriminator.\nThis is the perfect use case for a callable Discriminator . Tip When you're designing callable discriminators, remember that you might have to account\nfor both dict and model type inputs. This pattern is similar to that of mode='before' validators,\nwhere you have to anticipate various forms of input. But wait! You ask, I only anticipate passing in dict types, why do I need to account for models?\nPydantic uses callable discriminators for serialization as well, at which point the input to your callable is\nvery likely to be a model instance. In the following examples, you'll see that the callable discriminators are designed to handle both dict and model inputs.\nIf you don't follow this practice, it's likely that you'll, in the best case, get warnings during serialization,\nand in the worst case, get runtime errors during validation. Python 3.9 and above Python 3.10 and above from typing import Annotated, Any, Literal, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        Union[\n            Annotated[ApplePie, Tag('apple')],\n            Annotated[PumpkinPie, Tag('pumpkin')],\n        ],\n        Discriminator(get_discriminator_value),\n    ]\n\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n\"\"\"\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n\"\"\"\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n\"\"\"\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n\"\"\" from typing import Annotated, Any, Literal\n\nfrom pydantic import BaseModel, Discriminator, Tag\n\n\nclass Pie(BaseModel):\n    time_to_cook: int\n    num_ingredients: int\n\n\nclass ApplePie(Pie):\n    fruit: Literal['apple'] = 'apple'\n\n\nclass PumpkinPie(Pie):\n    filling: Literal['pumpkin'] = 'pumpkin'\n\n\ndef get_discriminator_value(v: Any) -> str:\n    if isinstance(v, dict):\n        return v.get('fruit', v.get('filling'))\n    return getattr(v, 'fruit', getattr(v, 'filling', None))\n\n\nclass ThanksgivingDinner(BaseModel):\n    dessert: Annotated[\n        (\n            Annotated[ApplePie, Tag('apple')] |\n            Annotated[PumpkinPie, Tag('pumpkin')]\n        ),\n        Discriminator(get_discriminator_value),\n    ]\n\n\napple_variation = ThanksgivingDinner.model_validate(\n    {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n)\nprint(repr(apple_variation))\n\"\"\"\nThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n\"\"\"\n\npumpkin_variation = ThanksgivingDinner.model_validate(\n    {\n        'dessert': {\n            'filling': 'pumpkin',\n            'time_to_cook': 40,\n            'num_ingredients': 6,\n        }\n    }\n)\nprint(repr(pumpkin_variation))\n\"\"\"\nThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n\"\"\" Discriminator s can also be used to validate Union types with combinations of models and primitive types. For example: Python 3.9 and above Python 3.10 and above from typing import Annotated, Any, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag, ValidationError\n\n\ndef model_x_discriminator(v: Any) -> str:\n    if isinstance(v, int):\n        return 'int'\n    if isinstance(v, (dict, BaseModel)):\n        return 'model'\n    else:\n        # return None if the discriminator value isn't found\n        return None\n\n\nclass SpecialValue(BaseModel):\n    value: int\n\n\nclass DiscriminatedModel(BaseModel):\n    value: Annotated[\n        Union[\n            Annotated[int, Tag('int')],\n            Annotated['SpecialValue', Tag('model')],\n        ],\n        Discriminator(model_x_discriminator),\n    ]\n\n\nmodel_data = {'value': {'value': 1}}\nm = DiscriminatedModel.model_validate(model_data)\nprint(m)\n#> value=SpecialValue(value=1)\n\nint_data = {'value': 123}\nm = DiscriminatedModel.model_validate(int_data)\nprint(m)\n#> value=123\n\ntry:\n    DiscriminatedModel.model_validate({'value': 'not an int or a model'})\nexcept ValidationError as e:\n    print(e)  # (1)!\n    \"\"\"\n    1 validation error for DiscriminatedModel\n    value\n      Unable to extract tag using discriminator model_x_discriminator() [type=union_tag_not_found, input_value='not an int or a model', input_type=str]\n    \"\"\" Notice the callable discriminator function returns None if a discriminator value is not found.\n   When None is returned, this union_tag_not_found error is raised. from typing import Annotated, Any\n\nfrom pydantic import BaseModel, Discriminator, Tag, ValidationError\n\n\ndef model_x_discriminator(v: Any) -> str:\n    if isinstance(v, int):\n        return 'int'\n    if isinstance(v, (dict, BaseModel)):\n        return 'model'\n    else:\n        # return None if the discriminator value isn't found\n        return None\n\n\nclass SpecialValue(BaseModel):\n    value: int\n\n\nclass DiscriminatedModel(BaseModel):\n    value: Annotated[\n        (\n            Annotated[int, Tag('int')] |\n            Annotated['SpecialValue', Tag('model')]\n        ),\n        Discriminator(model_x_discriminator),\n    ]\n\n\nmodel_data = {'value': {'value': 1}}\nm = DiscriminatedModel.model_validate(model_data)\nprint(m)\n#> value=SpecialValue(value=1)\n\nint_data = {'value': 123}\nm = DiscriminatedModel.model_validate(int_data)\nprint(m)\n#> value=123\n\ntry:\n    DiscriminatedModel.model_validate({'value': 'not an int or a model'})\nexcept ValidationError as e:\n    print(e)  # (1)!\n    \"\"\"\n    1 validation error for DiscriminatedModel\n    value\n      Unable to extract tag using discriminator model_x_discriminator() [type=union_tag_not_found, input_value='not an int or a model', input_type=str]\n    \"\"\" Notice the callable discriminator function returns None if a discriminator value is not found.\n   When None is returned, this union_tag_not_found error is raised. Note Using the annotated pattern can be handy to regroup\nthe Union and discriminator information. See the next example for more details. There are a few ways to set a discriminator for a field, all varying slightly in syntax. For str discriminators: some_field: Union[...] = Field(discriminator='my_discriminator')\nsome_field: Annotated[Union[...], Field(discriminator='my_discriminator')] For callable Discriminator s: some_field: Union[...] = Field(discriminator=Discriminator(...))\nsome_field: Annotated[Union[...], Discriminator(...)]\nsome_field: Annotated[Union[...], Field(discriminator=Discriminator(...))] Warning Discriminated unions cannot be used with only a single variant, such as Union[Cat] . Python changes Union[T] into T at interpretation time, so it is not possible for pydantic to\ndistinguish fields of Union[T] from T .","pageID":"Unions","abs_url":"/latest/concepts/unions/#discriminated-unions-with-callable-discriminator","title":"Unions - Discriminated Unions - Discriminated Unions with callable Discriminator","objectID":"/latest/concepts/unions/#discriminated-unions-with-callable-discriminator","rank":70},{"content":"Only one discriminator can be set for a field but sometimes you want to combine multiple discriminators.\nYou can do it by creating nested Annotated types, e.g.: from typing import Annotated, Literal, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass BlackCat(BaseModel):\n    pet_type: Literal['cat']\n    color: Literal['black']\n    black_name: str\n\n\nclass WhiteCat(BaseModel):\n    pet_type: Literal['cat']\n    color: Literal['white']\n    white_name: str\n\n\nCat = Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    name: str\n\n\nPet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]\n\n\nclass Model(BaseModel):\n    pet: Pet\n    n: int\n\n\nm = Model(pet={'pet_type': 'cat', 'color': 'black', 'black_name': 'felix'}, n=1)\nprint(m)\n#> pet=BlackCat(pet_type='cat', color='black', black_name='felix') n=1\ntry:\n    Model(pet={'pet_type': 'cat', 'color': 'red'}, n='1')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    pet.cat\n      Input tag 'red' found using 'color' does not match any of the expected tags: 'black', 'white' [type=union_tag_invalid, input_value={'pet_type': 'cat', 'color': 'red'}, input_type=dict]\n    \"\"\"\ntry:\n    Model(pet={'pet_type': 'cat', 'color': 'black'}, n='1')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    pet.cat.black.black_name\n      Field required [type=missing, input_value={'pet_type': 'cat', 'color': 'black'}, input_type=dict]\n    \"\"\" Tip If you want to validate data against a union, and solely a union, you can use pydantic's TypeAdapter construct instead of inheriting from the standard BaseModel . In the context of the previous example, we have the following: type_adapter = TypeAdapter(Pet)\n\npet = type_adapter.validate_python(\n    {'pet_type': 'cat', 'color': 'black', 'black_name': 'felix'}\n)\nprint(repr(pet))\n#> BlackCat(pet_type='cat', color='black', black_name='felix')","pageID":"Unions","abs_url":"/latest/concepts/unions/#nested-discriminated-unions","title":"Unions - Discriminated Unions - Nested Discriminated Unions","objectID":"/latest/concepts/unions/#nested-discriminated-unions","rank":65},{"content":"When Union validation fails, error messages can be quite verbose, as they will produce validation errors for\neach case in the union.\nThis is especially noticeable when dealing with recursive models, where reasons may be generated at each level of\nrecursion.\nDiscriminated unions help to simplify error messages in this case, as validation errors are only produced for\nthe case with a matching discriminator value. You can also customize the error type, message, and context for a Discriminator by passing\nthese specifications as parameters to the Discriminator constructor, as seen in the example below. from typing import Annotated, Union\n\nfrom pydantic import BaseModel, Discriminator, Tag, ValidationError\n\n\n# Errors are quite verbose with a normal Union:\nclass Model(BaseModel):\n    x: Union[str, 'Model']\n\n\ntry:\n    Model.model_validate({'x': {'x': {'x': 1}}})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    4 validation errors for Model\n    x.str\n      Input should be a valid string [type=string_type, input_value={'x': {'x': 1}}, input_type=dict]\n    x.Model.x.str\n      Input should be a valid string [type=string_type, input_value={'x': 1}, input_type=dict]\n    x.Model.x.Model.x.str\n      Input should be a valid string [type=string_type, input_value=1, input_type=int]\n    x.Model.x.Model.x.Model\n      Input should be a valid dictionary or instance of Model [type=model_type, input_value=1, input_type=int]\n    \"\"\"\n\ntry:\n    Model.model_validate({'x': {'x': {'x': {}}}})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    4 validation errors for Model\n    x.str\n      Input should be a valid string [type=string_type, input_value={'x': {'x': {}}}, input_type=dict]\n    x.Model.x.str\n      Input should be a valid string [type=string_type, input_value={'x': {}}, input_type=dict]\n    x.Model.x.Model.x.str\n      Input should be a valid string [type=string_type, input_value={}, input_type=dict]\n    x.Model.x.Model.x.Model.x\n      Field required [type=missing, input_value={}, input_type=dict]\n    \"\"\"\n\n\n# Errors are much simpler with a discriminated union:\ndef model_x_discriminator(v):\n    if isinstance(v, str):\n        return 'str'\n    if isinstance(v, (dict, BaseModel)):\n        return 'model'\n\n\nclass DiscriminatedModel(BaseModel):\n    x: Annotated[\n        Union[\n            Annotated[str, Tag('str')],\n            Annotated['DiscriminatedModel', Tag('model')],\n        ],\n        Discriminator(\n            model_x_discriminator,\n            custom_error_type='invalid_union_member',  # (1)!\n            custom_error_message='Invalid union member',  # (2)!\n            custom_error_context={'discriminator': 'str_or_model'},  # (3)!\n        ),\n    ]\n\n\ntry:\n    DiscriminatedModel.model_validate({'x': {'x': {'x': 1}}})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for DiscriminatedModel\n    x.model.x.model.x\n      Invalid union member [type=invalid_union_member, input_value=1, input_type=int]\n    \"\"\"\n\ntry:\n    DiscriminatedModel.model_validate({'x': {'x': {'x': {}}}})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for DiscriminatedModel\n    x.model.x.model.x.model.x\n      Field required [type=missing, input_value={}, input_type=dict]\n    \"\"\"\n\n# The data is still handled properly when valid:\ndata = {'x': {'x': {'x': 'a'}}}\nm = DiscriminatedModel.model_validate(data)\nprint(m.model_dump())\n#> {'x': {'x': {'x': 'a'}}} custom_error_type is the type attribute of the ValidationError raised when validation fails. custom_error_message is the msg attribute of the ValidationError raised when validation fails. custom_error_context is the ctx attribute of the ValidationError raised when validation fails. You can also simplify error messages by labeling each case with a .\nThis is especially useful when you have complex types like those in this example: from typing import Annotated, Union\n\nfrom pydantic import AfterValidator, Tag, TypeAdapter, ValidationError\n\nDoubledList = Annotated[list[int], AfterValidator(lambda x: x * 2)]\nStringsMap = dict[str, str]\n\n\n# Not using any `Tag`s for each union case, the errors are not so nice to look at\nadapter = TypeAdapter(Union[DoubledList, StringsMap])\n\ntry:\n    adapter.validate_python(['a'])\nexcept ValidationError as exc_info:\n    print(exc_info)\n    \"\"\"\n    2 validation errors for union[function-after[ (), list[int]],dict[str,str]]\n    function-after[ (), list[int]].0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    dict[str,str]\n      Input should be a valid dictionary [type=dict_type, input_value=['a'], input_type=list]\n    \"\"\"\n\ntag_adapter = TypeAdapter(\n    Union[\n        Annotated[DoubledList, Tag('DoubledList')],\n        Annotated[StringsMap, Tag('StringsMap')],\n    ]\n)\n\ntry:\n    tag_adapter.validate_python(['a'])\nexcept ValidationError as exc_info:\n    print(exc_info)\n    \"\"\"\n    2 validation errors for union[DoubledList,StringsMap]\n    DoubledList.0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n    StringsMap\n      Input should be a valid dictionary [type=dict_type, input_value=['a'], input_type=list]\n    \"\"\"","pageID":"Unions","abs_url":"/latest/concepts/unions/#union-validation-errors","title":"Unions - Union Validation Errors","objectID":"/latest/concepts/unions/#union-validation-errors","rank":60},{"content":"The  decorator allows the arguments passed to a function to be parsed\nand validated using the function's annotations before the function is called. While under the hood this uses the same approach of model creation and initialisation\n(see Validators for more details), it provides an extremely easy way to apply validation\nto your code with minimal boilerplate. Example of usage: from pydantic import ValidationError, validate_call\n\n\n@validate_call\ndef repeat(s: str, count: int, *, separator: bytes = b'') -> bytes:\n    b = s.encode()\n    return separator.join(b for _ in range(count))\n\n\na = repeat('hello', 3)\nprint(a)\n#> b'hellohellohello'\n\nb = repeat('x', '4', separator=b' ')\nprint(b)\n#> b'x x x x'\n\ntry:\n    c = repeat('hello', 'wrong')\nexcept ValidationError as exc:\n    print(exc)\n    \"\"\"\n    1 validation error for repeat\n    1\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='wrong', input_type=str]\n    \"\"\"","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#Validation Decorator","title":"Validation Decorator","objectID":"/latest/concepts/validation_decorator/#Validation Decorator","rank":100},{"content":"Parameter types are inferred from type annotations on the function, or as  if not annotated. All types listed in types can be validated, including Pydantic models and custom types .\nAs with the rest of Pydantic, types are by default coerced by the decorator before they're passed to the actual function: from datetime import date\n\nfrom pydantic import validate_call\n\n\n@validate_call\ndef greater_than(d1: date, d2: date, *, include_equal=False) -> date:  # (1)!\n    if include_equal:\n        return d1 >= d2\n    else:\n        return d1 > d2\n\n\nd1 = '2000-01-01'  # (2)!\nd2 = date(2001, 1, 1)\ngreater_than(d1, d2, include_equal=True) Because include_equal has no type annotation, it will be inferred as . Although d1 is a string, it will be converted to a  object. Type coercion like this can be extremely helpful, but also confusing or not desired (see model data conversion ). Strict mode can be enabled by using a custom configuration . Validating the return value By default, the return value of the function is not validated. To do so, the validate_return argument\nof the decorator can be set to True .","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#parameter-types","title":"Validation Decorator - Parameter types","objectID":"/latest/concepts/validation_decorator/#parameter-types","rank":95},{"content":"The  decorator is designed to work with functions\nusing all possible  and all possible combinations of these: Positional or keyword parameters with or without defaults. Keyword-only parameters: parameters after *, . Positional-only parameters: parameters before , / . Variable positional parameters defined via * (often *args ). Variable keyword parameters defined via ** (often **kwargs ). for keyword parameters and typed dictionaries can be used to annotate the variable\nkeyword parameters of a function: from typing_extensions import TypedDict, Unpack\n\nfrom pydantic import validate_call\n\n\nclass Point(TypedDict):\n    x: int\n    y: int\n\n\n@validate_call\ndef add_coords(**kwargs: Unpack[Point]) -> int:\n    return kwargs['x'] + kwargs['y']\n\n\nadd_coords(x=1, y=2) For reference, see the related specification section and PEP 692 .","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#function-signatures","title":"Validation Decorator - Function signatures","objectID":"/latest/concepts/validation_decorator/#function-signatures","rank":90},{"content":"The Field() function can also be used with the decorator to provide extra information about\nthe field and validations. If you don't make use of the default or default_factory parameter, it is\nrecommended to use the annotated pattern (so that type checkers\ninfer the parameter as being required). Otherwise, the  function can be used\nas a default value (again, to trick type checkers into thinking a default value is provided for the parameter). from typing import Annotated\n\nfrom pydantic import Field, ValidationError, validate_call\n\n\n@validate_call\ndef how_many(num: Annotated[int, Field(gt=10)]):\n    return num\n\n\ntry:\n    how_many(1)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for how_many\n    0\n      Input should be greater than 10 [type=greater_than, input_value=1, input_type=int]\n    \"\"\"\n\n\n@validate_call\ndef return_value(value: str = Field(default='default value')):\n    return value\n\n\nprint(return_value())\n#> default value Aliases can be used with the decorator as normal: from typing import Annotated\n\nfrom pydantic import Field, validate_call\n\n\n@validate_call\ndef how_many(num: Annotated[int, Field(gt=10, alias='number')]):\n    return num\n\n\nhow_many(number=42)","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#using-the-field-function-to-describe-function-parameters","title":"Validation Decorator - Using the  function to describe function parameters","objectID":"/latest/concepts/validation_decorator/#using-the-field-function-to-describe-function-parameters","rank":85},{"content":"The original function which was decorated can still be accessed by using the raw_function attribute.\nThis is useful if in some scenarios you trust your input arguments and want to call the function in the most efficient way (see notes on performance below): from pydantic import validate_call\n\n\n@validate_call\ndef repeat(s: str, count: int, *, separator: bytes = b'') -> bytes:\n    b = s.encode()\n    return separator.join(b for _ in range(count))\n\n\na = repeat('hello', 3)\nprint(a)\n#> b'hellohellohello'\n\nb = repeat.raw_function('good bye', 2, separator=b', ')\nprint(b)\n#> b'good bye, good bye'","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#accessing-the-original-function","title":"Validation Decorator - Accessing the original function","objectID":"/latest/concepts/validation_decorator/#accessing-the-original-function","rank":80},{"content":"can also be used on async functions: class Connection:\n    async def execute(self, sql, *args):\n        return 'testing@example.com'\n\n\nconn = Connection()\n# ignore-above\nimport asyncio\n\nfrom pydantic import PositiveInt, ValidationError, validate_call\n\n\n@validate_call\nasync def get_user_email(user_id: PositiveInt):\n    # `conn` is some fictional connection to a database\n    email = await conn.execute('select email from users where id=$1', user_id)\n    if email is None:\n        raise RuntimeError('user not found')\n    else:\n        return email\n\n\nasync def main():\n    email = await get_user_email(123)\n    print(email)\n    #> testing@example.com\n    try:\n        await get_user_email(-4)\n    except ValidationError as exc:\n        print(exc.errors())\n        \"\"\"\n        [\n            {\n                'type': 'greater_than',\n                'loc': (0,),\n                'msg': 'Input should be greater than 0',\n                'input': -4,\n                'ctx': {'gt': 0},\n                'url': 'https://errors.pydantic.dev/2/v/greater_than',\n            }\n        ]\n        \"\"\"\n\n\nasyncio.run(main())\n# requires: `conn.execute()` that will return `'testing@example.com'`","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#async-functions","title":"Validation Decorator - Async functions","objectID":"/latest/concepts/validation_decorator/#async-functions","rank":75},{"content":"As the  decorator preserves the decorated function's signature,\nit should be compatible with type checkers (such as mypy and pyright). However, due to current limitations in the Python type system,\nthe raw_function or other attributes won't be recognized and you will\nneed to suppress the error using (usually with a # type: ignore comment).","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#compatibility-with-type-checkers","title":"Validation Decorator - Compatibility with type checkers","objectID":"/latest/concepts/validation_decorator/#compatibility-with-type-checkers","rank":70},{"content":"Similarly to Pydantic models, the config parameter of the decorator can be used to specify a custom configuration: from pydantic import ConfigDict, ValidationError, validate_call\n\n\nclass Foobar:\n    def __init__(self, v: str):\n        self.v = v\n\n    def __add__(self, other: 'Foobar') -> str:\n        return f'{self} + {other}'\n\n    def __str__(self) -> str:\n        return f'Foobar({self.v})'\n\n\n@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef add_foobars(a: Foobar, b: Foobar):\n    return a + b\n\n\nc = add_foobars(Foobar('a'), Foobar('b'))\nprint(c)\n#> Foobar(a) + Foobar(b)\n\ntry:\n    add_foobars(1, 2)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    2 validation errors for add_foobars\n    0\n      Input should be an instance of Foobar [type=is_instance_of, input_value=1, input_type=int]\n    1\n      Input should be an instance of Foobar [type=is_instance_of, input_value=2, input_type=int]\n    \"\"\"","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#custom-configuration","title":"Validation Decorator - Custom configuration","objectID":"/latest/concepts/validation_decorator/#custom-configuration","rank":65},{"content":"In some cases, it may be helpful to separate validation of a function's arguments from the function call itself.\nThis might be useful when a particular function is costly/time consuming. Here's an example of a workaround you can use for that pattern: from pydantic import validate_call\n\n\n@validate_call\ndef validate_foo(a: int, b: int):\n    def foo():\n        return a + b\n\n    return foo\n\n\nfoo = validate_foo(a=1, b=2)\nprint(foo())\n#> 3","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#extension-validating-arguments-before-calling-a-function","title":"Validation Decorator - Extension — validating arguments before calling a function","objectID":"/latest/concepts/validation_decorator/#extension-validating-arguments-before-calling-a-function","rank":60},{"content":"","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#limitations","title":"Validation Decorator - Limitations","objectID":"/latest/concepts/validation_decorator/#limitations","rank":55},{"content":"Currently upon validation failure, a standard Pydantic  is raised\n(see model error handling for details). This is also true for missing required arguments,\nwhere Python normally raises a .","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#validation-exception","title":"Validation Decorator - Limitations - Validation exception","objectID":"/latest/concepts/validation_decorator/#validation-exception","rank":50},{"content":"We've made a big effort to make Pydantic as performant as possible. While the inspection of the decorated\nfunction is only performed once, there will still be a performance impact when making calls to the function\ncompared to using the original function. In many situations, this will have little or no noticeable effect. However, be aware that\n is not an equivalent or alternative to function\ndefinitions in strongly typed languages, and it never will be.","pageID":"Validation Decorator","abs_url":"/latest/concepts/validation_decorator/#performance","title":"Validation Decorator - Limitations - Performance","objectID":"/latest/concepts/validation_decorator/#performance","rank":45},{"content":"In addition to Pydantic's built-in validation capabilities ,\nyou can leverage custom validators at the field and model levels to enforce more complex constraints\nand ensure the integrity of your data. Tip Want to quickly jump to the relevant validator section? Field validators field after validators field before validators field plain validators field wrap validators Model validators model before validators model after validators model wrap validators","pageID":"Validators","abs_url":"/latest/concepts/validators/#Validators","title":"Validators","objectID":"/latest/concepts/validators/#Validators","rank":100},{"content":"In its simplest form, a field validator is a callable taking the value to be validated as an argument and returning the validated value . The callable can perform checks for specific conditions (see raising validation errors ) and make changes to the validated value (coercion or mutation). Four different types of validators can be used. They can all be defined using the annotated pattern or using the\n decorator, applied on a : After validators : run after Pydantic's internal validation. They are generally more type safe and thus easier to implement. Annotated pattern Decorator Here is an example of a validator performing a validation check, and returning the value unchanged. from typing import Annotated\n\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef is_even(value: int) -> int:\n    if value % 2 == 1:\n        raise ValueError(f'{value} is not an even number')\n    return value  # (1)!\n\n\nclass Model(BaseModel):\n    number: Annotated[int, AfterValidator(is_even)]\n\n\ntry:\n    Model(number=1)\nexcept ValidationError as err:\n    print(err)\n    \"\"\"\n    1 validation error for Model\n    number\n      Value error, 1 is not an even number [type=value_error, input_value=1, input_type=int]\n    \"\"\" Note that it is important to return the validated value. Here is an example of a validator performing a validation check, and returning the value unchanged,\nthis time using the  decorator. from pydantic import BaseModel, ValidationError, field_validator\n\n\nclass Model(BaseModel):\n    number: int\n\n    @field_validator('number', mode='after')  # (1)!\n    @classmethod\n    def is_even(cls, value: int) -> int:\n        if value % 2 == 1:\n            raise ValueError(f'{value} is not an even number')\n        return value  # (2)!\n\n\ntry:\n    Model(number=1)\nexcept ValidationError as err:\n    print(err)\n    \"\"\"\n    1 validation error for Model\n    number\n      Value error, 1 is not an even number [type=value_error, input_value=1, input_type=int]\n    \"\"\" 'after' is the default mode for the decorator, and can be omitted. Note that it is important to return the validated value. Before validators : run before Pydantic's internal parsing and validation (e.g. coercion of a str to an int ).\n  These are more flexible than after validators , but they also have to deal with the raw input, which\n  in theory could be any arbitrary object. You should also avoid mutating the value directly if you are raising a validation error later in your validator function, as the mutated value may be passed to other\n  validators if using unions . The value returned from this callable is then validated against the provided type annotation by Pydantic. Annotated pattern Decorator from typing import Annotated, Any\n\nfrom pydantic import BaseModel, BeforeValidator, ValidationError\n\n\ndef ensure_list(value: Any) -> Any:  # (1)!\n    if not isinstance(value, list):  # (2)!\n        return [value]\n    else:\n        return value\n\n\nclass Model(BaseModel):\n    numbers: Annotated[list[int], BeforeValidator(ensure_list)]\n\n\nprint(Model(numbers=2))\n#> numbers=[2]\ntry:\n    Model(numbers='str')\nexcept ValidationError as err:\n    print(err)  # (3)!\n    \"\"\"\n    1 validation error for Model\n    numbers.0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='str', input_type=str]\n    \"\"\" Notice the use of  as a type hint for value . Before validators take the raw input, which\n   can be anything. Note that you might want to check for other sequence types (such as tuples) that would normally successfully\n   validate against the list type. Before validators give you more flexibility, but you have to account for\n   every possible case. Pydantic still performs validation against the int type, no matter if our ensure_list validator\n   did operations on the original input type. from typing import Any\n\nfrom pydantic import BaseModel, ValidationError, field_validator\n\n\nclass Model(BaseModel):\n    numbers: list[int]\n\n    @field_validator('numbers', mode='before')\n    @classmethod\n    def ensure_list(cls, value: Any) -> Any:  # (1)!\n        if not isinstance(value, list):  # (2)!\n            return [value]\n        else:\n            return value\n\n\nprint(Model(numbers=2))\n#> numbers=[2]\ntry:\n    Model(numbers='str')\nexcept ValidationError as err:\n    print(err)  # (3)!\n    \"\"\"\n    1 validation error for Model\n    numbers.0\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='str', input_type=str]\n    \"\"\" Notice the use of  as a type hint for value . Before validators take the raw input, which\n   can be anything. Note that you might want to check for other sequence types (such as tuples) that would normally successfully\n   validate against the list type. Before validators give you more flexibility, but you have to account for\n   every possible case. Pydantic still performs validation against the int type, no matter if our ensure_list validator\n   did operations on the original input type. Plain validators : act similarly to before validators but they terminate validation immediately after returning,\n  so no further validators are called and Pydantic does not do any of its internal validation against the field type. Annotated pattern Decorator from typing import Annotated, Any\n\nfrom pydantic import BaseModel, PlainValidator\n\n\ndef val_number(value: Any) -> Any:\n    if isinstance(value, int):\n        return value * 2\n    else:\n        return value\n\n\nclass Model(BaseModel):\n    number: Annotated[int, PlainValidator(val_number)]\n\n\nprint(Model(number=4))\n#> number=8\nprint(Model(number='invalid'))  # (1)!\n#> number='invalid' Although 'invalid' shouldn't validate against the int type, Pydantic accepts the input. from typing import Any\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Model(BaseModel):\n    number: int\n\n    @field_validator('number', mode='plain')\n    @classmethod\n    def val_number(cls, value: Any) -> Any:\n        if isinstance(value, int):\n            return value * 2\n        else:\n            return value\n\n\nprint(Model(number=4))\n#> number=8\nprint(Model(number='invalid'))  # (1)!\n#> number='invalid' Although 'invalid' shouldn't validate against the int type, Pydantic accepts the input. Wrap validators : are the most flexible of all. You can run code before or after Pydantic and other validators\n  process the input, or you can terminate validation immediately, either by returning the value early or by raising an\n  error. Such validators must be defined with a mandatory extra handler parameter: a callable taking the value to be validated\nas an argument. Internally, this handler will delegate validation of the value to Pydantic. You are free to wrap the call\nto the handler in a try..except block, or not call it at all. Annotated pattern Decorator from typing import Any\n\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field, ValidationError, ValidatorFunctionWrapHandler, WrapValidator\n\n\ndef truncate(value: Any, handler: ValidatorFunctionWrapHandler) -> str:\n    try:\n        return handler(value)\n    except ValidationError as err:\n        if err.errors()[0]['type'] == 'string_too_long':\n            return handler(value[:5])\n        else:\n            raise\n\n\nclass Model(BaseModel):\n    my_string: Annotated[str, Field(max_length=5), WrapValidator(truncate)]\n\n\nprint(Model(my_string='abcde'))\n#> my_string='abcde'\nprint(Model(my_string='abcdef'))\n#> my_string='abcde' from typing import Any\n\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field, ValidationError, ValidatorFunctionWrapHandler, field_validator\n\n\nclass Model(BaseModel):\n    my_string: Annotated[str, Field(max_length=5)]\n\n    @field_validator('my_string', mode='wrap')\n    @classmethod\n    def truncate(cls, value: Any, handler: ValidatorFunctionWrapHandler) -> str:\n        try:\n            return handler(value)\n        except ValidationError as err:\n            if err.errors()[0]['type'] == 'string_too_long':\n                return handler(value[:5])\n            else:\n                raise\n\n\nprint(Model(my_string='abcde'))\n#> my_string='abcde'\nprint(Model(my_string='abcdef'))\n#> my_string='abcde' Validation of default values As mentioned in the fields documentation , default values of fields\nare not validated unless configured to do so, and thus custom validators will not be applied as well.","pageID":"Validators","abs_url":"/latest/concepts/validators/#field-validators","title":"Validators - Field validators","objectID":"/latest/concepts/validators/#field-validators","rank":95},{"content":"While both approaches can achieve the same thing, each pattern provides different benefits. Using the annotated pattern ¶ One of the key benefits of using the annotated pattern is to make\nvalidators reusable: from typing import Annotated\n\nfrom pydantic import AfterValidator, BaseModel\n\n\ndef is_even(value: int) -> int:\n    if value % 2 == 1:\n        raise ValueError(f'{value} is not an even number')\n    return value\n\n\nEvenNumber = Annotated[int, AfterValidator(is_even)]\n\n\nclass Model1(BaseModel):\n    my_number: EvenNumber\n\n\nclass Model2(BaseModel):\n    other_number: Annotated[EvenNumber, AfterValidator(lambda v: v + 2)]\n\n\nclass Model3(BaseModel):\n    list_of_even_numbers: list[EvenNumber]  # (1)! As mentioned in the annotated pattern documentation,\n   we can also make use of validators for specific parts of the annotation (in this case,\n   validation is applied for list items, but not the whole list). It is also easier to understand which validators are applied to a type, by just looking at the field annotation. Using the decorator pattern ¶ One of the key benefits of using the  decorator is to apply\nthe function to multiple fields: from pydantic import BaseModel, field_validator\n\n\nclass Model(BaseModel):\n    f1: str\n    f2: str\n\n    @field_validator('f1', 'f2', mode='before')\n    @classmethod\n    def capitalize(cls, value: str) -> str:\n        return value.capitalize() Here are a couple additional notes about the decorator usage: If you want the validator to apply to all fields (including the ones defined in subclasses), you can pass '*' as the field name argument. By default, the decorator will ensure the provided field name(s) are defined on the model. If you want to\n  disable this check during class creation, you can do so by passing False to the check_fields argument.\n  This is useful when the field validator is defined on a base class, and the field is expected to exist on\n  subclasses.","pageID":"Validators","abs_url":"/latest/concepts/validators/#which-validator-pattern-to-use","title":"Validators - Field validators - Which validator pattern to use","objectID":"/latest/concepts/validators/#which-validator-pattern-to-use","rank":90},{"content":"Validation can also be performed on the entire model's data using the \ndecorator. Three different types of model validators can be used: After validators : run after the whole model has been validated. As such, they are defined as instance methods and can be seen as post-initialization hooks. Important note: the validated instance\n  should be returned. from typing_extensions import Self\n\nfrom pydantic import BaseModel, model_validator\n\n\nclass UserModel(BaseModel):\n    username: str\n    password: str\n    password_repeat: str\n\n    @model_validator(mode='after')\n    def check_passwords_match(self) -> Self:\n        if self.password != self.password_repeat:\n            raise ValueError('Passwords do not match')\n        return self Before validators : are run before the model is instantiated. These are more flexible than after validators,\n  but they also have to deal with the raw input, which in theory could be any arbitrary object. You should also avoid\n  mutating the value directly if you are raising a validation error later in your validator\n  function, as the mutated value may be passed to other validators if using unions . from typing import Any\n\nfrom pydantic import BaseModel, model_validator\n\n\nclass UserModel(BaseModel):\n    username: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def check_card_number_not_present(cls, data: Any) -> Any:  # (1)!\n        if isinstance(data, dict):  # (2)!\n            if 'card_number' in data:\n                raise ValueError(\"'card_number' should not be included\")\n        return data Notice the use of  as a type hint for data . Before validators take the raw input, which\n   can be anything. Most of the time, the input data will be a dictionary (e.g. when calling UserModel(username='...') ). However,\n   this is not always the case. For instance, if the \n   configuration value is set, you might receive an arbitrary class instance for the data argument. Wrap validators : are the most flexible of all. You can run code before or after Pydantic and\n  other validators process the input data, or you can terminate validation immediately, either by returning\n  the data early or by raising an error. import logging\nfrom typing import Any\n\nfrom typing_extensions import Self\n\nfrom pydantic import BaseModel, ModelWrapValidatorHandler, ValidationError, model_validator\n\n\nclass UserModel(BaseModel):\n    username: str\n\n    @model_validator(mode='wrap')\n    @classmethod\n    def log_failed_validation(cls, data: Any, handler: ModelWrapValidatorHandler[Self]) -> Self:\n        try:\n            return handler(data)\n        except ValidationError:\n            logging.error('Model %s failed to validate with data %s', cls, data)\n            raise On inheritance A model validator defined in a base class will be called during the validation of a subclass instance. Overriding a model validator in a subclass will override the base class' validator, and thus only the subclass' version of said validator will be called.","pageID":"Validators","abs_url":"/latest/concepts/validators/#model-validators","title":"Validators - Model validators","objectID":"/latest/concepts/validators/#model-validators","rank":85},{"content":"To raise a validation error, three types of exceptions can be used: : this is the most common exception raised inside validators. : using the  statement also works, but be aware that these statements\n  are skipped when Python is run with the  optimization flag. : a bit more verbose, but provides extra flexibility: from pydantic_core import PydanticCustomError\n\nfrom pydantic import BaseModel, ValidationError, field_validator\n\n\nclass Model(BaseModel):\n    x: int\n\n    @field_validator('x', mode='after')\n    @classmethod\n    def validate_x(cls, v: int) -> int:\n        if v % 42 == 0:\n            raise PydanticCustomError(\n                'the_answer_error',\n                '{number} is the answer!',\n                {'number': v},\n            )\n        return v\n\n\ntry:\n    Model(x=42 * 2)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    x\n      84 is the answer! [type=the_answer_error, input_value=84, input_type=int]\n    \"\"\"","pageID":"Validators","abs_url":"/latest/concepts/validators/#raising-validation-errors","title":"Validators - Raising validation errors","objectID":"/latest/concepts/validators/#raising-validation-errors","rank":80},{"content":"Both the field and model validators callables (in all modes) can optionally take an extra\n argument, providing useful extra information, such as: already validated data user defined context the current validation mode: either 'python' or 'json' (see the  property) the current field name, if using a field validator (see the  property).","pageID":"Validators","abs_url":"/latest/concepts/validators/#validation-info","title":"Validators - Validation info","objectID":"/latest/concepts/validators/#validation-info","rank":75},{"content":"For field validators, the already validated data can be accessed using the \nproperty. Here is an example than can be used as an alternative to the after model validator example: from pydantic import BaseModel, ValidationInfo, field_validator\n\n\nclass UserModel(BaseModel):\n    password: str\n    password_repeat: str\n    username: str\n\n    @field_validator('password_repeat', mode='after')\n    @classmethod\n    def check_passwords_match(cls, value: str, info: ValidationInfo) -> str:\n        if value != info.data['password']:\n            raise ValueError('Passwords do not match')\n        return value Warning As validation is performed in the order fields are defined , you have to\nmake sure you are not accessing a field that hasn't been validated yet. In the code above, for example,\nthe username validated value is not available yet, as it is defined after password_repeat . The  property is None for model validators .","pageID":"Validators","abs_url":"/latest/concepts/validators/#validation-data","title":"Validators - Validation info - Validation data","objectID":"/latest/concepts/validators/#validation-data","rank":70},{"content":"You can pass a context object to the validation methods , which can be accessed\ninside the validator functions using the  property: from pydantic import BaseModel, ValidationInfo, field_validator\n\n\nclass Model(BaseModel):\n    text: str\n\n    @field_validator('text', mode='after')\n    @classmethod\n    def remove_stopwords(cls, v: str, info: ValidationInfo) -> str:\n        if isinstance(info.context, dict):\n            stopwords = info.context.get('stopwords', set())\n            v = ' '.join(w for w in v.split() if w.lower() not in stopwords)\n        return v\n\n\ndata = {'text': 'This is an example document'}\nprint(Model.model_validate(data))  # no context\n#> text='This is an example document'\nprint(Model.model_validate(data, context={'stopwords': ['this', 'is', 'an']}))\n#> text='example document' Similarly, you can use a context for serialization .","pageID":"Validators","abs_url":"/latest/concepts/validators/#validation-context","title":"Validators - Validation info - Validation context","objectID":"/latest/concepts/validators/#validation-context","rank":65},{"content":"When using the annotated pattern , the order in which validators are applied\nis defined as follows: before and wrap validators\nare run from right to left, and after validators are then run from left to right: from pydantic import AfterValidator, BaseModel, BeforeValidator, WrapValidator\n\n\nclass Model(BaseModel):\n    name: Annotated[\n        str,\n        AfterValidator(runs_3rd),\n        AfterValidator(runs_4th),\n        BeforeValidator(runs_2nd),\n        WrapValidator(runs_1st),\n    ] Internally, validators defined using the decorator are converted to their annotated\nform counterpart and added last after the existing metadata for the field. This means that the same ordering\nlogic applies.","pageID":"Validators","abs_url":"/latest/concepts/validators/#ordering-of-validators","title":"Validators - Ordering of validators","objectID":"/latest/concepts/validators/#ordering-of-validators","rank":60},{"content":"Pydantic provides a few special utilities that can be used to customize validation. can be used to validate that a value is an instance of a given class. from pydantic import BaseModel, InstanceOf, ValidationError\n\n\nclass Fruit:\n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass Banana(Fruit): ...\n\n\nclass Apple(Fruit): ...\n\n\nclass Basket(BaseModel):\n    fruits: list[InstanceOf[Fruit]]\n\n\nprint(Basket(fruits=[Banana(), Apple()]))\n#> fruits=[Banana, Apple]\ntry:\n    Basket(fruits=[Banana(), 'Apple'])\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Basket\n    fruits.1\n      Input should be an instance of Fruit [type=is_instance_of, input_value='Apple', input_type=str]\n    \"\"\" can be used to skip validation on a field. from pydantic import BaseModel, SkipValidation\n\n\nclass Model(BaseModel):\n    names: list[SkipValidation[str]]\n\n\nm = Model(names=['foo', 'bar'])\nprint(m)\n#> names=['foo', 'bar']\n\nm = Model(names=['foo', 123])  # (1)!\nprint(m)\n#> names=['foo', 123] Note that the validation of the second item is skipped. If it has the wrong type it will emit a\n   warning during serialization. can be used to validate an custom type from a\n  type natively supported by Pydantic. This is particularly useful when using custom types with multiple fields. from typing import Annotated\n\nfrom pydantic import BaseModel, TypeAdapter, ValidateAs\n\nclass MyCls:\n    def __init__(self, a: int) -> None:\n        self.a = a\n\n    def __repr__(self) -> str:\n        return f\"MyCls(a={self.a})\"\n\nclass ValModel(BaseModel):\n    a: int\n\n\nta = TypeAdapter(\n    Annotated[MyCls, ValidateAs(ValModel, lambda v: MyCls(a=v.a))]\n)\n\nprint(ta.validate_python({'a': 1}))\n#> MyCls(a=1) can be used to notify Pydantic that the default value\n  should be used. from typing import Annotated, Any\n\nfrom pydantic_core import PydanticUseDefault\n\nfrom pydantic import BaseModel, BeforeValidator\n\n\ndef default_if_none(value: Any) -> Any:\n    if value is None:\n        raise PydanticUseDefault()\n    return value\n\n\nclass Model(BaseModel):\n    name: Annotated[str, BeforeValidator(default_if_none)] = 'default_name'\n\n\nprint(Model(name=None))\n#> name='default_name'","pageID":"Validators","abs_url":"/latest/concepts/validators/#special-types","title":"Validators - Special types","objectID":"/latest/concepts/validators/#special-types","rank":55},{"content":"When using before , plain or wrap field validators, the accepted input type may be different from the field annotation. Consider the following example: from typing import Any\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Model(BaseModel):\n    value: str\n\n    @field_validator('value', mode='before')\n    @classmethod\n    def cast_ints(cls, value: Any) -> Any:\n        if isinstance(value, int):\n            return str(value)\n        else:\n            return value\n\n\nprint(Model(value='a'))\n#> value='a'\nprint(Model(value=1))\n#> value='1' While the type hint for value is str , the cast_ints validator also allows integers. To specify the correct\ninput type, the json_schema_input_type argument can be provided: from typing import Any, Union\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Model(BaseModel):\n    value: str\n\n    @field_validator(\n        'value', mode='before', json_schema_input_type=Union[int, str]\n    )\n    @classmethod\n    def cast_ints(cls, value: Any) -> Any:\n        if isinstance(value, int):\n            return str(value)\n        else:\n            return value\n\n\nprint(Model.model_json_schema()['properties']['value'])\n#> {'anyOf': [{'type': 'integer'}, {'type': 'string'}], 'title': 'Value'} As a convenience, Pydantic will use the field type if the argument is not provided (unless you are using\na plain validator, in which case json_schema_input_type defaults to\n as the field type is completely discarded).","pageID":"Validators","abs_url":"/latest/concepts/validators/#json-schema-and-field-validators","title":"Validators - JSON Schema and field validators","objectID":"/latest/concepts/validators/#json-schema-and-field-validators","rank":50},{"content":"Pydantic will raise a  whenever it finds an error in the data it's validating. Note Validation code should not raise the  itself,\nbut rather raise a  or a  (or subclass thereof) which will\nbe caught and used to populate the final . For more details, refer to the dedicated section of the validators documentation. That  will contain information about all the errors and how they happened. You can access these errors in several ways: Method Description Returns a list of  errors found in the input data. Returns the number of errors. Returns a JSON representation of the list errors. str(e) Returns a human-readable representation of the errors. The  object is a dictionary. It contains the following: Property Description An optional object which contains values required to render the error message. The input provided for validation. The error's location as a list. A human-readable explanation of the error. A computer-readable identifier of the error type. The documentation URL giving information about the error. The first item in the  list will be the field where the error occurred, and if the field is a sub-model , subsequent items will be present to indicate the nested location of the error. As a demonstration: from pydantic import BaseModel, Field, ValidationError, field_validator\n\n\nclass Location(BaseModel):\n    lat: float = 0.1\n    lng: float = 10.1\n\n\nclass Model(BaseModel):\n    is_required: float\n    gt_int: int = Field(gt=42)\n    list_of_ints: list[int]\n    a_float: float\n    recursive_model: Location\n\n    @field_validator('a_float', mode='after')\n    @classmethod\n    def validate_float(cls, value: float) -> float:\n        if value > 2.0:\n            raise ValueError('Invalid float value')\n        return value\n\n\ndata = {\n    'list_of_ints': ['1', 2, 'bad'],\n    'a_float': 3.0,\n    'recursive_model': {'lat': 4.2, 'lng': 'New York'},\n    'gt_int': 21,\n}\n\ntry:\n    Model(**data)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    5 validation errors for Model\n    is_required\n      Field required [type=missing, input_value={'list_of_ints': ['1', 2,...ew York'}, 'gt_int': 21}, input_type=dict]\n    gt_int\n      Input should be greater than 42 [type=greater_than, input_value=21, input_type=int]\n    list_of_ints.2\n      Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bad', input_type=str]\n    a_float\n      Value error, Invalid float value [type=value_error, input_value=3.0, input_type=float]\n    recursive_model.lng\n      Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='New York', input_type=str]\n    \"\"\"\n\ntry:\n    Model(**data)\nexcept ValidationError as e:\n    print(e.errors())\n    \"\"\"\n    [\n        {\n            'type': 'missing',\n            'loc': ('is_required',),\n            'msg': 'Field required',\n            'input': {\n                'list_of_ints': ['1', 2, 'bad'],\n                'a_float': 3.0,\n                'recursive_model': {'lat': 4.2, 'lng': 'New York'},\n                'gt_int': 21,\n            },\n            'url': 'https://errors.pydantic.dev/2/v/missing',\n        },\n        {\n            'type': 'greater_than',\n            'loc': ('gt_int',),\n            'msg': 'Input should be greater than 42',\n            'input': 21,\n            'ctx': {'gt': 42},\n            'url': 'https://errors.pydantic.dev/2/v/greater_than',\n        },\n        {\n            'type': 'int_parsing',\n            'loc': ('list_of_ints', 2),\n            'msg': 'Input should be a valid integer, unable to parse string as an integer',\n            'input': 'bad',\n            'url': 'https://errors.pydantic.dev/2/v/int_parsing',\n        },\n        {\n            'type': 'value_error',\n            'loc': ('a_float',),\n            'msg': 'Value error, Invalid float value',\n            'input': 3.0,\n            'ctx': {'error': ValueError('Invalid float value')},\n            'url': 'https://errors.pydantic.dev/2/v/value_error',\n        },\n        {\n            'type': 'float_parsing',\n            'loc': ('recursive_model', 'lng'),\n            'msg': 'Input should be a valid number, unable to parse string as a number',\n            'input': 'New York',\n            'url': 'https://errors.pydantic.dev/2/v/float_parsing',\n        },\n    ]\n    \"\"\"","pageID":"Error Handling","abs_url":"/latest/errors/errors/#Error Handling","title":"Error Handling","objectID":"/latest/errors/errors/#Error Handling","rank":100},{"content":"Pydantic attempts to provide useful default error messages for validation and usage errors, which can be found here: Validation Errors : Errors that happen during data validation. Usage Errors : Errors that happen when using Pydantic.","pageID":"Error Handling","abs_url":"/latest/errors/errors/#error-messages","title":"Error Handling - Error messages","objectID":"/latest/errors/errors/#error-messages","rank":95},{"content":"You can customize error messages by creating a custom error handler. from pydantic_core import ErrorDetails\n\nfrom pydantic import BaseModel, HttpUrl, ValidationError\n\nCUSTOM_MESSAGES = {\n    'int_parsing': 'This is not an integer! 🤦',\n    'url_scheme': 'Hey, use the right URL scheme! I wanted {expected_schemes}.',\n}\n\n\ndef convert_errors(\n    e: ValidationError, custom_messages: dict[str, str]\n) -> list[ErrorDetails]:\n    new_errors: list[ErrorDetails] = []\n    for error in e.errors():\n        custom_message = custom_messages.get(error['type'])\n        if custom_message:\n            ctx = error.get('ctx')\n            error['msg'] = (\n                custom_message.format(**ctx) if ctx else custom_message\n            )\n        new_errors.append(error)\n    return new_errors\n\n\nclass Model(BaseModel):\n    a: int\n    b: HttpUrl\n\n\ntry:\n    Model(a='wrong', b='ftp://example.com')\nexcept ValidationError as e:\n    errors = convert_errors(e, CUSTOM_MESSAGES)\n    print(errors)\n    \"\"\"\n    [\n        {\n            'type': 'int_parsing',\n            'loc': ('a',),\n            'msg': 'This is not an integer! 🤦',\n            'input': 'wrong',\n            'url': 'https://errors.pydantic.dev/2/v/int_parsing',\n        },\n        {\n            'type': 'url_scheme',\n            'loc': ('b',),\n            'msg': \"Hey, use the right URL scheme! I wanted 'http' or 'https'.\",\n            'input': 'ftp://example.com',\n            'ctx': {'expected_schemes': \"'http' or 'https'\"},\n            'url': 'https://errors.pydantic.dev/2/v/url_scheme',\n        },\n    ]\n    \"\"\" A common use case would be to translate error messages. For example, in the above example,\nwe could translate the error messages replacing the CUSTOM_MESSAGES dictionary with a\ndictionary of translations. Another example is customizing the way that the 'loc' value of an error is represented. Python 3.9 and above Python 3.10 and above from typing import Any, Union\n\nfrom pydantic import BaseModel, ValidationError\n\n\ndef loc_to_dot_sep(loc: tuple[Union[str, int], ...]) -> str:\n    path = ''\n    for i, x in enumerate(loc):\n        if isinstance(x, str):\n            if i > 0:\n                path += '.'\n            path += x\n        elif isinstance(x, int):\n            path += f'[{x}]'\n        else:\n            raise TypeError('Unexpected type')\n    return path\n\n\ndef convert_errors(e: ValidationError) -> list[dict[str, Any]]:\n    new_errors: list[dict[str, Any]] = e.errors()\n    for error in new_errors:\n        error['loc'] = loc_to_dot_sep(error['loc'])\n    return new_errors\n\n\nclass TestNestedModel(BaseModel):\n    key: str\n    value: str\n\n\nclass TestModel(BaseModel):\n    items: list[TestNestedModel]\n\n\ndata = {'items': [{'key': 'foo', 'value': 'bar'}, {'key': 'baz'}]}\n\ntry:\n    TestModel.model_validate(data)\nexcept ValidationError as e:\n    print(e.errors())  # (1)!\n    \"\"\"\n    [\n        {\n            'type': 'missing',\n            'loc': ('items', 1, 'value'),\n            'msg': 'Field required',\n            'input': {'key': 'baz'},\n            'url': 'https://errors.pydantic.dev/2/v/missing',\n        }\n    ]\n    \"\"\"\n    pretty_errors = convert_errors(e)\n    print(pretty_errors)  # (2)!\n    \"\"\"\n    [\n        {\n            'type': 'missing',\n            'loc': 'items[1].value',\n            'msg': 'Field required',\n            'input': {'key': 'baz'},\n            'url': 'https://errors.pydantic.dev/2/v/missing',\n        }\n    ]\n    \"\"\" By default, e.errors() produces a list of errors with loc values that take the form of tuples. With our custom loc_to_dot_sep function, we've modified the form of the loc representation. from typing import Any\n\nfrom pydantic import BaseModel, ValidationError\n\n\ndef loc_to_dot_sep(loc: tuple[str | int, ...]) -> str:\n    path = ''\n    for i, x in enumerate(loc):\n        if isinstance(x, str):\n            if i > 0:\n                path += '.'\n            path += x\n        elif isinstance(x, int):\n            path += f'[{x}]'\n        else:\n            raise TypeError('Unexpected type')\n    return path\n\n\ndef convert_errors(e: ValidationError) -> list[dict[str, Any]]:\n    new_errors: list[dict[str, Any]] = e.errors()\n    for error in new_errors:\n        error['loc'] = loc_to_dot_sep(error['loc'])\n    return new_errors\n\n\nclass TestNestedModel(BaseModel):\n    key: str\n    value: str\n\n\nclass TestModel(BaseModel):\n    items: list[TestNestedModel]\n\n\ndata = {'items': [{'key': 'foo', 'value': 'bar'}, {'key': 'baz'}]}\n\ntry:\n    TestModel.model_validate(data)\nexcept ValidationError as e:\n    print(e.errors())  # (1)!\n    \"\"\"\n    [\n        {\n            'type': 'missing',\n            'loc': ('items', 1, 'value'),\n            'msg': 'Field required',\n            'input': {'key': 'baz'},\n            'url': 'https://errors.pydantic.dev/2/v/missing',\n        }\n    ]\n    \"\"\"\n    pretty_errors = convert_errors(e)\n    print(pretty_errors)  # (2)!\n    \"\"\"\n    [\n        {\n            'type': 'missing',\n            'loc': 'items[1].value',\n            'msg': 'Field required',\n            'input': {'key': 'baz'},\n            'url': 'https://errors.pydantic.dev/2/v/missing',\n        }\n    ]\n    \"\"\" By default, e.errors() produces a list of errors with loc values that take the form of tuples. With our custom loc_to_dot_sep function, we've modified the form of the loc representation.","pageID":"Error Handling","abs_url":"/latest/errors/errors/#customize-error-messages","title":"Error Handling - Error messages - Customize error messages","objectID":"/latest/errors/errors/#customize-error-messages","rank":90},{"content":"Pydantic attempts to provide useful errors. The following sections provide details on common errors developers may\nencounter when working with Pydantic, along with suggestions for addressing the error condition.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#Usage Errors","title":"Usage Errors","objectID":"/latest/errors/usage_errors/#Usage Errors","rank":100},{"content":"This error is raised when a type referenced in an annotation of a pydantic-validated type\n(such as a subclass of BaseModel , or a pydantic dataclass ) is not defined: from typing import ForwardRef\n\nfrom pydantic import BaseModel, PydanticUserError\n\nUndefinedType = ForwardRef('UndefinedType')\n\n\nclass Foobar(BaseModel):\n    a: UndefinedType\n\n\ntry:\n    Foobar(a=1)\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'class-not-fully-defined' Or when the type has been defined after usage: from typing import Optional\n\nfrom pydantic import BaseModel, PydanticUserError\n\n\nclass Foo(BaseModel):\n    a: Optional['Bar'] = None\n\n\ntry:\n    # this doesn't work, see raised error\n    foo = Foo(a={'b': {'a': None}})\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'class-not-fully-defined'\n\n\nclass Bar(BaseModel):\n    b: 'Foo'\n\n\n# this works, though\nfoo = Foo(a={'b': {'a': None}}) For BaseModel subclasses, it can be fixed by defining the type and then calling .model_rebuild() : from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    a: Optional['Bar'] = None\n\n\nclass Bar(BaseModel):\n    b: 'Foo'\n\n\nFoo.model_rebuild()\n\nfoo = Foo(a={'b': {'a': None}}) In other cases, the error message should indicate how to rebuild the class with the appropriate type defined.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#class-not-fully-defined","title":"Usage Errors - Class not fully defined","objectID":"/latest/errors/usage_errors/#class-not-fully-defined","rank":95},{"content":"The __modify_schema__ method is no longer supported in V2. You should use the __get_pydantic_json_schema__ method instead. The __modify_schema__ used to receive a single argument representing the JSON schema. See the example below: from pydantic import BaseModel, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        @classmethod\n        def __modify_schema__(cls, field_schema):\n            field_schema.update(examples=['example'])\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'custom-json-schema' The new method __get_pydantic_json_schema__ receives two arguments: the first is a dictionary denoted as CoreSchema ,\nand the second a callable handler that receives a CoreSchema as parameter, and returns a JSON schema. See the example\nbelow: from typing import Any\n\nfrom pydantic_core import CoreSchema\n\nfrom pydantic import BaseModel, GetJsonSchemaHandler\n\n\nclass Model(BaseModel):\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> dict[str, Any]:\n        json_schema = super().__get_pydantic_json_schema__(core_schema, handler)\n        json_schema = handler.resolve_ref_schema(json_schema)\n        json_schema.update(examples=['example'])\n        return json_schema\n\n\nprint(Model.model_json_schema())\n\"\"\"\n{'examples': ['example'], 'properties': {}, 'title': 'Model', 'type': 'object'}\n\"\"\"","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#custom-json-schema","title":"Usage Errors - Custom JSON Schema","objectID":"/latest/errors/usage_errors/#custom-json-schema","rank":90},{"content":"This error is raised when you define a decorator with a field that is not valid. from typing import Any\n\nfrom pydantic import BaseModel, PydanticUserError, field_validator\n\ntry:\n\n    class Model(BaseModel):\n        a: str\n\n        @field_validator('b')\n        def check_b(cls, v: Any):\n            return v\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'decorator-missing-field' You can use check_fields=False if you're inheriting from the model and intended this. from typing import Any\n\nfrom pydantic import BaseModel, create_model, field_validator\n\n\nclass Model(BaseModel):\n    @field_validator('a', check_fields=False)\n    def check_a(cls, v: Any):\n        return v\n\n\nmodel = create_model('FooModel', a=(str, 'cake'), __base__=Model)","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#decorator-missing-field","title":"Usage Errors - Decorator on missing field","objectID":"/latest/errors/usage_errors/#decorator-missing-field","rank":85},{"content":"This error is raised when a model in discriminated unions doesn't define a discriminator field. Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-no-field' from typing import Literal\n\nfrom pydantic import BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Cat | Dog = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-no-field'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#discriminator-no-field","title":"Usage Errors - Discriminator no field","objectID":"/latest/errors/usage_errors/#discriminator-no-field","rank":80},{"content":"This error is raised when you define a non-string alias on a discriminator field. Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import AliasChoices, BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat'] = Field(\n        validation_alias=AliasChoices('Pet', 'PET')\n    )\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-alias-type' from typing import Literal\n\nfrom pydantic import AliasChoices, BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat'] = Field(\n        validation_alias=AliasChoices('Pet', 'PET')\n    )\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Cat | Dog = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-alias-type'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#discriminator-alias-type","title":"Usage Errors - Discriminator alias type","objectID":"/latest/errors/usage_errors/#discriminator-alias-type","rank":75},{"content":"This error is raised when you define a non- Literal type on a discriminator field. Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    pet_type: int\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-needs-literal' from typing import Literal\n\nfrom pydantic import BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    pet_type: int\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Cat | Dog = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-needs-literal'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#discriminator-needs-literal","title":"Usage Errors - Discriminator needs literal","objectID":"/latest/errors/usage_errors/#discriminator-needs-literal","rank":70},{"content":"This error is raised when you define different aliases on discriminator fields. Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat'] = Field(validation_alias='PET')\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog'] = Field(validation_alias='Pet')\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-alias' from typing import Literal\n\nfrom pydantic import BaseModel, Field, PydanticUserError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat'] = Field(validation_alias='PET')\n    c: str\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog'] = Field(validation_alias='Pet')\n    d: str\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Cat | Dog = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-alias'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#discriminator-alias","title":"Usage Errors - Discriminator alias","objectID":"/latest/errors/usage_errors/#discriminator-alias","rank":65},{"content":"This error is raised when you use a before, wrap, or plain validator on a discriminator field. This is disallowed because the discriminator field is used to determine the type of the model to use for validation,\nso you can't use a validator that might change its value. Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, PydanticUserError, field_validator\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n\n    @field_validator('pet_type', mode='before')\n    @classmethod\n    def validate_pet_type(cls, v):\n        if v == 'kitten':\n            return 'cat'\n        return v\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Union[Cat, Dog] = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-validator' from typing import Literal\n\nfrom pydantic import BaseModel, Field, PydanticUserError, field_validator\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n\n    @field_validator('pet_type', mode='before')\n    @classmethod\n    def validate_pet_type(cls, v):\n        if v == 'kitten':\n            return 'cat'\n        return v\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n\n\ntry:\n\n    class Model(BaseModel):\n        pet: Cat | Dog = Field(discriminator='pet_type')\n        number: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'discriminator-validator' This can be worked around by using a standard Union , dropping the discriminator: Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n\n    @field_validator('pet_type', mode='before')\n    @classmethod\n    def validate_pet_type(cls, v):\n        if v == 'kitten':\n            return 'cat'\n        return v\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n\n\nclass Model(BaseModel):\n    pet: Union[Cat, Dog]\n\n\nassert Model(pet={'pet_type': 'kitten'}).pet.pet_type == 'cat' from typing import Literal\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n\n    @field_validator('pet_type', mode='before')\n    @classmethod\n    def validate_pet_type(cls, v):\n        if v == 'kitten':\n            return 'cat'\n        return v\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n\n\nclass Model(BaseModel):\n    pet: Cat | Dog\n\n\nassert Model(pet={'pet_type': 'kitten'}).pet.pet_type == 'cat'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#discriminator-validator","title":"Usage Errors - Invalid discriminator validator","objectID":"/latest/errors/usage_errors/#discriminator-validator","rank":60},{"content":"This error is raised when a Union that uses a callable Discriminator doesn't have Tag annotations for all cases. Python 3.9 and above Python 3.10 and above from typing import Annotated, Union\n\nfrom pydantic import BaseModel, Discriminator, PydanticUserError, Tag\n\n\ndef model_x_discriminator(v):\n    if isinstance(v, str):\n        return 'str'\n    if isinstance(v, (dict, BaseModel)):\n        return 'model'\n\n\n# tag missing for both union choices\ntry:\n\n    class DiscriminatedModel(BaseModel):\n        x: Annotated[\n            Union[str, 'DiscriminatedModel'],\n            Discriminator(model_x_discriminator),\n        ]\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'callable-discriminator-no-tag'\n\n# tag missing for `'DiscriminatedModel'` union choice\ntry:\n\n    class DiscriminatedModel(BaseModel):\n        x: Annotated[\n            Union[Annotated[str, Tag('str')], 'DiscriminatedModel'],\n            Discriminator(model_x_discriminator),\n        ]\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'callable-discriminator-no-tag'\n\n# tag missing for `str` union choice\ntry:\n\n    class DiscriminatedModel(BaseModel):\n        x: Annotated[\n            Union[str, Annotated['DiscriminatedModel', Tag('model')]],\n            Discriminator(model_x_discriminator),\n        ]\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'callable-discriminator-no-tag' from typing import Annotated, Union\n\nfrom pydantic import BaseModel, Discriminator, PydanticUserError, Tag\n\n\ndef model_x_discriminator(v):\n    if isinstance(v, str):\n        return 'str'\n    if isinstance(v, (dict, BaseModel)):\n        return 'model'\n\n\n# tag missing for both union choices\ntry:\n\n    class DiscriminatedModel(BaseModel):\n        x: Annotated[\n            Union[str, 'DiscriminatedModel'],\n            Discriminator(model_x_discriminator),\n        ]\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'callable-discriminator-no-tag'\n\n# tag missing for `'DiscriminatedModel'` union choice\ntry:\n\n    class DiscriminatedModel(BaseModel):\n        x: Annotated[\n            Union[Annotated[str, Tag('str')], 'DiscriminatedModel'],\n            Discriminator(model_x_discriminator),\n        ]\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'callable-discriminator-no-tag'\n\n# tag missing for `str` union choice\ntry:\n\n    class DiscriminatedModel(BaseModel):\n        x: Annotated[\n            str | Annotated['DiscriminatedModel', Tag('model')],\n            Discriminator(model_x_discriminator),\n        ]\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'callable-discriminator-no-tag'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#callable-discriminator-no-tag","title":"Usage Errors - Callable discriminator case with no tag","objectID":"/latest/errors/usage_errors/#callable-discriminator-no-tag","rank":55},{"content":"This error is raised when you use \ninstead of typing_extensions.TypedDict on Python < 3.12.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#typed-dict-version","title":"Usage Errors - TypedDict version","objectID":"/latest/errors/usage_errors/#typed-dict-version","rank":50},{"content":"This error is raised when a field defined on a base class was overridden by a non-annotated attribute. from pydantic import BaseModel, PydanticUserError\n\n\nclass Foo(BaseModel):\n    a: float\n\n\ntry:\n\n    class Bar(Foo):\n        x: float = 12.3\n        a = 123.0\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'model-field-overridden'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#model-field-overridden","title":"Usage Errors - Model parent field overridden","objectID":"/latest/errors/usage_errors/#model-field-overridden","rank":45},{"content":"This error is raised when a field doesn't have an annotation. from pydantic import BaseModel, Field, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        a = Field('foobar')\n        b = None\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'model-field-missing-annotation' If the field is not meant to be a field, you may be able to resolve the error\nby annotating it as a ClassVar : from typing import ClassVar\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    a: ClassVar[str] Or updating model_config['ignored_types'] : from pydantic import BaseModel, ConfigDict\n\n\nclass IgnoredType:\n    pass\n\n\nclass MyModel(BaseModel):\n    model_config = ConfigDict(ignored_types=(IgnoredType,))\n\n    _a = IgnoredType()\n    _b: int = IgnoredType()\n    _c: IgnoredType\n    _d: IgnoredType = IgnoredType()","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#model-field-missing-annotation","title":"Usage Errors - Model field missing annotation","objectID":"/latest/errors/usage_errors/#model-field-missing-annotation","rank":40},{"content":"This error is raised when class Config and model_config are used together. from pydantic import BaseModel, ConfigDict, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        model_config = ConfigDict(from_attributes=True)\n\n        a: str\n\n        class Config:\n            from_attributes = True\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'config-both'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#config-both","title":"Usage Errors - Config and model_config both defined","objectID":"/latest/errors/usage_errors/#config-both","rank":35},{"content":"This error is raised when the keyword arguments are not available in Pydantic V2. For example, regex is removed from Pydantic V2: from pydantic import BaseModel, Field, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        x: str = Field(regex='test')\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'removed-kwargs'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#removed-kwargs","title":"Usage Errors - Keyword arguments removed","objectID":"/latest/errors/usage_errors/#removed-kwargs","rank":30},{"content":"This error is raised when a circular reference is found that would otherwise result in an infinite recursion. For example, this is a valid type alias: type A = list[A] | None while these are not: type A = A\n\ntype B = C\ntype C = B","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#circular-reference-schema","title":"Usage Errors - Circular reference schema","objectID":"/latest/errors/usage_errors/#circular-reference-schema","rank":25},{"content":"This error is raised when Pydantic fails to generate a JSON schema for some CoreSchema . from pydantic import BaseModel, ImportString, PydanticUserError\n\n\nclass Model(BaseModel):\n    a: ImportString\n\n\ntry:\n    Model.model_json_schema()\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'invalid-for-json-schema'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#invalid-for-json-schema","title":"Usage Errors - JSON schema invalid type","objectID":"/latest/errors/usage_errors/#invalid-for-json-schema","rank":20},{"content":"This error is raised when the JSON schema generator has already been used to generate a JSON schema.\nYou must create a new instance to generate a new JSON schema.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#json-schema-already-used","title":"Usage Errors - JSON schema already used","objectID":"/latest/errors/usage_errors/#json-schema-already-used","rank":15},{"content":"This error is raised when you instantiate BaseModel directly. Pydantic models should inherit from BaseModel . from pydantic import BaseModel, PydanticUserError\n\ntry:\n    BaseModel()\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'base-model-instantiated'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#base-model-instantiated","title":"Usage Errors - BaseModel instantiated","objectID":"/latest/errors/usage_errors/#base-model-instantiated","rank":10},{"content":"This error is raised when handling undefined annotations during CoreSchema generation. from pydantic import BaseModel, PydanticUndefinedAnnotation\n\n\nclass Model(BaseModel):\n    a: 'B'  # noqa F821\n\n\ntry:\n    Model.model_rebuild()\nexcept PydanticUndefinedAnnotation as exc_info:\n    assert exc_info.code == 'undefined-annotation'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#undefined-annotation","title":"Usage Errors - Undefined annotation","objectID":"/latest/errors/usage_errors/#undefined-annotation","rank":5},{"content":"This error is raised when Pydantic fails to generate a CoreSchema for some type. from pydantic import BaseModel, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        x: 43 = 123\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'schema-for-unknown-type'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#schema-for-unknown-type","title":"Usage Errors - Schema for unknown type","objectID":"/latest/errors/usage_errors/#schema-for-unknown-type","rank":0},{"content":"This error is raised when you try to import an object that was available in Pydantic V1, but has been removed in\nPydantic V2. See the Migration Guide for more information.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#import-error","title":"Usage Errors - Import error","objectID":"/latest/errors/usage_errors/#import-error","rank":-5},{"content":"This error is raised when you provide invalid field definitions in . from pydantic import PydanticUserError, create_model\n\ntry:\n    create_model('FooModel', foo=(str, 'default value', 'more'))\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'create-model-field-definitions' The fields definition syntax can be found in the dynamic model creation documentation.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#create-model-field-definitions","title":"Usage Errors - create_model field definitions","objectID":"/latest/errors/usage_errors/#create-model-field-definitions","rank":-10},{"content":"This error is raised when you use validator bare (with no fields). from pydantic import BaseModel, PydanticUserError, field_validator\n\ntry:\n\n    class Model(BaseModel):\n        a: str\n\n        @field_validator\n        def checker(cls, v):\n            return v\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validator-no-fields' Validators should be used with fields and keyword arguments. from pydantic import BaseModel, field_validator\n\n\nclass Model(BaseModel):\n    a: str\n\n    @field_validator('a')\n    def checker(cls, v):\n        return v","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validator-no-fields","title":"Usage Errors - Validator with no fields","objectID":"/latest/errors/usage_errors/#validator-no-fields","rank":-15},{"content":"This error is raised when you use a validator with non-string fields. from pydantic import BaseModel, PydanticUserError, field_validator\n\ntry:\n\n    class Model(BaseModel):\n        a: str\n        b: str\n\n        @field_validator(['a', 'b'])\n        def check_fields(cls, v):\n            return v\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validator-invalid-fields' Fields should be passed as separate string arguments: from pydantic import BaseModel, field_validator\n\n\nclass Model(BaseModel):\n    a: str\n    b: str\n\n    @field_validator('a', 'b')\n    def check_fields(cls, v):\n        return v","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validator-invalid-fields","title":"Usage Errors - Invalid validator fields","objectID":"/latest/errors/usage_errors/#validator-invalid-fields","rank":-20},{"content":"This error is raised when you apply a validator on an instance method. from pydantic import BaseModel, PydanticUserError, field_validator\n\ntry:\n\n    class Model(BaseModel):\n        a: int = 1\n\n        @field_validator('a')\n        def check_a(self, value):\n            return value\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validator-instance-method'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validator-instance-method","title":"Usage Errors - Validator on instance method","objectID":"/latest/errors/usage_errors/#validator-instance-method","rank":-25},{"content":"This error is raised when you explicitly specify a value for the json_schema_input_type argument and mode isn't set to either 'before' , 'plain' or 'wrap' . from pydantic import BaseModel, PydanticUserError, field_validator\n\ntry:\n\n    class Model(BaseModel):\n        a: int = 1\n\n        @field_validator('a', mode='after', json_schema_input_type=int)\n        @classmethod\n        def check_a(self, value):\n            return value\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validator-input-type' Documenting the JSON Schema input type is only possible for validators where the given\nvalue can be anything. That is why it isn't available for after validators, where\nthe value is first validated against the type annotation.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validator-input-type","title":"Usage Errors - json_schema_input_type used with the wrong mode","objectID":"/latest/errors/usage_errors/#validator-input-type","rank":-30},{"content":"If you use @root_validator with pre=False (the default) you MUST specify skip_on_failure=True .\nThe skip_on_failure=False option is no longer available. If you were not trying to set skip_on_failure=False , you can safely set skip_on_failure=True .\nIf you do, this root validator will no longer be called if validation fails for any of the fields. Please see the Migration Guide for more details.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#root-validator-pre-skip","title":"Usage Errors - Root validator, pre, skip_on_failure","objectID":"/latest/errors/usage_errors/#root-validator-pre-skip","rank":-35},{"content":"@model_serializer must be applied to instance methods. This error is raised when you apply model_serializer on an instance method without self : from pydantic import BaseModel, PydanticUserError, model_serializer\n\ntry:\n\n    class MyModel(BaseModel):\n        a: int\n\n        @model_serializer\n        def _serialize(slf, x, y, z):\n            return slf\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'model-serializer-instance-method' Or on a class method: from pydantic import BaseModel, PydanticUserError, model_serializer\n\ntry:\n\n    class MyModel(BaseModel):\n        a: int\n\n        @model_serializer\n        @classmethod\n        def _serialize(self, x, y, z):\n            return self\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'model-serializer-instance-method'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#model-serializer-instance-method","title":"Usage Errors - model_serializer instance methods","objectID":"/latest/errors/usage_errors/#model-serializer-instance-method","rank":-40},{"content":"The field and config parameters are not available in Pydantic V2.\nPlease use the info parameter instead. You can access the configuration via info.config ,\nbut it is a dictionary instead of an object like it was in Pydantic V1. The field argument is no longer available.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validator-field-config-info","title":"Usage Errors - validator, field, config, and info","objectID":"/latest/errors/usage_errors/#validator-field-config-info","rank":-45},{"content":"This error is raised when you use an unsupported signature for Pydantic V1-style validator. import warnings\n\nfrom pydantic import BaseModel, PydanticUserError, validator\n\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\ntry:\n\n    class Model(BaseModel):\n        a: int\n\n        @validator('a')\n        def check_a(cls, value, foo):\n            return value\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validator-v1-signature'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validator-v1-signature","title":"Usage Errors - Pydantic V1 validator signature","objectID":"/latest/errors/usage_errors/#validator-v1-signature","rank":-50},{"content":"This error is raised when a field_validator or model_validator function has the wrong signature. from pydantic import BaseModel, PydanticUserError, field_validator\n\ntry:\n\n    class Model(BaseModel):\n        a: str\n\n        @field_validator('a')\n        @classmethod\n        def check_a(cls):\n            return 'a'\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validator-signature'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validator-signature","title":"Usage Errors - Unrecognized field_validator signature","objectID":"/latest/errors/usage_errors/#validator-signature","rank":-55},{"content":"This error is raised when the field_serializer function has the wrong signature. from pydantic import BaseModel, PydanticUserError, field_serializer\n\ntry:\n\n    class Model(BaseModel):\n        x: int\n\n        @field_serializer('x')\n        def no_args():\n            return 'x'\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'field-serializer-signature' Valid field serializer signatures are: from pydantic import FieldSerializationInfo, SerializerFunctionWrapHandler, field_serializer\n\n# an instance method with the default mode or `mode='plain'`\n@field_serializer('x')  # or @field_serializer('x', mode='plain')\ndef ser_x(self, value: Any, info: FieldSerializationInfo): ...\n\n# a static method or function with the default mode or `mode='plain'`\n@field_serializer('x')  # or @field_serializer('x', mode='plain')\n@staticmethod\ndef ser_x(value: Any, info: FieldSerializationInfo): ...\n\n# equivalent to\ndef ser_x(value: Any, info: FieldSerializationInfo): ...\nserializer('x')(ser_x)\n\n# an instance method with `mode='wrap'`\n@field_serializer('x', mode='wrap')\ndef ser_x(self, value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo): ...\n\n# a static method or function with `mode='wrap'`\n@field_serializer('x', mode='wrap')\n@staticmethod\ndef ser_x(value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo): ...\n\n# equivalent to\ndef ser_x(value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo): ...\nserializer('x')(ser_x)\n\n# For all of these, you can also choose to omit the `info` argument, for example:\n@field_serializer('x')\ndef ser_x(self, value: Any): ...\n\n@field_serializer('x', mode='wrap')\ndef ser_x(self, value: Any, handler: SerializerFunctionWrapHandler): ...","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#field-serializer-signature","title":"Usage Errors - Unrecognized field_serializer signature","objectID":"/latest/errors/usage_errors/#field-serializer-signature","rank":-60},{"content":"This error is raised when the model_serializer function has the wrong signature. from pydantic import BaseModel, PydanticUserError, model_serializer\n\ntry:\n\n    class MyModel(BaseModel):\n        a: int\n\n        @model_serializer\n        def _serialize(self, x, y, z):\n            return self\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'model-serializer-signature' Valid model serializer signatures are: from pydantic import SerializerFunctionWrapHandler, SerializationInfo, model_serializer\n\n# an instance method with the default mode or `mode='plain'`\n@model_serializer  # or model_serializer(mode='plain')\ndef mod_ser(self, info: SerializationInfo): ...\n\n# an instance method with `mode='wrap'`\n@model_serializer(mode='wrap')\ndef mod_ser(self, handler: SerializerFunctionWrapHandler, info: SerializationInfo):\n\n# For all of these, you can also choose to omit the `info` argument, for example:\n@model_serializer(mode='plain')\ndef mod_ser(self): ...\n\n@model_serializer(mode='wrap')\ndef mod_ser(self, handler: SerializerFunctionWrapHandler): ...","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#model-serializer-signature","title":"Usage Errors - Unrecognized model_serializer signature","objectID":"/latest/errors/usage_errors/#model-serializer-signature","rank":-65},{"content":"This error is raised when multiple model_serializer functions are defined for a field. from pydantic import BaseModel, PydanticUserError, field_serializer\n\ntry:\n\n    class MyModel(BaseModel):\n        x: int\n        y: int\n\n        @field_serializer('x', 'y')\n        def serializer1(v):\n            return f'{v:,}'\n\n        @field_serializer('x')\n        def serializer2(v):\n            return v\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'multiple-field-serializers'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#multiple-field-serializers","title":"Usage Errors - Multiple field serializers","objectID":"/latest/errors/usage_errors/#multiple-field-serializers","rank":-70},{"content":"This error is raised when an annotation cannot annotate a type. from typing import Annotated\n\nfrom pydantic import BaseModel, FutureDate, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        foo: Annotated[str, FutureDate()]\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'invalid-annotated-type'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#invalid-annotated-type","title":"Usage Errors - Invalid annotated type","objectID":"/latest/errors/usage_errors/#invalid-annotated-type","rank":-75},{"content":"You will get this error if you try to pass config to TypeAdapter when the type is a type that\nhas its own config that cannot be overridden (currently this is only BaseModel , TypedDict and dataclass ): Python 3.9 and above Python 3.13 and above from typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, PydanticUserError, TypeAdapter\n\n\nclass MyTypedDict(TypedDict):\n    x: int\n\n\ntry:\n    TypeAdapter(MyTypedDict, config=ConfigDict(strict=True))\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'type-adapter-config-unused' from typing import TypedDict\n\nfrom pydantic import ConfigDict, PydanticUserError, TypeAdapter\n\n\nclass MyTypedDict(TypedDict):\n    x: int\n\n\ntry:\n    TypeAdapter(MyTypedDict, config=ConfigDict(strict=True))\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'type-adapter-config-unused' Instead you'll need to subclass the type and override or set the config on it: Python 3.9 and above Python 3.13 and above from typing_extensions import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter\n\n\nclass MyTypedDict(TypedDict):\n    x: int\n\n    # or `model_config = ...` for BaseModel\n    __pydantic_config__ = ConfigDict(strict=True)\n\n\nTypeAdapter(MyTypedDict)  # ok from typing import TypedDict\n\nfrom pydantic import ConfigDict, TypeAdapter\n\n\nclass MyTypedDict(TypedDict):\n    x: int\n\n    # or `model_config = ...` for BaseModel\n    __pydantic_config__ = ConfigDict(strict=True)\n\n\nTypeAdapter(MyTypedDict)  # ok","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#type-adapter-config-unused","title":"Usage Errors - config is unused with TypeAdapter","objectID":"/latest/errors/usage_errors/#type-adapter-config-unused","rank":-80},{"content":"Because RootModel is not capable of storing or even accepting extra fields during initialization, we raise an error\nif you try to specify a value for the config setting 'extra' when creating a subclass of RootModel : from pydantic import PydanticUserError, RootModel\n\ntry:\n\n    class MyRootModel(RootModel):\n        model_config = {'extra': 'allow'}\n        root: int\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'root-model-extra'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#root-model-extra","title":"Usage Errors - Cannot specify model_config['extra'] with RootModel","objectID":"/latest/errors/usage_errors/#root-model-extra","rank":-85},{"content":"Because type annotations are evaluated after assignments, you might get unexpected results when using a type annotation name\nthat clashes with one of your fields. We raise an error in the following case: from datetime import date\n\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    date: date = Field(description='A date') As a workaround, you can either use an alias or change your import: import datetime\n# Or `from datetime import date as _date`\n\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    date: datetime.date = Field(description='A date')","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#unevaluable-type-annotation","title":"Usage Errors - Cannot evaluate type annotation","objectID":"/latest/errors/usage_errors/#unevaluable-type-annotation","rank":-90},{"content":"Pydantic does not allow the specification of the extra='allow' setting on a dataclass\nwhile any of the fields have init=False set. Thus, you may not do something like the following: from pydantic import ConfigDict, Field\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass(config=ConfigDict(extra='allow'))\nclass A:\n    a: int = Field(init=False, default=1) The above snippet results in the following error during schema building for the A dataclass: pydantic.errors.PydanticUserError: Field a has `init=False` and dataclass has config setting `extra=\"allow\"`. This combination is not allowed.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#dataclass-init-false-extra-allow","title":"Usage Errors - Incompatible dataclass init and extra settings","objectID":"/latest/errors/usage_errors/#dataclass-init-false-extra-allow","rank":-95},{"content":"The init=False and init_var=True settings are mutually exclusive. Doing so results in the PydanticUserError shown in the example below. from pydantic import Field\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass Foo:\n    bar: str = Field(init=False, init_var=True)\n\n\n\"\"\"\npydantic.errors.PydanticUserError: Dataclass field bar has init=False and init_var=True, but these are mutually exclusive.\n\"\"\"","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#clashing-init-and-init-var","title":"Usage Errors - Incompatible init and init_var settings on dataclass field","objectID":"/latest/errors/usage_errors/#clashing-init-and-init-var","rank":-100},{"content":"This error is raised when model_config is used as the name of a field. from pydantic import BaseModel, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        model_config: str\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'model-config-invalid-field-name'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#model-config-invalid-field-name","title":"Usage Errors - model_config is used as a model field","objectID":"/latest/errors/usage_errors/#model-config-invalid-field-name","rank":-105},{"content":"This error is raised when the  decorator is used on a class which is already a Pydantic model (use the model_config attribute instead). from pydantic import BaseModel, PydanticUserError, with_config\n\ntry:\n\n    @with_config({'allow_inf_nan': True})\n    class Model(BaseModel):\n        bar: str\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'with-config-on-model'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#with-config-on-model","title":"Usage Errors - is used on a BaseModel subclass","objectID":"/latest/errors/usage_errors/#with-config-on-model","rank":-110},{"content":"This error is raised when the Pydantic dataclass decorator is used on a class which is already\na Pydantic model. from pydantic import BaseModel, PydanticUserError\nfrom pydantic.dataclasses import dataclass\n\ntry:\n\n    @dataclass\n    class Model(BaseModel):\n        bar: str\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'dataclass-on-model'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#dataclass-on-model","title":"Usage Errors - dataclass is used on a BaseModel subclass","objectID":"/latest/errors/usage_errors/#dataclass-on-model","rank":-115},{"content":"validate_call has some limitations on the callables it can validate. This error is raised when you try to use it with an unsupported callable.\nCurrently the supported callables are functions (including lambdas, but not built-ins) and methods and instances of .\nIn the case of , the function being partially applied must be one of the supported callables.","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validate-call-type","title":"Usage Errors - Unsupported type for validate_call","objectID":"/latest/errors/usage_errors/#validate-call-type","rank":-120},{"content":"These decorators must be put before validate_call . from pydantic import PydanticUserError, validate_call\n\n# error\ntry:\n\n    class A:\n        @validate_call\n        @classmethod\n        def f1(cls): ...\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validate-call-type'\n\n\n# correct\n@classmethod\n@validate_call\ndef f2(cls): ...","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#classmethod-staticmethod-and-property","title":"Usage Errors - Unsupported type for validate_call - @classmethod, @staticmethod, and @property","objectID":"/latest/errors/usage_errors/#classmethod-staticmethod-and-property","rank":-125},{"content":"While classes are callables themselves, validate_call can't be applied on them, as it needs to know about which method to use ( __init__ or __new__ ) to fetch type annotations. If you want to validate the constructor of a class, you should put validate_call on top of the appropriate method instead. from pydantic import PydanticUserError, validate_call\n\n# error\ntry:\n\n    @validate_call\n    class A1: ...\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validate-call-type'\n\n\n# correct\nclass A2:\n    @validate_call\n    def __init__(self): ...\n\n    @validate_call\n    def __new__(cls): ...","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#classes","title":"Usage Errors - Unsupported type for validate_call - Classes","objectID":"/latest/errors/usage_errors/#classes","rank":-130},{"content":"Although instances can be callable by implementing a __call__ method, currently the instances of these types cannot be validated with validate_call .\nThis may change in the future, but for now, you should use validate_call explicitly on __call__ instead. from pydantic import PydanticUserError, validate_call\n\n# error\ntry:\n\n    class A1:\n        def __call__(self): ...\n\n    validate_call(A1())\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validate-call-type'\n\n\n# correct\nclass A2:\n    @validate_call\n    def __call__(self): ...","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#callable-instances","title":"Usage Errors - Unsupported type for validate_call - Callable instances","objectID":"/latest/errors/usage_errors/#callable-instances","rank":-135},{"content":"This is generally less common, but a possible reason is that you are trying to validate a method that doesn't have at least one argument (usually self ). from pydantic import PydanticUserError, validate_call\n\ntry:\n\n    class A:\n        def f(): ...\n\n    validate_call(A().f)\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validate-call-type'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#invalid-signature","title":"Usage Errors - Unsupported type for validate_call - Invalid signature","objectID":"/latest/errors/usage_errors/#invalid-signature","rank":-140},{"content":"This error is raised when  is used with something other than\na  class object to type hint variadic keyword parameters. For reference, see the related specification section and PEP 692 . Python 3.9 and above Python 3.12 and above from typing_extensions import Unpack\n\nfrom pydantic import PydanticUserError, validate_call\n\ntry:\n\n    @validate_call\n    def func(**kwargs: Unpack[int]):\n        pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'unpack-typed-dict' from typing import Unpack\n\nfrom pydantic import PydanticUserError, validate_call\n\ntry:\n\n    @validate_call\n    def func(**kwargs: Unpack[int]):\n        pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'unpack-typed-dict'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#unpack-typed-dict","title":"Usage Errors - used without a","objectID":"/latest/errors/usage_errors/#unpack-typed-dict","rank":-145},{"content":"This error is raised when the typed dictionary used to type hint variadic keywords parameters has field names\noverlapping with other parameters (unless ). For reference, see the related specification section and PEP 692 . Python 3.9 and above Python 3.12 and above Python 3.13 and above from typing_extensions import TypedDict, Unpack\n\nfrom pydantic import PydanticUserError, validate_call\n\n\nclass TD(TypedDict):\n    a: int\n\n\ntry:\n\n    @validate_call\n    def func(a: int, **kwargs: Unpack[TD]):\n        pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'overlapping-unpack-typed-dict' from typing_extensions import TypedDict\nfrom typing import Unpack\n\nfrom pydantic import PydanticUserError, validate_call\n\n\nclass TD(TypedDict):\n    a: int\n\n\ntry:\n\n    @validate_call\n    def func(a: int, **kwargs: Unpack[TD]):\n        pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'overlapping-unpack-typed-dict' from typing import TypedDict, Unpack\n\nfrom pydantic import PydanticUserError, validate_call\n\n\nclass TD(TypedDict):\n    a: int\n\n\ntry:\n\n    @validate_call\n    def func(a: int, **kwargs: Unpack[TD]):\n        pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'overlapping-unpack-typed-dict'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#overlapping-unpack-typed-dict","title":"Usage Errors - Overlapping unpacked  fields and arguments","objectID":"/latest/errors/usage_errors/#overlapping-unpack-typed-dict","rank":-150},{"content":"Currently,  can only be used to annotate a field of a class (specifically, subclasses of , , , or dataclasses). Attempting to use  in any other ways will raise this error. Python 3.9 and above Python 3.11 and above from typing_extensions import Self\n\nfrom pydantic import PydanticUserError, validate_call\n\ntry:\n\n    @validate_call\n    def func(self: Self):\n        pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'invalid-self-type' from typing import Self\n\nfrom pydantic import PydanticUserError, validate_call\n\ntry:\n\n    @validate_call\n    def func(self: Self):\n        pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'invalid-self-type' The following example of  will also raise this error, even though it is correct from a type-checking perspective. This may be supported in the future. Python 3.9 and above Python 3.11 and above from typing_extensions import Self\n\nfrom pydantic import BaseModel, PydanticUserError, validate_call\n\ntry:\n\n    class A(BaseModel):\n        @validate_call\n        def func(self, arg: Self):\n            pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'invalid-self-type' from typing import Self\n\nfrom pydantic import BaseModel, PydanticUserError, validate_call\n\ntry:\n\n    class A(BaseModel):\n        @validate_call\n        def func(self, arg: Self):\n            pass\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'invalid-self-type'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#invalid-self-type","title":"Usage Errors - Invalid Self type","objectID":"/latest/errors/usage_errors/#invalid-self-type","rank":-155},{"content":"This error is raised when you set validate_by_alias and validate_by_name to False in the configuration. This is not allowed because it would make it impossible to populate attributes. from pydantic import BaseModel, ConfigDict, Field, PydanticUserError\n\ntry:\n\n    class Model(BaseModel):\n        a: int = Field(alias='A')\n\n        model_config = ConfigDict(\n            validate_by_alias=False, validate_by_name=False\n        )\n\nexcept PydanticUserError as exc_info:\n    assert exc_info.code == 'validate-by-alias-and-name-false'","pageID":"Usage Errors","abs_url":"/latest/errors/usage_errors/#validate-by-alias-and-name-false","title":"Usage Errors - validate_by_alias and validate_by_name both set to False","objectID":"/latest/errors/usage_errors/#validate-by-alias-and-name-false","rank":-160},{"content":"Pydantic attempts to provide useful validation errors. Below are details on common validation errors users\nmay encounter when working with pydantic, together with some suggestions on how to fix them.","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#Validation Errors","title":"Validation Errors","objectID":"/latest/errors/validation_errors/#Validation Errors","rank":100},{"content":"This error is raised when an object that would be passed as arguments to a function during validation is not\na tuple , list , or dict . Because NamedTuple uses function calls in its implementation, that is one way to\nproduce this error: from typing import NamedTuple\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass MyNamedTuple(NamedTuple):\n    x: int\n\n\nclass MyModel(BaseModel):\n    field: MyNamedTuple\n\n\ntry:\n    MyModel.model_validate({'field': 'invalid'})\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'arguments_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#arguments_type","title":"Validation Errors - arguments_type","objectID":"/latest/errors/validation_errors/#arguments_type","rank":95},{"content":"This error is raised when a failing assert statement is encountered during validation: from pydantic import BaseModel, ValidationError, field_validator\n\n\nclass Model(BaseModel):\n    x: int\n\n    @field_validator('x')\n    @classmethod\n    def force_x_positive(cls, v):\n        assert v > 0\n        return v\n\n\ntry:\n    Model(x=-1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'assertion_error'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#assertion_error","title":"Validation Errors - assertion_error","objectID":"/latest/errors/validation_errors/#assertion_error","rank":90},{"content":"This error is raised when the input value is a string that is not valid for coercion to a boolean: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: bool\n\n\nModel(x='true')  # OK\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'bool_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#bool_parsing","title":"Validation Errors - bool_parsing","objectID":"/latest/errors/validation_errors/#bool_parsing","rank":85},{"content":"This error is raised when the input value's type is not valid for a bool field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: bool\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'bool_type' This error is also raised for strict fields when the input value is not an instance of bool .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#bool_type","title":"Validation Errors - bool_type","objectID":"/latest/errors/validation_errors/#bool_type","rank":80},{"content":"This error is raised when a bytes value is invalid under the configured encoding.\nIn the following example, b'a' is invalid hex (odd number of digits). from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: bytes\n    model_config = {'val_json_bytes': 'hex'}\n\n\ntry:\n    Model(x=b'a')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'bytes_invalid_encoding'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#bytes_invalid_encoding","title":"Validation Errors - bytes_invalid_encoding","objectID":"/latest/errors/validation_errors/#bytes_invalid_encoding","rank":75},{"content":"This error is raised when the length of a bytes value is greater than the field's max_length constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: bytes = Field(max_length=3)\n\n\ntry:\n    Model(x=b'test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'bytes_too_long'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#bytes_too_long","title":"Validation Errors - bytes_too_long","objectID":"/latest/errors/validation_errors/#bytes_too_long","rank":70},{"content":"This error is raised when the length of a bytes value is less than the field's min_length constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: bytes = Field(min_length=3)\n\n\ntry:\n    Model(x=b't')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'bytes_too_short'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#bytes_too_short","title":"Validation Errors - bytes_too_short","objectID":"/latest/errors/validation_errors/#bytes_too_short","rank":65},{"content":"This error is raised when the input value's type is not valid for a bytes field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: bytes\n\n\ntry:\n    Model(x=123)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'bytes_type' This error is also raised for strict fields when the input value is not an instance of bytes .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#bytes_type","title":"Validation Errors - bytes_type","objectID":"/latest/errors/validation_errors/#bytes_type","rank":60},{"content":"This error is raised when the input value is not valid as a Callable : Python 3.9 and above Python 3.10 and above from typing import Any, Callable\n\nfrom pydantic import BaseModel, ImportString, ValidationError\n\n\nclass Model(BaseModel):\n    x: ImportString[Callable[[Any], Any]]\n\n\nModel(x='math:cos')  # OK\n\ntry:\n    Model(x='os.path')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'callable_type' from typing import Any\nfrom collections.abc import Callable\n\nfrom pydantic import BaseModel, ImportString, ValidationError\n\n\nclass Model(BaseModel):\n    x: ImportString[Callable[[Any], Any]]\n\n\nModel(x='math:cos')  # OK\n\ntry:\n    Model(x='os.path')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'callable_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#callable_type","title":"Validation Errors - callable_type","objectID":"/latest/errors/validation_errors/#callable_type","rank":55},{"content":"This error is raised when the input value is a string but cannot be parsed as a complex number because\nit does not follow the rule in Python: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    num: complex\n\n\ntry:\n    # Complex numbers in json are expected to be valid complex strings.\n    # This value `abc` is not a valid complex string.\n    Model.model_validate_json('{\"num\": \"abc\"}')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'complex_str_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#complex_str_parsing","title":"Validation Errors - complex_str_parsing","objectID":"/latest/errors/validation_errors/#complex_str_parsing","rank":50},{"content":"This error is raised when the input value cannot be interpreted as a complex number: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    num: complex\n\n\ntry:\n    Model(num=False)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'complex_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#complex_type","title":"Validation Errors - complex_type","objectID":"/latest/errors/validation_errors/#complex_type","rank":45},{"content":"This error is raised when validating a dataclass with strict=True and the input is not an instance of the dataclass: import pydantic.dataclasses\nfrom pydantic import TypeAdapter, ValidationError\n\n\n@pydantic.dataclasses.dataclass\nclass MyDataclass:\n    x: str\n\n\nadapter = TypeAdapter(MyDataclass)\n\nprint(adapter.validate_python(MyDataclass(x='test'), strict=True))\n#> MyDataclass(x='test')\nprint(adapter.validate_python({'x': 'test'}))\n#> MyDataclass(x='test')\n\ntry:\n    adapter.validate_python({'x': 'test'}, strict=True)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'dataclass_exact_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#dataclass_exact_type","title":"Validation Errors - dataclass_exact_type","objectID":"/latest/errors/validation_errors/#dataclass_exact_type","rank":40},{"content":"This error is raised when the input value is not valid for a dataclass field: from pydantic import ValidationError, dataclasses\n\n\n@dataclasses.dataclass\nclass Inner:\n    x: int\n\n\n@dataclasses.dataclass\nclass Outer:\n    y: Inner\n\n\nOuter(y=Inner(x=1))  # OK\n\ntry:\n    Outer(y=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'dataclass_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#dataclass_type","title":"Validation Errors - dataclass_type","objectID":"/latest/errors/validation_errors/#dataclass_type","rank":35},{"content":"This error is raised when the input datetime value provided for a date field has a nonzero time component.\nFor a timestamp to parse into a field of type date , the time components must all be zero: from datetime import date, datetime\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: date\n\n\nModel(x='2023-01-01')  # OK\nModel(x=datetime(2023, 1, 1))  # OK\n\ntry:\n    Model(x=datetime(2023, 1, 1, 12))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_from_datetime_inexact'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#date_from_datetime_inexact","title":"Validation Errors - date_from_datetime_inexact","objectID":"/latest/errors/validation_errors/#date_from_datetime_inexact","rank":30},{"content":"This error is raised when the input value is a string that cannot be parsed for a date field: from datetime import date\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: date\n\n\ntry:\n    Model(x='XX1494012000')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_from_datetime_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#date_from_datetime_parsing","title":"Validation Errors - date_from_datetime_parsing","objectID":"/latest/errors/validation_errors/#date_from_datetime_parsing","rank":25},{"content":"This error is raised when the input value provided for a FutureDate field is not in the future: from datetime import date\n\nfrom pydantic import BaseModel, FutureDate, ValidationError\n\n\nclass Model(BaseModel):\n    x: FutureDate\n\n\ntry:\n    Model(x=date(2000, 1, 1))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_future'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#date_future","title":"Validation Errors - date_future","objectID":"/latest/errors/validation_errors/#date_future","rank":20},{"content":"This error is raised when validating JSON where the input value is string that cannot be parsed for a date field: import json\nfrom datetime import date\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: date = Field(strict=True)\n\n\ntry:\n    Model.model_validate_json(json.dumps({'x': '1'}))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#date_parsing","title":"Validation Errors - date_parsing","objectID":"/latest/errors/validation_errors/#date_parsing","rank":15},{"content":"This error is raised when the value provided for a PastDate field is not in the past: from datetime import date, timedelta\n\nfrom pydantic import BaseModel, PastDate, ValidationError\n\n\nclass Model(BaseModel):\n    x: PastDate\n\n\ntry:\n    Model(x=date.today() + timedelta(1))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_past'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#date_past","title":"Validation Errors - date_past","objectID":"/latest/errors/validation_errors/#date_past","rank":10},{"content":"This error is raised when the input value's type is not valid for a date field: from datetime import date\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: date\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'date_type' This error is also raised for strict fields when the input value is not an instance of date .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#date_type","title":"Validation Errors - date_type","objectID":"/latest/errors/validation_errors/#date_type","rank":5},{"content":"This error is raised when the input value is a string that cannot be parsed for a datetime field: from datetime import datetime\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: datetime\n\n\ntry:\n    # there is no 13th month\n    Model(x='2023-13-01')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'datetime_from_date_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#datetime_from_date_parsing","title":"Validation Errors - datetime_from_date_parsing","objectID":"/latest/errors/validation_errors/#datetime_from_date_parsing","rank":0},{"content":"This error is raised when the value provided for a FutureDatetime field is not in the future: from datetime import datetime\n\nfrom pydantic import BaseModel, FutureDatetime, ValidationError\n\n\nclass Model(BaseModel):\n    x: FutureDatetime\n\n\ntry:\n    Model(x=datetime(2000, 1, 1))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'datetime_future'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#datetime_future","title":"Validation Errors - datetime_future","objectID":"/latest/errors/validation_errors/#datetime_future","rank":-5},{"content":"This error is raised when something about the datetime object is not valid: from datetime import datetime, tzinfo\n\nfrom pydantic import AwareDatetime, BaseModel, ValidationError\n\n\nclass CustomTz(tzinfo):\n    # utcoffset is not implemented!\n\n    def tzname(self, _dt):\n        return 'CustomTZ'\n\n\nclass Model(BaseModel):\n    x: AwareDatetime\n\n\ntry:\n    Model(x=datetime(2023, 1, 1, tzinfo=CustomTz()))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'datetime_object_invalid'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#datetime_object_invalid","title":"Validation Errors - datetime_object_invalid","objectID":"/latest/errors/validation_errors/#datetime_object_invalid","rank":-10},{"content":"This error is raised when the value is a string that cannot be parsed for a datetime field: import json\nfrom datetime import datetime\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: datetime = Field(strict=True)\n\n\ntry:\n    Model.model_validate_json(json.dumps({'x': 'not a datetime'}))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'datetime_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#datetime_parsing","title":"Validation Errors - datetime_parsing","objectID":"/latest/errors/validation_errors/#datetime_parsing","rank":-15},{"content":"This error is raised when the value provided for a PastDatetime field is not in the past: from datetime import datetime, timedelta\n\nfrom pydantic import BaseModel, PastDatetime, ValidationError\n\n\nclass Model(BaseModel):\n    x: PastDatetime\n\n\ntry:\n    Model(x=datetime.now() + timedelta(100))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'datetime_past'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#datetime_past","title":"Validation Errors - datetime_past","objectID":"/latest/errors/validation_errors/#datetime_past","rank":-20},{"content":"This error is raised when the input value's type is not valid for a datetime field: from datetime import datetime\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: datetime\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'datetime_type' This error is also raised for strict fields when the input value is not an instance of datetime .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#datetime_type","title":"Validation Errors - datetime_type","objectID":"/latest/errors/validation_errors/#datetime_type","rank":-25},{"content":"This error is raised when the value provided for a Decimal has too many digits: from decimal import Decimal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: Decimal = Field(max_digits=3)\n\n\ntry:\n    Model(x='42.1234')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'decimal_max_digits'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#decimal_max_digits","title":"Validation Errors - decimal_max_digits","objectID":"/latest/errors/validation_errors/#decimal_max_digits","rank":-30},{"content":"This error is raised when the value provided for a Decimal has too many digits after the decimal point: from decimal import Decimal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: Decimal = Field(decimal_places=3)\n\n\ntry:\n    Model(x='42.1234')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'decimal_max_places'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#decimal_max_places","title":"Validation Errors - decimal_max_places","objectID":"/latest/errors/validation_errors/#decimal_max_places","rank":-35},{"content":"This error is raised when the value provided for a Decimal could not be parsed as a decimal number: from decimal import Decimal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: Decimal = Field(decimal_places=3)\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'decimal_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#decimal_parsing","title":"Validation Errors - decimal_parsing","objectID":"/latest/errors/validation_errors/#decimal_parsing","rank":-40},{"content":"This error is raised when the value provided for a Decimal is of the wrong type: from decimal import Decimal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: Decimal = Field(decimal_places=3)\n\n\ntry:\n    Model(x=[1, 2, 3])\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'decimal_type' This error is also raised for strict fields when the input value is not an instance of Decimal .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#decimal_type","title":"Validation Errors - decimal_type","objectID":"/latest/errors/validation_errors/#decimal_type","rank":-45},{"content":"This error is raised when the value provided for a Decimal has more digits before the decimal point than max_digits - decimal_places (as long as both are specified): from decimal import Decimal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: Decimal = Field(max_digits=6, decimal_places=3)\n\n\ntry:\n    Model(x='12345.6')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'decimal_whole_digits'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#decimal_whole_digits","title":"Validation Errors - decimal_whole_digits","objectID":"/latest/errors/validation_errors/#decimal_whole_digits","rank":-50},{"content":"This error is raised when the input value's type is not dict for a dict field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: dict\n\n\ntry:\n    Model(x=['1', '2'])\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'dict_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#dict_type","title":"Validation Errors - dict_type","objectID":"/latest/errors/validation_errors/#dict_type","rank":-55},{"content":"This error is raised when the input value does not exist in an enum field members: from enum import Enum\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass MyEnum(str, Enum):\n    option = 'option'\n\n\nclass Model(BaseModel):\n    x: MyEnum\n\n\ntry:\n    Model(x='other_option')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'enum'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#enum","title":"Validation Errors - enum","objectID":"/latest/errors/validation_errors/#enum","rank":-60},{"content":"This error is raised when the input value contains extra fields, but model_config['extra'] == 'forbid' : from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: str\n\n    model_config = ConfigDict(extra='forbid')\n\n\ntry:\n    Model(x='test', y='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'extra_forbidden' You can read more about the extra configuration in the  section.","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#extra_forbidden","title":"Validation Errors - extra_forbidden","objectID":"/latest/errors/validation_errors/#extra_forbidden","rank":-65},{"content":"This error is raised when the value is infinite, or too large to be represented as a 64-bit floating point number\nduring validation: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n\ntry:\n    Model(x=2.2250738585072011e308)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'finite_number'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#finite_number","title":"Validation Errors - finite_number","objectID":"/latest/errors/validation_errors/#finite_number","rank":-70},{"content":"This error is raised when the value is a string that can't be parsed as a float : from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: float\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'float_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#float_parsing","title":"Validation Errors - float_parsing","objectID":"/latest/errors/validation_errors/#float_parsing","rank":-75},{"content":"This error is raised when the input value's type is not valid for a float field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: float\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'float_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#float_type","title":"Validation Errors - float_type","objectID":"/latest/errors/validation_errors/#float_type","rank":-80},{"content":"This error is raised when you attempt to assign a value to a field with frozen=True , or to delete such a field: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: str = Field('test', frozen=True)\n\n\nmodel = Model()\n\ntry:\n    model.x = 'test1'\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'frozen_field'\n\ntry:\n    del model.x\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'frozen_field'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#frozen_field","title":"Validation Errors - frozen_field","objectID":"/latest/errors/validation_errors/#frozen_field","rank":-85},{"content":"This error is raised when frozen is set in the configuration and you attempt to delete or assign a new value to\nany of the fields: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(frozen=True)\n\n\nm = Model(x=1)\n\ntry:\n    m.x = 2\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'frozen_instance'\n\ntry:\n    del m.x\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'frozen_instance'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#frozen_instance","title":"Validation Errors - frozen_instance","objectID":"/latest/errors/validation_errors/#frozen_instance","rank":-90},{"content":"This error is raised when the input value's type is not valid for a frozenset field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: frozenset\n\n\ntry:\n    model = Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'frozen_set_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#frozen_set_type","title":"Validation Errors - frozen_set_type","objectID":"/latest/errors/validation_errors/#frozen_set_type","rank":-95},{"content":"This error is raised when model_config['from_attributes'] == True and an error is raised while reading the attributes: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Foobar:\n    def __init__(self):\n        self.x = 1\n\n    @property\n    def y(self):\n        raise RuntimeError('intentional error')\n\n\nclass Model(BaseModel):\n    x: int\n    y: str\n\n    model_config = ConfigDict(from_attributes=True)\n\n\ntry:\n    Model.model_validate(Foobar())\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'get_attribute_error'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#get_attribute_error","title":"Validation Errors - get_attribute_error","objectID":"/latest/errors/validation_errors/#get_attribute_error","rank":-100},{"content":"This error is raised when the value is not greater than the field's gt constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: int = Field(gt=10)\n\n\ntry:\n    Model(x=10)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'greater_than'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#greater_than","title":"Validation Errors - greater_than","objectID":"/latest/errors/validation_errors/#greater_than","rank":-105},{"content":"This error is raised when the value is not greater than or equal to the field's ge constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: int = Field(ge=10)\n\n\ntry:\n    Model(x=9)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'greater_than_equal'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#greater_than_equal","title":"Validation Errors - greater_than_equal","objectID":"/latest/errors/validation_errors/#greater_than_equal","rank":-110},{"content":"This error is raised when you provide a float value for an int field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n\ntry:\n    Model(x=0.5)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'int_from_float'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#int_from_float","title":"Validation Errors - int_from_float","objectID":"/latest/errors/validation_errors/#int_from_float","rank":-115},{"content":"This error is raised when the value can't be parsed as int : from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'int_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#int_parsing","title":"Validation Errors - int_parsing","objectID":"/latest/errors/validation_errors/#int_parsing","rank":-120},{"content":"This error is raised when attempting to parse a python or JSON value from a string outside the maximum range that Python str to int parsing permits: import json\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n\n# from Python\nassert Model(x='1' * 4_300).x == int('1' * 4_300)  # OK\n\ntoo_long = '1' * 4_301\ntry:\n    Model(x=too_long)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'int_parsing_size'\n\n# from JSON\ntry:\n    Model.model_validate_json(json.dumps({'x': too_long}))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'int_parsing_size'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#int_parsing_size","title":"Validation Errors - int_parsing_size","objectID":"/latest/errors/validation_errors/#int_parsing_size","rank":-125},{"content":"This error is raised when the input value's type is not valid for an int field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'int_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#int_type","title":"Validation Errors - int_type","objectID":"/latest/errors/validation_errors/#int_type","rank":-130},{"content":"This error is raised when attempting to validate a dict that has a key that is not an instance of str : from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Model(BaseModel):\n    x: int\n\n    model_config = ConfigDict(extra='allow')\n\n\ntry:\n    Model.model_validate({'x': 1, b'y': 2})\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'invalid_key'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#invalid_key","title":"Validation Errors - invalid_key","objectID":"/latest/errors/validation_errors/#invalid_key","rank":-135},{"content":"This error is raised when the input value is not an instance of the expected type: from pydantic import BaseModel, ConfigDict, ValidationError\n\n\nclass Nested:\n    x: str\n\n\nclass Model(BaseModel):\n    y: Nested\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n\ntry:\n    Model(y='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'is_instance_of'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#is_instance_of","title":"Validation Errors - is_instance_of","objectID":"/latest/errors/validation_errors/#is_instance_of","rank":-140},{"content":"This error is raised when the input value is not a subclass of the expected type: from pydantic import BaseModel, ValidationError\n\n\nclass Nested:\n    x: str\n\n\nclass Model(BaseModel):\n    y: type[Nested]\n\n\ntry:\n    Model(y='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'is_subclass_of'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#is_subclass_of","title":"Validation Errors - is_subclass_of","objectID":"/latest/errors/validation_errors/#is_subclass_of","rank":-145},{"content":"This error is raised when the input value is not valid as an Iterable : from collections.abc import Iterable\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    y: Iterable[str]\n\n\ntry:\n    Model(y=123)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'iterable_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#iterable_type","title":"Validation Errors - iterable_type","objectID":"/latest/errors/validation_errors/#iterable_type","rank":-150},{"content":"This error is raised when an error occurs during iteration: from pydantic import BaseModel, ValidationError\n\n\ndef gen():\n    yield 1\n    raise RuntimeError('error')\n\n\nclass Model(BaseModel):\n    x: list[int]\n\n\ntry:\n    Model(x=gen())\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'iteration_error'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#iteration_error","title":"Validation Errors - iteration_error","objectID":"/latest/errors/validation_errors/#iteration_error","rank":-155},{"content":"This error is raised when the input value is not a valid JSON string: from pydantic import BaseModel, Json, ValidationError\n\n\nclass Model(BaseModel):\n    x: Json\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'json_invalid'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#json_invalid","title":"Validation Errors - json_invalid","objectID":"/latest/errors/validation_errors/#json_invalid","rank":-160},{"content":"This error is raised when the input value is of a type that cannot be parsed as JSON: from pydantic import BaseModel, Json, ValidationError\n\n\nclass Model(BaseModel):\n    x: Json\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'json_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#json_type","title":"Validation Errors - json_type","objectID":"/latest/errors/validation_errors/#json_type","rank":-165},{"content":"This error is raised when the input value is not less than the field's lt constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: int = Field(lt=10)\n\n\ntry:\n    Model(x=10)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'less_than'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#less_than","title":"Validation Errors - less_than","objectID":"/latest/errors/validation_errors/#less_than","rank":-170},{"content":"This error is raised when the input value is not less than or equal to the field's le constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: int = Field(le=10)\n\n\ntry:\n    Model(x=11)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'less_than_equal'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#less_than_equal","title":"Validation Errors - less_than_equal","objectID":"/latest/errors/validation_errors/#less_than_equal","rank":-175},{"content":"This error is raised when the input value's type is not valid for a list field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: list[int]\n\n\ntry:\n    Model(x=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'list_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#list_type","title":"Validation Errors - list_type","objectID":"/latest/errors/validation_errors/#list_type","rank":-180},{"content":"This error is raised when the input value is not one of the expected literal values: from typing import Literal\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: Literal['a', 'b']\n\n\nModel(x='a')  # OK\n\ntry:\n    Model(x='c')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'literal_error'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#literal_error","title":"Validation Errors - literal_error","objectID":"/latest/errors/validation_errors/#literal_error","rank":-185},{"content":"This error is raised when a problem occurs during validation due to a failure in a call to the methods from the Mapping protocol, such as .items() : from collections.abc import Mapping\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass BadMapping(Mapping):\n    def items(self):\n        raise ValueError()\n\n    def __iter__(self):\n        raise ValueError()\n\n    def __getitem__(self, key):\n        raise ValueError()\n\n    def __len__(self):\n        return 1\n\n\nclass Model(BaseModel):\n    x: dict[str, str]\n\n\ntry:\n    Model(x=BadMapping())\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'mapping_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#mapping_type","title":"Validation Errors - mapping_type","objectID":"/latest/errors/validation_errors/#mapping_type","rank":-190},{"content":"This error is raised when there are required fields missing from the input value: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: str\n\n\ntry:\n    Model()\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'missing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#missing","title":"Validation Errors - missing","objectID":"/latest/errors/validation_errors/#missing","rank":-195},{"content":"This error is raised when a required positional-or-keyword argument is not passed to a function decorated with validate_call : from pydantic import ValidationError, validate_call\n\n\n@validate_call\ndef foo(a: int):\n    return a\n\n\ntry:\n    foo()\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'missing_argument'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#missing_argument","title":"Validation Errors - missing_argument","objectID":"/latest/errors/validation_errors/#missing_argument","rank":-200},{"content":"This error is raised when a required keyword-only argument is not passed to a function decorated with validate_call : from pydantic import ValidationError, validate_call\n\n\n@validate_call\ndef foo(*, a: int):\n    return a\n\n\ntry:\n    foo()\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'missing_keyword_only_argument'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#missing_keyword_only_argument","title":"Validation Errors - missing_keyword_only_argument","objectID":"/latest/errors/validation_errors/#missing_keyword_only_argument","rank":-205},{"content":"This error is raised when a required positional-only argument is not passed to a function decorated with validate_call : from pydantic import ValidationError, validate_call\n\n\n@validate_call\ndef foo(a: int, /):\n    return a\n\n\ntry:\n    foo()\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'missing_positional_only_argument'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#missing_positional_only_argument","title":"Validation Errors - missing_positional_only_argument","objectID":"/latest/errors/validation_errors/#missing_positional_only_argument","rank":-210},{"content":"This error is raised when the experimental MISSING sentinel is the only value allowed, and wasn't\nprovided during validation: from pydantic import BaseModel, ValidationError\nfrom pydantic.experimental.missing_sentinel import MISSING\n\n\nclass Model(BaseModel):\n    f: MISSING\n\n\ntry:\n    Model(f=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'missing_sentinel_error'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#missing_sentinel_error","title":"Validation Errors - missing_sentinel_error","objectID":"/latest/errors/validation_errors/#missing_sentinel_error","rank":-215},{"content":"This error is raised when the input value is not a valid dictionary, model instance, or instance that fields can be extracted from: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    a: int\n    b: int\n\n\n# simply validating a dict\nprint(Model.model_validate({'a': 1, 'b': 2}))\n#> a=1 b=2\n\n\nclass CustomObj:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n\n# using from attributes to extract fields from an objects\nprint(Model.model_validate(CustomObj(3, 4), from_attributes=True))\n#> a=3 b=4\n\ntry:\n    Model.model_validate('not an object', from_attributes=True)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'model_attributes_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#model_attributes_type","title":"Validation Errors - model_attributes_type","objectID":"/latest/errors/validation_errors/#model_attributes_type","rank":-220},{"content":"This error is raised when the input to a model is not an instance of the model or dict: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    a: int\n    b: int\n\n\n# simply validating a dict\nm = Model.model_validate({'a': 1, 'b': 2})\nprint(m)\n#> a=1 b=2\n\n# validating an existing model instance\nprint(Model.model_validate(m))\n#> a=1 b=2\n\ntry:\n    Model.model_validate('not an object')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'model_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#model_type","title":"Validation Errors - model_type","objectID":"/latest/errors/validation_errors/#model_type","rank":-225},{"content":"This error is raised when you provide multiple values for a single argument while calling a function decorated with validate_call : from pydantic import ValidationError, validate_call\n\n\n@validate_call\ndef foo(a: int):\n    return a\n\n\ntry:\n    foo(1, a=2)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'multiple_argument_values'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#multiple_argument_values","title":"Validation Errors - multiple_argument_values","objectID":"/latest/errors/validation_errors/#multiple_argument_values","rank":-230},{"content":"This error is raised when the input is not a multiple of a field's multiple_of constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: int = Field(multiple_of=5)\n\n\ntry:\n    Model(x=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'multiple_of'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#multiple_of","title":"Validation Errors - multiple_of","objectID":"/latest/errors/validation_errors/#multiple_of","rank":-235},{"content":"This type of error is raised when validation is attempted from a format that cannot be converted to a Python object.\nFor example, we cannot check isinstance or issubclass from JSON: import json\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    bm: type[BaseModel]\n\n\ntry:\n    Model.model_validate_json(json.dumps({'bm': 'not a basemodel class'}))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'needs_python_object'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#needs_python_object","title":"Validation Errors - needs_python_object","objectID":"/latest/errors/validation_errors/#needs_python_object","rank":-240},{"content":"This error is raised when validate_assignment=True in the config, and you attempt to assign a value to an attribute\nthat is not an existing field: from pydantic import ConfigDict, ValidationError, dataclasses\n\n\n@dataclasses.dataclass(config=ConfigDict(validate_assignment=True))\nclass MyDataclass:\n    x: int\n\n\nm = MyDataclass(x=1)\ntry:\n    m.y = 10\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'no_such_attribute'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#no_such_attribute","title":"Validation Errors - no_such_attribute","objectID":"/latest/errors/validation_errors/#no_such_attribute","rank":-245},{"content":"This error is raised when the input value is not None for a field that requires None : from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: None\n\n\ntry:\n    Model(x=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'none_required' Note You may encounter this error when there is a naming collision in your model between a field name and its type. More specifically, this error is likely to be thrown when the default value of that field is None . For example, the following would yield the none_required validation error since the field int is set to a default value of None and has the exact same name as its type, which causes problems with validation. from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass M1(BaseModel):\n    int: Optional[int] = None\n\n\nm = M1(int=123)  # errors","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#none_required","title":"Validation Errors - none_required","objectID":"/latest/errors/validation_errors/#none_required","rank":-250},{"content":"This error is raised when a cyclic reference is detected: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: list['Model']\n\n\nd = {'x': []}\nd['x'].append(d)\ntry:\n    Model(**d)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'recursion_loop'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#recursion_loop","title":"Validation Errors - recursion_loop","objectID":"/latest/errors/validation_errors/#recursion_loop","rank":-255},{"content":"This error is raised when an unhashable value is validated against a  or a : from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: set[object]\n\n\nclass Unhashable:\n    __hash__ = None\n\n\ntry:\n    Model(x=[{'a': 'b'}, Unhashable()])\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'set_item_not_hashable'\n    print(repr(exc.errors()[1]['type']))\n    #> 'set_item_not_hashable'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#set_item_not_hashable","title":"Validation Errors - set_item_not_hashable","objectID":"/latest/errors/validation_errors/#set_item_not_hashable","rank":-260},{"content":"This error is raised when the value type is not valid for a set field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: set[int]\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'set_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#set_type","title":"Validation Errors - set_type","objectID":"/latest/errors/validation_errors/#set_type","rank":-265},{"content":"This error is raised when the input value doesn't match the field's pattern constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: str = Field(pattern='test')\n\n\ntry:\n    Model(x='1')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'string_pattern_mismatch'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#string_pattern_mismatch","title":"Validation Errors - string_pattern_mismatch","objectID":"/latest/errors/validation_errors/#string_pattern_mismatch","rank":-270},{"content":"This error is raised when the value is an instance of a strict subtype of str when the field is strict: from enum import Enum\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass MyEnum(str, Enum):\n    foo = 'foo'\n\n\nclass Model(BaseModel):\n    x: str = Field(strict=True)\n\n\ntry:\n    Model(x=MyEnum.foo)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'string_sub_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#string_sub_type","title":"Validation Errors - string_sub_type","objectID":"/latest/errors/validation_errors/#string_sub_type","rank":-275},{"content":"This error is raised when the input value is a string whose length is greater than the field's max_length constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: str = Field(max_length=3)\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'string_too_long'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#string_too_long","title":"Validation Errors - string_too_long","objectID":"/latest/errors/validation_errors/#string_too_long","rank":-280},{"content":"This error is raised when the input value is a string whose length is less than the field's min_length constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: str = Field(min_length=3)\n\n\ntry:\n    Model(x='t')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'string_too_short'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#string_too_short","title":"Validation Errors - string_too_short","objectID":"/latest/errors/validation_errors/#string_too_short","rank":-285},{"content":"This error is raised when the input value's type is not valid for a str field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: str\n\n\ntry:\n    Model(x=1)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'string_type' This error is also raised for strict fields when the input value is not an instance of str .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#string_type","title":"Validation Errors - string_type","objectID":"/latest/errors/validation_errors/#string_type","rank":-290},{"content":"This error is raised when the value cannot be parsed as a Unicode string: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: str\n\n\ntry:\n    Model(x=b'\\x81')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'string_unicode'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#string_unicode","title":"Validation Errors - string_unicode","objectID":"/latest/errors/validation_errors/#string_unicode","rank":-295},{"content":"This error is raised when the input value is a string that cannot be parsed for a timedelta field: from datetime import timedelta\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: timedelta\n\n\ntry:\n    Model(x='t')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'time_delta_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#time_delta_parsing","title":"Validation Errors - time_delta_parsing","objectID":"/latest/errors/validation_errors/#time_delta_parsing","rank":-300},{"content":"This error is raised when the input value's type is not valid for a timedelta field: from datetime import timedelta\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: timedelta\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'time_delta_type' This error is also raised for strict fields when the input value is not an instance of timedelta .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#time_delta_type","title":"Validation Errors - time_delta_type","objectID":"/latest/errors/validation_errors/#time_delta_type","rank":-305},{"content":"This error is raised when the input value is a string that cannot be parsed for a time field: from datetime import time\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: time\n\n\ntry:\n    Model(x='25:20:30.400')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'time_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#time_parsing","title":"Validation Errors - time_parsing","objectID":"/latest/errors/validation_errors/#time_parsing","rank":-310},{"content":"This error is raised when the value type is not valid for a time field: from datetime import time\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: time\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'time_type' This error is also raised for strict fields when the input value is not an instance of time .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#time_type","title":"Validation Errors - time_type","objectID":"/latest/errors/validation_errors/#time_type","rank":-315},{"content":"This error is raised when the datetime value provided for a timezone-aware datetime field\ndoesn't have timezone information: from datetime import datetime\n\nfrom pydantic import AwareDatetime, BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: AwareDatetime\n\n\ntry:\n    Model(x=datetime.now())\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'timezone_aware'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#timezone_aware","title":"Validation Errors - timezone_aware","objectID":"/latest/errors/validation_errors/#timezone_aware","rank":-320},{"content":"This error is raised when the datetime value provided for a timezone-naive datetime field\nhas timezone info: from datetime import datetime, timezone\n\nfrom pydantic import BaseModel, NaiveDatetime, ValidationError\n\n\nclass Model(BaseModel):\n    x: NaiveDatetime\n\n\ntry:\n    Model(x=datetime.now(tz=timezone.utc))\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'timezone_naive'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#timezone_naive","title":"Validation Errors - timezone_naive","objectID":"/latest/errors/validation_errors/#timezone_naive","rank":-325},{"content":"This error is raised when the input value's length is greater than the field's max_length constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: list[int] = Field(max_length=3)\n\n\ntry:\n    Model(x=[1, 2, 3, 4])\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'too_long'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#too_long","title":"Validation Errors - too_long","objectID":"/latest/errors/validation_errors/#too_long","rank":-330},{"content":"This error is raised when the value length is less than the field's min_length constraint: from pydantic import BaseModel, Field, ValidationError\n\n\nclass Model(BaseModel):\n    x: list[int] = Field(min_length=3)\n\n\ntry:\n    Model(x=[1, 2])\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'too_short'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#too_short","title":"Validation Errors - too_short","objectID":"/latest/errors/validation_errors/#too_short","rank":-335},{"content":"This error is raised when the input value's type is not valid for a tuple field: from pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: tuple[int]\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'tuple_type' This error is also raised for strict fields when the input value is not an instance of tuple .","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#tuple_type","title":"Validation Errors - tuple_type","objectID":"/latest/errors/validation_errors/#tuple_type","rank":-340},{"content":"This error is raised when you provide a value by keyword for a positional-only\nargument while calling a function decorated with validate_call : from pydantic import ValidationError, validate_call\n\n\n@validate_call\ndef foo(a: int, /):\n    return a\n\n\ntry:\n    foo(a=2)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[1]['type']))\n    #> 'unexpected_keyword_argument' It is also raised when using pydantic.dataclasses and extra=forbid : from pydantic import TypeAdapter, ValidationError\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass(config={'extra': 'forbid'})\nclass Foo:\n    bar: int\n\n\ntry:\n    TypeAdapter(Foo).validate_python({'bar': 1, 'foobar': 2})\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'unexpected_keyword_argument'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#unexpected_keyword_argument","title":"Validation Errors - unexpected_keyword_argument","objectID":"/latest/errors/validation_errors/#unexpected_keyword_argument","rank":-345},{"content":"This error is raised when you provide a positional value for a keyword-only\nargument while calling a function decorated with validate_call : from pydantic import ValidationError, validate_call\n\n\n@validate_call\ndef foo(*, a: int):\n    return a\n\n\ntry:\n    foo(2)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[1]['type']))\n    #> 'unexpected_positional_argument'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#unexpected_positional_argument","title":"Validation Errors - unexpected_positional_argument","objectID":"/latest/errors/validation_errors/#unexpected_positional_argument","rank":-350},{"content":"This error is raised when the input's discriminator is not one of the expected values: Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass BlackCat(BaseModel):\n    pet_type: Literal['blackcat']\n\n\nclass WhiteCat(BaseModel):\n    pet_type: Literal['whitecat']\n\n\nclass Model(BaseModel):\n    cat: Union[BlackCat, WhiteCat] = Field(discriminator='pet_type')\n\n\ntry:\n    Model(cat={'pet_type': 'dog'})\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'union_tag_invalid' from typing import Literal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass BlackCat(BaseModel):\n    pet_type: Literal['blackcat']\n\n\nclass WhiteCat(BaseModel):\n    pet_type: Literal['whitecat']\n\n\nclass Model(BaseModel):\n    cat: BlackCat | WhiteCat = Field(discriminator='pet_type')\n\n\ntry:\n    Model(cat={'pet_type': 'dog'})\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'union_tag_invalid'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#union_tag_invalid","title":"Validation Errors - union_tag_invalid","objectID":"/latest/errors/validation_errors/#union_tag_invalid","rank":-355},{"content":"This error is raised when it is not possible to extract a discriminator value from the input: Python 3.9 and above Python 3.10 and above from typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass BlackCat(BaseModel):\n    pet_type: Literal['blackcat']\n\n\nclass WhiteCat(BaseModel):\n    pet_type: Literal['whitecat']\n\n\nclass Model(BaseModel):\n    cat: Union[BlackCat, WhiteCat] = Field(discriminator='pet_type')\n\n\ntry:\n    Model(cat={'name': 'blackcat'})\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'union_tag_not_found' from typing import Literal\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass BlackCat(BaseModel):\n    pet_type: Literal['blackcat']\n\n\nclass WhiteCat(BaseModel):\n    pet_type: Literal['whitecat']\n\n\nclass Model(BaseModel):\n    cat: BlackCat | WhiteCat = Field(discriminator='pet_type')\n\n\ntry:\n    Model(cat={'name': 'blackcat'})\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'union_tag_not_found'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#union_tag_not_found","title":"Validation Errors - union_tag_not_found","objectID":"/latest/errors/validation_errors/#union_tag_not_found","rank":-360},{"content":"This error is raised when the input value cannot be parsed as a URL: from pydantic import AnyUrl, BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    x: AnyUrl\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'url_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#url_parsing","title":"Validation Errors - url_parsing","objectID":"/latest/errors/validation_errors/#url_parsing","rank":-365},{"content":"This error is raised when the URL scheme is not valid for the URL type of the field: from pydantic import BaseModel, HttpUrl, ValidationError\n\n\nclass Model(BaseModel):\n    x: HttpUrl\n\n\ntry:\n    Model(x='ftp://example.com')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'url_scheme'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#url_scheme","title":"Validation Errors - url_scheme","objectID":"/latest/errors/validation_errors/#url_scheme","rank":-370},{"content":"This error is raised when the URL syntax is not valid: from pydantic import BaseModel, Field, HttpUrl, ValidationError\n\n\nclass Model(BaseModel):\n    x: HttpUrl = Field(strict=True)\n\n\ntry:\n    Model(x='http:////example.com')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'url_syntax_violation'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#url_syntax_violation","title":"Validation Errors - url_syntax_violation","objectID":"/latest/errors/validation_errors/#url_syntax_violation","rank":-375},{"content":"This error is raised when the URL length is greater than 2083: from pydantic import BaseModel, HttpUrl, ValidationError\n\n\nclass Model(BaseModel):\n    x: HttpUrl\n\n\ntry:\n    Model(x='x' * 2084)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'url_too_long'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#url_too_long","title":"Validation Errors - url_too_long","objectID":"/latest/errors/validation_errors/#url_too_long","rank":-380},{"content":"This error is raised when the input value's type is not valid for a URL field: from pydantic import BaseModel, HttpUrl, ValidationError\n\n\nclass Model(BaseModel):\n    x: HttpUrl\n\n\ntry:\n    Model(x=None)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'url_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#url_type","title":"Validation Errors - url_type","objectID":"/latest/errors/validation_errors/#url_type","rank":-385},{"content":"This error is raised when the input value's type is not valid for a UUID field: from uuid import UUID\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    u: UUID\n\n\ntry:\n    Model(u='12345678-124-1234-1234-567812345678')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'uuid_parsing'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#uuid_parsing","title":"Validation Errors - uuid_parsing","objectID":"/latest/errors/validation_errors/#uuid_parsing","rank":-390},{"content":"This error is raised when the input value's type is not valid instance for a UUID field (str, bytes or UUID): from uuid import UUID\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    u: UUID\n\n\ntry:\n    Model(u=1234567812412341234567812345678)\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'uuid_type'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#uuid_type","title":"Validation Errors - uuid_type","objectID":"/latest/errors/validation_errors/#uuid_type","rank":-395},{"content":"This error is raised when the input value's type is not match UUID version: from pydantic import UUID5, BaseModel, ValidationError\n\n\nclass Model(BaseModel):\n    u: UUID5\n\n\ntry:\n    Model(u='a6cc5730-2261-11ee-9c43-2eb5a363657c')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'uuid_version'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#uuid_version","title":"Validation Errors - uuid_version","objectID":"/latest/errors/validation_errors/#uuid_version","rank":-400},{"content":"This error is raised when a ValueError is raised during validation: from pydantic import BaseModel, ValidationError, field_validator\n\n\nclass Model(BaseModel):\n    x: str\n\n    @field_validator('x')\n    @classmethod\n    def repeat_b(cls, v):\n        raise ValueError()\n\n\ntry:\n    Model(x='test')\nexcept ValidationError as exc:\n    print(repr(exc.errors()[0]['type']))\n    #> 'value_error'","pageID":"Validation Errors","abs_url":"/latest/errors/validation_errors/#value_error","title":"Validation Errors - value_error","objectID":"/latest/errors/validation_errors/#value_error","rank":-405},{"content":"This page provides example snippets for creating more complex, custom validators in Pydantic.\nMany of these examples are adapted from Pydantic issues and discussions, and are intended to showcase\nthe flexibility and power of Pydantic's validation system.","pageID":"Custom Validators","abs_url":"/latest/examples/custom_validators/#Custom Validators","title":"Custom Validators","objectID":"/latest/examples/custom_validators/#Custom Validators","rank":100},{"content":"In this example, we'll construct a custom validator, attached to an  type,\nthat ensures a  object adheres to a given timezone constraint. The custom validator supports string specification of the timezone, and will raise an error if the  object does not have the correct timezone. We use __get_pydantic_core_schema__ in the validator to customize the schema of the annotated type (in this case, ), which allows us to add custom validation logic. Notably, we use a wrap validator function so that we can perform operations both before and after the default pydantic validation of a . Python 3.9 and above Python 3.10 and above import datetime as dt\nfrom dataclasses import dataclass\nfrom pprint import pprint\nfrom typing import Annotated, Any, Callable, Optional\n\nimport pytz\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom pydantic import (\n    GetCoreSchemaHandler,\n    PydanticUserError,\n    TypeAdapter,\n    ValidationError,\n)\n\n\n@dataclass(frozen=True)\nclass MyDatetimeValidator:\n    tz_constraint: Optional[str] = None\n\n    def tz_constraint_validator(\n        self,\n        value: dt.datetime,\n        handler: Callable,  # (1)!\n    ):\n        \"\"\"Validate tz_constraint and tz_info.\"\"\"\n        # handle naive datetimes\n        if self.tz_constraint is None:\n            assert (\n                value.tzinfo is None\n            ), 'tz_constraint is None, but provided value is tz-aware.'\n            return handler(value)\n\n        # validate tz_constraint and tz-aware tzinfo\n        if self.tz_constraint not in pytz.all_timezones:\n            raise PydanticUserError(\n                f'Invalid tz_constraint: {self.tz_constraint}',\n                code='unevaluable-type-annotation',\n            )\n        result = handler(value)  # (2)!\n        assert self.tz_constraint == str(\n            result.tzinfo\n        ), f'Invalid tzinfo: {str(result.tzinfo)}, expected: {self.tz_constraint}'\n\n        return result\n\n    def __get_pydantic_core_schema__(\n        self,\n        source_type: Any,\n        handler: GetCoreSchemaHandler,\n    ) -> CoreSchema:\n        return core_schema.no_info_wrap_validator_function(\n            self.tz_constraint_validator,\n            handler(source_type),\n        )\n\n\nLA = 'America/Los_Angeles'\nta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(LA)])\nprint(\n    ta.validate_python(dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LA)))\n)\n#> 2023-01-01 00:00:00-07:53\n\nLONDON = 'Europe/London'\ntry:\n    ta.validate_python(\n        dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LONDON))\n    )\nexcept ValidationError as ve:\n    pprint(ve.errors(), width=100)\n    \"\"\"\n    [{'ctx': {'error': AssertionError('Invalid tzinfo: Europe/London, expected: America/Los_Angeles')},\n    'input': datetime.datetime(2023, 1, 1, 0, 0, tzinfo= ),\n    'loc': (),\n    'msg': 'Assertion failed, Invalid tzinfo: Europe/London, expected: America/Los_Angeles',\n    'type': 'assertion_error',\n    'url': 'https://errors.pydantic.dev/2.8/v/assertion_error'}]\n    \"\"\" The handler function is what we call to validate the input with standard pydantic validation We call the handler function to validate the input with standard pydantic validation in this wrap validator import datetime as dt\nfrom dataclasses import dataclass\nfrom pprint import pprint\nfrom typing import Annotated, Any\nfrom collections.abc import Callable\n\nimport pytz\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom pydantic import (\n    GetCoreSchemaHandler,\n    PydanticUserError,\n    TypeAdapter,\n    ValidationError,\n)\n\n\n@dataclass(frozen=True)\nclass MyDatetimeValidator:\n    tz_constraint: str | None = None\n\n    def tz_constraint_validator(\n        self,\n        value: dt.datetime,\n        handler: Callable,  # (1)!\n    ):\n        \"\"\"Validate tz_constraint and tz_info.\"\"\"\n        # handle naive datetimes\n        if self.tz_constraint is None:\n            assert (\n                value.tzinfo is None\n            ), 'tz_constraint is None, but provided value is tz-aware.'\n            return handler(value)\n\n        # validate tz_constraint and tz-aware tzinfo\n        if self.tz_constraint not in pytz.all_timezones:\n            raise PydanticUserError(\n                f'Invalid tz_constraint: {self.tz_constraint}',\n                code='unevaluable-type-annotation',\n            )\n        result = handler(value)  # (2)!\n        assert self.tz_constraint == str(\n            result.tzinfo\n        ), f'Invalid tzinfo: {str(result.tzinfo)}, expected: {self.tz_constraint}'\n\n        return result\n\n    def __get_pydantic_core_schema__(\n        self,\n        source_type: Any,\n        handler: GetCoreSchemaHandler,\n    ) -> CoreSchema:\n        return core_schema.no_info_wrap_validator_function(\n            self.tz_constraint_validator,\n            handler(source_type),\n        )\n\n\nLA = 'America/Los_Angeles'\nta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(LA)])\nprint(\n    ta.validate_python(dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LA)))\n)\n#> 2023-01-01 00:00:00-07:53\n\nLONDON = 'Europe/London'\ntry:\n    ta.validate_python(\n        dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LONDON))\n    )\nexcept ValidationError as ve:\n    pprint(ve.errors(), width=100)\n    \"\"\"\n    [{'ctx': {'error': AssertionError('Invalid tzinfo: Europe/London, expected: America/Los_Angeles')},\n    'input': datetime.datetime(2023, 1, 1, 0, 0, tzinfo= ),\n    'loc': (),\n    'msg': 'Assertion failed, Invalid tzinfo: Europe/London, expected: America/Los_Angeles',\n    'type': 'assertion_error',\n    'url': 'https://errors.pydantic.dev/2.8/v/assertion_error'}]\n    \"\"\" The handler function is what we call to validate the input with standard pydantic validation We call the handler function to validate the input with standard pydantic validation in this wrap validator We can also enforce UTC offset constraints in a similar way.  Assuming we have a lower_bound and an upper_bound , we can create a custom validator to ensure our datetime has a UTC offset that is inclusive within the boundary we define: Python 3.9 and above Python 3.10 and above import datetime as dt\nfrom dataclasses import dataclass\nfrom pprint import pprint\nfrom typing import Annotated, Any, Callable\n\nimport pytz\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom pydantic import GetCoreSchemaHandler, TypeAdapter, ValidationError\n\n\n@dataclass(frozen=True)\nclass MyDatetimeValidator:\n    lower_bound: int\n    upper_bound: int\n\n    def validate_tz_bounds(self, value: dt.datetime, handler: Callable):\n        \"\"\"Validate and test bounds\"\"\"\n        assert value.utcoffset() is not None, 'UTC offset must exist'\n        assert self.lower_bound <= self.upper_bound, 'Invalid bounds'\n\n        result = handler(value)\n\n        hours_offset = value.utcoffset().total_seconds() / 3600\n        assert (\n            self.lower_bound <= hours_offset <= self.upper_bound\n        ), 'Value out of bounds'\n\n        return result\n\n    def __get_pydantic_core_schema__(\n        self,\n        source_type: Any,\n        handler: GetCoreSchemaHandler,\n    ) -> CoreSchema:\n        return core_schema.no_info_wrap_validator_function(\n            self.validate_tz_bounds,\n            handler(source_type),\n        )\n\n\nLA = 'America/Los_Angeles'  # UTC-7 or UTC-8\nta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(-10, -5)])\nprint(\n    ta.validate_python(dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LA)))\n)\n#> 2023-01-01 00:00:00-07:53\n\nLONDON = 'Europe/London'\ntry:\n    print(\n        ta.validate_python(\n            dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LONDON))\n        )\n    )\nexcept ValidationError as e:\n    pprint(e.errors(), width=100)\n    \"\"\"\n    [{'ctx': {'error': AssertionError('Value out of bounds')},\n    'input': datetime.datetime(2023, 1, 1, 0, 0, tzinfo= ),\n    'loc': (),\n    'msg': 'Assertion failed, Value out of bounds',\n    'type': 'assertion_error',\n    'url': 'https://errors.pydantic.dev/2.8/v/assertion_error'}]\n    \"\"\" import datetime as dt\nfrom dataclasses import dataclass\nfrom pprint import pprint\nfrom typing import Annotated, Any\nfrom collections.abc import Callable\n\nimport pytz\nfrom pydantic_core import CoreSchema, core_schema\n\nfrom pydantic import GetCoreSchemaHandler, TypeAdapter, ValidationError\n\n\n@dataclass(frozen=True)\nclass MyDatetimeValidator:\n    lower_bound: int\n    upper_bound: int\n\n    def validate_tz_bounds(self, value: dt.datetime, handler: Callable):\n        \"\"\"Validate and test bounds\"\"\"\n        assert value.utcoffset() is not None, 'UTC offset must exist'\n        assert self.lower_bound <= self.upper_bound, 'Invalid bounds'\n\n        result = handler(value)\n\n        hours_offset = value.utcoffset().total_seconds() / 3600\n        assert (\n            self.lower_bound <= hours_offset <= self.upper_bound\n        ), 'Value out of bounds'\n\n        return result\n\n    def __get_pydantic_core_schema__(\n        self,\n        source_type: Any,\n        handler: GetCoreSchemaHandler,\n    ) -> CoreSchema:\n        return core_schema.no_info_wrap_validator_function(\n            self.validate_tz_bounds,\n            handler(source_type),\n        )\n\n\nLA = 'America/Los_Angeles'  # UTC-7 or UTC-8\nta = TypeAdapter(Annotated[dt.datetime, MyDatetimeValidator(-10, -5)])\nprint(\n    ta.validate_python(dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LA)))\n)\n#> 2023-01-01 00:00:00-07:53\n\nLONDON = 'Europe/London'\ntry:\n    print(\n        ta.validate_python(\n            dt.datetime(2023, 1, 1, 0, 0, tzinfo=pytz.timezone(LONDON))\n        )\n    )\nexcept ValidationError as e:\n    pprint(e.errors(), width=100)\n    \"\"\"\n    [{'ctx': {'error': AssertionError('Value out of bounds')},\n    'input': datetime.datetime(2023, 1, 1, 0, 0, tzinfo= ),\n    'loc': (),\n    'msg': 'Assertion failed, Value out of bounds',\n    'type': 'assertion_error',\n    'url': 'https://errors.pydantic.dev/2.8/v/assertion_error'}]\n    \"\"\"","pageID":"Custom Validators","abs_url":"/latest/examples/custom_validators/#custom-datetime-validator-via-annotated-metadata","title":"Custom Validators - Custom datetime Validator via  Metadata","objectID":"/latest/examples/custom_validators/#custom-datetime-validator-via-annotated-metadata","rank":95},{"content":"Here, we demonstrate two ways to validate a field of a nested model, where the validator utilizes data from the parent model. In this example, we construct a validator that checks that each user's password is not in a list of forbidden passwords specified by the parent model. One way to do this is to place a custom validator on the outer model: Python 3.9 and above Python 3.11 and above from typing_extensions import Self\n\nfrom pydantic import BaseModel, ValidationError, model_validator\n\n\nclass User(BaseModel):\n    username: str\n    password: str\n\n\nclass Organization(BaseModel):\n    forbidden_passwords: list[str]\n    users: list[User]\n\n    @model_validator(mode='after')\n    def validate_user_passwords(self) -> Self:\n        \"\"\"Check that user password is not in forbidden list. Raise a validation error if a forbidden password is encountered.\"\"\"\n        for user in self.users:\n            current_pw = user.password\n            if current_pw in self.forbidden_passwords:\n                raise ValueError(\n                    f'Password {current_pw} is forbidden. Please choose another password for user {user.username}.'\n                )\n        return self\n\n\ndata = {\n    'forbidden_passwords': ['123'],\n    'users': [\n        {'username': 'Spartacat', 'password': '123'},\n        {'username': 'Iceburgh', 'password': '87'},\n    ],\n}\ntry:\n    org = Organization(**data)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Organization\n      Value error, Password 123 is forbidden. Please choose another password for user Spartacat. [type=value_error, input_value={'forbidden_passwords': [...gh', 'password': '87'}]}, input_type=dict]\n    \"\"\" from typing import Self\n\nfrom pydantic import BaseModel, ValidationError, model_validator\n\n\nclass User(BaseModel):\n    username: str\n    password: str\n\n\nclass Organization(BaseModel):\n    forbidden_passwords: list[str]\n    users: list[User]\n\n    @model_validator(mode='after')\n    def validate_user_passwords(self) -> Self:\n        \"\"\"Check that user password is not in forbidden list. Raise a validation error if a forbidden password is encountered.\"\"\"\n        for user in self.users:\n            current_pw = user.password\n            if current_pw in self.forbidden_passwords:\n                raise ValueError(\n                    f'Password {current_pw} is forbidden. Please choose another password for user {user.username}.'\n                )\n        return self\n\n\ndata = {\n    'forbidden_passwords': ['123'],\n    'users': [\n        {'username': 'Spartacat', 'password': '123'},\n        {'username': 'Iceburgh', 'password': '87'},\n    ],\n}\ntry:\n    org = Organization(**data)\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Organization\n      Value error, Password 123 is forbidden. Please choose another password for user Spartacat. [type=value_error, input_value={'forbidden_passwords': [...gh', 'password': '87'}]}, input_type=dict]\n    \"\"\" Alternatively, a custom validator can be used in the nested model class ( User ), with the forbidden passwords data from the parent model being passed in via validation context. Warning The ability to mutate the context within a validator adds a lot of power to nested validation, but can also lead to confusing or hard-to-debug code. Use this approach at your own risk! from pydantic import BaseModel, ValidationError, ValidationInfo, field_validator\n\n\nclass User(BaseModel):\n    username: str\n    password: str\n\n    @field_validator('password', mode='after')\n    @classmethod\n    def validate_user_passwords(\n        cls, password: str, info: ValidationInfo\n    ) -> str:\n        \"\"\"Check that user password is not in forbidden list.\"\"\"\n        forbidden_passwords = (\n            info.context.get('forbidden_passwords', []) if info.context else []\n        )\n        if password in forbidden_passwords:\n            raise ValueError(f'Password {password} is forbidden.')\n        return password\n\n\nclass Organization(BaseModel):\n    forbidden_passwords: list[str]\n    users: list[User]\n\n    @field_validator('forbidden_passwords', mode='after')\n    @classmethod\n    def add_context(cls, v: list[str], info: ValidationInfo) -> list[str]:\n        if info.context is not None:\n            info.context.update({'forbidden_passwords': v})\n        return v\n\n\ndata = {\n    'forbidden_passwords': ['123'],\n    'users': [\n        {'username': 'Spartacat', 'password': '123'},\n        {'username': 'Iceburgh', 'password': '87'},\n    ],\n}\n\ntry:\n    org = Organization.model_validate(data, context={})\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Organization\n    users.0.password\n      Value error, Password 123 is forbidden. [type=value_error, input_value='123', input_type=str]\n    \"\"\" Note that if the context property is not included in model_validate , then info.context will be None and the forbidden passwords list will not get added to the context in the above implementation. As such, validate_user_passwords would not carry out the desired password validation. More details about validation context can be found here .","pageID":"Custom Validators","abs_url":"/latest/examples/custom_validators/#validating-nested-model-fields","title":"Custom Validators - Validating Nested Model Fields","objectID":"/latest/examples/custom_validators/#validating-nested-model-fields","rank":90},{"content":"pydantic is a great tool for validating data coming from various sources.\nIn this section, we will look at how to validate data from different types of files. Note If you're using any of the below file formats to parse configuration / settings, you might want to\nconsider using the  library, which offers builtin\nsupport for parsing this type of data.","pageID":"Validating File Data","abs_url":"/latest/examples/files/#Validating File Data","title":"Validating File Data","objectID":"/latest/examples/files/#Validating File Data","rank":100},{"content":".json files are a common way to store key / value data in a human-readable format.\nHere is an example of a .json file: { \"name\" : \"John Doe\" , \"age\" : 30 , \"email\" : \"john@example.com\" } To validate this data, we can use a pydantic model: import pathlib\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\njson_string = pathlib.Path('person.json').read_text()\nperson = Person.model_validate_json(json_string)\nprint(person)\n#> name='John Doe' age=30 email='john@example.com' If the data in the file is not valid, pydantic will raise a .\nLet's say we have the following .json file: { \"age\" : -30 , \"email\" : \"not-an-email-address\" } This data is flawed for three reasons: It's missing the name field. The age field is negative. The email field is not a valid email address. When we try to validate this data, pydantic raises a  with all of the\nabove issues: import pathlib\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt, ValidationError\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\njson_string = pathlib.Path('person.json').read_text()\ntry:\n    person = Person.model_validate_json(json_string)\nexcept ValidationError as err:\n    print(err)\n    \"\"\"\n    3 validation errors for Person\n    name\n    Field required [type=missing, input_value={'age': -30, 'email': 'not-an-email-address'}, input_type=dict]\n        For further information visit https://errors.pydantic.dev/2.10/v/missing\n    age\n    Input should be greater than 0 [type=greater_than, input_value=-30, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.10/v/greater_than\n    email\n    value is not a valid email address: An email address must have an @-sign. [type=value_error, input_value='not-an-email-address', input_type=str]\n    \"\"\" Often, it's the case that you have an abundance of a certain type of data within a .json file.\nFor example, you might have a list of people: [ { \"name\" : \"John Doe\" , \"age\" : 30 , \"email\" : \"john@example.com\" }, { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : \"jane@example.com\" } ] In this case, you can validate the data against a list[Person] model: import pathlib\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt, TypeAdapter\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\nperson_list_adapter = TypeAdapter(list[Person])  # (1)!\n\njson_string = pathlib.Path('people.json').read_text()\npeople = person_list_adapter.validate_json(json_string)\nprint(people)\n#> [Person(name='John Doe', age=30, email='john@example.com'), Person(name='Jane Doe', age=25, email='jane@example.com')] We use  to validate a list of Person objects.\n is a Pydantic construct used to validate data against a single type.","pageID":"Validating File Data","abs_url":"/latest/examples/files/#json-data","title":"Validating File Data - JSON data","objectID":"/latest/examples/files/#json-data","rank":95},{"content":"Similar to validating a list of objects from a .json file, you can validate a list of objects from a .jsonl file. .jsonl files are a sequence of JSON objects separated by newlines. Consider the following .jsonl file: { \"name\" : \"John Doe\" , \"age\" : 30 , \"email\" : \"john@example.com\" } { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : \"jane@example.com\" } We can validate this data with a similar approach to the one we used for .json files: import pathlib\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\njson_lines = pathlib.Path('people.jsonl').read_text().splitlines()\npeople = [Person.model_validate_json(line) for line in json_lines]\nprint(people)\n#> [Person(name='John Doe', age=30, email='john@example.com'), Person(name='Jane Doe', age=25, email='jane@example.com')]","pageID":"Validating File Data","abs_url":"/latest/examples/files/#json-lines-files","title":"Validating File Data - JSON lines files","objectID":"/latest/examples/files/#json-lines-files","rank":90},{"content":"CSV is one of the most common file formats for storing tabular data.\nTo validate data from a CSV file, you can use the csv module from the Python standard library to load\nthe data and validate it against a Pydantic model. Consider the following CSV file: name,age,email\nJohn Doe,30,john@example.com\nJane Doe,25,jane@example.com Here's how we validate that data: import csv\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\nwith open('people.csv') as f:\n    reader = csv.DictReader(f)\n    people = [Person.model_validate(row) for row in reader]\n\nprint(people)\n#> [Person(name='John Doe', age=30, email='john@example.com'), Person(name='Jane Doe', age=25, email='jane@example.com')]","pageID":"Validating File Data","abs_url":"/latest/examples/files/#csv-files","title":"Validating File Data - CSV files","objectID":"/latest/examples/files/#csv-files","rank":85},{"content":"TOML files are often used for configuration due to their simplicity and readability. Consider the following TOML file: name = \"John Doe\" age = 30 email = \"john@example.com\" Here's how we validate that data: import tomllib\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\nwith open('person.toml', 'rb') as f:\n    data = tomllib.load(f)\n\nperson = Person.model_validate(data)\nprint(person)\n#> name='John Doe' age=30 email='john@example.com'","pageID":"Validating File Data","abs_url":"/latest/examples/files/#toml-files","title":"Validating File Data - TOML files","objectID":"/latest/examples/files/#toml-files","rank":80},{"content":"YAML (YAML Ain't Markup Language) is a human-readable data serialization format that is often used for configuration files. Consider the following YAML file: name : John Doe age : 30 email : john@example.com Here's how we validate that data: import yaml\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\nwith open('person.yaml') as f:\n    data = yaml.safe_load(f)\n\nperson = Person.model_validate(data)\nprint(person)\n#> name='John Doe' age=30 email='john@example.com'","pageID":"Validating File Data","abs_url":"/latest/examples/files/#yaml-files","title":"Validating File Data - YAML files","objectID":"/latest/examples/files/#yaml-files","rank":75},{"content":"XML (eXtensible Markup Language) is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. Consider the following XML file: <?xml version=\"1.0\"?> <person> <name> John Doe </name> <age> 30 </age> <email> john@example.com </email> </person> Here's how we validate that data: import xml.etree.ElementTree as ET\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\ntree = ET.parse('person.xml').getroot()\ndata = {child.tag: child.text for child in tree}\nperson = Person.model_validate(data)\nprint(person)\n#> name='John Doe' age=30 email='john@example.com'","pageID":"Validating File Data","abs_url":"/latest/examples/files/#xml-files","title":"Validating File Data - XML files","objectID":"/latest/examples/files/#xml-files","rank":70},{"content":"INI files are a simple configuration file format that uses sections and key-value pairs. They are commonly used in Windows applications and older software. Consider the following INI file: [PERSON] name = John Doe age = 30 email = john@example.com Here's how we validate that data: import configparser\n\nfrom pydantic import BaseModel, EmailStr, PositiveInt\n\n\nclass Person(BaseModel):\n    name: str\n    age: PositiveInt\n    email: EmailStr\n\n\nconfig = configparser.ConfigParser()\nconfig.read('person.ini')\nperson = Person.model_validate(config['PERSON'])\nprint(person)\n#> name='John Doe' age=30 email='john@example.com'","pageID":"Validating File Data","abs_url":"/latest/examples/files/#ini-files","title":"Validating File Data - INI files","objectID":"/latest/examples/files/#ini-files","rank":65},{"content":"Pydantic serves as a great tool for defining models for ORM (object relational mapping) libraries.\nORMs are used to map objects to database tables, and vice versa.","pageID":"Databases","abs_url":"/latest/examples/orms/#Databases","title":"Databases","objectID":"/latest/examples/orms/#Databases","rank":100},{"content":"Pydantic can pair with SQLAlchemy, as it can be used to define the schema of the database models. Code Duplication If you use Pydantic with SQLAlchemy, you might experience some frustration with code duplication.\nIf you find yourself experiencing this difficulty, you might also consider SQLModel which integrates Pydantic with SQLAlchemy such that much of the code duplication is eliminated. If you'd prefer to use pure Pydantic with SQLAlchemy, we recommend using Pydantic models alongside of SQLAlchemy models\nas shown in the example below. In this case, we take advantage of Pydantic's aliases feature to name a Column after a reserved SQLAlchemy field, thus avoiding conflicts. import sqlalchemy as sa\nfrom sqlalchemy.orm import declarative_base\n\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass MyModel(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n\n    metadata: dict[str, str] = Field(alias='metadata_')\n\n\nBase = declarative_base()\n\n\nclass MyTableModel(Base):\n    __tablename__ = 'my_table'\n    id = sa.Column('id', sa.Integer, primary_key=True)\n    # 'metadata' is reserved by SQLAlchemy, hence the '_'\n    metadata_ = sa.Column('metadata', sa.JSON)\n\n\nsql_model = MyTableModel(metadata_={'key': 'val'}, id=1)\npydantic_model = MyModel.model_validate(sql_model)\n\nprint(pydantic_model.model_dump())\n#> {'metadata': {'key': 'val'}}\nprint(pydantic_model.model_dump(by_alias=True))\n#> {'metadata_': {'key': 'val'}} Note The example above works because aliases have priority over field names for\nfield population. Accessing SQLModel 's metadata attribute would lead to a ValidationError .","pageID":"Databases","abs_url":"/latest/examples/orms/#sqlalchemy","title":"Databases - SQLAlchemy","objectID":"/latest/examples/orms/#sqlalchemy","rank":95},{"content":"Pydantic is quite helpful for validating data that goes into and comes out of queues. Below,\nwe'll explore how to validate / serialize data with various queue systems.","pageID":"Queues","abs_url":"/latest/examples/queues/#Queues","title":"Queues","objectID":"/latest/examples/queues/#Queues","rank":100},{"content":"Redis is a popular in-memory data structure store. In order to run this example locally, you'll first need to install Redis and start your server up locally. Here's a simple example of how you can use Pydantic to: Serialize data to push to the queue Deserialize and validate data when it's popped from the queue import redis\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: EmailStr\n\n\nr = redis.Redis(host='localhost', port=6379, db=0)\nQUEUE_NAME = 'user_queue'\n\n\ndef push_to_queue(user_data: User) -> None:\n    serialized_data = user_data.model_dump_json()\n    r.rpush(QUEUE_NAME, serialized_data)\n    print(f'Added to queue: {serialized_data}')\n\n\nuser1 = User(id=1, name='John Doe', email='john@example.com')\nuser2 = User(id=2, name='Jane Doe', email='jane@example.com')\n\npush_to_queue(user1)\n#> Added to queue: {\"id\":1,\"name\":\"John Doe\",\"email\":\"john@example.com\"}\n\npush_to_queue(user2)\n#> Added to queue: {\"id\":2,\"name\":\"Jane Doe\",\"email\":\"jane@example.com\"}\n\n\ndef pop_from_queue() -> None:\n    data = r.lpop(QUEUE_NAME)\n\n    if data:\n        user = User.model_validate_json(data)\n        print(f'Validated user: {repr(user)}')\n    else:\n        print('Queue is empty')\n\n\npop_from_queue()\n#> Validated user: User(id=1, name='John Doe', email='john@example.com')\n\npop_from_queue()\n#> Validated user: User(id=2, name='Jane Doe', email='jane@example.com')\n\npop_from_queue()\n#> Queue is empty","pageID":"Queues","abs_url":"/latest/examples/queues/#redis-queue","title":"Queues - Redis queue","objectID":"/latest/examples/queues/#redis-queue","rank":95},{"content":"RabbitMQ is a popular message broker that implements the AMQP protocol. In order to run this example locally, you'll first need to install RabbitMQ and start your server. Here's a simple example of how you can use Pydantic to: Serialize data to push to the queue Deserialize and validate data when it's popped from the queue First, let's create a sender script. import pika\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: EmailStr\n\n\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nchannel = connection.channel()\nQUEUE_NAME = 'user_queue'\nchannel.queue_declare(queue=QUEUE_NAME)\n\n\ndef push_to_queue(user_data: User) -> None:\n    serialized_data = user_data.model_dump_json()\n    channel.basic_publish(\n        exchange='',\n        routing_key=QUEUE_NAME,\n        body=serialized_data,\n    )\n    print(f'Added to queue: {serialized_data}')\n\n\nuser1 = User(id=1, name='John Doe', email='john@example.com')\nuser2 = User(id=2, name='Jane Doe', email='jane@example.com')\n\npush_to_queue(user1)\n#> Added to queue: {\"id\":1,\"name\":\"John Doe\",\"email\":\"john@example.com\"}\n\npush_to_queue(user2)\n#> Added to queue: {\"id\":2,\"name\":\"Jane Doe\",\"email\":\"jane@example.com\"}\n\nconnection.close() And here's the receiver script. import pika\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: EmailStr\n\n\ndef main():\n    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n    channel = connection.channel()\n    QUEUE_NAME = 'user_queue'\n    channel.queue_declare(queue=QUEUE_NAME)\n\n    def process_message(\n        ch: pika.channel.Channel,\n        method: pika.spec.Basic.Deliver,\n        properties: pika.spec.BasicProperties,\n        body: bytes,\n    ):\n        user = User.model_validate_json(body)\n        print(f'Validated user: {repr(user)}')\n        ch.basic_ack(delivery_tag=method.delivery_tag)\n\n    channel.basic_consume(queue=QUEUE_NAME, on_message_callback=process_message)\n    channel.start_consuming()\n\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        pass To test this example: Run the receiver script in one terminal to start the consumer. Run the sender script in another terminal to send messages.","pageID":"Queues","abs_url":"/latest/examples/queues/#rabbitmq","title":"Queues - RabbitMQ","objectID":"/latest/examples/queues/#rabbitmq","rank":90},{"content":"ARQ is a fast Redis-based job queue for Python.\nIt's built on top of Redis and provides a simple way to handle background tasks. In order to run this example locally, you’ll need to Install Redis and start your server. Here's a simple example of how you can use Pydantic with ARQ to: Define a model for your job data Serialize data when enqueueing jobs Validate and deserialize data when processing jobs import asyncio\nfrom typing import Any\n\nfrom arq import create_pool\nfrom arq.connections import RedisSettings\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: EmailStr\n\n\nREDIS_SETTINGS = RedisSettings()\n\n\nasync def process_user(ctx: dict[str, Any], user_data: dict[str, Any]) -> None:\n    user = User.model_validate(user_data)\n    print(f'Processing user: {repr(user)}')\n\n\nasync def enqueue_jobs(redis):\n    user1 = User(id=1, name='John Doe', email='john@example.com')\n    user2 = User(id=2, name='Jane Doe', email='jane@example.com')\n\n    await redis.enqueue_job('process_user', user1.model_dump())\n    print(f'Enqueued user: {repr(user1)}')\n\n    await redis.enqueue_job('process_user', user2.model_dump())\n    print(f'Enqueued user: {repr(user2)}')\n\n\nclass WorkerSettings:\n    functions = [process_user]\n    redis_settings = REDIS_SETTINGS\n\n\nasync def main():\n    redis = await create_pool(REDIS_SETTINGS)\n    await enqueue_jobs(redis)\n\n\nif __name__ == '__main__':\n    asyncio.run(main()) This script is complete.\nIt should run \"as is\" both to enqueue jobs and to process them.","pageID":"Queues","abs_url":"/latest/examples/queues/#arq","title":"Queues - ARQ","objectID":"/latest/examples/queues/#arq","rank":85},{"content":"Pydantic models are a great way to validating and serializing data for requests and responses.\nPydantic is instrumental in many web frameworks and libraries, such as FastAPI, Django, Flask, and HTTPX.","pageID":"Web and API Requests","abs_url":"/latest/examples/requests/#Web and API Requests","title":"Web and API Requests","objectID":"/latest/examples/requests/#Web and API Requests","rank":100},{"content":"httpx is a HTTP client for Python 3 with synchronous and asynchronous APIs.\nIn the below example, we query the JSONPlaceholder API to get a user's data and validate it with a Pydantic model. import httpx\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: EmailStr\n\n\nurl = 'https://jsonplaceholder.typicode.com/users/1'\n\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nuser = User.model_validate(response.json())\nprint(repr(user))\n#> User(id=1, name='Leanne Graham', email='Sincere@april.biz') The  tool from Pydantic often comes in quite\nhandy when working with HTTP requests. Consider a similar example where we are validating a list of users: from pprint import pprint\n\nimport httpx\n\nfrom pydantic import BaseModel, EmailStr, TypeAdapter\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: EmailStr\n\n\nurl = 'https://jsonplaceholder.typicode.com/users/'  # (1)!\n\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nusers_list_adapter = TypeAdapter(list[User])\n\nusers = users_list_adapter.validate_python(response.json())\npprint([u.name for u in users])\n\"\"\"\n['Leanne Graham',\n 'Ervin Howell',\n 'Clementine Bauch',\n 'Patricia Lebsack',\n 'Chelsey Dietrich',\n 'Mrs. Dennis Schulist',\n 'Kurtis Weissnat',\n 'Nicholas Runolfsdottir V',\n 'Glenna Reichert',\n 'Clementina DuBuque']\n\"\"\" Note, we're querying the /users/ endpoint here to get a list of users.","pageID":"Web and API Requests","abs_url":"/latest/examples/requests/#httpx-requests","title":"Web and API Requests - httpx requests","objectID":"/latest/examples/requests/#httpx-requests","rank":95},{"content":"pydantic integrates well with AWS Lambda functions. In this guide, we'll discuss how to setup pydantic for an AWS Lambda function.","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#AWS Lambda","title":"AWS Lambda","objectID":"/latest/integrations/aws_lambda/#AWS Lambda","rank":100},{"content":"There are many ways to utilize Python libraries in AWS Lambda functions. As outlined in the AWS Lambda documentation , the most common approaches include: Using a .zip file archive to package your code and dependencies Using AWS Lambda Layers to share libraries across multiple functions Using a container image to package your code and dependencies All of these approaches can be used with pydantic . The best approach for you will depend on your specific requirements and constraints. We'll cover the first two cases more in-depth here, as dependency management with\na container image is more straightforward. If you're using a container image, you might find this comment helpful for installing pydantic . Tip If you use pydantic across multiple functions, you may want to consider AWS Lambda Layers, which support seamless sharing of libraries across multiple functions. Regardless of the dependencies management approach you choose, it's beneficial to adhere to these guidelines to ensure a smooth\ndependency management process.","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#installing-python-libraries-for-aws-lambda-functions","title":"AWS Lambda - Installing Python libraries for AWS Lambda functions","objectID":"/latest/integrations/aws_lambda/#installing-python-libraries-for-aws-lambda-functions","rank":95},{"content":"When you're building your .zip file archive with your code and dependencies or organizing your .zip file for a Lambda Layer, you'll likely use a local virtual environment to install and manage your dependencies. This can be a bit tricky if you're using pip because pip installs wheels compiled for your local platform, which may not be compatible with the Lambda environment. Thus, we suggest you use a command similar to the following: pip install \\ --platform manylinux2014_x86_64 \\ # (1)! --target = <your_package_dir> \\ # (2)! --implementation cp \\ # (3)! --python-version 3 .10 \\ # (4)! --only-binary = :all: \\ # (5)! --upgrade pydantic # (6)! Use the platform corresponding to your Lambda runtime. Specify the directory where you want to install the package (often python for Lambda Layers). Use the CPython implementation. The Python version must be compatible with the Lambda runtime. This flag ensures that the package is installed pre-built binary wheels. The latest version of pydantic will be installed.","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#installing-pydantic-for-aws-lambda-functions","title":"AWS Lambda - Installing pydantic for AWS Lambda functions","objectID":"/latest/integrations/aws_lambda/#installing-pydantic-for-aws-lambda-functions","rank":90},{"content":"","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#troubleshooting","title":"AWS Lambda - Troubleshooting","objectID":"/latest/integrations/aws_lambda/#troubleshooting","rank":85},{"content":"The no module named `pydantic_core._pydantic_core` error is a common issue that indicates you have installed pydantic incorrectly. To debug this issue, you can try the following steps (before the failing import): Check the contents of the installed pydantic-core package. Are the compiled library and its type stubs both present? from importlib.metadata import files\nprint([file for file in files('pydantic-core') if file.name.startswith('_pydantic_core')])\n\"\"\"\n[PackagePath('pydantic_core/_pydantic_core.pyi'), PackagePath('pydantic_core/_pydantic_core.cpython-312-x86_64-linux-gnu.so')]\n\"\"\" You should expect to see two files like those printed above. The compiled library file should have the .so or .pyd extension with a name that varies according to the OS and Python version. Check that your lambda's Python version is compatible with the compiled library version found above. import sysconfig\nprint(sysconfig.get_config_var(\"EXT_SUFFIX\"))\n#> '.cpython-312-x86_64-linux-gnu.so' You should expect to see the same suffix here as the compiled library, for example here we see this suffix .cpython-312-x86_64-linux-gnu.so indeed matches _pydantic_core.cpython-312-x86_64-linux-gnu.so . If these two checks do not match, your build steps have not installed the correct native code for your lambda's target platform. You should adjust your build steps to change the version of the installed library which gets installed. Most likely errors: Your OS or CPU architecture is mismatched (e.g. darwin vs x86_64-linux-gnu). Try passing correct --platform argument to pip install when installing your lambda dependencies, or build inside a linux docker container for the correct platform. Possible platforms at the moment include --platform manylinux2014_x86_64 or --platform manylinux2014_aarch64 , but these may change with a future Pydantic major release. Your Python version is mismatched (e.g. cpython-310 vs cpython-312 ). Try passing correct --python-version argument to pip install , or otherwise change the Python version used on your build.","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#missing-pydantic_core-module","title":"AWS Lambda - Troubleshooting - Missing pydantic_core module","objectID":"/latest/integrations/aws_lambda/#missing-pydantic_core-module","rank":80},{"content":"Pydantic uses version from importlib.metadata to check what version of email-validator is installed.\nThis package versioning mechanism is somewhat incompatible with AWS Lambda, even though it's the industry standard for versioning packages in Python. There\nare a few ways to fix this issue: If you're deploying your lambda with the serverless framework, it's likely that the appropriate metadata for the email-validator package is not being included in your deployment package. Tools like serverless-python-requirements remove metadata to reduce package size. You can fix this issue by setting the slim setting to false in your serverless.yml file: pythonRequirements : dockerizePip : non-linux slim : false fileName : requirements.txt You can read more about this fix, and other slim settings that might be relevant here . If you're using a .zip archive for your code and/or dependencies, make sure that your package contains the required version metadata. To do this, make sure you include the dist-info directory in your .zip archive for the email-validator package. This issue has been reported for other popular python libraries like jsonschema , so you can\nread more about the issue and potential fixes there as well.","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#no-package-metadata-was-found-for-email-validator","title":"AWS Lambda - Troubleshooting - No package metadata was found for email-validator","objectID":"/latest/integrations/aws_lambda/#no-package-metadata-was-found-for-email-validator","rank":75},{"content":"","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#extra-resources","title":"AWS Lambda - Extra Resources","objectID":"/latest/integrations/aws_lambda/#extra-resources","rank":70},{"content":"If you're still struggling with installing pydantic for your AWS Lambda, you might consult with this issue , which covers a variety of problems and solutions encountered by other developers.","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#more-debugging-tips","title":"AWS Lambda - Extra Resources - More Debugging Tips","objectID":"/latest/integrations/aws_lambda/#more-debugging-tips","rank":65},{"content":"Check out our blog post to learn more about how to use pydantic to validate event and context data in AWS Lambda functions.","pageID":"AWS Lambda","abs_url":"/latest/integrations/aws_lambda/#validating-event-and-context-data","title":"AWS Lambda - Extra Resources - Validating event and context data","objectID":"/latest/integrations/aws_lambda/#validating-event-and-context-data","rank":60},{"content":"The datamodel-code-generator project is a library and command-line utility to generate pydantic models from just about any data source, including: OpenAPI 3 (YAML/JSON) JSON Schema JSON/YAML/CSV Data (which will be converted to JSON Schema) Python dictionary (which will be converted to JSON Schema) GraphQL schema Whenever you find yourself with any data convertible JSON but without pydantic models, this tool will allow you to generate type-safe model hierarchies on demand.","pageID":"datamodel-code-generator","abs_url":"/latest/integrations/datamodel_code_generator/#code-generation-with-datamodel-code-generator","title":"datamodel-code-generator","objectID":"/latest/integrations/datamodel_code_generator/#code-generation-with-datamodel-code-generator","rank":100},{"content":"pip install datamodel-code-generator","pageID":"datamodel-code-generator","abs_url":"/latest/integrations/datamodel_code_generator/#installation","title":"datamodel-code-generator - Installation","objectID":"/latest/integrations/datamodel_code_generator/#installation","rank":95},{"content":"In this case, datamodel-code-generator creates pydantic models from a JSON Schema file. datamodel-codegen --input person.json --input-file-type jsonschema --output model.py person.json: { \"$id\" : \"person.json\" , \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"title\" : \"Person\" , \"type\" : \"object\" , \"properties\" : { \"first_name\" : { \"type\" : \"string\" , \"description\" : \"The person's first name.\" }, \"last_name\" : { \"type\" : \"string\" , \"description\" : \"The person's last name.\" }, \"age\" : { \"description\" : \"Age in years.\" , \"type\" : \"integer\" , \"minimum\" : 0 }, \"pets\" : { \"type\" : \"array\" , \"items\" : [ { \"$ref\" : \"#/definitions/Pet\" } ] }, \"comment\" : { \"type\" : \"null\" } }, \"required\" : [ \"first_name\" , \"last_name\" ], \"definitions\" : { \"Pet\" : { \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" } } } } } model.py: # generated by datamodel-codegen:\n#   filename:  person.json\n#   timestamp: 2020-05-19T15:07:31+00:00\nfrom __future__ import annotations\n\nfrom typing import Any\n\nfrom pydantic import BaseModel, Field, conint\n\n\nclass Pet(BaseModel):\n    name: str | None = None\n    age: int | None = None\n\n\nclass Person(BaseModel):\n    first_name: str = Field(description=\"The person's first name.\")\n    last_name: str = Field(description=\"The person's last name.\")\n    age: conint(ge=0) | None = Field(None, description='Age in years.')\n    pets: list[Pet] | None = None\n    comment: Any | None = None More information can be found on the official documentation","pageID":"datamodel-code-generator","abs_url":"/latest/integrations/datamodel_code_generator/#example","title":"datamodel-code-generator - Example","objectID":"/latest/integrations/datamodel_code_generator/#example","rank":90},{"content":"Note Admission: I (the primary developer of Pydantic) also develop python-devtools. python-devtools ( pip install devtools ) provides a number of tools which\nare useful during Python development, including debug() an alternative to print() which formats output in a way\nwhich should be easier to read than print as well as giving information about which file/line the print statement\nis on and what value was printed. Pydantic integrates with devtools by implementing the __pretty__ method on most public classes. In particular debug() is useful when inspecting models: from datetime import datetime\n\nfrom devtools import debug\n\nfrom pydantic import BaseModel\n\n\nclass Address(BaseModel):\n    street: str\n    country: str\n    lat: float\n    lng: float\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    signup_ts: datetime\n    friends: list[int]\n    address: Address\n\n\nuser = User(\n    id='123',\n    name='John Doe',\n    signup_ts='2019-06-01 12:22',\n    friends=[1234, 4567, 7890],\n    address=dict(street='Testing', country='uk', lat=51.5, lng=0),\n)\ndebug(user)\nprint('\\nshould be much easier read than:\\n')\nprint('user:', user) Will output in your terminal: devtools_example.py : 30 <module> user : User ( id = 123 , name = ' John Doe ' , signup_ts = datetime . datetime ( 2019 , 6 , 1 , 12 , 22 ) , friends = [ 1234 , 4567 , 7890 , ] , address = Address ( street = ' Testing ' , country = ' uk ' , lat = 51.5 , lng = 0.0 , ) , ) (User) should be much easier read than:\n\nuser: id=123 name='John Doe' signup_ts=datetime.datetime(2019, 6, 1, 12, 22) friends=[1234, 4567, 7890] address=Address(street='Testing', country='uk', lat=51.5, lng=0.0) Note python-devtools doesn't yet support Python 3.13.","pageID":"devtools","abs_url":"/latest/integrations/devtools/#devtools","title":"devtools","objectID":"/latest/integrations/devtools/#devtools","rank":100},{"content":"Pydantic uses MkDocs for documentation, together with mkdocstrings . As such, you can make use of Pydantic's\nSphinx object inventory to cross-reference the Pydantic API documentation. Sphinx mkdocstrings In your Sphinx configuration ,\nadd the following to the intersphinx extension configuration : intersphinx_mapping = {\n    'pydantic': ('https://docs.pydantic.dev/latest', None),  # (1)!\n} You can also use dev instead of latest to target the latest documentation build, up to date\n   with the main branch. In your MkDocs configuration , add the following\nimport to your mkdocstrings plugin configuration : plugins : - mkdocstrings : handlers : python : import : - https://docs.pydantic.dev/latest/objects.inv # (1)! You can also use dev instead of latest to target the latest documentation build, up to date\n   with the main branch.","pageID":"Documentation","abs_url":"/latest/integrations/documentation/#Documentation","title":"Documentation","objectID":"/latest/integrations/documentation/#Documentation","rank":100},{"content":"Hypothesis is the Python library for property-based testing .\nHypothesis can infer how to construct type-annotated classes, and supports builtin types,\nmany standard library types, and generic types from the typing and typing_extensions modules by default. Pydantic v2.0 drops built-in support for Hypothesis and no more ships with the integrated Hypothesis plugin. Warning We are temporarily removing the Hypothesis plugin in favor of studying a different mechanism. For more information, see the issue annotated-types/annotated-types#37 . The Hypothesis plugin may be back in a future release. Subscribe to pydantic/pydantic#4682 for updates.","pageID":"Hypothesis","abs_url":"/latest/integrations/hypothesis/#Hypothesis","title":"Hypothesis","objectID":"/latest/integrations/hypothesis/#Hypothesis","rank":100},{"content":"If using Flake8 in your project, a plugin is available\nand can be installed using the following: pip install flake8-pydantic The lint errors provided by this plugin are namespaced under the PYDXXX code. To ignore some unwanted\nrules, the Flake8 configuration can be adapted: [flake8] extend-ignore = PYD001,PYD002","pageID":"Linting","abs_url":"/latest/integrations/linting/#flake8-plugin","title":"Linting - Flake8 plugin","objectID":"/latest/integrations/linting/#flake8-plugin","rank":100},{"content":"The Pydantic documentation is available in the llms.txt format.\nThis format is defined in Markdown and suited for large language models. Two formats are available: llms.txt : a file containing a brief description\n  of the project, along with links to the different sections of the documentation. The structure\n  of this file is described in details here . llms-full.txt : Similar to the llms.txt file,\n  but every link content is included. Note that this file may be too large for some LLMs. As of today, these files cannot be natively leveraged by LLM frameworks or IDEs. Alternatively,\na MCP server can be implemented to properly parse the llms.txt file.","pageID":"LLMs","abs_url":"/latest/integrations/llms/#LLMs","title":"LLMs","objectID":"/latest/integrations/llms/#LLMs","rank":100},{"content":"Pydantic integrates seamlessly with Pydantic Logfire , an observability platform built by us on the same belief as our open source library — that the most powerful tools can be easy to use.","pageID":"Pydantic Logfire","abs_url":"/latest/integrations/logfire/#Pydantic Logfire","title":"Pydantic Logfire","objectID":"/latest/integrations/logfire/#Pydantic Logfire","rank":100},{"content":"Logfire has an out-of-the-box Pydantic integration that lets you understand the data passing through your Pydantic models and get analytics on validations. For existing Pydantic users, it delivers unparalleled insights into your usage of Pydantic models. Getting started with Logfire can be done in three simple steps: Set up your Logfire account. Install the Logfire SDK. Instrument your project.","pageID":"Pydantic Logfire","abs_url":"/latest/integrations/logfire/#getting-started","title":"Pydantic Logfire - Getting Started","objectID":"/latest/integrations/logfire/#getting-started","rank":95},{"content":"Once you've got Logfire set up, you can start using it to monitor your Pydantic models and get insights into your data validation: from datetime import date\n\nimport logfire\n\nfrom pydantic import BaseModel\n\nlogfire.configure()  # (1)!\n\n\nclass User(BaseModel):\n    name: str\n    country_code: str\n    dob: date\n\n\nuser = User(name='Anne', country_code='USA', dob='2000-01-01')\nlogfire.info('user processed: {user!r}', user=user)  # (2)! The logfire.configure() call is all you need to instrument your project with Logfire. The logfire.info() call logs the user object to Logfire, with builtin support for Pydantic models.","pageID":"Pydantic Logfire","abs_url":"/latest/integrations/logfire/#basic-usage","title":"Pydantic Logfire - Getting Started - Basic Usage","objectID":"/latest/integrations/logfire/#basic-usage","rank":90},{"content":"You can even record information about the validation process automatically by\nusing the builtin Pydantic integration : from datetime import date\n\nimport logfire\n\nfrom pydantic import BaseModel\n\nlogfire.configure()\nlogfire.instrument_pydantic()  # (1)!\n\n\nclass User(BaseModel):\n    name: str\n    country_code: str\n    dob: date\n\n\nUser(name='Anne', country_code='USA', dob='2000-01-01')\nUser(name='David', country_code='GBR', dob='invalid-dob') The logfire.instrument_pydantic() call automatically logs validation information for all Pydantic models in your project. You'll see each successful and failed validation logged in Logfire: And you can investigate each of the corresponding spans to get validation details:","pageID":"Pydantic Logfire","abs_url":"/latest/integrations/logfire/#pydantic-instrumentation","title":"Pydantic Logfire - Getting Started - Pydantic Instrumentation","objectID":"/latest/integrations/logfire/#pydantic-instrumentation","rank":85},{"content":"Pydantic works well with mypy right out of the box. However, Pydantic also ships with a mypy plugin that adds a number of important Pydantic-specific\nfeatures that improve its ability to type-check your code. For example, consider the following script: Python 3.9 and above Python 3.10 and above from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: Optional[str] = None\n    signup_ts: Optional[datetime] = None\n    list_of_ints: list[int]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\nprint(m.middle_name)  # not a model field!\nModel()  # will raise a validation error for age and list_of_ints from datetime import datetime\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    age: int\n    first_name = 'John'\n    last_name: str | None = None\n    signup_ts: datetime | None = None\n    list_of_ints: list[int]\n\n\nm = Model(age=42, list_of_ints=[1, '2', b'3'])\nprint(m.middle_name)  # not a model field!\nModel()  # will raise a validation error for age and list_of_ints Without any special configuration, mypy does not catch the missing model field annotation and errors about the list_of_ints argument which Pydantic parses correctly: 15: error: List item 1 has incompatible type \"str\"; expected \"int\"  [list-item] 15: error: List item 2 has incompatible type \"bytes\"; expected \"int\"  [list-item] 16: error: \"Model\" has no attribute \"middle_name\"  [attr-defined] 17: error: Missing named argument \"age\" for \"Model\"  [call-arg] 17: error: Missing named argument \"list_of_ints\" for \"Model\"  [call-arg] But with the plugin enabled , it gives the correct errors: 9: error: Untyped fields disallowed  [pydantic-field] 16: error: \"Model\" has no attribute \"middle_name\"  [attr-defined] 17: error: Missing named argument \"age\" for \"Model\"  [call-arg] 17: error: Missing named argument \"list_of_ints\" for \"Model\"  [call-arg] With the pydantic mypy plugin, you can fearlessly refactor your models knowing mypy will catch any mistakes\nif your field names or types change. Note that mypy already supports some features without using the Pydantic plugin, such as synthesizing a __init__ method for Pydantic models and dataclasses. See the mypy plugin capabilities for a list\nof additional features. The Pydantic mypy plugin is tested against the latest mypy version. Older versions might work but won't be tested.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#Mypy","title":"Mypy","objectID":"/latest/integrations/mypy/#Mypy","rank":100},{"content":"To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file : mypy.ini pyproject.toml [mypy] plugins = pydantic.mypy [tool.mypy] plugins = [ 'pydantic.mypy' ] Note If you're using pydantic.v1 models, you'll need to add pydantic.v1.mypy to your list of plugins. See the plugin configuration for more details.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#enabling-the-plugin","title":"Mypy - Enabling the Plugin","objectID":"/latest/integrations/mypy/#enabling-the-plugin","rank":95},{"content":"","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#mypy-plugin-capabilities","title":"Mypy - Mypy plugin capabilities","objectID":"/latest/integrations/mypy/#mypy-plugin-capabilities","rank":90},{"content":"Any required fields that don't have dynamically-determined aliases will be included as required\n  keyword arguments. If the  model configuration value is set to True , the generated signature will use the field names rather than aliases. The init_forbid_extra and init_typed plugin configuration\n  values can further fine-tune the synthesized __init__ method.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#generate-a-__init__-signature-for-pydantic-models","title":"Mypy - Mypy plugin capabilities - Generate a __init__ signature for Pydantic models","objectID":"/latest/integrations/mypy/#generate-a-__init__-signature-for-pydantic-models","rank":85},{"content":"The  method is an alternative to model validation when input data is\n  known to be valid and should not be parsed (see the documentation ).\n  Because this method performs no runtime validation, static checking is important to detect errors.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#generate-a-typed-signature-for-model_construct","title":"Mypy - Mypy plugin capabilities - Generate a typed signature for model_construct","objectID":"/latest/integrations/mypy/#generate-a-typed-signature-for-model_construct","rank":80},{"content":"If the  configuration is set to True , you will get\n  an error if you try mutating a model field (see faux immutability )","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#support-for-frozen-models","title":"Mypy - Mypy plugin capabilities - Support for frozen models","objectID":"/latest/integrations/mypy/#support-for-frozen-models","rank":75},{"content":"Field with both a default and a default_factory will result in an error during static checking. The type of the default and default_factory value must be compatible with the one of the field.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#respect-the-type-of-the-fields-default-and-default_factory","title":"Mypy - Mypy plugin capabilities - Respect the type of the Field's default and default_factory","objectID":"/latest/integrations/mypy/#respect-the-type-of-the-fields-default-and-default_factory","rank":70},{"content":"While defining a field without an annotation will result in a runtime error ,\n  the plugin will also emit a type checking error.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#warn-about-the-use-of-untyped-fields","title":"Mypy - Mypy plugin capabilities - Warn about the use of untyped fields","objectID":"/latest/integrations/mypy/#warn-about-the-use-of-untyped-fields","rank":65},{"content":"See the documentation of the warn_required_dynamic_aliases plugin configuration value.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#prevent-the-use-of-required-dynamic-aliases","title":"Mypy - Mypy plugin capabilities - Prevent the use of required dynamic aliases","objectID":"/latest/integrations/mypy/#prevent-the-use-of-required-dynamic-aliases","rank":60},{"content":"To change the values of the plugin settings, create a section in your mypy config file called [pydantic-mypy] ,\nand add any key-value pairs for settings you want to override. A configuration file with all plugin strictness flags enabled (and some other mypy strictness flags, too) might look like: mypy.ini pyproject.toml [mypy] plugins = pydantic.mypy follow_imports = silent warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True no_implicit_reexport = True disallow_untyped_defs = True [pydantic-mypy] init_forbid_extra = True init_typed = True warn_required_dynamic_aliases = True [tool.mypy] plugins = [ \"pydantic.mypy\" ] follow_imports = \"silent\" warn_redundant_casts = true warn_unused_ignores = true disallow_any_generics = true no_implicit_reexport = true disallow_untyped_defs = true [tool.pydantic-mypy] init_forbid_extra = true init_typed = true warn_required_dynamic_aliases = true","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#configuring-the-plugin","title":"Mypy - Configuring the Plugin","objectID":"/latest/integrations/mypy/#configuring-the-plugin","rank":55},{"content":"Because Pydantic performs data conversion by default, the following is still valid at runtime: class Model(BaseModel):\n    a: int\n\n\nModel(a='1') For this reason, the plugin will use  for field annotations when synthesizing the __init__ method,\nunless init_typed is set or strict mode is enabled on the model.","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#init_typed","title":"Mypy - Configuring the Plugin - init_typed","objectID":"/latest/integrations/mypy/#init_typed","rank":50},{"content":"By default, Pydantic allows (and ignores) any extra provided argument: class Model(BaseModel):\n    a: int = 1\n\n\nModel(unrelated=2) For this reason, the plugin will add an extra **kwargs: Any parameter when synthesizing the __init__ method, unless init_forbid_extra is set or the  is set to 'forbid' .","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#init_forbid_extra","title":"Mypy - Configuring the Plugin - init_forbid_extra","objectID":"/latest/integrations/mypy/#init_forbid_extra","rank":45},{"content":"Whether to error when using a dynamically-determined alias or alias generator on a model with\n set to False . If such aliases are\npresent, mypy cannot properly type check calls to __init__ . In this case, it will default to\ntreating all arguments as not required. Compatibility with Any being disallowed Some mypy configuration options (such as disallow_any_explicit )\nwill error because the synthesized __init__ method contains  annotations. To circumvent the issue, you will have\nto enable both init_forbid_extra and init_typed .","pageID":"Mypy","abs_url":"/latest/integrations/mypy/#warn_required_dynamic_aliases","title":"Mypy - Configuring the Plugin - warn_required_dynamic_aliases","objectID":"/latest/integrations/mypy/#warn_required_dynamic_aliases","rank":40},{"content":"While pydantic will work well with any IDE out of the box, a PyCharm plugin offering improved pydantic integration is available on the JetBrains Plugins Repository for PyCharm.\nYou can install the plugin for free from the plugin marketplace\n(PyCharm's Preferences -> Plugin -> Marketplace -> search \"pydantic\"). The plugin currently supports the following features: For pydantic.BaseModel.__init__ : Inspection Autocompletion Type-checking For fields of pydantic.BaseModel : Refactor-renaming fields updates __init__ calls, and affects sub- and super-classes Refactor-renaming __init__ keyword arguments updates field names, and affects sub- and super-classes More information can be found on the official plugin page and Github repository .","pageID":"PyCharm","abs_url":"/latest/integrations/pycharm/#PyCharm","title":"PyCharm","objectID":"/latest/integrations/pycharm/#PyCharm","rank":100},{"content":"Pydantic models may be printed with the Rich library which will add additional formatting and color to the output. Here's an example: See the Rich documentation on pretty printing for more information.","pageID":"Rich","abs_url":"/latest/integrations/rich/#Rich","title":"Rich","objectID":"/latest/integrations/rich/#Rich","rank":100},{"content":"Pydantic works well with any editor or IDE out of the box because it's made on top of standard Python type annotations. When using Visual Studio Code (VS Code) , there are some additional editor features supported, comparable to the ones provided by the PyCharm plugin . This means that you will have autocompletion (or \"IntelliSense\") and error checks for types and required arguments even while creating new Pydantic model instances.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#Visual Studio Code","title":"Visual Studio Code","objectID":"/latest/integrations/visual_studio_code/#Visual Studio Code","rank":100},{"content":"To take advantage of these features, you need to make sure you configure VS Code correctly, using the recommended settings. In case you have a different configuration, here's a short overview of the steps.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#configure-vs-code","title":"Visual Studio Code - Configure VS Code","objectID":"/latest/integrations/visual_studio_code/#configure-vs-code","rank":95},{"content":"You should use the Pylance extension for VS Code. It is the recommended, next-generation, official VS Code plug-in for Python. Pylance is installed as part of the Python Extension for VS Code by default, so it should probably just work. Otherwise, you can double check it's installed and enabled in your editor.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#install-pylance","title":"Visual Studio Code - Configure VS Code - Install Pylance","objectID":"/latest/integrations/visual_studio_code/#install-pylance","rank":90},{"content":"Then you need to make sure your editor knows the Python environment (probably a virtual environment) for your Python project. This would be the environment in where you installed Pydantic.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#configure-your-environment","title":"Visual Studio Code - Configure VS Code - Configure your environment","objectID":"/latest/integrations/visual_studio_code/#configure-your-environment","rank":85},{"content":"With the default configurations, you will get support for autocompletion, but Pylance might not check for type errors. You can enable type error checks from Pylance with these steps: Open the \"User Settings\" Search for Type Checking Mode You will find an option under Python › Analysis: Type Checking Mode Set it to basic or strict (by default it's off ) Now you will not only get autocompletion when creating new Pydantic model instances but also error checks for required arguments . And you will also get error checks for invalid data types . Technical Details Pylance is the VS Code extension, it's closed source, but free to use. Underneath, Pylance uses an open source tool (also from Microsoft) called Pyright that does all the heavy lifting. You can read more about it in the Pylance Frequently Asked Questions .","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#configure-pylance","title":"Visual Studio Code - Configure VS Code - Configure Pylance","objectID":"/latest/integrations/visual_studio_code/#configure-pylance","rank":80},{"content":"You might also want to configure mypy in VS Code to get mypy error checks inline in your editor (alternatively/additionally to Pylance). This would include the errors detected by the Pydantic mypy plugin , if you configured it. To enable mypy in VS Code, do the following: Open the \"User Settings\" Search for Mypy Enabled You will find an option under Python › Linting: Mypy Enabled Check the box (by default it's unchecked)","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#configure-mypy","title":"Visual Studio Code - Configure VS Code - Configure mypy","objectID":"/latest/integrations/visual_studio_code/#configure-mypy","rank":75},{"content":"Here are some additional tips and tricks to improve your developer experience when using VS Code with Pydantic.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#tips-and-tricks","title":"Visual Studio Code - Tips and tricks","objectID":"/latest/integrations/visual_studio_code/#tips-and-tricks","rank":70},{"content":"The way this additional editor support works is that Pylance will treat your Pydantic models as if they were Python's pure dataclasses . And it will show strict type error checks about the data types passed in arguments when creating a new Pydantic model instance. In this example you can see that it shows that a str of '23' is not a valid int for the argument age . It would expect age=23 instead of age='23' . Nevertheless, the design, and one of the main features of Pydantic, is that it is very lenient with data types . It will actually accept the str with value '23' and will convert it to an int with value 23 . These strict error checks are very useful most of the time and can help you detect many bugs early . But there are cases, like with age='23' , where they could be inconvenient by reporting a \"false positive\" error. This example above with age='23' is intentionally simple, to show the error and the differences in types. But more common cases where these strict errors would be inconvenient would be when using more sophisticated data types, like int values for datetime fields, or dict values for Pydantic sub-models. For example, this is valid for Pydantic: from pydantic import BaseModel\n\n\nclass Knight(BaseModel):\n    title: str\n    age: int\n    color: str = 'blue'\n\n\nclass Quest(BaseModel):\n    title: str\n    knight: Knight\n\n\nquest = Quest(\n    title='To seek the Holy Grail', knight={'title': 'Sir Lancelot', 'age': 23}\n) The type of the field knight is declared with the class Knight (a Pydantic model) and the code is passing a literal dict instead. This is still valid for Pydantic, and the dict would be automatically converted to a Knight instance. Nevertheless, it would be detected as a type error: In those cases, there are several ways to disable or ignore strict errors in very specific places, while still preserving them in the rest of the code. Below are several techniques to achieve it. Disable type checks in a line ¶ You can disable the errors for a specific line using a comment of: # type: ignore or (to be specific to pylance/pyright): # pyright: ignore ( pyright is the language server used by Pylance.). coming back to the example with age='23' , it would be: from pydantic import BaseModel\n\n\nclass Knight(BaseModel):\n    title: str\n    age: int\n    color: str = 'blue'\n\n\nlancelot = Knight(title='Sir Lancelot', age='23')  # pyright: ignore that way Pylance and mypy will ignore errors in that line. Pros : it's a simple change in that line to remove errors there. Cons : any other error in that line will also be omitted, including type checks, misspelled arguments, required arguments not provided, etc. Override the type of a variable ¶ You can also create a variable with the value you want to use and declare its type explicitly with Any . from typing import Any\n\nfrom pydantic import BaseModel\n\n\nclass Knight(BaseModel):\n    title: str\n    age: int\n    color: str = 'blue'\n\n\nage_str: Any = '23'\nlancelot = Knight(title='Sir Lancelot', age=age_str) that way Pylance and mypy will interpret the variable age_str as if they didn't know its type, instead of knowing it has a type of str when an int was expected (and then showing the corresponding error). Pros : errors will be ignored only for a specific value, and you will still see any additional errors for the other arguments. Cons : it requires importing Any and a new variable in a new line for each argument that needs ignoring errors. Override the type of a value with cast ¶ The same idea from the previous example can be put on the same line with the help of cast() . This way, the type declaration of the value is overridden inline, without requiring another variable. from typing import Any, cast\n\nfrom pydantic import BaseModel\n\n\nclass Knight(BaseModel):\n    title: str\n    age: int\n    color: str = 'blue'\n\n\nlancelot = Knight(title='Sir Lancelot', age=cast(Any, '23')) cast(Any, '23') doesn't affect the value, it's still just '23' , but now Pylance and mypy will assume it is of type Any , which means, they will act as if they didn't know the type of the value. So, this is the equivalent of the previous example, without the additional variable. Pros : errors will be ignored only for a specific value, and you will still see any additional errors for the other arguments. There's no need for additional variables. Cons : it requires importing Any and cast , and if you are not used to using cast() , it could seem strange at first.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#strict-errors","title":"Visual Studio Code - Tips and tricks - Strict errors","objectID":"/latest/integrations/visual_studio_code/#strict-errors","rank":65},{"content":"Pydantic has a rich set of  available. These configurations can be set in an internal class Config on each model: from pydantic import BaseModel\n\n\nclass Knight(BaseModel):\n    model_config = dict(frozen=True)\n    title: str\n    age: int\n    color: str = 'blue' or passed as keyword arguments when defining the model class: from pydantic import BaseModel\n\n\nclass Knight(BaseModel, frozen=True):\n    title: str\n    age: int\n    color: str = 'blue' The specific configuration frozen (in beta) has a special meaning. It prevents other code from changing a model instance once it's created, keeping it \"frozen\" . When using the second version to declare frozen=True (with keyword arguments in the class definition),\nPylance can use it to help you check in your code and detect errors when something is trying to set values\nin a model that is \"frozen\".","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#config-in-class-arguments","title":"Visual Studio Code - Tips and tricks - Config in class arguments","objectID":"/latest/integrations/visual_studio_code/#config-in-class-arguments","rank":60},{"content":"Pylance/pyright requires default to be a keyword argument to Field in order to infer that the field is optional. from pydantic import BaseModel, Field\n\n\nclass Knight(BaseModel):\n    title: str = Field(default='Sir Lancelot')  # this is okay\n    age: int = Field(\n        23\n    )  # this works fine at runtime but will case an error for pyright\n\n\nlance = Knight()  # error: Argument missing for parameter \"age\" This is a limitation of dataclass transforms and cannot be fixed in pydantic.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#adding-a-default-with-field","title":"Visual Studio Code - Adding a default with Field","objectID":"/latest/integrations/visual_studio_code/#adding-a-default-with-field","rank":55},{"content":"Warning As a Pydantic user, you don't need the details below. Feel free to skip the rest of this section. These details are only useful for other library authors, etc. This additional editor support works by making use of the @dataclass_transform decorator (introduced by PEP 681 ). The standard provides a way for libraries like Pydantic and others to tell editors and tools that they (the editors) should treat these libraries (e.g. Pydantic) as if they were , providing autocompletion, type checks, etc.","pageID":"Visual Studio Code","abs_url":"/latest/integrations/visual_studio_code/#technical-details","title":"Visual Studio Code - Technical Details","objectID":"/latest/integrations/visual_studio_code/#technical-details","rank":50},{"content":"Note This section is part of the internals documentation, and is partly targeted to contributors. Starting with Pydantic V2, part of the codebase is written in Rust in a separate package called pydantic-core .\nThis was done partly in order to improve validation and serialization performance (with the cost of limited\ncustomization and extendibility of the internal logic). This architecture documentation will first cover how the two pydantic and pydantic-core packages interact\ntogether, then will go through the architecture specifics for various patterns (model definition, validation,\nserialization, JSON Schema). Usage of the Pydantic library can be divided into two parts: Model definition, done in the pydantic package. Model validation and serialization, done in the pydantic-core package.","pageID":"Architecture","abs_url":"/latest/internals/architecture/#Architecture","title":"Architecture","objectID":"/latest/internals/architecture/#Architecture","rank":100},{"content":"Whenever a Pydantic  is defined, the metaclass\nwill analyze the body of the model to collect a number of elements: Defined annotations to build model fields (collected in the  attribute). Model configuration, set with . Additional validators/serializers. Private attributes, class variables, identification of generic parametrization, etc.","pageID":"Architecture","abs_url":"/latest/internals/architecture/#model-definition","title":"Architecture - Model definition","objectID":"/latest/internals/architecture/#model-definition","rank":95},{"content":"We then need a way to communicate the collected information from the model definition to pydantic-core ,\nso that validation and serialization is performed accordingly. To do so, Pydantic uses the concept\nof a core schema: a structured (and serializable) Python dictionary (represented using\n definitions) describing a specific validation and serialization\nlogic. It is the core data structure used to communicate between the pydantic and pydantic-core packages. Every core schema has a required type key, and extra properties depending on this type . The generation of a core schema is handled in a single place, by the GenerateSchema class\n(no matter if it is for a Pydantic model or anything else). Note It is not possible to define a custom core schema. A core schema needs to be understood by the pydantic-core package, and as such we only support a fixed number of core schema types.\nThis is also part of the reason why the GenerateSchema isn't truly exposed and properly\ndocumented. The core schema definitions can be found in the  module. In the case of a Pydantic model, a core schema will be constructed and set as the\n attribute. To illustrate what a core schema looks like, we will take the example of the\n core schema: class BoolSchema(TypedDict, total=False):\n    type: Required[Literal['bool']]\n    strict: bool\n    ref: str\n    metadata: Any\n    serialization: SerSchema When defining a Pydantic model with a boolean field: from pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    foo: bool = Field(strict=True) The core schema for the foo field will look like: {\n    'type': 'bool',\n    'strict': True,\n} As seen in the  definition,\nthe serialization logic is also defined in the core schema.\nIf we were to define a custom serialization function for foo (1), the serialization key would look like: For example using the  decorator: class Model(BaseModel):\n    foo: bool = Field(strict=True)\n\n    @field_serializer('foo', mode='plain')\n    def serialize_foo(self, value: bool) -> Any:\n        ... {\n    'type': 'function-plain',\n    'function': ,\n    'is_field_serializer': True,\n    'info_arg': False,\n    'return_schema': {'type': 'int'},\n} Note that this is also a core schema definition, just that it is only relevant for pydantic-core during serialization. Core schemas cover a broad scope, and are used whenever we want to communicate between the Python and Rust side.\nWhile the previous examples were related to validation and serialization, it could in theory be used for anything:\nerror management, extra metadata, etc.","pageID":"Architecture","abs_url":"/latest/internals/architecture/#communicating-between-pydantic-and-pydantic-core-the-core-schema","title":"Architecture - Model definition - Communicating between pydantic and pydantic-core: the core schema","objectID":"/latest/internals/architecture/#communicating-between-pydantic-and-pydantic-core-the-core-schema","rank":90},{"content":"You may have noticed that the previous serialization core schema has a return_schema key.\nThis is because the core schema is also used to generate the corresponding JSON Schema. Similar to how the core schema is generated, the JSON Schema generation is handled by the\n class.\nThe  method\nis the main entry point and is given the core schema of that model. Coming back to our bool field example, the \nmethod will be given the previously generated \nand will return the following JSON Schema: { { \"type\" : \"boolean\" } }","pageID":"Architecture","abs_url":"/latest/internals/architecture/#json-schema-generation","title":"Architecture - Model definition - JSON Schema generation","objectID":"/latest/internals/architecture/#json-schema-generation","rank":85},{"content":"Usage Documentation Custom types Implementing __get_pydantic_core_schema__ Implementing __get_pydantic_json_schema__ While the GenerateSchema and  classes handle\nthe creation of the corresponding schemas, Pydantic offers a way to customize them in some cases, following a wrapper pattern.\nThis customization is done through the __get_pydantic_core_schema__ and __get_pydantic_json_schema__ methods. To understand this wrapper pattern, we will take the example of metadata classes used with ,\nwhere the __get_pydantic_core_schema__ method can be used: from typing import Annotated, Any\n\nfrom pydantic_core import CoreSchema\n\nfrom pydantic import GetCoreSchemaHandler, TypeAdapter\n\n\nclass MyStrict:\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: Any, handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        schema = handler(source)  # (1)!\n        schema['strict'] = True\n        return schema\n\n\nclass MyGt:\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: Any, handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        schema = handler(source)  # (2)!\n        schema['gt'] = 1\n        return schema\n\n\nta = TypeAdapter(Annotated[int, MyStrict(), MyGt()]) MyStrict is the first annotation to be applied. At this point, schema = {'type': 'int'} . MyGt is the last annotation to be applied. At this point, schema = {'type': 'int', 'strict': True} . When the GenerateSchema class builds the core schema for Annotated[int, MyStrict(), MyGt()] , it will\ncreate an instance of a GetCoreSchemaHandler to be passed to the MyGt.__get_pydantic_core_schema__ method. (1) In the case of our  pattern, the GetCoreSchemaHandler is defined in a nested way.\n    Calling it will recursively call the other __get_pydantic_core_schema__ methods until it reaches the int annotation,\n    where a simple {'type': 'int'} schema is returned. The source argument depends on the core schema generation pattern. In the case of ,\nthe source will be the type being annotated. When defining a custom type ,\nthe source will be the actual class where __get_pydantic_core_schema__ is defined.","pageID":"Architecture","abs_url":"/latest/internals/architecture/#customizing-the-core-schema-and-json-schema","title":"Architecture - Model definition - Customizing the core schema and JSON schema","objectID":"/latest/internals/architecture/#customizing-the-core-schema-and-json-schema","rank":80},{"content":"While model definition was scoped to the class level (i.e. when defining your model), model validation\nand serialization happens at the instance level. Both these concepts are handled in pydantic-core (providing a 5 to 20 performance increase compared to Pydantic V1), by using the previously built core schema. pydantic-core exposes a  and\n class to perform these tasks: from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    foo: int\n\n\nmodel = Model.model_validate({'foo': 1})  # (1)!\ndumped = model.model_dump()  # (2)! The provided data is sent to pydantic-core by using the\n    method. pydantic-core will validate (following the core schema of the model) the data and populate\n   the model's __dict__ attribute. The model instance is sent to pydantic-core by using the\n    method. pydantic-core will read the instance's __dict__ attribute and built the appropriate result\n   (again, following the core schema of the model).","pageID":"Architecture","abs_url":"/latest/internals/architecture/#model-validation-and-serialization","title":"Architecture - Model validation and serialization","objectID":"/latest/internals/architecture/#model-validation-and-serialization","rank":75},{"content":"Note This section is part of the internals documentation, and is partly targeted to contributors. Pydantic heavily relies on  at runtime to build schemas for validation, serialization, etc. While type hints were primarily introduced for static type checkers (such as Mypy or Pyright ), they are\naccessible (and sometimes evaluated) at runtime. This means that the following would fail at runtime,\nbecause Node has yet to be defined in the current module: class Node:\n    \"\"\"Binary tree node.\"\"\"\n\n    # NameError: name 'Node' is not defined:\n    def __init__(self, l: Node, r: Node) -> None:\n        self.left = l\n        self.right = r To circumvent this issue, forward references can be used (by wrapping the annotation in quotes). In Python 3.7, PEP 563 introduced the concept of postponed evaluation of annotations , meaning\nwith the from __future__ import annotations future statement , type hints are stringified by default: from __future__ import annotations\n\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    f: MyType\n    # Given the future import above, this is equivalent to:\n    # f: 'MyType'\n\n\ntype MyType = int\n\nprint(Foo.__annotations__)\n#> {'f': 'MyType'}","pageID":"Resolving Annotations","abs_url":"/latest/internals/resolving_annotations/#Resolving Annotations","title":"Resolving Annotations","objectID":"/latest/internals/resolving_annotations/#Resolving Annotations","rank":100},{"content":"Static type checkers make use of the AST to analyze the defined annotations.\nRegarding the previous example, this has the benefit of being able to understand what MyType refers to when analyzing\nthe class definition of Foo , even if MyType isn't yet defined at runtime. However, for runtime tools such as Pydantic, it is more challenging to correctly resolve these forward annotations.\nThe Python standard library provides some tools to do so (,\n), but they come with some limitations. Thus, they are\nbeing re-implemented in Pydantic with improved support for edge cases. As Pydantic as grown, it's adapted to support many edge cases requiring irregular patterns for annotation evaluation.\nSome of these use cases aren't necessarily sound from a static type checking perspective. In v2.10, the internal\nlogic was refactored in an attempt to simplify and standardize annotation evaluation. Admittedly, backwards compatibility\nposed some challenges, and there is still some noticeable scar tissue in the codebase because of this.There's a hope that PEP 649 (introduced in Python 3.14) will greatly simplify the process, especially when it comes to dealing with locals\nof a function. To evaluate forward references, Pydantic roughly follows the same logic as described in the documentation of the\n function. That is, the built-in  function is used\nby passing the forward reference, a global, and a local namespace. The namespace fetching logic is defined in the\nsections below.","pageID":"Resolving Annotations","abs_url":"/latest/internals/resolving_annotations/#the-challenges-of-runtime-evaluation","title":"Resolving Annotations - The challenges of runtime evaluation","objectID":"/latest/internals/resolving_annotations/#the-challenges-of-runtime-evaluation","rank":95},{"content":"The following example will be used as a reference throughout this section: # module1.py:\ntype MyType = int\n\nclass Base:\n    f1: 'MyType'\n\n# module2.py:\nfrom pydantic import BaseModel\n\nfrom module1 import Base\n\ntype MyType = str\n\n\ndef inner() -> None:\n    type InnerType = bool\n\n    class Model(BaseModel, Base):\n        type LocalType = bytes\n\n        f2: 'MyType'\n        f3: 'InnerType'\n        f4: 'LocalType'\n        f5: 'UnknownType'\n\n    type InnerType2 = complex When the Model class is being built, different  are at play. For each base class\nof the Model 's  (in reverse order — that is, starting with Base ), the\nfollowing logic is applied: Fetch the __annotations__ key from the current base class' __dict__ , if present. For Base , this will be {'f1': 'MyType'} . Iterate over the __annotations__ items and try to evaluate the annotation 1 using a custom wrapper around\n   the built-in  function. This function takes two globals and locals arguments: The current module's __dict__ is naturally used as globals . For Base , this will be sys.modules['module1'].__dict__ . For the locals argument, Pydantic will try to resolve symbols in the following namespaces, sorted by highest priority: A namespace created on the fly, containing the current class name ( {cls.__name__: cls} ). This is done\n   in order to support recursive references. The locals of the current class (i.e. cls.__dict__ ). For Model , this will include LocalType . The parent namespace of the class, if different from the globals described above. This is the\n    of the frame where the class is being defined. For Base , because the class is being\n   defined in the module directly, this namespace won't be used as it will result in the globals being used again.\n   For Model , the parent namespace is the locals of the frame of inner() . If the annotation failed to evaluate, it is kept as is, so that the model can be rebuilt at a later stage. This will\n   be the case for f5 . The following table lists the resolved type annotations for every field, once the Model class has been created: Field name Resolved annotation f1 f2 f3 f4 f5 'UnknownType'","pageID":"Resolving Annotations","abs_url":"/latest/internals/resolving_annotations/#resolving-annotations-at-class-definition","title":"Resolving Annotations - Resolving annotations at class definition","objectID":"/latest/internals/resolving_annotations/#resolving-annotations-at-class-definition","rank":90},{"content":"While the namespace fetching logic is trying to be as accurate as possible, we still face some limitations: The locals of the current class ( cls.__dict__ ) may include irrelevant entries, most of them being dunder attributes.\n  This means that the following annotation: f: '__doc__' will successfully (and unexpectedly) be resolved. When the Model class is being created inside a function, we keep a copy of the  of the frame.\n  This copy only includes the symbols defined in the locals when Model is being defined, meaning InnerType2 won't be included\n  (and will not be if doing a model rebuild at a later point!). To avoid memory leaks, we use  to the locals of the function, meaning some forward references might\nnot resolve outside the function (1). Locals of the function are only taken into account for Pydantic models, but this pattern does not apply to dataclasses, typed\ndictionaries or named tuples. Here is an example: def func():\n    A = int\n\n    class Model(BaseModel):\n        f: 'A | Forward'\n\n    return Model\n\n\nModel = func()\n\nModel.model_rebuild(_types_namespace={'Forward': str})\n# pydantic.errors.PydanticUndefinedAnnotation: name 'A' is not defined For backwards compatibility reasons, and to be able to support valid use cases without having to rebuild models,\nthe namespace logic described above is a bit different when it comes to core schema generation.\nTaking the following example: from dataclasses import dataclass\n\nfrom pydantic import BaseModel\n\n\n@dataclass\nclass Foo:\n    a: 'Bar | None' = None\n\n\nclass Bar(BaseModel):\n    b: Foo Once the fields for Bar have been collected (meaning annotations resolved), the GenerateSchema class converts\nevery field into a core schema. When it encounters another class-like field type (such as a dataclass), it will\ntry to evaluate annotations, following roughly the same logic as described above .\nHowever, to evaluate the 'Bar | None' annotation, Bar needs to be present in the globals or locals, which is normally not the case: Bar is being created, so it is not \"assigned\" to the current module's __dict__ at that point. To avoid having to call  on Bar , both the parent namespace\n(if Bar was to be defined inside a function, and the namespace provided during a model rebuild )\nand the {Bar.__name__: Bar} namespace are included in the locals during annotations evaluation of Foo (with the lowest priority) (1). This backwards compatibility logic can introduce some inconsistencies, such as the following: from dataclasses import dataclass\n\nfrom pydantic import BaseModel\n\n\n@dataclass\nclass Foo:\n    # `a` and `b` shouldn't resolve:\n    a: 'Model'\n    b: 'Inner'\n\n\ndef func():\n    Inner = int\n\n    class Model(BaseModel):\n        foo: Foo\n\n    Model.__pydantic_complete__\n    #> True, should be False.","pageID":"Resolving Annotations","abs_url":"/latest/internals/resolving_annotations/#limitations-and-backwards-compatibility-concerns","title":"Resolving Annotations - Resolving annotations at class definition - Limitations and backwards compatibility concerns","objectID":"/latest/internals/resolving_annotations/#limitations-and-backwards-compatibility-concerns","rank":85},{"content":"When a forward reference fails to evaluate, Pydantic will silently fail and stop the core schema\ngeneration process. This can be seen by inspecting the __pydantic_core_schema__ of a model class: from pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    f: 'MyType'\n\n\nFoo.__pydantic_core_schema__\n#> If you then properly define MyType , you can rebuild the model: type MyType = int\n\nFoo.model_rebuild()\nFoo.__pydantic_core_schema__\n#> {'type': 'model', 'schema': {...}, ...} The  method uses a rebuild namespace , with the following semantics: If an explicit _types_namespace argument is provided, it is used as the rebuild namespace. If no namespace is provided, the namespace where the method is called will be used as the rebuild namespace. This rebuild namespace will be merged with the model's parent namespace (if it was defined in a function) and used as is\n(see the backwards compatibility logic described above). This is done unconditionally, as forward annotations can be only present as part of a type hint (e.g. Optional['int'] ), as dictated by\n  the typing specification . ↩","pageID":"Resolving Annotations","abs_url":"/latest/internals/resolving_annotations/#resolving-annotations-when-rebuilding-a-model","title":"Resolving Annotations - Resolving annotations when rebuilding a model","objectID":"/latest/internals/resolving_annotations/#resolving-annotations-when-rebuilding-a-model","rank":80}]